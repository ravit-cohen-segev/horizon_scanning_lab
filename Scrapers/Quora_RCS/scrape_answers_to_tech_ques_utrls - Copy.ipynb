{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import ssl\n",
    "import os\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For ignoring SSL ceritficate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class infine_scroll(object): \n",
    "  def __init__(self, last):\n",
    "    self.last = last\n",
    "\n",
    "  def __call__(self, driver):\n",
    "    new = driver.execute_script('return document.body.scrollHeight')  \n",
    "    if new > self.last:\n",
    "        return new\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_html_from_url(url):\n",
    "  browser = webdriver.Chrome('C:\\Program Files\\chromedriver_win32 (1)\\chromedriver') \n",
    "  browser.set_page_load_timeout(30) \n",
    "  browser.get(url)\n",
    "  \n",
    "  html= '' \n",
    "  \n",
    "  last_height = browser.execute_script('return document.body.scrollHeight')\n",
    "\n",
    "  html += browser.page_source\n",
    "  flag=1\n",
    "\n",
    "  while flag==1:\n",
    "    browser.execute_script('window.scrollTo(0,document.body.scrollHeight)')\n",
    " \n",
    "    html += browser.page_source\n",
    "    try:\n",
    "       wait = WebDriverWait(browser, 10)\n",
    "\n",
    "       new_height = wait.until(infinite_scroll(last_height))\n",
    "       last_height = new_height\n",
    "\n",
    "    except:\n",
    "        print(\"End of page reached\")\n",
    "        flag = 0\n",
    "  return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function get first number in string\n",
    "\n",
    "def str_first_num(s):\n",
    "    first_num = ''\n",
    "    flag_digit=False\n",
    "    for el in s: \n",
    "        if el.isdigit():\n",
    "            first_num += el\n",
    "            flag_digit = True\n",
    "        else:\n",
    "            if flag_digit:\n",
    "                break\n",
    "    #If there are no upvotes replace '' with 0\n",
    "    if first_num == '':\n",
    "        first_num = 0\n",
    "    return first_num\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a helper function that removes unwanted words/sections like url links\n",
    "def strip_rgx_words(inp, regex_ = ['image','url']):\n",
    "    #remove urls or images\n",
    "    rgx_word = None\n",
    "    for rgx in regex_:\n",
    "      if rgx in inp:\n",
    "          rgx_word = rgx\n",
    "          break\n",
    "    if rgx_word is not None:\n",
    "      out = inp.split(rgx_word)[0]\n",
    "    else:\n",
    "      out =inp\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_two_str(st1, st2):\n",
    "    '''\n",
    "    input: two strings \n",
    "    output: differences between two strings and indexes'''\n",
    "    unsimilar_id = {}\n",
    "    for i, l1 in enumerate(st1):\n",
    "        l2 = st2[i]\n",
    "        if l1 != l2:\n",
    "            unsimilar_id[i] = [l1, l2]\n",
    "    return unsimilar_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read question urls from csv file\n",
    "src_path = r\"C:\\Users\\Ravit\\Documents\\horizon_scanning_lab\\Scrapers\\Quora_RCS\\answers_and_info\\quet_indexes.csv\"\n",
    "df = pd.read_csv(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of urls\n",
    "url_list = list(df['url'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.quora.com/What-will-the-worlds-technology-be-like-in-50-years',\n",
       " 'https://www.quora.com/How-long-could-the-brain-survive-theoretically-if-we-had-the-technology-to-replace-all-other-body-parts-as-you-aged-with-functioning-organs-that-are-grown-in-a-lab-using-your-DNA',\n",
       " 'https://www.quora.com/What-are-the-top-10-emerging-technologies-in-the-next-5-10-years-2020%E2%80%932025',\n",
       " 'https://www.quora.com/What-are-the-upcoming-emerging-technologies-in-software-industry',\n",
       " 'https://www.quora.com/What-will-the-worlds-technology-be-like-in-50-years',\n",
       " 'https://www.quora.com/What-are-the-most-advanced-technologies-that-people-dont-know-about-yet',\n",
       " 'https://www.quora.com/When-will-Fusion-reactors-become-a-reality',\n",
       " 'https://www.quora.com/In-future-IOT-Internet-of-things-is-trending-technology-or-not',\n",
       " 'https://www.quora.com/What-will-the-worlds-technology-be-like-in-50-years',\n",
       " 'https://www.quora.com/What-are-the-solutions-to-emerging-issues-in-communication']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answers_into_df(url):\n",
    "    '''input: url for scrapping answers to a particular question.\n",
    "        out: df with answers for each url/question'''\n",
    "    #request url html\n",
    "    \n",
    "    #create soup object with htmal parser\n",
    "    page_html = get_html_from_url(url)\n",
    "    soup = BeautifulSoup(page_html)\n",
    "\n",
    "    answers = soup.find_all('p', {'class':\"q-text qu-display--block qu-wordBreak--break-word qu-textAlign--start\"})\n",
    "    answers = list(set(answers))\n",
    "    \n",
    "    #get all answers in span\n",
    "    answers = [ans.find('span') for ans in answers]\n",
    "    answers = [ans.find_next(text=True).strip() for ans in answers]\n",
    "    #remove duplicates \n",
    "    answers = list(set(answers))\n",
    "    \n",
    "    upvotes = soup.find_all('span', {'class':\"q-text qu-whiteSpace--nowrap qu-display--inline-flex qu-alignItems--center qu-justifyContent--center\"})\n",
    "    upvotes = [up.find_next(text=True).strip() for up in upvotes]\n",
    "    \n",
    "\n",
    "    \n",
    "    #convert soup object to string\n",
    "    answers = [str(a) for a in answers][0]\n",
    "    #split answers with text separator\n",
    "    answers_list = answers.split('\"text\":')\n",
    "    #first element is just the header of a page. remove it\n",
    "    answers_list = answers_list[1:]\n",
    "    \n",
    "    #create df with nans to fill it later with values\n",
    "    columns = [\"Answer\", \"upvoteCount\", \"answerCount\", \"followerCount\", \"name\"]\n",
    "    #columns = [\"Answer\", \"upvoteCount\"]\n",
    "    x_shape = (len(answers_list), len(columns))\n",
    "    x = np.tile(np.nan, x_shape)\n",
    "   \n",
    "    answers_df = pd.DataFrame(x, columns = columns)\n",
    "    columns = columns[1:]\n",
    "\n",
    "    for i, ans in enumerate(answers_list):    \n",
    "        #get list of categories that exist in answer post\n",
    "        existing_columns = [col for col in columns if col in ans]\n",
    "        missing_columns = [col for col in columns if col not in ans]\n",
    "\n",
    "        #fill all missing features in post with UNKNOWN\n",
    "        answers_df.loc[i, missing_columns] = 'UNKNOWN'         \n",
    "        \n",
    "        n = len(existing_columns)\n",
    "        c = iter(existing_columns)\n",
    "        curr_c = next(c)\n",
    "\n",
    "        for j in range(n):          \n",
    "            #Fill first element column which is 'Answer'   \n",
    "            try:\n",
    "                next_c = next(c)\n",
    "            except StopIteration:\n",
    "                pass\n",
    "                        \n",
    "            if j==0:\n",
    "                split_post = ans.split(curr_c)\n",
    "                ans_text = split_post[0]\n",
    "                answers_df['Answer'].iloc[i] = ans_text\n",
    "                split_post = split_post[1] \n",
    "               \n",
    "                \n",
    "            #numeric categories have Count in their names        \n",
    "            if \"Count\" in curr_c:\n",
    "                #in case the category is\n",
    "                if curr_c!=existing_columns[-1]:\n",
    "                    #get text relevant to c -column\n",
    "                    split_post = split_post.split(curr_c)\n",
    "                    inter_split = split_post[-1].split(next_c)\n",
    "                    #if category is numeric get numbe\n",
    "                    answers_df[curr_c].iloc[i] = str_first_num(inter_split[0])\n",
    "                    split_post = inter_split[-1]\n",
    "                    curr_c = next_c\n",
    "                    continue \n",
    "                else:\n",
    "                    answers_df[curr_c].iloc[i] = str_first_num(split_post)\n",
    "                    continue\n",
    "        \n",
    "            #insert regex helper function here----!\n",
    "            split_post = strip_rgx_words(split_post)\n",
    "            answers_df[curr_c].iloc[i] = split_post \n",
    "            curr_c = next_c \n",
    "    return answers_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ravit\\AppData\\Local\\Temp\\ipykernel_32528\\789551367.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome('C:\\Program Files\\chromedriver_win32 (1)\\chromedriver')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of page reached\n"
     ]
    }
   ],
   "source": [
    "#show one example\n",
    "answers_into_df(url_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"question\", \"Answer\", \"upvoteCount\", \"answerCount\", \"followerCount\", \"name\"]\n",
    "df = pd.DataFrame(data = [], columns = columns)\n",
    "\n",
    "for i, url in enumerate(url_list):\n",
    "    temp_df = answers_into_df(url)\n",
    "    temp_df[\"question\"] = url\n",
    "    #rearrange columns order to match main df columns order\n",
    "    temp_df = temp_df[columns]\n",
    "\n",
    "    df = df.append(temp_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy of function before making changes\n",
    "\n",
    "def answers_into_df_copy(url):\n",
    "    '''input: url for scrapping answers to a particular question.\n",
    "        out: df with answers for each url/question'''\n",
    "    #request url html\n",
    "    \n",
    "    #create soup object with htmal parser\n",
    "    page_html = get_html_from_url(url)\n",
    "    soup = BeautifulSoup(page_html)\n",
    "\n",
    "    answers = soup.find_all('p', {'class':\"q-text qu-display--block qu-wordBreak--break-word qu-textAlign--start\"})\n",
    "    #convert soup object to string\n",
    "    answers = [str(a) for a in answers][0]\n",
    "    #split answers with text separator\n",
    "    answers_list = answers.split('\"text\":')\n",
    "    #first element is just the header of a page. remove it\n",
    "    answers_list = answers_list[1:]\n",
    "    \n",
    "    #create df with nans to fill it later with values\n",
    "    columns = [\"Answer\", \"upvoteCount\", \"answerCount\", \"followerCount\", \"name\"]\n",
    "    #columns = [\"Answer\", \"upvoteCount\"]\n",
    "    x_shape = (len(answers_list), len(columns))\n",
    "    x = np.tile(np.nan, x_shape)\n",
    "   \n",
    "    answers_df = pd.DataFrame(x, columns = columns)\n",
    "    columns = columns[1:]\n",
    "\n",
    "    for i, ans in enumerate(answers_list):    \n",
    "        #get list of categories that exist in answer post\n",
    "        existing_columns = [col for col in columns if col in ans]\n",
    "        missing_columns = [col for col in columns if col not in ans]\n",
    "\n",
    "        #fill all missing features in post with UNKNOWN\n",
    "        answers_df.loc[i, missing_columns] = 'UNKNOWN'         \n",
    "        \n",
    "        n = len(existing_columns)\n",
    "        c = iter(existing_columns)\n",
    "        curr_c = next(c)\n",
    "\n",
    "        for j in range(n):          \n",
    "            #Fill first element column which is 'Answer'   \n",
    "            try:\n",
    "                next_c = next(c)\n",
    "            except StopIteration:\n",
    "                pass\n",
    "                        \n",
    "            if j==0:\n",
    "                split_post = ans.split(curr_c)\n",
    "                ans_text = split_post[0]\n",
    "                answers_df['Answer'].iloc[i] = ans_text\n",
    "                split_post = split_post[1] \n",
    "               \n",
    "                \n",
    "            #numeric categories have Count in their names        \n",
    "            if \"Count\" in curr_c:\n",
    "                #in case the category is\n",
    "                if curr_c!=existing_columns[-1]:\n",
    "                    #get text relevant to c -column\n",
    "                    split_post = split_post.split(curr_c)\n",
    "                    inter_split = split_post[-1].split(next_c)\n",
    "                    #if category is numeric get numbe\n",
    "                    answers_df[curr_c].iloc[i] = str_first_num(inter_split[0])\n",
    "                    split_post = inter_split[-1]\n",
    "                    curr_c = next_c\n",
    "                    continue \n",
    "                else:\n",
    "                    answers_df[curr_c].iloc[i] = str_first_num(split_post)\n",
    "                    continue\n",
    "        \n",
    "            #insert regex helper function here----!\n",
    "            split_post = strip_rgx_words(split_post)\n",
    "            answers_df[curr_c].iloc[i] = split_post \n",
    "            curr_c = next_c \n",
    "    return answers_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save df to csv\n",
    "save_path = r\"C:\\Users\\Ravit\\Documents\\horizon_scanning_lab\\Scrapers\\Quora_RCS\\answers_and_info\"\n",
    "final_path = os.path.join(save_path, \"ET_RCS_22SEP15_quora_data.csv\")\n",
    "df.to_csv(final_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO -  scrape all pages from this urls,\n",
    "        #scrape all groups related to technology trends\n",
    "     \n",
    "         "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
