,Answer,followerCount,name,upvoteCount,answerCount,answerViews,question
0,"Factory ECU calibrations are the result of person-years of work to meet a bewildering number of often-conflicting requirements for a production vehicle, including but not limited to:Factory ECU calibrations are the result of person-years of work to meet a bewildering number of often-conflicting requirements for a production vehicle, including but not limited to:For just one example, running an engine fuel mixture very lean - which is possible under low-load conditions with direct injection - is a tremendous help to fuel consumption, but raises exhaust gas temperatures (EGTs), which produces more oxides of nitrogen (NOx) emissions, which are difficult to after-treat as it’s a reduction reaction rather than an oxidation reaction.So using an aftermarket calibration to boost performance or accommodate hardware mods like ported heads, bigger injectors, some form of boost, less-restrictive intake and exhaust, etc., will change that balance. Often, the first thing to go with an aftermarket cal is emissions compliance. And the next thing that takes a hit will be durability/longevity, unless a lot of hardware is upgraded to live safely with increased power output from the engine, at which point we’re not just talking about a simple reflash.One of the long-running memes of the hot-rodding world is that the OEMs “deliberately detune” their offerings and you can “unlock” that built-in performance with a simple calibration change. There’s a grain of truth: yes, you can see power gains doing that. But you’ll pay a price in a number of other areas to do so.And if you’re under warranty, be prepared to kiss any coverage of the powertrain (engine/transmisison/final drive) goodbye. Anyone who says the Magnusson-Moss Warranty Act protects your warranty if you mod your vehicle is lying to you. The MMWA exists to allow car owners to get regular routine service of things like oil and air filters, brake pads, etc., from sources other than franchised dealers, using “equivalent to standard” parts. It doesn’t cover modifications, at all.And “flashing it back to stock” before taking it to the dealer for service may not prevent them from detecting your mods, as there’s a checksum value associated with the original ECM cal that will most likely be different.Some of the better aftermarket shops and suppliers will offer what amounts to a replacement warranty for the portions of the OEM warranty their products void. For the rest, though, “you pays your money and you takes your chances”. Something to keep in mind if you want to modify an under-warranty vehicle.(Funny story: back in the day, I worked on a high-performance vehicle program for a large OEM. It so happened that on this particular engine, a couple of popular mods would cause particular problems - underdrive pulleys would cause overheats due to greatly - like 40% lower - reduced coolant flow, and a centrifugal supercharger or nitrous could crack piston ring lands or pound one or two connecting rod bearings into mulch if pushed past a very low limit. We had a number of owners discover these things the hard way, and a few tried to get warranty replacement engines. The dealer techs would call the hotline, and we’d walk them through inspecting the dead engines for some tell-tale signs of mods. Far more often than not, we’d catch it then. And the VIN would be flagged in the computer, and the customer would be told we knew they’d modded the car and caused the problem, and they’d be offered a very good price on a new engine.(One slipped by until the owner came back to the dealer to ask if he could get his high-flow injectors, aluminum flywheel, and dual-disc racing clutch back from the dead engine, which had already been shipped back to us at HQ. We took a look at his engine once it arrived, and sure enough, high-flow injectors, aluminum flywheel, racing clutch. And 2 pistons with broken ring lands and 3 connecting rod bearings essentially extruded into an aluminum/copper/tin foil (with a lot of it in the oil sump). Needless to say, his warranty claim was reviewed and rejected.(And one poor schnook showed up at his dealer with the car on a trailer, the blower, nitrous bottle, and drag radials still on the car. The dealer didn’t even bother calling us at the time, and just told him they’d be happy to sell him a new engine, flagged the VIN in the computer, and when he decided to not buy a new engine, he left. Over the next 3 months, he hauled that car - now minus the drag tires, blower, and nitrous system - to easily 2 dozen dealers in an ever-increasing radius from his home until one finally took pity on him and told him about his VIN being flagged, and thus no dealer would ever warranty that thing. He eventually went back to his selling dealer and bought an engine.(What were the tell-tales? For the pulleys, it would be tool marks on the crank damper bolt, water pump pulley bolts, and alternator pulley bolt, as well as a different-color sealer applied to the crankshaft bolt. And very very cooked - maybe even with melted electrodes - spark plugs in a couple of cylinders. For blowers, same deal with the crank damper bolt, plus there were a few bolt bosses on the engine’s front cover that were unused in this application , but were still present in the cover, and the blower brackets used them. Installing a steel bolt into an aluminmum cover leaves a witness mark in/on the cover. Nitrous was easy - pull up the trunk mat and look for the mounting holes for the bottle brackets or a hole for the nitrous hose. As some systems are “dry” and don’t require tapping the fuel plumbing, that was the easiest and most reliable way to find evidence of nitrous use. Well, that and the bottom-end knock in the engine. :D(And of course almost all of these folks flashed their ECUs, so a quick OBD-II checksum dump added an additional tell-tale.)",349 followers,Shaun Love,853,119,2.3M,https://www.quora.com/Is-there-any-disadvantages-to-ECU-tuning-remapping-I-doubt-you-can-increase-horsepower-and-fuel-economy-that-easily-without-seriously-costing-you-something-right
1,"AI is quite stupid and often trained on biased but convenient datasets. We’re pretty good at adding rules to make AI look intelligent, but AI mostly combines look-up tables and a couple algorithms. At the rate we’re going in the field, I doubt the singularity will happen at any point without significant progress. It certainly won’t be in my lifetime.",UNKNOWN,Ben Y. Zhao,1.2K,1.3K,33.6M,https://www.quora.com/Is-there-any-disadvantages-to-ECU-tuning-remapping-I-doubt-you-can-increase-horsepower-and-fuel-economy-that-easily-without-seriously-costing-you-something-right
2,"As a 68-year-old recently retired Silicon Valley engineer with a M.S. in Computer Science from Texas A&M University that focused on AI, I have a surprising answer for you to that question. We are no closer to strong AI today then we were in the 1970s. It is not clear that we will ever be closer. That is the reason why the field has shifted to machine learning. That field has entirety different goals than strong AI.The assumption that human-style thinking is algorithmic and independent of the underlying hardware is probably false. Nothing short of an organic biological brain in an actual humanAs a 68-year-old recently retired Silicon Valley engineer with a M.S. in Computer Science from Texas A&M University that focused on AI, I have a surprising answer for you to that question. We are no closer to strong AI today then we were in the 1970s. It is not clear that we will ever be closer. That is the reason why the field has shifted to machine learning. That field has entirety different goals than strong AI.The assumption that human-style thinking is algorithmic and independent of the underlying hardware is probably false. Nothing short of an organic biological brain in an actual human will ever replicate what we humans are doing when we are thinking. A majority of people in my field now believe this.It’s time to move on folks.C’est la vie.","284,819 followers",James Altucher,989,878,91.6M,https://www.quora.com/Which-company-do-you-think-will-be-the-first-to-create-the-singularity-for-artificial-intelligence
3,"Still very far, in my opinion. Let me explain.Around 50 years ago AI and related problems were thought to be easy and computer scientists were optimistic to solve them within a few years. There was even a famous professor who had said that “computer vision” is an easy problem, which can be solved by a graduate student during a summer internship. In those days computer scientists were doing many far-fetched promises, for example to NASA and to US military. The research in last 50 years showed us that these problems are actually more difficult than scientists thought. All those unfulfilled promiStill very far, in my opinion. Let me explain.Around 50 years ago AI and related problems were thought to be easy and computer scientists were optimistic to solve them within a few years. There was even a famous professor who had said that “computer vision” is an easy problem, which can be solved by a graduate student during a summer internship. In those days computer scientists were doing many far-fetched promises, for example to NASA and to US military. The research in last 50 years showed us that these problems are actually more difficult than scientists thought. All those unfulfilled promises caused a distrust towards the AI field and fundings were decreased for a long period, which is referred to as “AI winter”.Now we have “deep learning” and another period of optimism has emerged. Deep learning really seems to achieve many difficult tasks and already causing revolutionary changes in many different industries. Thousands of start-up companies have emerged that are based on deep learning technologies and big companies have started pouring money into the field. Some are already getting quick return of their investments. Face recognition technology is just one already working example, but there are many others.But I think in the following years a similar kind of enlightenment will occur: we will understand that the AI problem is even more difficult than we think it is today. In the following years I expect people will start to recognize that developing a general AI is one of the most difficult problems in science, along with understanding the brain. And we will understand that some of the current promises are far-fetched.Because, if you look at the problems that deep learning (or in general machine learning) is solving, they are all kinds of problems that brain does automatically. We don’t know how we recognize faces, our brains just do it automatically. And this is only one part of brain function. And it is the most primitive one. The more interesting, and the more complicated side of our intelligence is our reasoning. The things that we are doing consciously, not automatically. Our “thinking” ability. Just an example: we watch a movie and think on it, solve causal relations between events, reason why some character behaved like that, draw conclusions, etc. We have not seen a huge progress at such reasoning part yet in AI.Evolution of animal brain took billions of years, over countless number of trials and errors. Even then, the most advanced brains in nature, such as dogs, or cats, are very limited in capabilities such as reasoning and analyzing. Only humans have reached that level so far, and that is with thousands of years of cultural evolution added to the biological evolution. Now we are trying to mimic the result of all this process of billions of years, just by doing a few decades of research. The bitter fact is, the only example to the intelligence level we aim is the human intelligence, and we are still far from understanding how it works.To sum up, we thought that AI problem was easy 50 years ago, and things proved that we were wrong. Now we start thinking “it is difficult but can be solved soon with some more research”, which will also be proved to be wrong within a few years. Soon we will understand that developing a general AI is an extremely difficult problem, which will require at least (optimistically) several decades of more research, and will probably require understanding the human brain.",36 followers,Adam Ambrozy,579,0,0,https://www.quora.com/Which-company-do-you-think-will-be-the-first-to-create-the-singularity-for-artificial-intelligence
4,"Elon Musk’s argument:However, the argument seems to lack an understanding ofA quick example:1. What is the probability that a fair coin toss results in heads?2. If you are told a red team is playing a blue team at soccer this afternoon, and you are asked to predict the result - what is theElon Musk’s argument:However, the argument seems to lack an understanding ofA quick example:1. What is the probability that a fair coin toss results in heads?2. If you are told a red team is playing a blue team at soccer this afternoon, and you are asked to predict the result - what is the probability that the blue team will win?3. If you are now told that the “blue team” is the French national soccer team, and the “red team” is made up of local school children, what is the probability that the blue team will win?….What’s going on here? Whilst in examples 1. and 2. both probabilities are 50% at face-value, they are actually quite different.As you can see above, whilst both probabilities are stated as 50%, the meta-probabilities are shown by the curves. The curves reveal that the coin toss result is known to be a 50% chance fairly accurately, but the soccer game result is known to be 50% with a large amount of uncertainty.In example 3. we acquire more information about the teams playing, and this allows us to make a better prediction of the result. In other words:So whilst I agree that: “I’m not pretending to know whether we live in base reality or in a simulation. But I can say that the",82 followers,Jonathan Armstrong,9.2K,0,0,https://www.quora.com/Is-there-any-disadvantages-to-ECU-tuning-remapping-I-doubt-you-can-increase-horsepower-and-fuel-economy-that-easily-without-seriously-costing-you-something-right
5,"“Do we only really exist as a brain in a jar and our lives are hallucinations?Is this universe universe simply the product of super intelligent being’s imagination?Was this universe created by two giants giving high fives so hard that it spawns a big bang that grew into our universe?Point being, right now, it’s just speculation, so who knows? Even the laws of space and time as we know them are products of our universe. It’s possible those don’t even exist outside. It’s all very“Do we only really exist as a brain in a jar and our lives are hallucinations?Is this universe universe simply the product of super intelligent being’s imagination?Was this universe created by two giants giving high fives so hard that it spawns a big bang that grew into our universe?Point being, right now, it’s just speculation, so who knows? Even the laws of space and time as we know them are products of our universe. It’s possible those don’t even exist outside. It’s all very","21,738 followers",Glyn Williams,12.4K,9.1K,53M,https://www.quora.com/Which-company-do-you-think-will-be-the-first-to-create-the-singularity-for-artificial-intelligence
6,"I’m working towards a PhD right now, but I formerly spent a few years working on algorithms and software related to missiles.When you go about designing new algorithms for missiles, an obvious step you need to take is testing your algorithms and seeing how they perform relative to some desirable metrics. Suppose we have an air-to-ground weapon that we want to test to see how well it is performing against some metrics. We know this weapon will have some latitude-longitude-altitude (LLA) position, the target will have an LLA position, the target will have some specified velocity, the missile wilI’m working towards a PhD right now, but I formerly spent a few years working on algorithms and software related to missiles.When you go about designing new algorithms for missiles, an obvious step you need to take is testing your algorithms and seeing how they perform relative to some desirable metrics. Suppose we have an air-to-ground weapon that we want to test to see how well it is performing against some metrics. We know this weapon will have some latitude-longitude-altitude (LLA) position, the target will have an LLA position, the target will have some specified velocity, the missile will have some initial velocity and heading, and there will be some wind profile that needs to be taken into account. We also know that there is variance in the weapon properties (like mass, inertia, control surface locations, propellant mass, etc) and quantities based on sensor measurements (like target velocity, wind velocity, missile launch velocity, etc).How can we even hope to adequately test a new algorithm when there’s so many possible situations we should run into? Not to mention, flight mechanics of a weapon are nonlinear and in turn can be extremely sensitive to initial conditions. We certainly do not want to do physical tests just to see if a new algorithm is good enough, that would be much too expensive and have a much larger estimate variance for the number of samples you could practically obtain. No, instead we need to useNow as you can imagine, performing an adequate performance study includes doing millions of Monte Carlo simulations just to get a reasonable estimate of our weapon’s performance with the new algorithm(s). For a high fidelity simulation, each Monte Carlo sim could take a non-trivial amount of time, say two seconds on average. If you need to perform 250 million simulations to get a sufficient estimate of our performance, we are looking at running for over 15 years in CPU time, which divided across a cluster with 2000 processors is going to take on the order of a few days to complete.Say you dump this performance study to your cluster and sit back and let it run for a few days while you do something else, in turn passively generating a large dataset of information tied to those 250 million simulations. The day comes and you must now post process this dataset and extract the information you need to crunch a range of performance metrics.You sit thereNopeI was a new, young guy on a team comprised of people at least 8 years older than myself, some being 40 years older. Nobody knew me yet or really thought I would bring anything drastically new to the table, essentially meaning they were constantly giving me skeptical looks.They sat me down and made sure I could get up and running with their tools and process. What they did not realize was I get bored really quick and liked to minimize this sort of thing happening.As the young blood, they wanted me to get familiar with running these performance tests since I would depend on them to test algorithms I developed down the road. Made sense. Unfortunately, I did not have a lot more to do, leaving me super bored since generating and processing the data took so long. I sat there studying things and developing skills that might be useful down the road while I waited, until I started to really get fed up with the data analysis process.Why in the world were we taking days to just load our dataset for processing?I investigated the kinds of metrics we cared about and realized that, generally speaking, most metrics were on a per-simulation basis. In our analysis, we would then pool them together at different subspaces of our test space to gather interesting statistical insights. Any other metrics were basically reducing a subset of our data into a single value, which could be done at run-time in Matlab, if necessary. My mind immediately thought about our cluster that we could take advantage of and my previous distributed computing experience lit up a great idea! I came to the realization that writing aWith the embarrassingly parallel nature of our metrics, I quickly put together a distributed code in C++ using my down time spent waiting for things to run. I did not tell anyone on my team about my plan because I wanted to make sure it worked before I tried to sell the idea.. it’s much easier to sell ideas to people who do not trust you yet if you can show it works.I built this distributed code to make effective use of our SLURM cluster and generate the desired metrics, dumping the results into a single memory-light file for Matlab to work with. Amazingly, this new software reduced the data analysis runtime from a few days to literally less than a second! If we assume the new analysis took a second and the normal analysis took 2 days, we are talking about aSoon after, I went to my supervisor and pitched my new tool. He was shocked and soon, this tool became the new standard on my team.From that point on, people started to take me much more seriously and I was",18 followers,Christian Howard,1.4K,933,1.8M,https://www.quora.com/Is-there-any-disadvantages-to-ECU-tuning-remapping-I-doubt-you-can-increase-horsepower-and-fuel-economy-that-easily-without-seriously-costing-you-something-right
7,Our new Royalty and Copyright monthly batch system was ridiculously slow (and also a bit prone to crashing but that's a different story). I was asked to look at it and try to improve it. When I examined it I discover that the authors had designed quite a clever database with 8 flat files and 4 indices and then written a single module for all the other programs to use to access it. This was in the days before real databases. VSAM was the best we had and he had stored the entire database on one VSAM file.I guessed that because of this the access module everyone on the system was using was unableOur new Royalty and Copyright monthly batch system was ridiculously slow (and also a bit prone to crashing but that's a different story). I was asked to look at it and try to improve it. When I examined it I discover that the authors had designed quite a clever database with 8 flat files and 4 indices and then written a single module for all the other programs to use to access it. This was in the days before real databases. VSAM was the best we had and he had stored the entire database on one VSAM file.I guessed that because of this the access module everyone on the system was using was unable to use its buffers efficently and was spending all its time thrashing around the disk.I Rewrote the database access module into 8 sub programs each doing exactly what they did before just one sub-program for each record type. I was able to crib most of the code and this allowed each record type to have its own buffer which should make it run fare more efficiently.So I implemented it and about 8:00 that night I got a call saying that the ops thought there was something wrong with the system. It had finished in under 15 minutes . They were used to the monthly run taking several hours. They took a little convincing that that could be correct.I got some Kudos for that.,3 followers,Steve Baker,4K,27.3K,162.4M,https://www.quora.com/Which-company-do-you-think-will-be-the-first-to-create-the-singularity-for-artificial-intelligence
8,"In my master thesis, I developed a scheduling algorithm for the machine shop of a manufacturing company. The algorithm was inferred using reinforcement learning.Once a work order was given priority on a machine, it was scheduled according to the Just-In-Time principle. That is, the algorithm would prefer to start the work order on the machine as late as possible while still being able to finish it before the deadline.I wrote an algorithm that would find the latest point in time that the machine still had capacity to finish the work order. It ran in a few hundred milliseconds. Fine. Done.To getIn my master thesis, I developed a scheduling algorithm for the machine shop of a manufacturing company. The algorithm was inferred using reinforcement learning.Once a work order was given priority on a machine, it was scheduled according to the Just-In-Time principle. That is, the algorithm would prefer to start the work order on the machine as late as possible while still being able to finish it before the deadline.I wrote an algorithm that would find the latest point in time that the machine still had capacity to finish the work order. It ran in a few hundred milliseconds. Fine. Done.To get training data, I was simulating the scheduling of work orders and varying execution times of those work orders across all the machines in the machine shop using discrete-event simulation. For each iteration in my reinforcement learning, I needed to simulate months of work in the machine shop.I fired up the training experiment, and it was… slow. How slow? Well, I did some back-of-the-envelope calculations, and realized that my training experiment would take about half a year to finish! By that time, my thesis deadline would have passed. Something had to be done.I inspected my code, and to my surprise, the CPU hog was the innocent-looking algorithm that found the latest starting point according to JIT.In a panicked coding session that lasted long into the night, I was finally able to get the running time down to less than a millisecond, which meant the experiment would","1,249 followers",Rishabh Agrahari,7.9K,0,0,https://www.quora.com/Which-company-do-you-think-will-be-the-first-to-create-the-singularity-for-artificial-intelligence
9,"One of my earliest speedups of this sort was when I had just graduated from college and was working at Lockheed in their space systems division.We had a very large piece of FORTRAN code that took a whole lot of parameters - up to a million, and usually a few tens of thousands - that described a specific type of satellite, loaded them into an array, and then ran a simulation of them in a whole bunch of satelliteOne of my earliest speedups of this sort was when I had just graduated from college and was working at Lockheed in their space systems division.We had a very large piece of FORTRAN code that took a whole lot of parameters - up to a million, and usually a few tens of thousands - that described a specific type of satellite, loaded them into an array, and then ran a simulation of them in a whole bunch of satelliteThe problem was it was very slow, and running a satellite configuration through a largish package of constellations took about a week on a dedicatedI was asked to speed it up.Everyone figured that the big performance problem was deep in the constellation simulator code, which had a ton of complex physics and orbital mechanics algorithms, and they were considering various options for rewriting it in assembly, running the sim on aOK, so I start looking at the code.And I notice that the level of the code above where it initializes the results array, it has a big loop to set the array to 0. The array had a million entries and was a double-precision array, so it took (slightly less than) 8MB of RAM, which was a big deal in the Vax 11/780 days.And then the sim did its thing.The number of results were an output of the sim.And there was a call to a sort routine. It actually took the array and the number of results, and sorted the results array.…but…it sorted the whole frikin’ results array! UsingI immediately recoded the sort routine to use aThis improved performance to the point where a whole sim package could be run in about 45 minutes, versus a week. And it hadn’t occurred to them that something on the “periphery” of the core work of the sim would have such a massive impact. (And this was long enough ago that profilers and such were not all that commonly used.)The aerospace engineers did a very careful code-review as they couldn’t believe that I didn’t break something someplace.But after they did, everyone was happy, except an engineer who was looking forward to his very own Cray X-MP to play with.It ran fast enough that we were able to bid on several satellite contracts that people didn’t think we could bid on as the sim would take too long to get the bid prepared in time.",627 followers,Dipt Chaudhary,9.5K,0,0,https://www.quora.com/Which-company-do-you-think-will-be-the-first-to-create-the-singularity-for-artificial-intelligence
10,"Many, many of them.One was where a piece of software was doing a bazillion string searches - and I replaced it with a hash-code lookup and got about a 50x speedup - that was just poorly written code in the first place though.Another was a physics simulation involving a lot of blocks moving around in three dimensions that had to be updated regularly. Moving the code onto the GPU (via some cunning restructuring of the problem) I got at least a 1000x speedup!But some of the best stories aren’t always time improvements.I recall when I worked at Midway Games - we were writing a game for Xbox 360 anMany, many of them.One was where a piece of software was doing a bazillion string searches - and I replaced it with a hash-code lookup and got about a 50x speedup - that was just poorly written code in the first place though.Another was a physics simulation involving a lot of blocks moving around in three dimensions that had to be updated regularly. Moving the code onto the GPU (via some cunning restructuring of the problem) I got at least a 1000x speedup!But some of the best stories aren’t always time improvements.I recall when I worked at Midway Games - we were writing a game for Xbox 360 and the problem with that platform was that there was NEVER enough memory. Our lead engineer (along with the rest of us) always hated the inevitable final crunch panic when the game was close to release and you were running out of memory - and everyone would have to start working all hours to save enough memory to get the last features into the game.It would all be an enormous panic - people would have to work long hours - it would be utterly miserable!So at the VERY beginning of the project - three years before the game was due to ship - he sneakily declared:static char xxx [ 1024*1024 ] ;…somewhere buried in a file that nobody ever needed to change…then, about 3 weeks before our delivery date, he quietly deleted it, freeing a megabyte of RAM and totally avoiding the last minute panic! When we figured out what he’d done, we engineers were torn between loving him and hating him for that one!My favorite optimization that I personally pulled off was hardware cost optimization.When I was working on a low cost F16 flight simulator for the US military. This was before 3D graphics hardware for PC’s existed - and we were using Silicon Graphics hardware. Each graphics unit (capable of driving just one display) cost about $70,000!The F16 sim needed the usual graphics for the “out of the window” view - plus three small monitors - two “MultiFunction” displays mounted inside the cockpit and one projecting upwards to reflect off of a sheet of glass for the “heads up display”.Each of those three displays were costing us $70,000 in Silicon Graphics hardware…and we were contracted to build 100 of these simulators for the US Air Force - so these additional displays weren’t cheap.I woke up one morning at 4am with an idea that had somehow popped into my head.Each of those three displays were monochrome…no color.So I figured that if we took the wire that carried the RED part of the image and connected it to the Red, Green and Blue inputs of one monitor, the GREEN to the three inputs of another and the BLUE to the third - and made a one line OpenGL change:glColorMask ( channelId==LEFT_MFD, channelId==RIGHT_MFD, channelId==HUD, false ) ;…so my graphics software would draw the three instrument display in different colors onto ONE video output instead of three!So with one small change to the cockpit cabling diagram - and one line of code - this saved the company $70,000 x 2 x 100 = $14,000,000 - and I had the software change done and working by 8am when my boss came into work!Him: “Oh - hi Steve! You’re in early!”Me: “Yes - I just saved the company fourteen million dollars!”That wasThe company gave me a $100 bonus on my next pay check…cheapskates!","29,877 followers",Barry Rountree,4.3K,7.3K,39.6M,https://www.quora.com/Are-we-living-in-a-universe-which-is-just-a-computer-simulation
11,"I was asked to do a piece of consultancy for a team at work - they had developed a fully compliant 7 layer ISO stack (it was a fad). it was written in C so in theory was source code portable all the way down.They had load tested it at around 500 messages per minute on Unix, but when they ported it to DEC/VMS performance fell through the floor - to around 20 messages per minute. I was known as a bit of an expert on DEC/VMS so I was asked to assist.I looked at all the key things - CPU load, memory load, swap thrashing and they all seemed normal. I then noticed a very large amount of disk I/O, anI was asked to do a piece of consultancy for a team at work - they had developed a fully compliant 7 layer ISO stack (it was a fad). it was written in C so in theory was source code portable all the way down.They had load tested it at around 500 messages per minute on Unix, but when they ported it to DEC/VMS performance fell through the floor - to around 20 messages per minute. I was known as a bit of an expert on DEC/VMS so I was asked to assist.I looked at all the key things - CPU load, memory load, swap thrashing and they all seemed normal. I then noticed a very large amount of disk I/O, and apart from the executable itself, the load tested was opening only one other file - which held the addresses, ports, Application Points etc.This last file wasn’t opened once - it was being opened hundreds of times per message - every single time and address, port or anything else needed to be converted, the file would be opened, read and closed.From my DEC/VMS experience opening/closing files was actually pretty slow - and the solution was a 10 line code change to open the file once and cache the contents; the performance improvement was about 22 fold - and the DEC/VMS performance was more or less comparable with Unix.Addendum: One small detail I neglected to mention - my consultancy was a day in Belfast - in 1993 or so (before the Good Friday Agreement). It was a fun taxi ride from the airport to the hotel - with driver admitting that a week before on the same road a sniper had shot one of his colleagues. I then managed to get a bout of food-poisoning in the hotel, so my meeting the next day I was already starting to feel ill and at least a little bit scared.My day ended at Belfast airport with two armed policemen asking if would be ok to travel after I finally succumbed to the effects of the dodgy food.","4,734 followers",Ben Podgursky,1.3K,337,9.7M,https://www.quora.com/Are-we-living-in-a-universe-which-is-just-a-computer-simulation
12,"BackstoryThe InterviewOne key item on their list they had mentioned was a report that took about 3.5 hours to run. The trouble was, they needed itBackstoryThe InterviewOne key item on their list they had mentioned was a report that took about 3.5 hours to run. The trouble was, they needed it every half hour. So I said, let me just look at it quickly and see what’s going on.Bad, Bad ProgrammerI did a couple of quick tweaks, and used an appropriate index with a range. After that, it was running with just 1 pass, and only needed to access a much smaller percentage of the records. So after about 5 or 10 minutes work, I had it then running every 10 minutes!This was a 20 times speed increase and 3 times better than what they needed. Percentage wise, it was not even close to my most impressive work. However, the fact that I did it so fast, with no development tools with me and made something go from unusable to extremely usable while I was doing an interview, I consider that very impressive.Obviously, they decided to hire me to do the work. That company was a client for many years, and I spent a lot of time tailoring the system to their needs.And now,,…the rest of the story:D",18 followers,Asim Qureshi,2.9K,996,155.4M,https://www.quora.com/Can-you-describe-your-most-impressive-speedup-of-programming-code-you-did-in-a-project-that-was-already-essentially-done
13,"I once had the pleasure of doing the impossible - impossible, that is, in the eyes of the otherwise very computer-savvy theoretical astrophysics professor who was my master’s thesis advisor. His main area of expertise was hydrodynamics simulations, but he also dabbled in n-particle simulations of galaxies, and that was the topic of my thesis.He had given me some Fortran code he had written to simulate a galaxy. In principle the algorithm that was used is rather simple: build a galaxy from a lot of (about 100 000 in those days, nowadays you could have millions) stars, and then repeatedly calculI once had the pleasure of doing the impossible - impossible, that is, in the eyes of the otherwise very computer-savvy theoretical astrophysics professor who was my master’s thesis advisor. His main area of expertise was hydrodynamics simulations, but he also dabbled in n-particle simulations of galaxies, and that was the topic of my thesis.He had given me some Fortran code he had written to simulate a galaxy. In principle the algorithm that was used is rather simple: build a galaxy from a lot of (about 100 000 in those days, nowadays you could have millions) stars, and then repeatedly calculate the forces of gravity acting on all the stars and update their positions and velocities according to Newton’s laws.Now, you have to calculate the force of gravity between all pairs of stars, but, at least in principle you only need to calculate half of them: Newton’s 3rd law ensures that the force acted on star j by star i is the same as the force acted on star i by star j, but in the opposite direction. And if the number of stars was limited, my prof’s code did exactly that: it stored all the forces in a large matrix, and retrieved a result calculated earlier whenever possible.However, the amount of memory needed to store this matrix gets out of hand really quickly, so if the number of stars was to big for this matrix to fit into memory, the code did not use the matrix, but calculated all the forces twice.I, however, thought of a little trick (it turns out to be just a few lines of code) that ensures that Newton’s 3rd law is used whenever possible but without needing this force matrix. With this trick, my code all of a sudden was twice as fast as the version my professor gave me, and it had a much smaller memory footprint.What I did was this: instead of storing each force between each individual pair of stars separately, I only stored a vector representing the total force enacted on each star. Then, when calculating the force between stars i and j, I added this force to the total force on star i and subtracted it from the total force on star j. Now you need only consider the pairs with i>j and you end up with the correct forces, albeit that they are calculated in a different order (which can have an effect in floating point calculations, but it turned out to be ok).When I told my professor he went from “this student is bullshitting me” to disbelief to genuine delight that his student had been more clever than him (more credit to him: I’ve run into numerous professors who turned out to be insecure, insuffereable know-it-alls in my lifetime), to desperately wanting to have my code, all within the time of a few minutes that it took for me to explain what I had done.","82,155 followers",Quincy Larson,2.9K,358,29.1M,https://www.quora.com/Can-you-describe-your-most-impressive-speedup-of-programming-code-you-did-in-a-project-that-was-already-essentially-done
14,"Around 1988 a teacher showed me a basic program he called the apple men. They were basically mandelbrot fractals. On a cga monitor in 4 colors it looked like apple shaped little men.They wouldn’t look as nice as this one, just crude colors and low res.On a pc-xt the program would be left overnight to run. Overnight… we’re talking 8–10 hours of rendering time for one picture.The first thing I did when I got the basic program was to reprogram it in pascal, which is a compiled language instead of an interpreted one.This brought the rendering time back to under an hour.Next I took a look at all thAround 1988 a teacher showed me a basic program he called the apple men. They were basically mandelbrot fractals. On a cga monitor in 4 colors it looked like apple shaped little men.They wouldn’t look as nice as this one, just crude colors and low res.On a pc-xt the program would be left overnight to run. Overnight… we’re talking 8–10 hours of rendering time for one picture.The first thing I did when I got the basic program was to reprogram it in pascal, which is a compiled language instead of an interpreted one.This brought the rendering time back to under an hour.Next I took a look at all the calculations. This thing was put together by a mathematician with no optimization at all. The heart of the code was a nesting of 3 loops, an x,y, and z coordinate if I remember correctly. A lot of the calculations were done over and over again even thought parts of them would be always the same within a loop. So I moved those out of the inner loop and put them in variables. Now the code took 20 ish minutes to run.I have a memory of also using a precalculated table for divisions bringing the time further down and some graphics optimizations in assembler but it’s 30 years ago :)Anyway, the teacher was pretty flabbergasted.","8,693 followers",Mario Galindo Queralt,526,637,8.8M,https://www.quora.com/Can-you-describe-your-most-impressive-speedup-of-programming-code-you-did-in-a-project-that-was-already-essentially-done
15,"I was the new, junior (relative to everyone else) dev on the team. I had just about one year of experience past college, and I was the young hotshot.There was a tool that people needed to use to edit data for the app (a game). Every time they generated new data to test, it would take 2–5 minutes to run. This made it pretty slow to iterate on new designs, but it was their standard tool for this job, and they’d been using it for years before I arrived on the scene.A quick look at the tool and I figured out why it was so slow, and how I could speed it up, but it meant that I needed to allocate aI was the new, junior (relative to everyone else) dev on the team. I had just about one year of experience past college, and I was the young hotshot.There was a tool that people needed to use to edit data for the app (a game). Every time they generated new data to test, it would take 2–5 minutes to run. This made it pretty slow to iterate on new designs, but it was their standard tool for this job, and they’d been using it for years before I arrived on the scene.A quick look at the tool and I figured out why it was so slow, and how I could speed it up, but it meant that I needed to allocate a big buffer to act as a hash table. When you’re optimizing, a really common tradeoff is between speed and space; this was one of those.This was back in the days of 640k RAM limits for PC apps; even if your system had more RAM, if it was a 16-bit app, you had 640k and that was it.The tool was already allocatingI know everyone these days is totally casual about “just grab more RAM!”, but back then we didn’t have virtual memory. We had our 1Mb memory map that included 320k of hard-coded video and other hardware and ROM addresses. There was 640k shared between our app and the OS, and that wasToday most apps won’t evenMy goal was to speed up the app, but I didn’t have any RAM I could allocate. So I dug through the app to look at the buffers that were already allocated…and I found a graphics buffer that,The new performance? Instead of 2–5 minutes, it took about 0.1 seconds. You’d hit the command and it would beI told the team who were using the tool, and they loved the new performance. II have of course optimized many things since then. Some may have even been a higher speed improvement multiple. But that one still stands out for me as one of my favorite stories to tell.","358,169 followers",Sean Kernan,2.5K,5.6K,723.5M,https://www.quora.com/Can-you-describe-your-most-impressive-speedup-of-programming-code-you-did-in-a-project-that-was-already-essentially-done
16,"I have several such examples; but I think it's appropriate to describe a more general speedup opportunity that I've faced many times.One of the most common performance mistakes in I.T. is the act of letting a programmer design an intensive SQL database query - instead of turning the task over to a database architect/engineer or even a good DBA. I've seen several examples of this.Keep in mind that I'm not a noob. In fact I have designed and built special-purpose database engines myself, including a complete b-tree-based DBMS in C, from scratch. Yet I have been shamed by the performance of my quI have several such examples; but I think it's appropriate to describe a more general speedup opportunity that I've faced many times.One of the most common performance mistakes in I.T. is the act of letting a programmer design an intensive SQL database query - instead of turning the task over to a database architect/engineer or even a good DBA. I've seen several examples of this.Keep in mind that I'm not a noob. In fact I have designed and built special-purpose database engines myself, including a complete b-tree-based DBMS in C, from scratch. Yet I have been shamed by the performance of my queries on several occasions.Because of the unique nature of SQL, programmers often think in a way that impairs their ability to design an optimal query. Often a good DB person can make a query run multiple times as fast.I remember one such incident where I had proudly delivered DDL (schema) and DML (queries, etc) for an analytical system. The database architect quickly looked it over, politely suggested some changes, and sent me off with my tail between my legs. After implementing his changes the entire operation ran nearly twenty times as fast.Specialized knowledge is an amazing thing.","12,703 followers",Jean Yang,715,60,2M,https://www.quora.com/Can-you-describe-your-most-impressive-speedup-of-programming-code-you-did-in-a-project-that-was-already-essentially-done
17,"A lot of the other answers are far more profound than mine. But it was a fun moment for me. It started me on the path toward learning more Perl.The late 90’s was a time of explosive growth for cable modem ISPs. We were constantly getting new, big blocks of IP address network delegations from ARIN. One of the tedious parts of that fell on me. DNS records. Every IP address we assigned to a customer had to have a forward (A) and reverse (PTR) DNS record. (This was before the days where $GENERATE statements rendered this completely unnecessary.)It was not uncommon for me to have to add 30,000+ recA lot of the other answers are far more profound than mine. But it was a fun moment for me. It started me on the path toward learning more Perl.The late 90’s was a time of explosive growth for cable modem ISPs. We were constantly getting new, big blocks of IP address network delegations from ARIN. One of the tedious parts of that fell on me. DNS records. Every IP address we assigned to a customer had to have a forward (A) and reverse (PTR) DNS record. (This was before the days where $GENERATE statements rendered this completely unnecessary.)It was not uncommon for me to have to add 30,000+ records at a time. (32,768 to be exact). I had automated it with a shell script, but it was ridiculously slow, using `expr` statements to do the math. If I remember correctly, I think a /19 (8,192 records) took the script 90 minutes to complete on a little Sun Ultra 5.Eventually, it got to the point where I really needed to speed this thing up. I took a crack at doing it in Perl. I didn’t know Perl at all, I just knew other people were using it for things like this a lot. So I gotOk, I thought, looks good, let’s give it a spin.On that same ultra 5, it ran in 4 seconds. Huh? No way. I ran it again. 4 seconds. No way. I looked at the file it produced. Looked good. Checked the line count. Looked good. But… no way. So I ran the old script, waited the 90 minutes, did a diff of the files produced by the old and new scripts, and it was a perfect match.Ok, I think I’m onto something here…Wrote a lot of Perl, good and bad, since then. ;)",39 followers,Eric DeFazio,879,0,0,https://www.quora.com/Can-you-describe-your-most-impressive-speedup-of-programming-code-you-did-in-a-project-that-was-already-essentially-done
18,"As a reactor physicist, my job is to calculate in detail the reactions driven by neutrons in nuclear reactors, for the ultimate purposes of design and maintenance ofA unique feature of this reactor type is that it uses relatively small fuel assemblies, and replaces them during normal operation. This feature has several important advantages: mingling of fuel at various levels of depletion maximizes lifetime energy yield, characteristics of the core overall are nearly constant in time, spatial distribution of power can be optimized, and there are no shutdowns for refuellingAs a reactor physicist, my job is to calculate in detail the reactions driven by neutrons in nuclear reactors, for the ultimate purposes of design and maintenance ofA unique feature of this reactor type is that it uses relatively small fuel assemblies, and replaces them during normal operation. This feature has several important advantages: mingling of fuel at various levels of depletion maximizes lifetime energy yield, characteristics of the core overall are nearly constant in time, spatial distribution of power can be optimized, and there are no shutdowns for refuelling.However, this design is computationally demanding, because each fuel assembly (indeed, each end and the middle of each) has its own history, and there are thousands of assemblies in a power reactor. In calculating a reactor’s history or anticipated history, a “lattice cell calculation” has to update the detailed distribution of elements and of neutrons within each section of each assembly for each time step. A computer program can struggle to calculate a reactor’s history any faster than it happens in real time to a real reactor, because of the thousands of lattice cell calculations for each time step.In a lattice cell calculation, the assembly is notionally divided into many regions, and the approximation made that within each region the distribution of elements and of neutrons is constant. A similar approximation is made for neutrons with different kinetic energies. A very fine distribution of regions will yield a result with accuracy limited only by roundoff error, but take several minutes on a fast computer. A coarse distribution of regions can yield an answer on the same computer in under a second, but with reduced accuracy.A good standard method to create such a division into computational regions is an “adaptive mesh”, in which divisions are finer where spatial variation is more rapid. However, adaptive mesh procedures were stymied by the fact that, at different energies, the distribution of neutrons has very different spatial characteristics.Because of the the time squeeze, the CANDU power industry previously adopted a “standard model”, which yielded a compromise of acceptable accuracy and acceptable computation time. There are hundreds of parameters involved in setting up those computational regions, and because of the inapplicability of standard adaptive mesh procedures, previous generations of reactor physicists had created the standard model largely by g̶u̶e̶s̶s̶w̶o̶r̶k̶ professional judgement.In response to a version change in the lattice cell calculation, I was given the seemingly routine job of confirming that the standard model remained an acceptable compromise.However, I discerned certain commonalities in certain characteristics of the spatial distributions of neutrons at different energies, obscured by the large differences of the actual distributions. I made up a fictitious neutron distribution that had those same certain characteristics, and a program that made up an adaptive mesh for that fictitious distribution. Thus, I was able to reduce the options in the mesh design to ten parameters, corresponding to ten basically different types of regions in each lattice cell.With much fiddling of those ten parameters, I obtained hundreds of results, all on the same computer, like this:I was able to improve computation time several-fold versus the previous standard model, and my model makesBy use of my program with its fictitious distribution, one can feed in different fuel assembly designs, with the same set of ten parameters, and have a good chance of achieving similarly good analyses for those other designs.","28,523 followers",Garry Tan,1.2K,108,1.5M,https://www.quora.com/Can-you-describe-your-most-impressive-speedup-of-programming-code-you-did-in-a-project-that-was-already-essentially-done
19,"Back when I was in college my major was EE (Computer Science) but I had a job programming for the Civil Engineering department. Since that was not my field of expertise I typically simply translated what the professors and grad students wished into computerese. Actually it might have been useful if IIn one notable case I was tasked to write a tunnel cost model. This was a fairly involved program which modeled the geology of each tunnel as a string of short segments each with its own length and geologBack when I was in college my major was EE (Computer Science) but I had a job programming for the Civil Engineering department. Since that was not my field of expertise I typically simply translated what the professors and grad students wished into computerese. Actually it might have been useful if IIn one notable case I was tasked to write a tunnel cost model. This was a fairly involved program which modeled the geology of each tunnel as a string of short segments each with its own length and geology: a Markov Tree (The program was largely finished and, when a number of simulations were run each against varying geology, generated a beautiful scattergram at the end showing time versus cost. It looked something like the image below except that there was no color (that was back in the day of the line printer): the number of runs which came up with a particular value was represented by a number or letter.Then, to get some sort of base reference, the professor in charge of the project asked for a set of simulations for a fixed geology with the digging of the tunnel the sole variable. The resulting “scattergram” wasMy first instinct was that there was some bug in my program. Upon reflection, the reason was obvious to all. The act of digging a tunnel segment was simply randomly picking a number from the distribution specified by the Markov Tree as many times as necessary to finish that segment. It was a beta distribution as I recall, but that detail hardly mattered. When one adds random variables from the same distribution repeatedly theNow at first glance the Markov tree we were using looked like a complicated beast that had to be processed by running simulations against it. But, with the result we found, I subsequently figured out thatSo that part of the program, at least, could now run multiple orders of magnitude faster.","29,877 followers",Barry Rountree,702,7.3K,39.6M,https://www.quora.com/Can-you-describe-your-most-impressive-speedup-of-programming-code-you-did-in-a-project-that-was-already-essentially-done
20,"When you talk to your friends, the sound travels through the air, i.e. empty medium. According to physics, this empty space or medium isn't actually empty. It weighs something, it's full of activities and it contains so many things. This sound is called analog sound.Let's consider this sound asSurprisingly, this sound or data can be converted into some form of numbers, called binary numbers, i.e.When you talk to your friends, the sound travels through the air, i.e. empty medium. According to physics, this empty space or medium isn't actually empty. It weighs something, it's full of activities and it contains so many things. This sound is called analog sound.Let's consider this sound asSurprisingly, this sound or data can be converted into some form of numbers, called binary numbers, i.e.When thisDigital wave has very much different characteristics than analog. Meaning digital waves are readable by computing device. So like sound, image, video, text and almost everything can be converted into binary format. And it’s possible to carry these numbers via empty medium.How?Not so easy to answer. But I will put this in a metaphor what explains most of it.Sara writesWhen the data packets were formed, the destination router's (John's router) address were specified within the packets. So the textFinally when John's computer will accept it, the text will be reconstructed from binary numbers. John’s Facebook will display theWrapping up-Energy and particle can travel through the air. For example, light consists of particles and it can reach anywhere in this universe. The reason is that every particle, every matter, every molecule and everything in this universe are somehow connected to one another. What you see below is an wonderful illustration of Super-string Universe that an artist can imagine.That is how the wave also can run from one place to another place without needing any physical medium.",25 followers,Ian Beyer,823,2K,5.9M,https://www.quora.com/Is-there-any-disadvantages-to-ECU-tuning-remapping-I-doubt-you-can-increase-horsepower-and-fuel-economy-that-easily-without-seriously-costing-you-something-right
21,"Wireless Technology works on the principle of transmission of data modulated over EM waves.  So for example, as I type this answer out, I am generating 1's and 0's, which is my data (in the binary form). These 1's and 0's then go to my wireless card where, it is modulated onto a carrier frequency, which we say is fc (2.4 GHz for wifi; 900 MHz - 2100 MHz for LTE, 3G and GSM standards). This modulated carrier is then fed to the antenna which generates EM waves. The EM waves are then received at the other end (receiver) by the antenna and the data is demodulated from the carrier. That data is theWireless Technology works on the principle of transmission of data modulated over EM waves.  So for example, as I type this answer out, I am generating 1's and 0's, which is my data (in the binary form). These 1's and 0's then go to my wireless card where, it is modulated onto a carrier frequency, which we say is fc (2.4 GHz for wifi; 900 MHz - 2100 MHz for LTE, 3G and GSM standards). This modulated carrier is then fed to the antenna which generates EM waves. The EM waves are then received at the other end (receiver) by the antenna and the data is demodulated from the carrier. That data is then fed to us in a human readable form, i.e., text and picture.","1,473 followers",Derek Schatz,1.7K,4.3K,5.9M,https://www.quora.com/Which-company-do-you-think-will-be-the-first-to-create-the-singularity-for-artificial-intelligence
