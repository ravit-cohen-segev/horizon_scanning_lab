,Answer,followerCount,name,upvoteCount,answerCount,answerViews,question
0,"I love this question, because it allows the coverage of many key space topics.So ok, let’s imagine this is year 2520.We have sent humans onto pretty much all bodies in the solar system where a human presence is reasonably feasible. After the Moon in 1969,In the years 2200-2270, we managed to set foot onI love this question, because it allows the coverage of many key space topics.So ok, let’s imagine this is year 2520.We have sent humans onto pretty much all bodies in the solar system where a human presence is reasonably feasible. After the Moon in 1969,In the years 2200-2270, we managed to set foot onFinally, in the years 2300-2360, we managed to conquerAll these missions were driven mainly by prestige and glory. Once a destination was reached, the public quickly lost interest and there was no funding to repeat further manned missions. A few human colonies have been started here and there. Although, as they were all lacking a sustainable business model, they all went bankrupt after a while. Space is now only populated withThe only place we still maintain aOn Earth, after a lot of trials and errors, we eventually found a way to manage our limited planet resources in a sustainable way. The consequence was that the worldRemember, this is year 2520. There is an ambitiousThe project got back on track, but it still remains a long shot. At best, it will take another 200 years to get the atmosphere to the desired level of pressure. And this is assuming funding will not be reduced, which is not a given in our stagnant economy. For plants to grow, and the air to be somewhat breathable, the most optimistic forecast puts this at least a couple of thousand years from now.Given the uncertainties remaining on the project, it’s hard to sell any real estate to investors and the project has to rely mostly onThe last frontier remainsIn the year 2100, we managed to send a small probe, at 2% of light speed, using an innovative fusion reactor. It arrived around New Earth 400 years later and by the year 2508, we started to get detailed coverage of the planet.Key parameters like pressure, temperature, gravity, and the magnetosphere are all confirmed to be ok. This New Earth hosts some basic unicellular life, but the probe found no trace of any multicellular organisms. A perfect place to start a new home!This New Earth seems indeed a much better place for humans than Mars, (or any other spot in our solar system), even if we assume the ongoing Martian terraforming efforts will be successful, (which is still a big if).So now again, it’s the year 2520. How do we proceed to colonize this New Earth?In our stagnant economy, there is only that much we can dedicate to interstellar travel, a project that generates no substantial benefits to politicians on Earth who control the funding. Still, they managed to secure $80 billion financing per year, (2020 equivalent), for this exciting project. Quite an impressive achievement for something which is seen by a large part of the world population asFaster than light travel is obviously out of the question. Worse, our space transportation technology isIn the last 500 years, AI has made tremendous progress. Well, to be honest most progress came in the first 100 years. After 2120, we started to face diminishing returns. Even if our AI are today capable of super impressive feats, we never quite reached the holy grail of Artificial General Intelligence. In particular, we realized that consciousness required a biological human body.Our robots would always remain sophisticated machines although they would have no purpose of their own. In a way, it’s a relief. Who wants its washing machine to be self-aware? For the same reasons,Medicine has also achieved impressive progress within the last centuries. We now routinely live healthy lives until 110 years old. Which is great! However, beyond that age, the body breaks down. ThisThere has been many attempts to put the human body intoOn the other hand, an area where we have made a lot of progress isIn theory, this gives us the option to send an interstellar ship with a largeUnfortunately, the results had been catastrophic. All children experienced severe psychological trauma beyond repair. For obvious ethical reasons, those experiments have been shut down since then.For our 200-year trip, it leaves us with the only remaining option, of aThe crew would beThere has been a lot of debate about how many people would join the ship. More people provide more expertise. Although, at the same time, the limited volume of the pressurized habitat is a key constraint. Eventually, it was decided that 3 people would be optimal.A simple calculation showed that with a couple of babies born after 20 years, and then every 50 years afterwards, the whole crew would always remain under 9 people, (assuming that people would be living, on average, up to 100 years). This ensures, at all times, a reasonable amount of living space for each crew member.The whole success of the mission relies on maintaining aTo maximize the odds, it has been decided to send, at the same time, two twin ships, following the same course. Beyond obvious mission redundancy, this has another advantage. It provides each crew with continuous real time communications with someone within close range of the ship.On top of the psychological comfort, to know that others are facing a similar fate, this could prove useful to deal with any troubleshooting, as opposed to mission support from Earth, which due to very long transmission delays, would quickly prove to be useless in the case of any emergencies.Dealing with the arrival on this New Earth is a whole new challenge on its own. The success of the settlement will depend on how muchIf on the other hand, the local ecosystem is hostile or just not practical, it means thatGiven the small population of the colony, it will indeed be impossible to maintain the right level of expertise over time. All mission critical material brought by the mission will slowly but certainly break. As no one will be capable of replacing it, (unless the technology has been cataloged into a library that would easily be accessible to the crew), the whole colony would collapse. Rescue missions sent from Earth would of course not be an option.After such a huge effort, this would be quite sad. So no need to rush. What’s key is to pick the perfect destination for our civilizations New Earth. Hopefully it exists in a 20 light year radius from us. Otherwise interstellar colonization will remain off the table for ever.",95 followers,JB Rudelle,1.3K,0,0,https://www.quora.com/If-humans-manage-not-to-destroy-themselves-what-do-you-think-might-be-achievable-technologically-in-space-travel-500-years-from-now
1,"Tremendous. Tremendous leaps in technology! Simply mind blowing!If a certain country doesn’t go on waging wars and quits trying to enforce its version of government and a certain other country stops expansionist exploitation of its neighbors, the progress we will make with space travel in 5 centuries will be incredible (for today’s person).Artificial Intelligence is going to really take off within 20 years from now and we will seeTremendous. Tremendous leaps in technology! Simply mind blowing!If a certain country doesn’t go on waging wars and quits trying to enforce its version of government and a certain other country stops expansionist exploitation of its neighbors, the progress we will make with space travel in 5 centuries will be incredible (for today’s person).Artificial Intelligence is going to really take off within 20 years from now and we will seeImage 1. Within the next 6 decades, the real issue facing scientists and engineers will not be how to make more intelligent software systems. It will be: how to stop criminals employing intelligent machines for their crimes!Computer chip hardware technology will come to a halt within 10 years from now, due to straining the miniaturization limits but it will not halt the development of faster computers. We will see more and more processors on the same board. Within a century, the need to develop faster computers will vanish, due to the inability of the human brain to comprehend the shockingly high amount of data those systems will be spitting out. There would still be competitions to build faster computers, but they will be for academic and curiosity purposes only.When a single computer system will automatically manage your financial accounts, build new video games for you with specifications provided by you, have the detailed map of the entire Earth on it and accomplish complex office tasks with little interference from humans, why would there be a need for a faster computer anyway? A common household computer will have sufficient computing power to plan a complete tour to and from Mars and Earth. NASA and EuSI will have “supercomputers” only for mapping the dark matter and running extremely detailed simulations of singularities.The technological boom produced by broadly intelligent software systems and incredibly fast hardware is difficult to imagine. One of the key improvements will be in propulsion methods. It will become very (relatively) cheap to create antimatter and safely store it, too. The implications will be incredible!Image 2. With affordable antimatter fuel, the real challenge concerning space travel would shift from achieving high velocity to safe travel.With speeds up to 10% the speed of light enabled by antimatter propulsion systems, the real challenge would become how to safely navigate a spaceship. The speed will be simply far exceeding human comprehension and the spaceships’ routes will be planned entirely by intelligent navigation systems, meticulously mapping the paths of individual asteroids and comets. However, even in the case of the slightest error, the ensuing disaster would be incomprehensible. A collision with a mere 0.1 cubic meter space rock/ice will not only completely obliterate the colliding part of the spaceship, but will also generate enough thermal energy to melt the entire section of the spaceship!For travel within a planetary system, speeds no more exceeding 1% the speed of light will be considered safe. It will only be for interstellar travel that the true potential of antimatter propulsion will be unleashed.The availability of cheap, incomprehensibly high energy fuel and incredibly intelligent software systems (and fast hardware to run on) will result in the construction of confoundingly large space telescopes. Within 2 centuries, there will be space telescopes as large as entire countries, orbiting the sun beyond Jovian orbit. They will employ an array of 60 square meters wide mirrors (of course) instead of lens or one colossal mirror (which would obviously fail due to gravity). The data transmitted by these gigantic telescopes would be comprehensible only by intelligent software since it would be sending hundreds of petabytes of dataImage 3. When transporting materials to space cheaply becomes possible, extremely large space stations and extremely large space telescopes will be the natural outcome.It will not only enable a far, far more detailed mapping of the entire solar system (obviously), but it will also become possible to not only visuallyHowever, it might still not be possible to sendWe will not just needWhat we will be sending a lot of spacecrafts to “nearby” exoplanets of course. However, these spacecrafts will contain nothing. The entire spacecraft itself will be a gigantic research instrument, capable of anchoring itself in a stable orbit around any star or planet, and studying it in detail, relaying the data back to Earth. Cloning spacecrafts will become a reality after 2 centuries. These would land on planets and use its resources to build clones of themselves and launch them into space. There might also be missions meant to extract precious resources from asteroids/planets and bringing them back to Earth.The last (after 4 centuries) and most shocking development weWhile we will not be able to physically “travel” to other planets and stars, through our cloning spacecrafts, we will be virtually everywhere within a 20–30 light years radius. And yes, that will mean we will know for sure how developed are life forms on other planets.Image 4. Controlling the quantum state of a quantum entangled pair will be the ultimate milestone in space travel, enabling us to communicate with our spacecrafts anywhere in the universe, in less than a moment.",UNKNOWN,Devin O'Keefe,21.7K,0,0,https://www.quora.com/If-humans-manage-not-to-destroy-themselves-what-do-you-think-might-be-achievable-technologically-in-space-travel-500-years-from-now
2,"As of today, and in fact, as of the 60’s, we have had the technology to reach nearby stars within a single human lifetime (barely, but yes). We have chosen not to develop those technologies (rotating space habitats, nuclear pulse rockets, medically induced comas, etc) for space travel for various reasons, which means they still have engineering concerns that need to be addressed, mostly those we don’t know about yet. Sustainable fusion technology would only help this.I personally believe that we either establish a significant colony in deep space by the end of the middle 3rd of this century, oAs of today, and in fact, as of the 60’s, we have had the technology to reach nearby stars within a single human lifetime (barely, but yes). We have chosen not to develop those technologies (rotating space habitats, nuclear pulse rockets, medically induced comas, etc) for space travel for various reasons, which means they still have engineering concerns that need to be addressed, mostly those we don’t know about yet. Sustainable fusion technology would only help this.I personally believe that we either establish a significant colony in deep space by the end of the middle 3rd of this century, or humanity will send itself to early technology and sub Billion populations over the next 3–4 centuries. And who and why has some impact on whether that is sufficient to prevent the reversion.Once we establish one or more mostly self sustaining colonies, competition will not need to include mass violence, as there will be enough, just who can get to it most efficiently. Instead of “the world is your oyster” it will be “the solar system is your oyster”.In 500 years even with continued population growth, we would barely be scratching the surface of the available resources in the solar system, so there would be no physical need to travel to other stars yet, but that doesn’t mean some group won’t want to do so, mainly to get away from the bureaucracy in the solar system, even though it would not be economical (puritans fleeing to the new world, ring any bells?)",4 followers,Dan Bradbury,4.2K,21.7K,235.2M,https://www.quora.com/If-humans-manage-not-to-destroy-themselves-what-do-you-think-might-be-achievable-technologically-in-space-travel-500-years-from-now
3,"Look back at any speculative TV program from a few decades ago and many of their ideas raise a smile. Flying cars, Martian colonies, nutritional pills as food- all of these were presented with breathless enthusiasm. Remember the early 1980`s mobile phones with their huge batteries ? The point is that reality overtakes speculation. There are nascent technologies and theories that “could” lead to successful interstellar travel. On the other hand they might turn out to be dead ends. Just for fun: I suspect that we will be travelling within our solar system reasonably quickly within 500 years. TraLook back at any speculative TV program from a few decades ago and many of their ideas raise a smile. Flying cars, Martian colonies, nutritional pills as food- all of these were presented with breathless enthusiasm. Remember the early 1980`s mobile phones with their huge batteries ? The point is that reality overtakes speculation. There are nascent technologies and theories that “could” lead to successful interstellar travel. On the other hand they might turn out to be dead ends. Just for fun: I suspect that we will be travelling within our solar system reasonably quickly within 500 years. Travel time to Mars could be weeks rather than months. There are a lot of resources for us to gather within our own system, so even 500 years from now interstellar travel will remain within the realm of science - unmanned probes are more likely than lantern jawed astronauts. There may be a manned expedition to one of our closest stars, launched with great fanfare and as much a political `stunt` as anything. Realistically they will spend decades in transit, probably spending long periods in some form of tupor / `suspended animation` if such a thing can be achieved. What we can hope for here in the dark ages is that our descendants will discover game changing new science that allows the famous warp drive of Sci-fi fame","358,158 followers",Sean Kernan,3.1K,5.6K,723.4M,https://www.quora.com/If-humans-manage-not-to-destroy-themselves-what-do-you-think-might-be-achievable-technologically-in-space-travel-500-years-from-now
4,"I have to say that I enjoy reading everyone's comments on this question. However, I am somewhat setback by the dire outlook for the projected expectations for the human race’s drive for exploration and incentives for space exploration. This is in part due to the current technology baseline recognized and misleading awareness of people not taking seriously what many people now know to be true.For example, the publicized state of the artI have to say that I enjoy reading everyone's comments on this question. However, I am somewhat setback by the dire outlook for the projected expectations for the human race’s drive for exploration and incentives for space exploration. This is in part due to the current technology baseline recognized and misleading awareness of people not taking seriously what many people now know to be true.For example, the publicized state of the artThat being said, a new baseline for what is possible today changes the whole outlook on what is possible in the next 50 years let alone 500 years. Let’s say the Star Trek or Star Wars vision for the future, as far as technology, is much closer to reality.This new technology will open the gates to exploring and capitalizing on the vast mineral deposits in the asteroid belt. This will expand the reach of manufacturing within the solar system which will pay for itself reducing the need for funding from governments. The drive for human exploring the unknown will be unleashed. Within 100 years we will have ventured beyond the boundaries of this solar system. Within 150 years we will have colonies on planets in other star systems. Within 500 year Mars and Earth will be in economic competition for resources beyond the asteroid belt. Travel to any point within our galaxy will take only minutes not decades. To days laws of physics will be remembered as only the beginning of what is possible.This assumes that the original question is still accurate and we have not met our counter parts in space which may not agree with our boldness.",64 followers,Shivansh Singh,1.9K,401,924.5K,https://www.quora.com/If-humans-manage-not-to-destroy-themselves-what-do-you-think-might-be-achievable-technologically-in-space-travel-500-years-from-now
5,"Awesome question! Assuming innovation will continue at its current pace without any major deviations I think that in 500 years we will be transitioning to a truly spacefaring civilization.The limiting factors in space exploration and expansion are time and distance. To solve these problems we either need to:The most basic problem we should have solved within 500 years will be energy storage and creation. Batteries are a huge problem even now, once they are able toAwesome question! Assuming innovation will continue at its current pace without any major deviations I think that in 500 years we will be transitioning to a truly spacefaring civilization.The limiting factors in space exploration and expansion are time and distance. To solve these problems we either need to:The most basic problem we should have solved within 500 years will be energy storage and creation. Batteries are a huge problem even now, once they are able to equal or exceed chemicals in energy density we will see an enormous change in the world. Energy creation is actually in my opinion only going to be an issue for deep space interstellar voyages beyond the range of a star's light to sufficiently be collected with solar panels. That said I believe that we will have finally solved the issues associated with fusion reactors and have them available for use on our spacecraft.Inertia is the biggest problem in space exploration that I don’t see a lot of discussion around. The importance of this can be seen with how long it will take to accurate from rest to half-light speed(0.5c) would take 5,376 hours at an acceleration of 9.8m/s2. And it will take the same amount of time for deceleration from that speed. While people can survive at higher accelerations it’s going to be uncomfortable real fast, even for the most fit pilots you can think of. But just having an engine with the endurance needed for the above-mentioned performance will make things feasible if not fast.Moving onto the holy grail of space exploration is Faster Thank Light(FTL) travel. This is a staple of science-fiction because of the vast distances involved. It takes light over 4 years to reach the nearest stars from our sun. Alpha Centauri is 40 trillion km from earth and at 0.5c would take 9 years without the accel and decel portions needed. Accounting for those portions of the trip it would take almost 12 years to get from the earth to the nearest start.Hibernation is where biological functions are slowed or halted in a living organism that can then be returned to normal at a later time. This would be useful for any trip outside of the solar system assuming that FTL technology is not available.Increased lifespans would be another way to offset the time it would take to travel between stars. This would make it possible to have trained personnel available for much longer periods reducing the need in some ways of needing to preserve and pass on institutional learning and experience.My assumption is that we will not find a way to implement FTL so our expansion and exploration will be somewhat confined. Even expanding at 0.5c we can reach the other side of the galaxy in 100,000 years, not great but not bad either. Another assumption is that we will be able to store and create near unlimited energy within 500 years, we will be well on our way to exploring the galaxy. With hibernation and life extension, it will also be possible that someone born at the end of this century could see these things take place.To answer your question I think we will have machine probes as far as 300 lightyears (LY) away and manned missions as far as 50 LY within 500 years. The solar system will be full of human activity from mining to travel. Mars and Venus will be colonized with more than a billion people living between those 2 planets. There will be another billion people living throughout the solar system. Earth will have a population of 20 billion people.",18 followers,Asim Qureshi,17K,996,155.4M,https://www.quora.com/If-humans-manage-not-to-destroy-themselves-what-do-you-think-might-be-achievable-technologically-in-space-travel-500-years-from-now
6,"Both are actually quite closely related, and it’s strange to me that people don’t pick up on this fact.Some people have this weird idea, that humanity is going to destroy the planet, and thus should go to space.This is really, really, really dumb.How do you think that people will survive in space? In a spaceship, or in a space station, or on Mars, or wherever?If on earth, with all the resources available, we cannot find a way to have something sustainable, not destroy our environment, and are always waging wars against each other an killing ourselves…All of these issues are orders of magnitudeBoth are actually quite closely related, and it’s strange to me that people don’t pick up on this fact.Some people have this weird idea, that humanity is going to destroy the planet, and thus should go to space.This is really, really, really dumb.How do you think that people will survive in space? In a spaceship, or in a space station, or on Mars, or wherever?If on earth, with all the resources available, we cannot find a way to have something sustainable, not destroy our environment, and are always waging wars against each other an killing ourselves…All of these issues are orders of magnitude worse when looking at something like a space colony.If you can’t figure out a sustainable system on earth, then on a colony on mars, an unsustainable system will also not work, it will fail much much faster.If you ever want to be able to go to space, it’s not just about technology, it’s also about our social organization, and actually managing to create sustainable systems.Some people think space travel is important, because in a couple of hundred thousand or millions of years, there might be an issue with earth itself.Sure, in that sense, it would be a good option to have space stuff within the next thousands of years.But out of the 2 things that are needed, the technology to go to space, and the technology and organization to survive sustainably in systems with a limited amount of resources without destroying each other…That second part is much more important right now.If we can’t learn not to destroy our environment and each other, then it doesn’t matter if we are here or in space, we are doomed anyway.How do you think that space will be organized?A capitalist’s wet dream, where one person owns the spaceship/space station/colony, and runs it as a totalitarian dictatorship, and sets the price for oxygen?And if anyone mentions the idea that everyone need to breathe, and maybe air should be free, they get executed for being a communist, and told that it can never work?",2 followers,Richard Muller,6.3K,2.3K,202M,https://www.quora.com/If-humans-manage-not-to-destroy-themselves-what-do-you-think-might-be-achievable-technologically-in-space-travel-500-years-from-now
7,"[I’ll ignore your first seven words.]In 500 years, the human lifetime will not have changed significantly, nor will the limitation on velocity imposed by the speed of light (which will not have changed at all).So, the outer limit for human travel away from Earth will be Titan — and I can see no attraction to make anyone devote their lifetime to leaving Earth.","284,819 followers",James Altucher,988,877,91.6M,https://www.quora.com/If-humans-manage-not-to-destroy-themselves-what-do-you-think-might-be-achievable-technologically-in-space-travel-500-years-from-now
8,"It is possible for humans. It has been done many times. It’s happening at the moment.If you mean, without artificial support… well, humans evolved to need oxygen, and there isn’t any in space. Also, to get into orbit, you need to be going 17,000+ miles per hour, and we can’t do that with our feet.","5,353 followers",Valerio Cietto,677,2.4K,9.1M,https://www.quora.com/If-humans-manage-not-to-destroy-themselves-what-do-you-think-might-be-achievable-technologically-in-space-travel-500-years-from-now
9,"Even if tecxhnology would make no big advancements, especially no readily available way to exploit fusion energy, you can take it to the bankWhether humanity will already be on its way to the stras in 500 years is hard to predict.",0 followers,Ashok Bishnoi,12.8K,314,7.6M,https://www.quora.com/If-humans-manage-not-to-destroy-themselves-what-do-you-think-might-be-achievable-technologically-in-space-travel-500-years-from-now
10,"Great question. This answer is based on an investigation done by Sahara Reporters in New York.Philip Emeagwali stirs up diverse emotions in Nigerians, Africans, and black people around the world. His claim of being a father of the Internet, of having invented the Connection Machine, of possessing 41patented inventions, of winning “the Nobel Prize of Computing” and of being a “doctor” and/or professor” have been conclusively debunked with widely documented evidence.Yet, the figure of Emeagwali as a black scientific, engineering, and information technology genius and pioneer continues to loomGreat question. This answer is based on an investigation done by Sahara Reporters in New York.Philip Emeagwali stirs up diverse emotions in Nigerians, Africans, and black people around the world. His claim of being a father of the Internet, of having invented the Connection Machine, of possessing 41patented inventions, of winning “the Nobel Prize of Computing” and of being a “doctor” and/or professor” have been conclusively debunked with widely documented evidence.Yet, the figure of Emeagwali as a black scientific, engineering, and information technology genius and pioneer continues to loom large over discussions of black achievement. The legend of Philip Emeagwali’s purported inventions, widely proven to emanate from the perverse deceptive genius of the man himself, endures and proliferates among Nigerian and black groups around the world.Only recently, the USAfricadialogue googlegroups listserv managed by Professor Toyin Falola of the University of Texas hosted a discussion on Philip Emeagwali’s vast fraud. Participants in the discussion included Nigerian and African intellectuals, scientists, engineers, and IT professionals. Overall, the discussion reinforced and reiterated one of the worst kept secrets in the Nigerian Diaspora, especially in its online community: that none of Emeagwali’s highfalutin claims, on whose strength he has curried and continues to curry favor and recognition from gullible and hero-hungry black people, is true. Yet, just a few days ago, one of Nigeria’s more visible dailies,Nigerians and black people deserve to know who the real Philip Emeagwali is. This will save them from the embarrassment of continuing to celebrate a fraud while real black scientific achievers and pioneers starve for attention and recognition. To correct Nigeria’s scientific and technological lag there is a need for investments — both financial and motivational — in the sciences, engineering, and IT fields. Nigerian youths need inspiration in the quantitative and scientific disciplines, but they should get it from actual, not pretending, black scientific, computing, and engineering heroes, not from phonies like Mr. Emeagwali.Patented Inventions Or The Invention Of Patents?Debunking the many myths of Mr. Emeagwali’s “achievements” is one the easiest things to do on earth if you have a computer with Internet access. Let us start with his claim of possessing 41 (32 by some accounts on some hero-worshipping black websites) patents for various inventions. A simple search at the website of the US Patent and Trade Mark Office (here:Specifically, Mr. Emeagwali claims to have invented the Connection Machine (CM-2). This false claim is displayed boldly and shamelessly onInternet Pioneer?Mr. Emeagwali claims to have used the CM-2 Machine to carry out billions of calculations by connecting over 65,000 processors (computers) around the world. He claims that this was the rudimentary foundation of the Internet. It is on this ground that he has aggrandized to himself the title of “father of the internet.” But this is a barefaced lie at worst and an egregious exaggeration at best. And it is so absurd in its circular logic that it is hilarious. First, as stated earlier, Emeagwali did not invent the Connection Machine on which his “experiment” relied. Second, Emeagwali used more than 65,000 independent processors ""around the world"" (meaning on the Internet) to do his calculation. This means that the Internet already existed and that he RELIED ON it for his calculations. Unless the Internet he claims to have fathered is different from the Internet that already existed at the time of his experiment (and which we all know as the existing internet today), he COULD NOT have invented the Internet or fathered it. He could not have been using an internet that, by his claim, did not exist until he invented it. As this websitePhilip Emeagwali did work in supercomputing in the [late] eighties……. But supercomputing and the Internet are very different areas. And Emeagwali did not contribute to even one of the hundreds of Internet standards, or RFCs (Requests For Comments), that were created in the early decades of the Internet—an open process that anyone could participate in. His supercomputing research was completely unrelated to the Internet.Emeagwali’s research was thus irrelevant to the evolution of the internet. Emeagwali did his supercomputing experiment in the late 1980s. By then, the “core standards” and protocols for information and data flow on the Internet already existed. And although, improvements have been made to the template since then, Emeagwali did not make any of those improvements and cannot therefore claim credit for them.Emeagwali's tenuous—and fraudulent—claim to internet fatherhood rests on his assertion that ""the Supercomputer is the father of the Internet,” “because both are networks of computers working together.” This, experts agree, is not true, as supercomputing is just one component of the Internet and in fact RELIES ON the rudiments of what we know as the internet to work. So, if anything, the internet concept is the father of supercomputing, not vice versa. But even if we accept Emeagwali’s wrong logic, the fact that he did not invent or pioneer supercomputing means that even on this flawed premise and logic he cannot be considered a father of the internet.Authentic histories of the internet are accessible all over the web. One can be found here:The only “history of the internet” source to even recognize Emeagwali as a legitimate computer scientist to be mentioned when chronicling the history of the internet is the book History of the Internet: A Chronology, 1843 to the Present by Christos J. P. Moschovitis, Hilary Poole, Tami Schuyler, Theresa M. Senft. The book was published in 2001. Although Mr. Emeagwali proudly displays the book’s reference to him on his website and claims that the “father of the internet” moniker (which has since been lazily picked up by several media platforms) originated in the book, there is absolutely no such reference in the book. The book’s reference to Emeagwali only states how Emeagwali’s research “effectively stimulate[d] petroleum reserves” by “harnessing the power of parallel computing.” And it is clear from a cursory analysis of the linguistic properties of this specific reference to Emeagwali that Emeagwali himself supplied the material and the claims articulated in it. It is also clear from the reference that it has nothing to do with the internet but is about improving the modeling of oilfields or oil reservoirs. The content and prose are eerily identical to the autobiographical write-ups and claims onThe Nobel Prize Of Computing?Emeagwali’s other claim is that of winning the “Nobel Prize of Computing.” He is, of course, referring to the Gordon Bell Prize, which he won in 1989. Many uninformed observers have since picked up this fraudulent reference, which emanated from• The cash award for the prize is a mere $1000. Often, the amount of an award is a good guide to its prestige and significance in the field.For all these reasons, it is the height of self-promotion and delusional exaggeration for Mr. Emeagwali to claim that he won the Nobel Prize of Computing or that the Gordon Bell is regarded as the Nobel of Computing. Nobody except Mr. Emeagwali regards the prize as such.It is noteworthy that both Emeagwali and the Mobile/TMC Team relied on the CM-2 Machine (the Connection Machine) for their calculations, the same machine that Emeagwali falsely claims to have invented!A final point to note here is that the research for which he won the Gordon Bell Prize (by default) has application and relevance only in the narrow area of oil flow reservoir modeling and oil prospecting. His entry for the competition utilized and optimized the capacity of parallel computing, that is, relied on an already existing Internet. Emeagwali’s own website states that he “accessed the supercomputers over the Internet from local workstations.” Neither the research nor the prize had anything to do with the Internet. The Internet was already invented and fairly perfected by then; otherwise he would not be, in his own words, “accessing the supercomputers over the Internet.” This clarification is necessary and important because some of Emeagwali’s supporters and victims tend to assume wrongly that his purported fatherhood of the internet derives from the research for which he won the Gordon Bell Prize. All these facts can be easily accessed here:“Dr.” Emeagwali Or Doctored Emeagwali?Emeagwali’s final fraudulent claim is that of being a “doctor” and “professor.” Several years ago, before eagle-eyed Nigerians and Africans decided to scrutinize his eye-popping claims, his website audaciously referred to him as “doctor” and “Professor.” Because of recent exposures of his scam, he no longer refers to himself on his website as “Dr. Emeagwali” or “Professor Emeagwali.” However, in what is typical of the Emeagwali scam, his website is still littered with many media references to “Dr Emeagwali” and “Professor Emeagwali.” These stealthily promoted references then get picked up by unsuspecting black media people who are eager to promote black achievement and excellence. Sometimes, he approaches black websites and organizations, asking them to link to or publish his false claims. In the course of the discussion on the USAfricadialogue forum, Ms. Funmi Okelola , the owner and webmaster ofBut many proprietors of black websites and publications have not been as alert to Emeagwali’s antics as Ms. Okelola and have been falling for his scam. In their eagerness to embrace what they believe to be the proud achievements of a “brother,” they have inadvertently donated space and platform to Emeagwali to consolidate and spread his false claims. Because of the virility of the internet, even some non-black websites have picked up these ubiquitous references that are patently false. Here, on this websiteHe will not correct what is clearly a false reference, preferring to take cover in the deniability of being able to say that it is others, not him, who use these false, unearned titles to refer to him. The reason he will not correct this falsehood is that it emanated from him in the first place; most of the references were picked up from his website in the days before scrutiny spooked him into avoiding such direct self-referencing. The clearest evidence yet of his complicity and culpability in this misrepresentation is that he sits through interviews where the clueless, awed interviewers refer to him as “Doctor Emeagwali” and “Professor Emeagwali” and he does not correct them. There is a particularly revolting video on youtube <The false references to him as “doctor” and “professor” are not the only falsehoods that Emeagwali coyly and deftly promotes; he routinely lets interlocutors repeat the many false claims that are based on his own prolific misrepresentations. On this websiteHere is Mr. Emeagwali’s response to her question: “Inventors are reluctant to provide expanded details of their inventions until they receive full patent protection. The reason is that the Patent and Trademark Office can deny patents to inventors that publicly provide details of their invention.” But the truth is that he has neither registered patents for his non-existent inventions nor a patent-pending status. He has no inventions or technologies to patent! The response itself contains a lie. Contrary to Emeagwali’s insinuation that inventors cannot publicly discuss their work until they are patented or that doing so would jeopardize their patent application with the US Patent and Trademark Office (USPTO), inventions and products with “Patent Pending” status are routinely discussed, advertized, and marketed on American television. In fact these public discussions of unpatented inventions always carry the disclaimer that patents are pending, meaning that applications have been made. If public discussion of inventions and technologies were detrimental to patent applications, none of these unpatented and “patent pending” technologies and inventions would be on the American market or be advertized on television. This was Emeagwali seeking to perpetuate the myth that he has several technological inventions that are patented or awaiting patents but avoiding having to mention or discussion the specific fictitious inventions for which he claims to have patents in order to have deniability when checks are made at the USPTO and he is confronted with the truth of his falsehood.Racism Or Laziness?The case of Philip Emeagwali is a cautionary tale on the pitfalls of self-delusion, laziness, and a sense of entitlement. Mr. Emeagwali enrolled in a doctoral program in Civil Engineering at the University of Michigan in 1987. His coursework over, he took the comprehensive examination that qualifies one for candidacy. He failed the exam twice and did not take it a third time. In the meantime, he conducted the research that would later win him the Gordon Bell Prize, a research he began as a class project for one of his graduate courses. In 1991, two years after winning the Gordon Bell by default, he petitioned the Dean of the School of Engineering to be allowed to submit a dissertation (despite not having passed his candidacy exam and therefore not being a doctoral candidate) in a different department — the Department of Electrical and Electronic Engineering. His request was curiously granted in what was clearly a sidestepping of standard procedure. Emeagwali submitted the dissertation, basically a rework of his entry for the Gordon Bell competition, on July 24, 1992. A team of internal and external evaluators examined it and found it unworthy of a doctorate and turned it down.Emeagwali then sued the University of Michigan for racial discrimination. The lawsuit was dismissed for lacking merit and also failed on appeal in 1999. The details of Emeagwali’s graduate school records and of the dueling contentions in the lawsuit are all documented hereA dispassionate analysis of the details, affidavits, and arguments submitted in the lawsuit and in the appeals process reveals the following:• Emeagwali was a fairly brilliant student but he was lazy and would not put in the work necessary to earn his degree.• He had a sense of entitlement, feeling that since he was black and had made it into the University of Michigan, he was entitled to a special treatment and academic favors.• This sense of entitlement escalated after he won the Gordon Bell Prize. He thought that he was entitled to a PhD on the strength of the Gordon Bell competition entry when in fact he was not even a doctoral candidate, having failed his comprehensive examination twice.• Emeagwali was more concerned with parlaying his newfound default Gordon Bell fame into profitable self-promotion than with the serious academic effort required to complete the PhD.• He petitioned to be allowed to submit a dissertation only after he realized that he would not be taken seriously as a researcher and may not be able to find a secure job in research or teaching if he did not possess a PhD.This is a story of how a promising, modestly brilliant graduate student was destroyed by his own hubris, entitlement mentality, and laziness. What Emeagwali failed to earn through hard work and diligence, he has since appropriated to himself by calling himself and getting others to call him “doctor” and “professor.”Emeagwali is not a doctor of whatever kind. He is not a professor. He has not held any research or teaching job in any educational or research institution since he failed to get a doctorate degree at Michigan. He has also not done any new research. Emeagwali has no single publication in any scientific journal. A search of the most comprehensive scientific publication database (which can be done online) yields only a reference to his Masters Degree dissertation.Here we have a man who is unemployed, has no serious standing in the scientific, engineering, or computing communities. Yet he is widely referred to as “a father of the internet,” “an internet pioneer,” “the greatest black scientist that ever lived,” “Bill Gates of Africa,” among other over-the-top and unearned titles. The question to pose is: how did the world get so deceived and why did many reputable people and organizations buy into Emeagwali’s con job? Emeagwali is a very industrious, persistent, and successful scam artist; you have to give him that. Very few intellectual frauds have successfully mainstreamed their false claims as Emeagwali has done.Intellectual Fraud And Its Unwitting ValidatorsEven former president Bill Clinton was suckered by the fraud, famously referring to Emeagwali as “one of the great minds of the information age” in his speech to the Nigerian National Assembly in 2000. The Clinton reference has provided cover and alibi for Emeagwali to perfect and spread his false claims. Predictably, Emeagwali’s defenders point to the Clinton reference and to CNN’s and TIME Magazine’s references to him as “a father of the internet” and “the unsung hero” of the internet age respectively. These references are boldly displayed onObviously Clinton was pandering to his Nigerian hosts who believed Emeagwali to be a scientific genius and national hero. Clinton, the savvy politician that he is, and a man who perfected white liberal outreach and pandering to black/African peoples, was relying on the image and descriptions of Emeagwali that was already in the black and mainstream press —descriptions that are traceable to Emeagwali's own misrepresentations on his websites. What Emeagwali does is so clever as to ensnare even a skeptical and vigilant observer, especially one that is already inclined to believe or seek out claims of black scientific achievement for whatever reason. As indicated earlier, Emeagwali plants these autobiographical write-ups that are ridden with falsehoods and misrepresentations in unsuspecting black publications. He does this by aggressively pitching these claims to their editors as he tried to do unsuccessfully with Ms. Okelola. Then, fired by liberal guilt and a desire to seem welcoming to black achievement and excellence, the mainstream media like TIME and CNN, lazily pick up these references and descriptions. Emeagwali then links to, disseminates, and publicizes these mainstream press references and descriptions (which are actually based on his own descriptions of himself and his ""achievements”), thus perfecting and furthering the fraud. This way, he creates deniability for himself. The deception comes full circle but the cycle continues to repeat itself, populating and repopulating the Internet with Emeagwali’s falsehoods.The sophistication and complexity of the fraud notwithstanding, there is no excuse for reputable organizations like CNN and TIME not to have done a simple due diligence on the false claims of Emeagwali. It is true that at the time that TIME and CNN made the glowing references to Emeagwali, the now widely available refutations of his claims were probably not yet available on the internet. Even so, a basic inquiry from the appropriate quarters would have revealed the truth about the claims on which the references to Emeagwali were based. The two reputable organizations failed to carry out this basic fact checking, an elemental reportorial and investigative duty of journalists. Instead, they relied on Emeagwali’s widely disseminated falsehoods for their stories. The case of TIME is particularly scandalous. The story in which it extols Emeagwali is clearly directly based on Emeagwali’s own autobiographical claims onThere is similarly no excuse for President Clinton’s speech writers not to have done basic checks or asked some of the president’s own appointees and advisers who are engineers and scientists about the true value of Emeagwali’s work. Had they done this relatively simply investigation, they would have realized that being a default winner in one category of a minor supercomputing competition for work that has a specific, limited application in the narrow field of oil reservoir modeling does not qualify one to be called “one of the greatest minds of the information age.”A Self-Replicating FraudWhen challenged, Emeagwali and his supporters can say he is merely repeating and linking to what others call him and say about him and that he does not call himself a father of the internet or a doctor or a professor. But the fraud is a self-replicating one, perpetuating and proliferating itself across both the print and virtual media worlds. Other publications that are searching for black scientific achievers do an internet search and then rely on the previous press descriptions of Emeagwali, which ultimately lead back to the man’s fraudulent biographical claims on his own website. The lazy journalists and Pan-African activists lift these published claims and references (which emanated from the man himself), concluding that they must be established facts if other media outlets had already published them. And on and on it goes. It is a very sophisticated fraud that is aided by the virility of the Internet. This is precisely how even the prestigious Law School Admission Test (LSAT) ended up including a passage about Emeagwali in their test, a passage that is exactly the same as what Emeagwali published onWhite liberal patronage of black people can be that shallow and sloppy — and insulting to the very people it purports to promote. It is political correctness and pandering marinated in a political agenda--that of ingratiating white liberal politicians and figures to blacks for political support and multicultural validation. The peak of this phenomenon is Black History Month in February when white liberal organizations and black institutions alike pull out all the stops to have self-promoting “black achievers” like Emeagwali speak to them. That's when they get invited by white liberal and African American organizations to showcase black achievement and innovation! White liberal patronage is a big industry in America. It takes many forms; one of them is what Pius Adesanmi calls the Mercy Industrial Complex (MIC). But the MIC is not as offensive as the false flattery and the silly excuses and defenses that white liberals advance for cuddling black failures and frauds. Hero-seeking black organizations have not helped matters with their patronage of people like Emeagwali. Emeagwali’s deception succeeds so well because of a multiracial coalition of consumers and enablers.Emeagwali is a very clever, self-conscious scam artist. That is however no excuse for the black community to allow itself to be used to actively promote a fraud.",5 followers,Matthew Lai,3K,3.9K,13.2M,https://www.quora.com/If-humans-manage-not-to-destroy-themselves-what-do-you-think-might-be-achievable-technologically-in-space-travel-500-years-from-now
11,"I'm not a historian of any kind, but I've had a computer for 37 years since I was 13 and my entire career has been in electronics, computers, networking and the internet. I know a fair amount about the pioneers and innovators in this space, I know a lot of obscure stuff.I've never heard of this person.That's not to say that he existed and independently invented these things, but it seems","17,351 followers",Vint Cerf,1.6K,71,1.2M,https://www.quora.com/If-humans-manage-not-to-destroy-themselves-what-do-you-think-might-be-achievable-technologically-in-space-travel-500-years-from-now
12,"Who?Name isn’t Cray.Name isn’t Tayor, nor Kahn nor Cerf. Nor Kleinrock. Nor Clark. Nor Neumann. Nor Engelbart.Nor Culler. Nor Sutherland. Nor any of the other names of people that I know and have worked with.And what program?","6,489 followers",Victor Xing,624,1.1K,6.1M,https://www.quora.com/If-humans-manage-not-to-destroy-themselves-what-do-you-think-might-be-achievable-technologically-in-space-travel-500-years-from-now
13,"They get re-sold on the second hand market. My computer, for example, is a 20 core workstation converted from a compute node from a cluster that was dumped on eBay by a big internet company (rumours say it’s Facebook - I have no idea if it’s true or not) decommissioning a bunch of their compute nodes at the same time, and pushing the price of those parts down to peanuts - for example, we had 8 core SandyBridge EP Xeons going for $30 to $50, back when it was only 2–3 generations old, and Intel hasn’t been doing much between the past few generations.When I got it, it came in a 1U rack-mounted caThey get re-sold on the second hand market. My computer, for example, is a 20 core workstation converted from a compute node from a cluster that was dumped on eBay by a big internet company (rumours say it’s Facebook - I have no idea if it’s true or not) decommissioning a bunch of their compute nodes at the same time, and pushing the price of those parts down to peanuts - for example, we had 8 core SandyBridge EP Xeons going for $30 to $50, back when it was only 2–3 generations old, and Intel hasn’t been doing much between the past few generations.When I got it, it came in a 1U rack-mounted case, and sounded like a jet engine.This is what the inside looked like. The black things on the right are fans. They are tiny, and they spin really really fast to cool the 2 Xeons CPUs with tiny heatsinks. The only reason those CPUs don’t overheat despite such tiny heatsinks is there is an incredible amount of airflow over them. That’s why the fans sound like jet engines.The SuperMicro motherboard uses a proprietary form factor, and only has a VGA output. PCI-E slots are available if you plug in an expansion card (so the video card would be mounted on an expansion card that’s plugged into the motherboard). That’s how they can fit something like a video card in a case that thin. The video card would be parallel to the motherboard, not perpendicular like they usually are on consumer motherboards.So I had to design and use a laser cutter to build my own case out of acrylic, after finding new (and bigger) heatsinks for the CPUs.This is what I ended up with:Why go through all this trouble? I got a dual Xeon (2x8 cores then, 2x10 cores now) workstation with 64GB of memory for A LOT cheaper than what they normally cost. Was it worth it? That depends on how much I value my time spent getting it to work… but that’s part of the fun.I am typing this answer on that computer (though it has been upgraded to water cooling).More details.","45,949 followers",Steven Haddock,1K,25.5K,342M,https://www.quora.com/If-humans-manage-not-to-destroy-themselves-what-do-you-think-might-be-achievable-technologically-in-space-travel-500-years-from-now
14,"Computer Science is NOT Web dev.Computer Science is NOT Web dev.Computer Science is NOT Web dev.Computer science is AI, Game programming, app programming, microcontroller programming, and on and on and on.Repeat after meComputer Science is NOT Web dev.","35,359 followers",Jeff Erickson,3K,2.3K,53M,https://www.quora.com/If-humans-manage-not-to-destroy-themselves-what-do-you-think-might-be-achievable-technologically-in-space-travel-500-years-from-now
15,"You won't learn how to code with a computer science bachelor's degree, and you shouldn't expect to. A bachelor's degree in software engineering might.The degree will introduce you to a variety of languages and technologies. You'll leave this programme with a basic knowledge of how processors function, the importance of a network protocol stack, how to build a compiler, etc.If you want to work as a programmer, you should always be programming. It doesn't matter where you start; that's unimportant. I studied C so that I could create fractals on my computer screen, and I later studied assembly soYou won't learn how to code with a computer science bachelor's degree, and you shouldn't expect to. A bachelor's degree in software engineering might.The degree will introduce you to a variety of languages and technologies. You'll leave this programme with a basic knowledge of how processors function, the importance of a network protocol stack, how to build a compiler, etc.If you want to work as a programmer, you should always be programming. It doesn't matter where you start; that's unimportant. I studied C so that I could create fractals on my computer screen, and I later studied assembly so that I could do so more quickly. You can complete it without enrolling in any classes. My education had a strong emphasis on this, and that was what first introduced me to programming. You learned to programme through these classes. In fact, before I started these seminars, I hardly ever even programmed. I had made an effort to get a head start, but all I had managed to do was loop.Data Structures: I enjoyed this course. really good The primary building component of the vast majority of computer programmes is a data structure. They give us the ability to store data in a form that our programmes can use it. The ability to analyse data and choose the best storage method for performance across time and space is quite important.I became familiar with the several kinds of these data structures, such as arrays, stacks, linked lists, doubly linked lists, trees, graphs, heaps, and more (like self-balancing trees). Thanks to this class, which also helped me understand data structures, I was able to organise the data I was preserving more effectively.Analyzing algorithms: This course was just marginally useful. Algorithms are sometimes useful. The best software in the world is made possible by them, and they are strong enough to be useful in the modern era. However, there were some issues in the class.The most useful part of this was learning Big O notation. You can evaluate a piece of code's performance in terms of time and space using Big O notation. When assessing the performance of a piece of code within a business, timers are typically employed to keep track of the intervals between encounters. You can evaluate the efficiency of your software using this. To do this, you must write code, then evaluate the results.I am aware that starting a project from scratch is difficult, but you will be required to do so, so you should begin developing that skill right away. If you are unable to think of any projects you would like to work on, spend at least a couple hours each day learning competitive programming. You will still be coding, which is something you should be doing more of right now, even though it won't teach you many of the skills you'll need for daily programming.",546 followers,Scott Gartner,1.6K,730,4.2M,https://www.quora.com/If-humans-manage-not-to-destroy-themselves-what-do-you-think-might-be-achievable-technologically-in-space-travel-500-years-from-now
16,"Yeah, the whole obsession with web and mobile apps presents a very skewed view of what you can actually do with a good CS degree. There's so much press on consumer apps it's easy to forget there are other rich areas of computer science that just don't have good PR! The whole Silicon Valley hype machine doesn't help.I loved doing real-time embedded systems. Very hard problems to solve, a real senseYeah, the whole obsession with web and mobile apps presents a very skewed view of what you can actually do with a good CS degree. There's so much press on consumer apps it's easy to forget there are other rich areas of computer science that just don't have good PR! The whole Silicon Valley hype machine doesn't help.I loved doing real-time embedded systems. Very hard problems to solve, a real sense of accomplishment when you could get the system to behave correctly and respond in real time, resource restrictions that forced you to be resourceful to overcome them, and some cool projects: I worked on systems that went into the Boeing 777 and International Space Station. Not a UI in sight :-)About a year ago, at a conference, I met a very interesting guy from the East Bay working on Super Computers for the government. He blew my mind with some of the hardware and software they were working on f...",139 followers,Lizzie Siegle,1.3K,177,747.6K,https://www.quora.com/If-humans-manage-not-to-destroy-themselves-what-do-you-think-might-be-achievable-technologically-in-space-travel-500-years-from-now
17,"Computers and software are everwhere. The web is just the tip of the iceberg. Avionics and flight control systems. Banking and finance. Shop floor. Animation and special effects.  The list goes on and on.  If all you're seeing is web stuff, you need to reevaluate your school and what it's exposing you to.",18 followers,Asim Qureshi,2.6K,996,155.4M,https://www.quora.com/If-humans-manage-not-to-destroy-themselves-what-do-you-think-might-be-achievable-technologically-in-space-travel-500-years-from-now
18,"There are lots of good answers already, but I was asked to answer so I'll add my $0.02. I started doing web programming when the web was invented (yes, you read that correctly.) It was new territory, it was exciting, and it was rewarding. However, it soon became routine and quite dull for me, and I haven't done it in years. It turns out that I love distributed systems and all kinds of server-side work that does not involve SQL or simply servicing the front-end of a web application. Sure, there might be somewhat fewer jobs in these areas and you may have to ""pay your dues"" as a full-stack web dThere are lots of good answers already, but I was asked to answer so I'll add my $0.02. I started doing web programming when the web was invented (yes, you read that correctly.) It was new territory, it was exciting, and it was rewarding. However, it soon became routine and quite dull for me, and I haven't done it in years. It turns out that I love distributed systems and all kinds of server-side work that does not involve SQL or simply servicing the front-end of a web application. Sure, there might be somewhat fewer jobs in these areas and you may have to ""pay your dues"" as a full-stack web developer for a while, but there are plenty of other areas and opportunities to explore--both in industry and in research. Don't give up just because the jobs you've seen so far are not inspiring. You'll find something that excites you eventually.",26 followers,Thomas Cormen,2.3K,826,35.4M,https://www.quora.com/If-humans-manage-not-to-destroy-themselves-what-do-you-think-might-be-achievable-technologically-in-space-travel-500-years-from-now
19,"They graduate with a CS degree. Then they either go on to good, well-paying jobs and have fulfilling careers, or they don’t, just like the CS majors at the top half of the curve.Some of us even become computer science professors! (I was solidly in the bottom quartile of my undergraduate class.)","22,266 followers",Gene Spafford,1.2K,3.6K,16.9M,https://www.quora.com/If-humans-manage-not-to-destroy-themselves-what-do-you-think-might-be-achievable-technologically-in-space-travel-500-years-from-now
20,"There is a terrific demand for embedded programmers. They never touch SQL, and the UI, if any, involves sensors and indicators, not screens. We are much less visible than the Web developers, because our main desire is not to be seen: the devices we build should ""just work"". But at the moment, demand is high and the prospects open ended.",215 followers,Mike West,646,10.2K,55M,https://www.quora.com/If-humans-manage-not-to-destroy-themselves-what-do-you-think-might-be-achievable-technologically-in-space-travel-500-years-from-now
21,"Yes, there are positions for practical Networking and Embedded Systems development. These are less theoretical and start off being completely development oriented, but you can work your way up to architectural work.",130 followers,Philip Nguyen,704,0,0,https://www.quora.com/If-humans-manage-not-to-destroy-themselves-what-do-you-think-might-be-achievable-technologically-in-space-travel-500-years-from-now
22,"Supercomputers are, by definition, the fastest research computers of their time.These days that doesn’t mean faster CPUs, it means lots of them. Tens of thousands of CPUs. They will often be a little bit slower than desktop cores, but will have more cache.It also means a lot of memory per CPU… at least 10 GB per core, but often a lot more than that. Fast memory, but again probably not quite as fast as a late model desktop.Current supercomputers also usually have a ‘GPU’ or four per CPU, although it probably will not have monitor connectors (since they cost money and aren’t going to be used). TSupercomputers are, by definition, the fastest research computers of their time.These days that doesn’t mean faster CPUs, it means lots of them. Tens of thousands of CPUs. They will often be a little bit slower than desktop cores, but will have more cache.It also means a lot of memory per CPU… at least 10 GB per core, but often a lot more than that. Fast memory, but again probably not quite as fast as a late model desktop.Current supercomputers also usually have a ‘GPU’ or four per CPU, although it probably will not have monitor connectors (since they cost money and aren’t going to be used). That may well be exactly the same as a desktop GPU, although probably one of the CAD versions rather than a gaming GPU.Some people argue that Google’s clusters are not supercomputers for a variety of technical reasons. They’re wrong, because at least some of them are used for research in the way a supercomputer should be. Mostly AI research.But in any case, Google’s data centres don’t look that much different from any other system in this class, so I’ll use them for examples.A row of computers in a Google data centre looks like this:Each shelf in the racks on the right is a computer, with some hard drives. The black, blue, yellow and green cables are the network cables, orange is power. The black units on top of the rack are part of the network.These are a set of a different model of computer, with the specialised TPUs that are used for AI work:That rack on its own probably qualifies as a supercomputer, without the other dozen racks of normal machines that hold the data.The whole room is like this:Bear in mind, that’s just a third of it in the picture… but you could call this one supercomputer.The cooling system looks like this:Yep, water cooled.The really different thing, that there is no equivalent to in any desktop class system, is the network.These days, a supercomputer node (usually two CPUs, each with 36 or more cores) that has less than 200 Gbps (yes, gigabits per second) is considered a bit slow. That will be two 100 Gbps network interfaces, and for comparison they use the same 16x PCIe slots as a GPU.The network switches in between are configured to allow pretty much full bandwidth from any node to any other node, so that you can do things like borrow memory from the other side of the cluster if required.Often the network is not Ethernet (your familiar wired connection on your internet modem at home) but something else, and usually a lot of the cabling is fibre optic rather than copper.The networking fabric might look like this:That’s just one superblock… there can be hundreds of those in a cluster. Each of those green cables is 100 Gbps, so you’re looking at 2.5 terabits per second.Finally… what kind of case do you put a computer like that in?That’s nine supercomputers on the right, three in each building (two different models of building there), and the blue things are the power supply. You can see the lakes for the cooling system reservoirs, and the edge of the wind farm that supplies a lot of the power.",9 followers,Andrew McGregor,516,12.9K,66.9M,https://www.quora.com/If-humans-manage-not-to-destroy-themselves-what-do-you-think-might-be-achievable-technologically-in-space-travel-500-years-from-now
23,"These people are harvesting tea:We don't know whether any of them are exceptionally good at it, but it doesn't really matter. Even if you won the Nobel prize in tea plucking, this crew would finish a field roughly ten times faster than you, simply because they outnumber you ten to oneThis guy is climbing a mountain:He has trained individually for that; no random team of ten people can reach the summit before him, because they can’t sensibly divide the task into ten simultaneous parts.GPUs are more powerful than CPUs because GPUs feature a much greater number of relatively unexceptional procesThese people are harvesting tea:We don't know whether any of them are exceptionally good at it, but it doesn't really matter. Even if you won the Nobel prize in tea plucking, this crew would finish a field roughly ten times faster than you, simply because they outnumber you ten to oneThis guy is climbing a mountain:He has trained individually for that; no random team of ten people can reach the summit before him, because they can’t sensibly divide the task into ten simultaneous parts.GPUs are more powerful than CPUs because GPUs feature a much greater number of relatively unexceptional processing cores. We don't use them for everything because they require the type of collective work where an overwhelming strength in numbers translates into performance improvements.","4,793 followers",Jan Christian Meyer,15.5K,2.2K,6.7M,https://www.quora.com/If-humans-manage-not-to-destroy-themselves-what-do-you-think-might-be-achievable-technologically-in-space-travel-500-years-from-now
24,"GPUs were initially built specifically for 3D graphics processing in things like video games.3D graphics rendering is actually quite computationally intensive: you need to generate the vertices for each polygon and then create the primitives associated with them. Once that’s done you need to rasterize the image , apply textures and lighting, and then push the image for the screen. This needs to be done for every polygon in the scene, and there areBecause of the fairly straightforward task and the sheer amount of grunt needed, it thus makes sense to build dedicated hardware to hanGPUs were initially built specifically for 3D graphics processing in things like video games.3D graphics rendering is actually quite computationally intensive: you need to generate the vertices for each polygon and then create the primitives associated with them. Once that’s done you need to rasterize the image , apply textures and lighting, and then push the image for the screen. This needs to be done for every polygon in the scene, and there areBecause of the fairly straightforward task and the sheer amount of grunt needed, it thus makes sense to build dedicated hardware to handle these graphical workloads. Hence the appearance of 3D graphics accelerators for the mainstream PC market during the 90s.Here’s the thing though: CPUs are pretty ill-equipped to deal with these types of tasks. They’ve been optimized for absolute single threaded performance, not absolute throughput. 3D graphics rendering consists of many simple operation performed in parallel. All the hardware optimizations found in CPUs (OOOE, caching , speculative execution)? Yeah, those are effectively useless here because the code is already embarrassing parallel here and doesn’t exhibit much locality.The closest analogy i can think of right now relates to transportation: a jet can carry up to 100 tons of cargo and land basically anywhere on earth within a day. That’s great if you’re dealing with similar quantities of material. However, if you have 100 THOUSAND tons of cargo to ship, well you’re better off using a container ship. While the time to transport said 100 tons will be quicker by plane, the sheer capacity of a boat will allow you to ship more cargo faster in the end when dealing with massive loads.Getting back to processors for a minute:So people designed these pieces of hardware for accelerating 3D graphics processing. Great. What about general workloads?GPGPU (General purpose compute with a GPU) is a relatively new concept. In the mid 2000s, people realized they could try and use the vertex and shading units present in GPUs to perform parallel computations on the graphics processor itself. There was no OpenCL or CUDA back then, and graphics processors were still relatively fixed functions, so people actually resorted toAnyway, manufacturers eventually caught on, which brought us into the era of programmable, GPGPU-capable processors we have today.These modern GPUs are optimized for throughput, as were their predecessors. If you have a very parallel workload, then this throughput can absolutely help performance, which is why they are frequently used in compute-intensive workloads such as (and not limited to): rendering, machine learning, video encoding and decoding, crypto-currency mining and scientific computation (SETI, BOINC). These tasks all work well with graphics processors because they are compute intensive and are highly parralel.Conclusion/tl,dr:",260 followers,Andrew Lane,739,1.2K,7.3M,https://www.quora.com/If-humans-manage-not-to-destroy-themselves-what-do-you-think-might-be-achievable-technologically-in-space-travel-500-years-from-now
25,"Well, I could point you to this screen, that is used for visualization of the output from some of our clusters:but really, the truth is more prosaic asWell, I could point you to this screen, that is used for visualization of the output from some of our clusters:but really, the truth is more prosaic as","21,424 followers",Vladislav Zorov,1.4K,16.1K,63.7M,https://www.quora.com/If-humans-manage-not-to-destroy-themselves-what-do-you-think-might-be-achievable-technologically-in-space-travel-500-years-from-now
26,"As others have pointed out, this isn't universally true. However, it is a general trend. For instance, Intel's Skylake devotes a much larger part of its area to the GPU:The reason has to do with the type of workload each type of processor is targeted at.The reason has to do with the type of workload each type of processor is targeted at.CPU's:As others have pointed out, this isn't universally true. However, it is a general trend. For instance, Intel's Skylake devotes a much larger part of its area to the GPU:The reason has to do with the type of workload each type of processor is targeted at.The reason has to do with the type of workload each type of processor is targeted at.CPU's:Unless specifically told otherwise, all code is executed on the CPU. This means the CPU needs to be a jack-of-all-trades type of processor. It needs to handle OS routines like allocating memory, copying memory, doing arithmetic, sending commands to IO devices and handling user input equally well. It also needs to be reasonably fast at application routines that don't easily fit in the execution scheme of any of the other processors -- e.g. a custom photo filter, video codec or text processing algorithm.For something like graphics workloads or very-very-parallelBut often -- and this is becoming more and more true in mobile devices -- programs (that won't be offloaded to a dedicated processor) contain little in the way of gigantic blocks of computation and more and more code-heavy, run-a-lot-of-different-instructions type of workloads. E.g. a mobile browser.To speed up such workloads, the CPU has to be able to do many different types of instruction routines well, including, but not limited to:Improvements to gain some or all of the above would require things like:2. Processing more instructions in parallel. But that also has limitations as most software routines are written with sequential routines -- e.g. each instruction follows another and some depends on the result of an instruction before it.3. Pipelining (A general trend is that for the types of workloads that CPU's work on, there isn't any one thing that can be scaled indefinitely in order to increase performance. Increasing any single feature will bring about performance increases up to a certain point. This naturally limits the number of transistors a CPU can use; at some point, throwing more cache or instruction pipelines really won't produce a worthwhile gain.* In modern CPU's, they aren't exactly processed one after another. Multiple instructions can be processed in parallel when possible.GPU'sGPU's work on a very specific -- though it's getting broader -- set of tasks. Specifically, because of their history as graphics processors, they work on tasks that have a lot (an almost infinite amount) of computation that aren't inter-dependent. If you throw the same workload at 2 GPU's (and 2 memory controllers and 2 sets of DRAM), they can perfectly divide the workload between themselves and the result would be a speed increase of 2x.This ability to scale seemingly to infinity means that the easiest and most straight-forward way to increase GPU performance is to increase the number of processing pipelines. There's basically no logical bound on this -- though there is obviously a physical bound in terms of number of transistors available, memory bandwidth and power consumption. This also means that if a chip manufacturer wants to increase their GPU's speed, they'll figure out a way to either provide more power to the chip or to design more power and area efficient processing elements, and then throw more onto the chip in parallel.Moreover, compute tasks on a typical modern device often requires a disproportionate amount of compute power for graphical purposes rather than code-heavy routines. Think of a 4k display and the number of pixels that need to be pushed for every pretty effect at a perfect 60fps without any dropped frames. Or games with any number of graphical effects that will need to be processed in parallel.These two factors -- unbounded ability to scale processing power with area and high demand for processing power -- naturally leads to GPU's being very very large.",5 followers,Wasim Momin,1.1K,0,0,https://www.quora.com/If-humans-manage-not-to-destroy-themselves-what-do-you-think-might-be-achievable-technologically-in-space-travel-500-years-from-now
27,"Nope.Zoom alone is not enough to resolve Saturn’s rings.You need absolute aperture of around 70mm minimum, to resolve Saturn’s rings. That means the outer entrance of your lens needs to be 70mm in diameter, and you should fully utilize the whole 70mm to have a chance to see Saturn’s rings.My 50mm binoculars just barely see Saturn elongated, but there is no details in the rings at all.The S20 telephoto camera has an absolute aperture of around 5.8mm.","41,026 followers",Dave Haynie,708,6.9K,95.4M,https://www.quora.com/If-humans-manage-not-to-destroy-themselves-what-do-you-think-might-be-achievable-technologically-in-space-travel-500-years-from-now
28,"I don’t have a 100x smartphone camera, or the planet Saturn visible right now, but hey, let’s find out!!This was shot by Grant Petersen with a Samsung Galaxy S8. You could do the same with the Galaxy S20 Ultra, though I wouldn’t recommend using anything but the standard wide-angle 1x mode. Petersen also used an 8″ Dobsonian telescope! The phone, of course, is typically mounted over the telescope’s eyepiece. It works… I do a similar thing all the time with cameras or phones and my office microscope. Though I get a better result with my home microscope trinocular microscope, which has a third poI don’t have a 100x smartphone camera, or the planet Saturn visible right now, but hey, let’s find out!!This was shot by Grant Petersen with a Samsung Galaxy S8. You could do the same with the Galaxy S20 Ultra, though I wouldn’t recommend using anything but the standard wide-angle 1x mode. Petersen also used an 8″ Dobsonian telescope! The phone, of course, is typically mounted over the telescope’s eyepiece. It works… I do a similar thing all the time with cameras or phones and my office microscope. Though I get a better result with my home microscope trinocular microscope, which has a third port allowing direct mounting of a real mirrorless or DSLR camera.But of course, you’re talking about seeing Saturn’s rings directly with the phone camera’s 100x “zoom”. And at least at the time of this writing, since you said “100x”, you mean the Samsung Galaxy S20 Ultra, which is the only phone currently claiming a 100x “zoom”. Well, actually they claim a 100x “Space Zoom”, which, not already being defined as something (in the way, say, a zoom lens has an actual definition), I guess I can just write Space Zoom, without the quotes.None of these new phones have actual zoom lenses. You see pundits and even phone makers using the term “zoom” all the time. However, I did take an e-trip to Samsung’s web site, and while they do like to say “Space Zoom”, they actually do not mislabel any of their cameras as zoom lens cameras. Good on Samsung!The Samsung Galaxy S20 Ultra has a 12 megapixel 1/2.55″ camera ultra wide angle camera, a 108 megapixel 1/1.33″ “normal” wide angle camera, and 48 megapixel, 1/2″ telephoto camera. In 35mm equivalents, these correspond to a 13mm f/2.2 lens, a 26mm f/1.6 lens, and a 105mm “periscope” f/3.5 lens. In absolute or telescope terms (however you’d like to call it), the magnification is not rated relative a wide-angle lens, but your eye. So the eye-relative effect here is 2x magnification.The normal operation of this phone is to shoot at 12 megapixels in all modes and use cropping of the image to deliver a software-driven “zoom”. So call the 108 megapixel camera 1x. It does a 2x or 3x crop, to deliver a 27 megapixel or 12 megapixel image. It will not be as good as the 1x image, but it’s using actual pixels, technically not “digital zoom”, which is upscaling and cropping. Going to 4x, they switch over to the telephoto camera. Crop that 2x and you still get a real 12 megapixel image at an overall 8x magnification. With a little digital zoom mixed in they get it to 10x still looking pretty good, with about 7.5 megapixels of actual information in the 12 megapixel result. But keep in mind that 10x is referenced against the wide angle lens, so against your eye, it’s about 5x.Then we get to “zoom” settings above 10x. These are computationally derived from taking multiple photos and applying a software technique called DRIZZLE. This was used by the Google Pixel 3 to deliver a “zoom” function with its single camera. But the technique was actually invented for the Hubble Space Telescope. The code-name for what they call Space Zoom today, when in development, was “Hubble.” Imagine that. Samsung doesn’t detail the specifics of their technique, but Google wrote about the Pixel version in their AI blog.Google managed to get a decent 1.5x-2.0x magnification. So that puts the actual telescopic, eye-relative “not bad” limit of the S20 Ultra probably around 20–25x (eye-relative 10x-12.5x). Yes, it’ll go to 100x (eye-relative 50x), but it’ll look pretty nasty. More like a sketch than a photograph.To see Saturn as anything but a point of light, you probably want at least a 30–50x magnification, at least with a real camera or telescope. To get much detail of the rings, 50x-100x or so. I can’t claim you won’t see anything at 30x, but I rather doubt it.And certainly not hand-held. I can get to 24x with my camera gear, maybe push it a bit with stacking images and drizzling, but I’d want the real telescope to shoot Saturn. This is an uncropped shot of the moon at an absolute 24x, taken with the Olympus OM-D E-M1 Mark II, Olympus M.Zuiko 300mm f/4.0 lens, and the MC20 teleconverter, to deliver a 600mm result (1200mm in full frame terms).This set of images was apparently released by Samsung…. I guess you just can’t get photographers who know moon photography anymore, eh? Not terrible when shown at 100 pixels high, eh, but too small to really tell. I did not find full sized images of much of anything at 100x, but I'm sure they'lll be posted by users and reviewers.GizGuide compared a couple of whole-moon shots, S20 Ultra vs. the Huawei P30 Pro, with cropping on the Huawei. Cropping for the full disc makes this a fair comparison to my 600mm shot. If I crop about 3x, I get this … so an effective 1800mm or 72x.My cropped version isn’t quite as tight. But it’s clearer than the phone shots. It had better be, given that gear cost about 3x the price of the S20 Ultra. It also doesn’t make phone calls.My shot was, incidently, hand-held. You can expect to be able to hand hold a moon shot, given the brightness of the moon, as long as you’re setting the exposure correctly and using a fast enough shutter speed. However, if I were trying to get a shot of Saturn, I’d want a tripod. For the camera, the phone, or my telescope. I rather suspect the GizGuide photo is made worse due to shaking, but given the newness of the S20 Ultra, I didn’t have alternative examples.And yes, you asked about Saturn, not the moon, I didn’t forget. That first photo, with the moon and Saturn, should give you an appreciation of the size of Saturn relative to a full moon in the night sky. Now consider the lack of detail in the phone images. My conclusion is that, with its real not-quite-5x absolute magnification and 50x of additional software that’s not quite as magical as you’d want, the Galaxy S20 Ultra will not be able to show you the rings of Saturn. At best a fuzzy blob.Unless your S20 Ultra is looking though a decent telescope! However……that doesn’t mean you can’t do some kinds of astrophotography with a phone. In fact, Google’s Pixel 4 has a customized adaptation of their Night Sight mode specifically for shooting night skies.Brent Hall, a photographer based in New Mexico (I’m absolutely jealous over the kind of dark sky he can see practically every night) has a good video demonstrating Milky Way shots on a Samsung S20 Ultra. He’s using “Pro” mode, which allows full control of the camera. And because Samsung’s “Pro” mode only runs on the main 108 megapixel camera, he’s adding on a Moment wide-angle lens. Add-on lenses are usually pretty sketchy, but Moment’s are definitely worth consideration. When I shoot the Milky Way, I’m probably using an 8mm fisheye or a 7mm wide angle on a Micro Four Thirds camera, as I want to take in the whole sky in most cases.Read MoreYou won’t believe this image of Saturn was taken on a Galaxy S8Smartphone Astrophotography: How I Capture the Moon and Planets with My PhoneAstronomy SourceHow is 100x 'Space Zoom' possible on the Galaxy S20 Ultra?The Galaxy S20 Ultra 5G’s 100x zoom is frustrating excessSee Better and Further with Super Res Zoom on the Pixel 3Can You Photograph the Milky Way With Just a Phone?",UNKNOWN,Robert Frost,3.3K,9.4K,195.4M,https://www.quora.com/If-humans-manage-not-to-destroy-themselves-what-do-you-think-might-be-achievable-technologically-in-space-travel-500-years-from-now
29,I took these pictures of the moon with my Note 20 Ultra without a Telescope. They probably would have come out better mounted on a tripod. Kind of hard with shaky hands when it is freezing outside.These were taken in the Philadelphia Suburbs. So not the darkest environment.I took these pictures of the moon with my Note 20 Ultra without a Telescope. They probably would have come out better mounted on a tripod. Kind of hard with shaky hands when it is freezing outside.These were taken in the Philadelphia Suburbs. So not the darkest environment.,5 followers,Bill Otto,7.6K,7.5K,43.2M,https://www.quora.com/If-humans-manage-not-to-destroy-themselves-what-do-you-think-might-be-achievable-technologically-in-space-travel-500-years-from-now
