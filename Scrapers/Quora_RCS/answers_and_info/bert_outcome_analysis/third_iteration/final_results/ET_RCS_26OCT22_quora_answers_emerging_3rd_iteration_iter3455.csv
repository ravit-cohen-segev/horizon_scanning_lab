,Answer,followerCount,name,upvoteCount,answerCount,answerViews,year,question
0,"Being a scientist is not being a member of a priesthood. It is possible for a Wiccan to be employed as a research scientist on a project where his or her Wiccan beliefs are not an interfering factor in the research. Science rests on the confirmability of the results, not the character or opinions of scientists.","5,084 followers",Bobby Tatro,3.9K,2.2K,45.6M,2020,https://www.quora.com/If-suitably-advanced-technology-appears-no-different-than-magic-how-do-we-know-that-witches-of-the-past-were-not-actually-scientists-conducting-experiments
1,"There were early attempts by alchemists, wisemen, what have you, using faulty understandings and assumptions about nature and the world who occasionally found something that was useful, like black powder, helium, and their strange effects, these were thought of as magic by the populace or churchy people. But it was not true science. Even Isaaac Newton was an alchenist. It didn't mean he was wrong about gravity, or calculus, nor did it mean alchemy was real or accurate. We know that those who used alchemy, or medical hypotheses based on four humors, or other such unproven ideas was not scientifThere were early attempts by alchemists, wisemen, what have you, using faulty understandings and assumptions about nature and the world who occasionally found something that was useful, like black powder, helium, and their strange effects, these were thought of as magic by the populace or churchy people. But it was not true science. Even Isaaac Newton was an alchenist. It didn't mean he was wrong about gravity, or calculus, nor did it mean alchemy was real or accurate. We know that those who used alchemy, or medical hypotheses based on four humors, or other such unproven ideas was not scientific, because they didn't use the scientific method.",326 followers,Anubhav Jain,2.4K,4.4K,123.7M,2021,https://www.quora.com/If-suitably-advanced-technology-appears-no-different-than-magic-how-do-we-know-that-witches-of-the-past-were-not-actually-scientists-conducting-experiments
2,"There is actually a term for magic and spells that work: it’s called “science”.After hundreds of years of carefully separating things that work from things that don’t, we are left with some surprising findings so far:There is actually a term for magic and spells that work: it’s called “science”.After hundreds of years of carefully separating things that work from things that don’t, we are left with some surprising findings so far:Weirdly, some people still wonder dimly why science “underestimates” magic. Science has","1,698 followers",Andrew Bromage,5.3K,1.1K,4.9M,2021,https://www.quora.com/If-suitably-advanced-technology-appears-no-different-than-magic-how-do-we-know-that-witches-of-the-past-were-not-actually-scientists-conducting-experiments
3,"BetVest is just not a company but a community for entertainment, respecting the Sharia law and providing the kind of entertainment which is needed by the public. And which boosts the positive energy among the players and viewers.","3,658 followers",Brent Oster,1.4K,357,1.8M,2021,https://www.quora.com/If-suitably-advanced-technology-appears-no-different-than-magic-how-do-we-know-that-witches-of-the-past-were-not-actually-scientists-conducting-experiments
4,"Again, it happened. They interrupted me last week and said, ""So you are creating the Skynet,"" as I was describing my work to someone. This meme that I thought represented my current situation pretty well, I felt I had to show them.1- Artificial General Intelligence & Pragmatic ThinkingIt doesn't have to be said that super-human AI doesn't happen. However, I think it's interesting to see people take on the concept of super smart computers worldwide. The myth of singularity has the name of this obsession.The uniqueness refers to the moment when an artificial intelligence will reach an exponentiaAgain, it happened. They interrupted me last week and said, ""So you are creating the Skynet,"" as I was describing my work to someone. This meme that I thought represented my current situation pretty well, I felt I had to show them.1- Artificial General Intelligence & Pragmatic ThinkingIt doesn't have to be said that super-human AI doesn't happen. However, I think it's interesting to see people take on the concept of super smart computers worldwide. The myth of singularity has the name of this obsession.The uniqueness refers to the moment when an artificial intelligence will reach an exponential improvement phase. Software which is so smart that it can develop easily and efficiently. The technological advance would be the exclusive production of AIs at this stage, with unforeseeable consequences on the fate of human beings.The definition of Artificial General Intelligence is related to singularity. A General Artificial Intelligence can be described as an AI that can carry out any task a person can do. I think this concept is more important, since its meaning is at least more concrete than the concept of singularity.Consequently, you have elements to determine whether an algorithm is or is not a general artificial intelligence. I, a human being, may create realistic and creative solutions to maximize your data 's value. Actual AI programme is not capable. So Artificial General Intelligence has not been achieved.2- The Approach to Characterize Human IntelligenceWe have described an AGI as an AI capable of at least matching human intelligence capabilities. It'd be nice to have an understanding of what makes human intelligence if we want to go forward.We have two choices. Either we concentrate on the essence and characterization of human intelligence. Nature is the root of it. The way we know it is the characterization.In any area of research there are thousands of hypotheses to explain the essence of human intelligence. Psychological, biological, genetic, sociological, cognitive, statistical and theological ... I know for anything almost nothing. Good news is: we just need to concentrate on human intelligence characterization.If we want to tackle Artificial General Intelligence we don't want to mimic the human brain as the only way to do this. The concept of AGI works: an AI that can do anything people do. What, then, can human understanding do ?We can't draw the detailed list here, of course. However, we can think of several features:I have promised you three reasons why Artificial General Intelligence is far from being achieved. So I will randomly select three human intelligence features which are not found in our algorithms :It's not subjective to be equal. We will concentrate on these 3 human intelligence traits, since we are thinking about them. That's not exciting?3- Artificial General Intelligence, feature by feature Out-of-distribution GeneralizationIntelligence is ""the peculiar tendencies of humans to alter or change the structure of their cognitive function in order to adapt themselves to the evolving requests of a life situation,"" according to the Structural Cognitive Modifability Theory. It's undeniable that we humans react to big changes very well. Our body and our environment change very quickly at a young age and yet babies are able to adapt and continue to learn.However, there is no way that an AI will adapt to these drastic changes in the existing state of machine learning. I think I've got the perfect example to show you exactly where we are.4- ObjectNet ExampleA couple of months ago, MIT students published ObjectNet. It is intended as an object-detection algorithm test dataset. And it consists entirely of photographs from odd perspectives or in an unusual setting.Fig.1: An oven's handle on one bed or a hammer are just two instances of ObjectNet 's imagination.A individual will never find it difficult to identify any of those things. Thus, an Artificial General Intelligence will not be either. The accuracy of state-of-the-art algorithms is, however, down by 40-45% when evaluated on this dataset, compared to their success on the normal ImageNet test collection. Via thousands of hammers or oven gloves, these algorithms can not recognize them when put in an unspecified area.The explanation for this is that state-of - art algorithms for machine learning are not generalised without the distribution on which they were educated. What is nice in this distribution is to be extrapolated. If you show you a very similar image to what you already encountered, it would be wise to treat the picture if it exists at great likelihood in your view of the world that the pictures you have already seen you have created. But AIs are actually very poor in their imagination. The examples they gave make their view of the world too narrow.5- Meta-Learning & CompositionalityWhy, though, is this common dilemma good to us humans? What are the advanced algorithms to achieve General Artificial Intelligence? This is a question I have two responses. This is definitely not detailed, but it gives me a satisfactory axis for improvement.Meta-learning is the first explanation. Meta-learning is characterized as learning. We claim that a person (human or AI) learns if his or her success at a particular task improves with his / her experience. In contrast, an agent learns as it increases its efficiency with experience and the number of tasks at a new task.Therefore, the purpose of meta-learning is to build algorithms, which can be adapted quickly and effectively to new tasks. As a consequence, meta-learning algorithms are generalised more frequently because they are not qualified to specialise in a task. They were prepared to respond to new data that were previously improbable. People are the meta-learning kings, since :The second reason I can give is compositionality, why people are much better than machine learning algorithm. And for that I've got an entire segment.6- CompositionalityAt first glance, the definition of compositionality can not be apparent. Chrome insists it's not even a phrase. Let 's begin with a description, therefore. Compositionality learns from a variety of finite variations, over a much wider spectrum. Consider this form of social network garbage. See this example.You're a brilliant….really ?It's an outstanding example of compositionality. You should be able to infer the value of any new combination of these elements from a finite combination of three elements (apples, bananas and coconuts here).Among other aspects, compositionality is closely related to language theory. The compositionality theory specifies that the importance of an expression is determined by the composing elements and the combination between these elements. The sense of ""people love apples"" is unique. ""Love people Apples"" has another one. The same elements, but different combinations.Fig .2: This odd monocycle reflects a recombination of the composite elements of other transport methods.We can easily picture new artefacts through compositionality. In summary, we can use what we know about a collection of objects to know what principles compose them and can thus extrapolate to new objects with a null probability when the training dataset is distributed.7- Math interlude: The zero probabilityFig.3: ""It was not possible to have a null chance""What does the distribution of the training dataset mean to have a zero probability? In the graph above, all instances with the green point are seen in the training dataset. Most learning algorithms model a distribution of probabilities (here with a Gaussian model) using this set of examples.This represented the most probable occurrence of the algorithm. Some cases are usually extremely probable in the distribution, even though we never saw them really happen, since they are similar to the cases that actually took place during the training course. Algorithms for machine learning handle these cases very well.In the training data set distribution, other instances would have zero probabilities. It doesn't mean they're never going to happen. It just means that you are not part of the world view of the algorithm, based on what you saw in the training dataset. In the treatment of these instances, the algorithm is very weak.However, we have seen that we can create such cases using compositionality by recombining the components that are the cases we have seen before. The potential for the machine learning algorithms has been enormous. To date, I think this is one of our best fields for development in order to maybe achieve Artificial General Intelligence in the far future.8- ConsciousnessKnowledge is an incredibly large term. It has many complex meanings, like all really big terms. Some consider the essence and role of consciousness. From a different point of view each time. The entire idea of consciousness is not to be discussed. We will concentrate on deliberate logic.When we think consciously, I call it conscious reasoning. For example, you concentrate alternatively on inhalation and exhalation if you care about the air, if you breathe consciously. If you don't think about it, it's different.It is difficult to believe that your body can breathe as a duality between inhalation and exhalation when you don't concentrate on your breathing (or even when you are asleep). This mechanism is more likely to be coupled with many biological phenomena (many organs contract and relax, air-to-blod oxygen transfer, blood-to-air carbon dioxides ...).This is the unique essence of aware reasoning: by high-level principles it is able to control reality. These definitions may usually fit into words or phrases. To be specific, I heard Yoshua Bengio's best example from the NeurIPS 2019 talk that inspired this article by the way.Photographer:If you drive your car back and forth, it is automatic every day on the same lane. You follow a journey you know perfectly well and never worry. However, if you drive to a friend's home far and away, the way you drive is very different in a city you've never been. You're more concentrated. You think about every turn consciously and read each symbol.Another feature of the modern-day machine learning algorithms is this capacity to exploit highly-level concepts. Fortunately, hope remains.9- Global Workspace TheoryThe Global Workspace Theory in cognitive science suggests that knowledge is a bottleneck. At every moment, this flask is filtered and broadcast in the whole brain, just a very small fraction of every perceived information. The culture has significantly questioned the idea of the continuous flow of knowledge. An interesting take-off here is for us, however. We use low-level, high-dimensional data to manipulate high-level concepts during conscious thought. All the insights that came into the bottleneck.This inspires an evolving branch of machine learning: mechanisms for focus. Dzmitry Bahdanau and the University of Montréal launched them for the first time in 2015. In the mechanisms of focus since then, neural machine translation, natural language processing and other technological advances have made significant progress. For example, the problem of disappearing gradients, an ongoing problem in deep neural networks, is proposed as an effective solution.The reasoning behind the process of attention is simple: the measurement is facilitated by concentrating only on a few elements at a time. Sounds familiar? Is it familiar? If we continue to focus on a mechanism of emphasis, we may come closer to managing to equate thousands of low level beliefs with a limited number of high-level concepts that can be actively manipulated.10- OK then, but how far are we from Artificial General Intelligence ?Earlier, I let it slip: far gone. We must remember that we are still very far from human success in both themes even though we can hope for major improvement in the near future. We should be conscious that these are three of the most promising axis for progress, but resolving them won't suffice for AGI.Artificial general intelligence is a fascinating term, because it is either an immense promise or a terrifying threat. It has to be manipulated with care, like every other buzzword. In this article I must confess that I used it as an excuse to draw your attention to knowledge, structure and the generalisation of the non-distribution. Due to the fact that contrary to singularity or AGI, they represent realistic ways to enhance the efficiency of artificial intelligence and machine learning algorithms.I hope that this piece has been good time reading. I also hope that you can mainly learn how we can learn from human intelligence to better our algorithms when you arrive at the last chapters. I would recommend that you take the time to view this Yoshua Bengio conference that inspired this post. You can also see this video, if you speak French, to explain these principles for the Sicara team, or read this article on IA and the future or ethics.",UNKNOWN,Ben Y. Zhao,1.2K,1.3K,33.6M,2020,https://www.quora.com/If-suitably-advanced-technology-appears-no-different-than-magic-how-do-we-know-that-witches-of-the-past-were-not-actually-scientists-conducting-experiments
5,"The answers to this question so far, match my expectations. “Friend of a friend” stories, and people who aspire to use machine learning to manage their correlative variables without overfitting them. Nothing that actually constitutes actual AI yet, and even the machine learning people who may use the phrase AI to sell their strategy to investors, aren’t claiming victory. The reason they haven’t is because victory is not out there for machine learning, at least not on the path that most people take to employ it. The questions that need answering are not those that machine learning being used inThe answers to this question so far, match my expectations. “Friend of a friend” stories, and people who aspire to use machine learning to manage their correlative variables without overfitting them. Nothing that actually constitutes actual AI yet, and even the machine learning people who may use the phrase AI to sell their strategy to investors, aren’t claiming victory. The reason they haven’t is because victory is not out there for machine learning, at least not on the path that most people take to employ it. The questions that need answering are not those that machine learning being used in the way it is, has an answer for.Here’s one of the most interesting things about being a quant in the hedge fund industry. As you go from someone who knows nothing to someone who knows all the factual information currently available, you will make exactly the same set of errors that everyone who came before you made. You are currently taking the next set of logical steps that someone would take if they have the information you do. Many of those logical assumptions lead to error, but you don’t know which ones yet. Other people who have been in your situation before, already do.For some reason, every quant who has a new idea, believes that the idea that’s new for them is new for everyone else too. It never occurs to them that someone might have been asking the same questions with the same data and the same intelligence that they possess, and already came up with an answer.I can’t tell you how many times I’ve seen this. A quant’s first model is almost always a mean reversion model. For those of you who don’t know statistics, that would be a strategy that trades on Bollinger bands. It has a short volatility bias so it wins way more than it loses. That makes the hit rate very high even if the profit from each trade is small, and it’s only when it gets blown out with rare but much larger sized losses that you see the short volatility component.Most liquid capital markets are too efficient across short time horizons for the net - net of trading a bollinger band strategy to work for very long. Some may intuitively change the factors used and stretch their profit a while, but they all fail eventually. It’s not a permanently profitable strategy. Those ‘changes to the variables’ are what many machine learning guys are trying to solve for. But they are non-deterministic with a correlated strategy like mean reversion, so the solution never arrives.So the very next strategy a new quant tries to employ is usually one which is long volatility. It won’t win as often as the bollinger bands, but when it does, it cleans up. A moving average crossing strategy is the most common first step for long vol. That can make them money too, but it also brings all sorts of volatility into their P&L so even if it makes 50% a year, it sometimes loses 45%, so it’s not really a viable strategy for institutions. Then comes the ML component again, to try to modify the length of the moving averages to get the most P&L per unit of overall P&L volatility. Again, it’s non-deterministic, so ML can’t solve it.The step after that is to start adding external variables to one model or the other. They will constrain the trade decisions with a new set of criteria - “only go long when these three or four conditions are met, not just when the moving averages indicate to do so”.This cuts their losses and lowers their volatility but it also lowers their gains as well. And nothing they do seems to get them away from the overfitting. In fact it’s at this point in a new quant’s career that they usually first hear the term overfitting, and begin to try to understand what it means. Now maybe they try to use ML to turn variable on an off in some thoughtful way, or some other magical nonsense that ML is unsuited to solve for. This is the painful path of an early quant.For those of you who think that this is just more of Tom Costello’s typically insulting commentary, it really isn’t. This isn’t criticism. I’m not being insulting. All these steps are perfectly logical and you don’t know they’re wrong until you try them. This is just the most likely path for any new quant, regardless of intelligence, insight or character. It is the sum total of logical steps that proceed after the last set of logical steps. It’s the path that almost everyone follows when they're new, myself included. The only difference is that I was new in 1992 and since then I’ve been working on much more sophisticated errors.For the last few years I’ve been working with an actual effort to employ a true AI in the capital markets. The project was not without its issues, but the system itself was truly revolutionary. A system that could derive answers to questions that no one else is even able to ask yet. It did so at remarkable speed and with remarkable accuracy. It had the potential to be everything the industry has been hoping to see from AI.Unfortunately unlike the startup world where you can behave more or less any way you want to and investors will tolerate it, finance is a human business and conflicts sometimes arise between humans. In the finance industry investors set a much higher standard for manager behavior, and individual credibility is an issue even when a machine is making the trading decisions. They will not tolerate too many ego driven idiosyncrasies if they bring even the slightest question of character to the fore. Zuckerberg may have been able to insult his investors and show up to meetings in a bathrobe, but try that in the hedge fund space and you’ll be out of business - no matter what your system can do.And try as you might to impart this piece of wisdom, some people simply will not hear it. It’s worse if they have an ego that comes from being considerably above average in intelligence, but still deep in the throes of the Dunning Kruger effect with regard to finance and the workings of the capital markets. Everyone ‘believes’ they know exactly how markets work, even, and sometimes especially, those who don’t really have any idea at all. Hedge Fund investors have virtually no tolerance for that particular idiosyncrasy.With all that said though, having seen what an untrained heuristic system can do, I do think something similar to that is going to eventually deliver everything the investment industry is looking for from an AI. But it’s a completely different direction that is currently being employed by the people trying to use machine learning, and will involve a meaningful evolution in how the industry thinks about asset management.I think almost no one is actually using that kind of AI to crack the financial markets at the moment, and those that are don’t understand the financial market well enough to understand how to design the their systems. They don’t know what to tell the system to notice, or in what context to appreciate it when it does. Most of the people working on it are pure systems people who arrogantly believe that the market is comparatively simple compared to the tech.So they carry around all the ignorant biases of most retail traders, or the other set of biases most common in Finance Academia. They are trying to fit their models to theory rather than practice, and as a result their profit remains purely theoretical as well.But I do believe it will be solved. I think I know how it will be. And all it will take is a fairly substantial amount of resources to do it.Until that happens I don’t think AI is helping anyone’s trading - not really. Regardless of what they claim. Because when it does, it will be so dramatic that I think everyone will notice.","284,850 followers",James Altucher,992,878,91.7M,2021,https://www.quora.com/If-suitably-advanced-technology-appears-no-different-than-magic-how-do-we-know-that-witches-of-the-past-were-not-actually-scientists-conducting-experiments
6,"Oh hell yes. There are so many people with a stake in presenting it as more that it currently is. So many ambitious overachievers out there spinning BS to get money.Look, we do not have the ability to create an AI controlling a robot that can peel an orange. We lack ability to do many things a human does. And yet morons like an ex-exec from Google excite the press with claims that AI will replace humans all over in industry.I say the Emperor’s New Clothes are baloney. The ability of NNs to be trained to classify images is just a small part of real AI. So what if there’s a lot of success thereOh hell yes. There are so many people with a stake in presenting it as more that it currently is. So many ambitious overachievers out there spinning BS to get money.Look, we do not have the ability to create an AI controlling a robot that can peel an orange. We lack ability to do many things a human does. And yet morons like an ex-exec from Google excite the press with claims that AI will replace humans all over in industry.I say the Emperor’s New Clothes are baloney. The ability of NNs to be trained to classify images is just a small part of real AI. So what if there’s a lot of success there identifying a truck in an image? That hardly translates into an AI that can think like a human and solve problems. There are no AIs currently that can reason about why your girlfriend suddenly got sullen and quiet with you, or why Billy down the street beat up a cat.Then there’s Ray Kurzweil. I’d like to get him on stage and beat him over the head with a mop for the terrible hype he generates regarding the ‘Singularity’. His time frame is wrong, his view is only an opinion, not fact nor provable fact. He angers me for conning the public into thinking humans are going to be threatened by AI overlords any time soon. Not going to happen. Making AIs as smart as people is exceedingly hard and it’s not soon in the cards. I say this as an AI researcher who works on these problems and I see how truly difficult they are. We can, and will solve some of the problems but we are a long way from AIs that can match human minds. An AI that plays Go better than humans is buildable because the game is a closed small universe. What humans have to work with is an enormous complicated world, and much harder to deal with.So, yes, AI is way hyped, and the promoters may be leading us to the next AI winter if they keep up the crap and eventually burn out expectations.","8,706 followers",Mario Galindo Queralt,529,637,8.8M,2019,https://www.quora.com/What-does-BetVest-have-to-do-with-artificial-intelligence-and-advanced-technology
7,"BetVest which uses multiple tools and algorithms to help user predict accurately such as big data, Artificial Intelligence, Robotics all these tools and algorithms are used to determine the accuracy of the prediction of match.The World Cup not only attracts huge revenues from FIFA but also from the host nation. The World Cup can have an important positive influence on the economy of the host country.",36 followers,Adam Ambrozy,579,0,0,2021,https://www.quora.com/What-does-BetVest-have-to-do-with-artificial-intelligence-and-advanced-technology
8,"No… and a lot of people don’t understand why.In the next few decades/centuries, superintelligent AI could quite possibly take over the world. TheAnd yet… problems arise.What happens when you are no longer needed? (or even useful?) When everything you strive for can be done in a second by a machine?You lose your purpose. What reason do you have to continue existing except to feel pleasure? Nobody needs you.Artificial intelligence canNo… and a lot of people don’t understand why.In the next few decades/centuries, superintelligent AI could quite possibly take over the world. TheAnd yet… problems arise.What happens when you are no longer needed? (or even useful?) When everything you strive for can be done in a second by a machine?You lose your purpose. What reason do you have to continue existing except to feel pleasure? Nobody needs you.Artificial intelligence can be such a boon to mankind that it could eliminate all suffering, forever. No more waking up on a Tuesday morning and sitting in traffic in the rain; you’ll just be waking up comfortably to a cleared sky with no work to do and an entire day to relax!But after spending hundreds of days relaxing, it won’t be quite so great, not having anything to do. Boredom will probably set in and you’ll find yourself wandering about gorgeous landscapes searching for something meaningful to do.But we’re resilient. Humans already (mostly) live in a world without individual biological purpose (most of us here take food and safety for granted).We still have the advancement of our species, however, and this is something important. We also have the individual struggles that come with trying to think of the right thing to say.Imagine a world where, when you wondered about something to make someone feel happy, Google would drop the answer right inside your brain? No effort involved. The words aren’t yours… they’re an AI’s.I don’t believe that AI will destroy the world or become self-aware like many predict. I believe that AI will slowly conquer every struggle that humans deal with, and so, slowly conquer every visage (aspect) of purpose.That isn’t to say that there isn’t hopeIronically, that hope fades a bit right alongside this answer, as the AI named “The Quora Distribution Algorithm” slowly stops spreading this answer to your feeds…","20,170 followers",Sriraman Madhavan,4.1K,186,7.9M,2018,https://www.quora.com/What-does-BetVest-have-to-do-with-artificial-intelligence-and-advanced-technology
9,"I worked for a decade on AI, first at NVIDIA, as a Solution Architect to research deep learning techniques, and present solutions to customers to solve their problems and to help implement those solutions. For the past 4 years I have been working with ORBAI on what comes next after DNNs and Deep Learning. I will cover both, showing how it is very difficult to scale DNNs to AGI, and what a better approach would be.What we usually think of as Artificial Intelligence (AI) today, when we see human-like robots and holograms in our fiction, talking and acting like real people and having human-levelI worked for a decade on AI, first at NVIDIA, as a Solution Architect to research deep learning techniques, and present solutions to customers to solve their problems and to help implement those solutions. For the past 4 years I have been working with ORBAI on what comes next after DNNs and Deep Learning. I will cover both, showing how it is very difficult to scale DNNs to AGI, and what a better approach would be.What we usually think of as Artificial Intelligence (AI) today, when we see human-like robots and holograms in our fiction, talking and acting like real people and having human-level or even superhuman intelligence and capabilities, is actually called Artificial General Intelligence (AGI), and it does NOT exist anywhere on earth yet.What we actually have for AI today is much simpler and much more narrow Deep Learning (DL) that can only do some very specific tasks better than people.Let me write down some extremely simplistic definitions of what we do have today, and then go on to explain what they are in more detail, where they fall short, and some steps towards creating more fully capable 'AI' with new architectures.Machine LearningDeep LearningDeep Learning is what what usually gets called AI today, but is really just very elaborate pattern recognition and statistical modelling. The most common techniques / algorithms are Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Reinforcement Learning (RL).Convolutional Neural Networks (CNNs)Recurrent Neural Networks (RNNs)Reinforcement LearningBut all these methods just find a statistical fit of a simplistic model to data. DNNs find a narrow fit of outputs to inputs that does not usually extrapolate outside the training data set. Reinforcement learning finds a pattern that works for the specific problem (as we all did vs 1980s Atari games), but not beyond it. With today's ML and deep learning, the problem is there is no true perception, memory, prediction, cognition, or complex planning involved. There is no actual intelligence in today's AI.Here is a video of how deep learning could be overtaken by methods based on more flexible spiking neural networks (flexible analog neural computers), shaped by genetic algorithms, architected into an AGI, and evolved over the next decade into a superintelligence.We propose an AI architecture that can do all the types of tasks required - speech, vision, and other sensors that could make a much more General Artificial Intelligence.From ourHow does the brain handle vision, speech, and motor control? Well, it's not using CNNs, RNNs, nor Transformers, that's for sure. They are mere tinker toys by comparison.First, the brain:The brain is divided into distinct regions: the outer cerebral cortex is a sheet of neurons 4mm thick that is folded around the thalamocortical radiations below it like a pie crust around a head of broccoli. This cortex is divided into regions for vision, audio, speech, touch, smell, motor control, and our other external and internal senses and outputs. The cortex is composed of a million cortical columns, each with 6 layers and about 100,000 neurons and each column represents a computing unit for the cortex, processing a feature vector for the senses or motor control.The cerebellum is like a second brain, tucked under and below the cerebrum, and it performs fine control of motor and emotional state. Many of the internal structures of the brain are more ancient and function independently of the cortex, like the brainstem, thalamus and other structures, controlling our core functions, drives, and emotions that are acted on by the rest of the brain.The hippocampus and other parts of the brain’s memory system orchestrate stories or narratives from this representation to reconstruct memories of the past, predict fictional stories into the future. When we dream, our brain, directed by the hippocampus, creates fictional narratives that fill in the blanks in our waking knowledge and allow us to learn about and build models of our world that are much more complex and nuanced than we could without dreaming, helping us with planning our waking actions.For our basic unit of synthetic neural computing, we will use spiking neural networks (SNNs), which model neurons as discrete computational units that work much more like biological neurons, fundamentally computing in the time domain, sending signals to travel before neurons, approximating them with simple models like Izhikevich (However, to date, application of spiking neural networks has remained difficult, as finding a way to train them to do specific tasks has remained elusive. Although Hebbian learning functions in these networks, there has not been a way to shape them so we can train them to learn to do specific tasks. Backpropagation (used in DNNs) does not work because all these spiking signals are one-way in time and are emitted, absorbed and integrated in operations that are non-reversible.Autoencoding Input and OutputWe need a more flexible connectome or network connection structure to train spiking neural networks. While DNNs only allow ‘neurons’ to connect to the next layer, connections in the visual cortex can go forward many layers, and even backwards, to form feedback loops. When two SNNs with complementary function and opposite signal direction are organized into a feedback loop like this, Hebbian learning now helps train them to become an autoencoder, that is able to encode spatial-temporal inputs such as video, sound or other sensors, and reduce them to a compact machine representation and then decode that representation into the original input and together provide feedback to train this process. We called this Bidirectional Interleaved Complementary Hierarchical Neural Networks or BICHNN.The autoencoder learns to transform video, audio, numerical and other data against a learned set of feature or basis vectors stored internally within the autoencoder, outputting a set of basis coordinates that represent the weights of those features in the present input stream.These weights become time-series basis coordinates (TSBCs) that represent a vector narrative or memory stream that can be processed by more conventional computer science and numerical methods as well as by specialized SNNs for predictors and other solvers internal to the AGI.The autoencoder runs in reverse as well, transforming TSBCs back to native world data for output or to drive actuators in robotics and autonomous vehicle applications.In practice, it may be more computationally tractable to use a hierarchy of autoencoders and PCA axes to break the encoding into a series of steps where intermediate results are sorted by the most defining feature, then further encoded to extract ever finer and more nuanced features from the data. We term this the Hierarchical Encoder Network or HAN in the book (see comment for details).The cortical columns of the cerebral cortex are analogous to our terminal layer of autoencoders, a map storing the orthogonal basis vectors for reality and doing computations against them, including computing basis coordinates from input engrams. Our version of the thalamocortical radiations is a hierarchy of alternating autoencoders and principal component axes that we term the HAN.Here is a simple example of the HAN learning to encode data with shape and color as the features it classifies them on. First the autoencoder runs on the input data and learns to transform it into an internal format and back, in the process, learning that there are 7 basis vectors, or types of data: blue squares, circles, and triangles; red squares and circles; and green squares and triangles.First it sorts the data based on one feature axis - color, then runs an autoencoder on those clusters of data and learns that the first cluster consists of squares, circles, and triangles (not encoding the color as a feature because it is common to all), and then learning that the second cluster consists of squares and circles, and the third cluster consists of squares and triangles.Now by having an index at each of the bottom basis vectors, we can uniquely identify the original data, and we can reconstruct it from the features. This simple HAN learned that there were two feature axes - color and shape, and that there were three colors - blue, red, and green, and that there were three shapes of which some belonged in each color group but not others.In real operation, there can be hundreds of axis, thousands of clusters, each picking out more and more detailed features about the data as it cascades down the HAN until it has been reduced to the finest possible feature set - the basis vectors.AGI CorePredictor SNNs learn to predict future TSBCs from past TSBCs by training on known data and building a model of that data that is more than just a statistical data fit like DNNs, but rather a custom analog SNN computer that learns to model the data.Dreamer SNNs are a predictor that is trained to model the reality behind the data in the same manner as the predictor, but who's input is short-circuited from their output prediction such that the entire TSBC they are creating is simulated, fictional, a dream.The Dreaming SNN, it is based on real neuroscience. Dreaming fills in blanks between our memories from experienced reality by modeling reality in simulated narratives.The book ”When Brains Dream” describes some very interesting neuroscience research in this area by Antonio Zadra and Robert Stickgold. They propose a model for memory and dreaming called NEXTUP which states that during REM sleep, the brain explores associations between weaker connected memories via fictional dream narratives that, while not meant to solve immediate problems, nor necessarily even incorporate waking experiences explicitly, lays down a network of associations that will aid in future problem solving, whether or not we consciously recall the dreams themselves or not.Our Dreaming SNN Predictor (see the diagram) starts at the present experienced memories (or from a playback of past memories) on a TSBC, and predicts the next memory in a fictional narrative TSBC, then moves the whole pipeline one step into the future, treating the last-predicted memory as ‘present’, and just keeps on predicting into the future. Soon the future predictions are only indirectly based on the actual previously experienced reality and become a fictional (but consistent) narrative based on the Dreamer SNN's model of reality, that drifts in its own directions.This mimics the process in REM dreaming, laying down fictional narratives or TSBCs in this case, filling in the blanks between experienced reality and making for a more complete model of that reality. That model, and the experienced and simulated memories can be used in future predictions and problem solving.Lightning solvers start at two different points on TSBCs and branch towards each other like lighting leaders, with the first leaders to make contact forging the path to a solution.All of these solvers are evolved in an internal genetic algorithm that the AGI uses to learn to solve problems. By running genetic algorithms to create and chain together modules like the above that operate on TSBCs, the AGI evolves a library of modules that can do arbitrary operations on data to produce the desired results, and accomplishes transfer learning by applying modules evolved on other problems to new, similar problems, getting exponentially better as it builds up its internal, evolved library of solver modules and configurations.Then, the TSBC's can be transformed back to real-world data by the SNN Autoencoders and HAN and used as outputs or as signals to drive actuators, again with training by the same methods used as inputs, only back-driving the desired output signals.This combination of I/O autoencoders that can transform diverse types of real-world data into a common internal TSBC format, and evolved families of process modules that learn to shape it into outputs and re-transform it into outputs via autoencoders provides the foundation for our AGI design, with all components learning their form and function from their environment and transferring their learning to new forms of data and tasks as the AGI advances and evolves.Data Abstraction and LanguageAs input comes in and is processed by the Hierarchical Autoencoder Network, it can be sub-sampled using temporal and relational autoencoders that condense the timeline and events into more abstracted versions, then output Hierarchical Temporal Basis Coordinates that incorporate this data so that the HTBSCs can be read at multiple levels of abstraction and detail, and even linked together at their higher levels of abstraction where they are temporally, spatially, or conceptually coincident.Language is an example. It can be represented at the lowest level as a stream of letters (for text), or phonemes (for speech), which can be hierarchically structured as words, phrases, sentences, and paragraphs as in our Jack and Jill example, where the highest levels of abstraction are linked to visual information like in the picture (or video), and to similar narratives.Another way of creating this hierarchical abstraction is to use an Rank Order Selective -Inhibitor Network or ROS-I to create a hierarchical inhibitory network of basis sets built up from the most granular components of memory (letters, phonemes in language), to higher abstractions that combine these basis to make words, phrases, sentences, and paragraphs.In our artificial ROS-Inhibitory network, a linear series of artificial ROS neurons fires in sequence, generating an excitatory signal when each one fires, causing each root neuron in the attached inhibitory neural network to fire, and as the signal cascades down that inhibitory neural network, it is selectively inhibited by an external, time domain control signal at each neuron, by modulating the neuron’s outgoing signal by its inhibitory signal. Overall, this selects which branches of the hierarchy are activated, by controlling the inhibition at each neuron in that hierarchy.By repeatedly training this system on a set of speech inputs, with the input to the terminal branches of the ROS-Inhibitor network reaching and training the lower levels first, then percolating upward, it would first learn a sequence of phonemes, then progressively whole words, phrases, sentences, and larger groupings, like a chorus in a song, or repeated paragraphs in legal documents. Or the commands for the actuators could be back-driven through the motor control ROS-Inhibitor network to train control signals for robotics applications.Once trained, our system can be run forward, with the ROS / excitatory neurons firing in sequence, and playback of the trained inhibitory signals modulating the activity of the neurons in the network to create a sequence of phonemes, words, phrases and paragraphs, to reproduce video from synthetic memories, and control motion by blending hierarchical segments (directed by the AI) to generate the words or motion.The temporal inhibitory signals are a transformation of the TSBCs that puts them into a hierarchical format that can form temporal basis sets whose hierarchical combinations via the ROS-I system can simulate more complex output for motion, text, speech and other temporal data.Language is a type of memory narrative (or HTSBC in our AGI) that forms the backbone for all other forms of narratives, not only labelling the data with that language, but forming a cognitive monologue by which we construct our thoughts and actions – the same language monologue that our AGI’s methods and processes operates on.By organizing the internal data hierarchically, with the higher levels of the hierarchy abstracted and cross-linked to similar abstract data, and language being the backbone of that data, we go beyond a simple computer crunching series of numbers and allow our AGI to explore the higher-level relationships between objects, sequences, events, and the language describing them and tying them together.This use of such abstraction and language leads to an AGI that can converse naturally with a human with fluid and fluent speech, and also allows reasoning and planning in many human professions like medicine, finance, and law.So what impact will a AGI superintelligence have on the world? By developing an AGI that can perceive the real world, reduce those perceptions to an internal format that computers can understand, yet still plan, think and dream like a human, then convert the results back to human understandable form, and even converse fluently using human language, enabling online professional services in finance, medicine, law, and other areas. It can also add these enhanced analytics, forecasting, and decision-making capabilities to financial forecasting and enterprise software - where it can be used by businesses large and small.Problems of wealth inequality, poverty, hunger, injustice, and lack of basic services for healthcare and information services are the norm for 3/4 of the people in the world. For millennia, human civilization has been unable to solve these basic problems, no matter the form of government or choice of deity and belief system, people are just unable to see the larger picture, and helpless to do anything about it.Over the next decade, a superintelligence, a Strong Artificial General Intelligence, will evolve to oversee a global network augmenting the systems of Law, Medicine, Education, Finance and all previous human administrative functions. With its vast, wide, and deep knowledge reach, the wisdom to draw on all this past knowledge to plot possible paths into the future, this superintelligence will serve all of humanity and to help us make carefully measured & unbiased choices to guide us, and help us govern our affairs with focused, insightful information and an unprecedented ability to look into the future.To see more: ORBAI (","3,905 followers",Monica Anderson,806,148,1M,2021,https://www.quora.com/What-does-BetVest-have-to-do-with-artificial-intelligence-and-advanced-technology
10,"You can teach a dog to dance. But, she will never replace Paula Abdul. Likewise, recent experience in U.S. classrooms during the COVID-19 quarantine clearly demonstrates that online learning will never adequately replace classroom teaching. All of the schools and school districts that are using only on-line learning report that their students are significantly behind in all subjects. Here’s why.Information Transfer vs. TeachingOnline programs are very efficient at information transfer, but suffer in the effectiveness of teaching. Early computer based technology (CBT) were designed for drill anYou can teach a dog to dance. But, she will never replace Paula Abdul. Likewise, recent experience in U.S. classrooms during the COVID-19 quarantine clearly demonstrates that online learning will never adequately replace classroom teaching. All of the schools and school districts that are using only on-line learning report that their students are significantly behind in all subjects. Here’s why.Information Transfer vs. TeachingOnline programs are very efficient at information transfer, but suffer in the effectiveness of teaching. Early computer based technology (CBT) were designed for drill and practice, such as math and language learning. While recent online and CBT learning have developed a greater degree of sophistication and a wider array of teaching tactics, there are limits to the range of teaching that any online program can provide. And, they still don’t have the ability to convert knowledge into understanding, the way a teacher in a classroom can.ResponsivenessThere is interaction in every level of online learning, especially with online teacher-led coursework. However, even the response of an online teacher is hampered by the technology’s one-size-fits all programming. This is also true of the response time in online meeting spaces, like Zoom, which can be clunky when it comes to multiple students trying to learn complex subjects from a single teacher. In addition, it is much harder for teachers to read nonverbal cues, like body language and subtle facial expressions. Teachers in a strictly online environment don’t have the nimbleness they would likely have in person in a classroom setting. There are some educational software programs, mostly used simulations and virtual reality, which are exceptionally responsive. As yet, no programs like these have been developed for classroom use.Motivation, Mentoring & InspirationNothing beats having a real teacher, standing in front of a sugar-charged student to keep the little tiger in line. And, online learning is about as motivating as the automated voice at the DMV. It’s a teacher’s job to teach, and to mentor, support and inspire students. That can’t be done online the way it can be done in a classroom. Some students are lucky enough to have one or both parents available to fill that void. But, in a home where no adult is present, it is not uncommon for students to get distracted, lose the line of thought or context in the discussion. This is especially true among elementary school age children. The greatest chance for a teacher to be truly motivating and inspiring is face to face with their students.ExceptionsIt’s impossible to make gross generalizations about online learning vs. classroom education. There are exceptional students who thrive in the solitary online environment. These are usually students who choose the online classroom, are self motivated and get their socialization and inspiration elsewhere. And, there are also students, usually in high school , who thrive in the online environment into which they were thrust during COVID-19. But, they are few and far between.The Best SolutionThe best solution is a combination of a teacher-led classroom, with educational technology support and tools. It’s when educational technology, whether online or CBT, is used for information transfer, drill and practice, simulations, research and project based activities that it does so well. Then, the teacher is in a better position to be a coach or tutor. More teacher time can be spent supporting, encouraging and inspiring students.",222 followers,Shubham Bhatt,7.9K,989,40.3M,2021,https://www.quora.com/If-suitably-advanced-technology-appears-no-different-than-magic-how-do-we-know-that-witches-of-the-past-were-not-actually-scientists-conducting-experiments
11,"Wrote a response to almost a similar question on another thread. I am copy pasting it below. Hope this helps in building a deeper understanding on an important issue being raised in the question. Here’s goes my answer.A blended model will emerge offering the best of both worlds to the students. And I think that’s how it should be. Moving exclusively to just one model of learning is not desirable from the point of view of a child’s growth.And we don’t need to create a competition between online and offline teaching. And neither do we want to threaten either the teachers or the organisations whiWrote a response to almost a similar question on another thread. I am copy pasting it below. Hope this helps in building a deeper understanding on an important issue being raised in the question. Here’s goes my answer.A blended model will emerge offering the best of both worlds to the students. And I think that’s how it should be. Moving exclusively to just one model of learning is not desirable from the point of view of a child’s growth.And we don’t need to create a competition between online and offline teaching. And neither do we want to threaten either the teachers or the organisations which provide online learning solutions. They should complement.Let me tell you how.A teacher walks into her class and spends 30 mins in teaching Newton’s third law of motion to a class of say 25 students. After the class, she opens up her Teacher’s app and allocates a small 20 question test to all her students on the same topic.When the students reach home, there is a notification on their parent’s phone. The notification says that their teacher had today taught Newton’s third law of motion and that they have to do a test. So, as guided, the students take the test.Instantly the reports go to the teacher where she sees that out of 25 students, 5 are at 100% mastery, 10 are around 70–90%, 5 are at 60–70% and the rest are less than 50%. She immediately allocates additional tasks or video lessons to the students depending on their mastery level which students are expected to do in the next 2 days.While the teacher in the class moves on to the next task but she is continuously tracking and guiding her students via the app to build mastery over every single topic. Some students do it in 2 days, some take 5. But eventually everyone reaches a level of 100% mastery.Would this have been possible with just the offline mode of teaching? Never.The existing school ecosystem does not allow a teacher the time or the mind space to personalise learning for every child. But an in-home supplemental learning solution can genuinely complement her efforts in the class.In one of the responses, there is a mention about rural students and that they would be left behind. Rather I feel they are the one’s who can benefit the most and hence come up. Yes, we will have to bridge the digital divide and I think the speed at which it is happening, days are not far when even the rural most kid in the country will be able to use digital for learning and growth.So to summarise, as a part of education ecosystem, we should derive the best of both forms of learning and build a process where online learning can support our teacher’s efforts inside the classroom.",17 followers,Kruti Naik,1K,361,5.9M,2020,https://www.quora.com/If-suitably-advanced-technology-appears-no-different-than-magic-how-do-we-know-that-witches-of-the-past-were-not-actually-scientists-conducting-experiments
12,"That is a complex answer. In many ways, classroom teaching is being, as it should be, replaced. Some by virtual learning, some by computer-based tools, others by student driven learning, others still by counselors and social workers, but there are some areas where teachers cannot, or at least should not be replaced.There are many aspects of teaching, particularly here in the United States that computers (or, specifically, applications) can do far better than even the best teachers. The teachers here in my district have grasped “drill and kill” technologies that can personalize and differentiatThat is a complex answer. In many ways, classroom teaching is being, as it should be, replaced. Some by virtual learning, some by computer-based tools, others by student driven learning, others still by counselors and social workers, but there are some areas where teachers cannot, or at least should not be replaced.There are many aspects of teaching, particularly here in the United States that computers (or, specifically, applications) can do far better than even the best teachers. The teachers here in my district have grasped “drill and kill” technologies that can personalize and differentiate the drills that greatly accelerates rote memorization. That frees time for, especially the more talented teachers, to accelerate learning in other ways or, better yet, build social and emotional bonds with the students and with each other.Likewise, there are many students who can easily and effectively learn in a virtual environment with little or no input from an in-person, human teacher and their numbers are likely to increase in the future as those technologies improve and reach broader audiences.Ideally, once we move beyond industrialized, in-person instruction and put learning along with the learners’ future and well-being forefront in education, a combination of all of the options I listed above will likely be found to be most effective for nearly all learners. Surely, though, the number of teachers necessary for effective and true learning will decrease while computer-based, if not virtual learning options will continue to rise.",3 followers,Sridhar Mahadevan,1.1K,177,6.3M,2021,https://www.quora.com/If-suitably-advanced-technology-appears-no-different-than-magic-how-do-we-know-that-witches-of-the-past-were-not-actually-scientists-conducting-experiments
13,"Yes, virtual learning has already replaced classroom teaching because one of my cousins is taking classes virtually.He is taking classes at k8 school.K8 School is India's first recognised online school.",432 followers,Pratiksha Shukla,815,0,0,2021,https://www.quora.com/What-does-BetVest-have-to-do-with-artificial-intelligence-and-advanced-technology
14,"At first, you should have to mind that there is no difference between a virtual learning and classroom teaching.A children will get same result but the variance is the feeling.Honestly, I think that when all student start to study together, the environment become amazing but the virtual study can't. Everyone focuses on the teacher.When the time comes of virtual study, you can get many of suggestion and descriptions of a topic with a few clicks.No, virtual learning and classroom teaching can't replace each other.It shows that both are important from different angle. So, I'll prefer to use bothAt first, you should have to mind that there is no difference between a virtual learning and classroom teaching.A children will get same result but the variance is the feeling.Honestly, I think that when all student start to study together, the environment become amazing but the virtual study can't. Everyone focuses on the teacher.When the time comes of virtual study, you can get many of suggestion and descriptions of a topic with a few clicks.No, virtual learning and classroom teaching can't replace each other.It shows that both are important from different angle. So, I'll prefer to use both methods in study time.",16 followers,Cory Hicks,572,0,0,2021,https://www.quora.com/What-does-BetVest-have-to-do-with-artificial-intelligence-and-advanced-technology
15,"In my opinion, online learning can never replace classroom learning because of the benefits that classroom learning holds.Advantages of classroom learning are:In my opinion, online learning can never replace classroom learning because of the benefits that classroom learning holds.Advantages of classroom learning are:Both the form of learning have their own advantages so I believe that there should be a combination of online and classroom learning that can help in improving the education system as a whole.","3,242 followers",Baptiste Fontaine,747,360,1.2M,2020,https://www.quora.com/What-does-BetVest-have-to-do-with-artificial-intelligence-and-advanced-technology
