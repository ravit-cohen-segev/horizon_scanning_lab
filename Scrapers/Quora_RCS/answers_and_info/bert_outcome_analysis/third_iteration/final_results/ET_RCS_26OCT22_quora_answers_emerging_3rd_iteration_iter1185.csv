,Answer,followerCount,name,upvoteCount,answerCount,answerViews,year,question
0,We all take money from ATM machines very often and sometimes at mid night too.What if you are trying to take money and someone sneaks in to steal?There is a saviour calledThough this plan is said to be followed at few places there are no such assurance.Also that this idea has few drawbacks like reversing palindrome numbers will be difficult.AnyWe all take money from ATM machines very often and sometimes at mid night too.What if you are trying to take money and someone sneaks in to steal?There is a saviour calledThough this plan is said to be followed at few places there are no such assurance.Also that this idea has few drawbacks like reversing palindrome numbers will be difficult.Anyway such smart security system if implemented will be of great use,18 followers,Asim Qureshi,2.9K,996,156M,2016,https://www.quora.com/What-will-be-the-next-up-and-coming-programming-technologies
1,I,0 followers,Lucas Martin,6.4K,65,4.2M,2012,https://www.quora.com/What-will-be-the-next-up-and-coming-programming-technologies
2,"From the increasing security headache of the internet of things to machine learning everywhere, the future of programming keeps getting harder to predict.The cloud will defeat Moore’s LawThere are naysayers who claim the chip companies have hit a wall. They’re no longer doubling chip speed every two years as they did during the halcyon years of the ’80s and ’90s. Perhaps -- but it doesn’t matter anymore because the boundaries between chips are less defined than ever.In the past, the speed of the CPU in the box on your desk mattered because, well, you could only go as fast as the silicon hamsteFrom the increasing security headache of the internet of things to machine learning everywhere, the future of programming keeps getting harder to predict.The cloud will defeat Moore’s LawThere are naysayers who claim the chip companies have hit a wall. They’re no longer doubling chip speed every two years as they did during the halcyon years of the ’80s and ’90s. Perhaps -- but it doesn’t matter anymore because the boundaries between chips are less defined than ever.In the past, the speed of the CPU in the box on your desk mattered because, well, you could only go as fast as the silicon hamster inside could spin its wheel. Buying a bigger, faster hamster every few years doubled your productivity, too.But now the CPU on your desk barely displays information on the screen. Most of the work is done in the cloud where it’s not clear how many hamsters are working on your job. When you search Google, their massive cloud could devote 10, 20, even 1,000 hamsters to finding the right answer for you.The challenge for programmers is finding clever ways to elastically deploy just enough computing power to each user’s problem so that the solution comes fast enough and the user doesn’t get bored and wander off to a competitor’s site. There’s plenty of power available. The cloud companies will let you handle the crush of users, but you have to find algorithms that work easily in parallel, then arrange for the servers to work in synchrony.IoT security will only get scarierThe Mirai botnet that unfolded in this past fall was a wake-up call for programmers who are creating the next generation of the internet of things. These clever little devices can be infected like any other computer, and they can use their internet connection to wreak havoc and let slip the dogs of war.The trouble is that the current supply chain for gadgets doesn’t have any mechanism for fixing software. The lifecycle of a gadget usually begins with a long trip from a manufacturing plant to a warehouse and finally to the user. It’s not usual for up to 10 months to unfold between assembly and first use. The gadgets are shipped halfway around the world over those long, lingering months. They sit in boxes waiting in shipping containers. Then they sit on pallets at big box stores or in warehouses. By the time they’re unpacked, anything could have happened to them.The challenge is keeping track of it all. It’s hard enough to update the batteries in the smoke detectors every time the clocks change. But now we’ll have to wonder about our toaster oven, our clothes dryer, and pretty much everything in the house. Is the software up-to-date? Have all the security patches been applied? The number of devices is making it harder to do anything intelligent about monitoring the home network. There are more than 30 devices with IP addresses connected to my wireless router, and I know the identity of only 24 of them. If I wanted to maintain a smart firewall, I would go nuts opening up the right ports for the right smart things.Giving these devices the chance to run arbitrary code is a blessing and a curse. If programmers want to perform clever tasks and let users have maximum flexibility, the platforms should be open. That’s how the maker revolution and open source creativity flourishes. But this also gives virus writers more opportunity than ever before. All they need to do is find one brand of widget that hasn’t updated a particular driver -- voilà, they’ve found millions of widgets primed to host bots.Video will dominate the web in new waysWhen the HTML standards committee started embedding video tags into HTML itself, they probably didn’t have grand plans of remaking entertainment. They probably only wanted to solve the glitches from plugins. But the basic video tags respond to JavaScript commands, and that makes them essentially programmable.That is a big change. In the past, most videos have been consumed very passively. You sit down at the couch, push the play button, and see what the video’s editor decided you should see. Everyone watching that cat video sees the cats in the same sequence decided by the cat video’s creator. Sure, a few fast-forward but videos head to their conclusion with as much regularity as Swiss trains.JavaScript’s control of video is limited, but the slickest web designers are figuring clever ways to integrate video with the rest of the web page in a seamless canvas. This opens up the possibility for the user to control how the narrative unfolds and interact with the video. No one can be sure what the writers, artists, and editors will imagine but they’ll require programming talent to make it happen.Many of the slickest websites already have video tightly running in clever spots. Soon they’ll all want moving things. It won’t be enough to put anConsoles will continue to replace PCsIt’s hard to be mad at gaming consoles. The games are great, and the graphics are amazing. They’ve built great video cards and relatively stable software platforms for us to relax in the living room and dream about shooting bad guys or throwing a football.Living room consoles are only the beginning. The makers of items for the rest of the house are following the same path. They could have chosen an open source ecosystem, but the manufacturers are building their own closed platforms.This fragments the marketplace and makes it harder for programmers to keep everything straight. What runs on one light switch won’t run on another. The hair dryer may speak the same protocol as the toaster, but it probably won’t. It's more work for programmers on getting up to speed and fewer opportunities to reuse our work.Data will remain kingAfter the 2016 U.S. presidential election, word-slinging pundits made fun of data-slinging pundits, suggesting that all of their statistical analysis was an exercise in foolishness. Predictions were dramatically wrong, and the big data people looked bad.How did they come to this conclusion? By comparing one set of numbers (the predictions) with another set of numbers (the election results). They still needed the data.Data is the way we see in the internet. Light brings us information about the real world, but numbers tell us about everything online. Some people may make bad predictions based on imperfect numbers, but that doesn’t mean we should stop gathering and interpreting the numbers.Data gathering, collating, curating, and parsing will continue to be one of the most important jobs for the enterprise. The decision makers need the numbers, and the programmers will continue to be tasked with delivering data in a way that’s easier to understand. This doesn’t mean the answers will be perfect. Context and intuition will continue to have a role, but the need to wrangle data won’t go away simply because a few folks predicted that Donald Trump wouldn’t be elected. This means more work for programmers, as there is no end in sight for our need to build bigger, faster, more data-intensive software.Machine learning will become the new standard featureWhen kids in college take a course called “Data Structures,” they get to learn what life was like when their grandparents wrote code and couldn’t depend on the existence of a layer called “the database.” Real programmers had to store, sort, and join tables full of data, without the help of Oracle, MySQL, or MongoDB.Machine learning algorithms are a few short years away from making that jump. Right now programmers and data scientists need to write much of their own code to perform complex analysis. Soon, languages like R and some of the cleverest business intelligence tools will stop being special and start being a regular feature in most software stacks. They’ll go from being four or five special slides in the PowerPoint sales deck to a little rectangle in the architecture drawing that’s taken for granted.It won’t happen overnight, and it’s not clear exactly what shape it will be, but it’s clear that more and more business plans depend on machine learning algorithms finding the best solutions.UI design will get more complicated as PCs continue to fadeEach day it seems like there is one fewer reason for you to use a PC. Between the rise of smartphones, living room consoles, and the tablet, the only folks who still seem to cling to PCs are office workers and students who need to turn in an assignment.This can be a challenge for programmers. It used to be easy to assume that software or website users would have a keyboard and a mouse. Now many users don’t have either. Smartphone users are mashing their fingers into a glass screen that barely has room for all 26 letters. Console users are pushing arrow keys on a remote.Designing websites is getting trickier because a touch event is slightly different from a click event. Users have different amounts of precision and screens vary greatly in size. It’s not easy to keep it all straight, and it’s only going to get worse in the years ahead.The end of opennessThe passing of the PC isn’t only the slow death of a particular form factor. It’s the dying of a particularly open and welcoming marketplace. The death of the PC will be a closing of possibilities.When the PCs first shipped, a programmer could compile code, copy it onto disks, pop those disks into ziplock bags, and the world could buy it. There was no middle man, no gatekeeper, no stern central force asking us to say, “Mother, may I?”Consoles are tightly locked down. No one gets into that marketplace without an investment of capital. The app stores are a bit more open, but they’re still walled gardens that limit what we can do. Sure, they are still open to programmers who jump through the right hoops but anyone who makes a false move can be tossed. (Somehow they’re always delaying our apps while the malware slips through. Go figure.)This distinction is important for open source. It’s not solely about selling floppy disks in baggies. We’re losing the ability to share code because we’re losing the ability to compile and run code. The end of the PC is a big part of the end of openness. For now, most of the people reading this probably have a decent desktop that can compile and run code, but that’s slowly changing.Fewer people have the opportunity to write code and share it. For all of the talk about the need to teach the next generation to program, there are fewer practical vectors for open code to be distributed.Autonomous transportation is here to stayIt’s not cars alone. Some want to make autonomous planes that aren’t encumbered by the need for roads. Others want to create autonomous skateboards for very lightweight travel. If it moves, some hacker has dreams of telling it where to go.Programmers won’t control what people see on the screen. They’ll control where people go and how they interact with the world. And people are only part of the game. All of our stuff will also move autonomously.If you want dinner from a famous chef downtown, an autonomous skateboard with a heated chamber may bring it to your house. If you want your lawn mowed, an autonomous lawn mower will replace the neighborhood kid.And programmers can use all of the cool ideas they had during the first internet revolution. If you thought pop-up ads were bad on the internet, wait until programmers are paid to divert your autonomous roller skates past the kitchen vent of a new restaurant. Hungry yet?The law will find new limitsThe ink was barely dry on the Bill of Rights when debates over what it means for a search of our papers to be reasonable began. Now, more than 200 years later, we’re still arguing the details.Changes in technology open up new avenues for the law. A few years ago, the Supreme Court decided that vehicle tracking technology requires a warrant. But that’s only when the police plant the tracker in the car. No one really knows what rules apply when someone subpoenas the tracking data from Waze, Google Maps, or any of the hundreds of other apps that cache our locations.","135,532 followers",Hector Quintanilla,582,1.2K,88.6M,2018,https://www.quora.com/What-will-be-the-next-up-and-coming-programming-technologies
3,"First there is to note, that programming languages live almost forever. Even Fortran and Cobol still live. What definitely doesn’t go out of trend is C, for it’s the base for every operating system and that’s something that C++ can’t do. They tried, it can’t and the problems with C++ are too big and too many to put them in an answer like this.The OOP paradigm is at the moment replaced by the data orientated programming, which is a nice way to say, that the old guys were right and the modern OOP is crap in most cases. The problems with OOP are widely discussed on the internet, there are some veFirst there is to note, that programming languages live almost forever. Even Fortran and Cobol still live. What definitely doesn’t go out of trend is C, for it’s the base for every operating system and that’s something that C++ can’t do. They tried, it can’t and the problems with C++ are too big and too many to put them in an answer like this.The OOP paradigm is at the moment replaced by the data orientated programming, which is a nice way to say, that the old guys were right and the modern OOP is crap in most cases. The problems with OOP are widely discussed on the internet, there are some very serious talks on YouTube about that, which will sum the problems up.The typical “fashion” languages of today are, asI can’t come up with such a daft idea myself, you need Python for that. And the language Space does mock Python for it. That’s a language completely written in space characters, which is just a bit worse than Python does. But at least I had a good laugh.Ruby is like Python but they managed to make it even slower. So Ruby will be discarded and fall out of fashion.Haskell is a scholars language, forget about it, Go is like Swift just a company driven stuff, I tried it, I wasn’t impressed and I can’t imagine that this thing will survive a phase of hype. Still, languages even the worst will live for very long. A friend of mine is quite enthusiastic about it, but actually he’s not doing much with it.TypeScript as a precompiler language to JavaScript has its point and will survive as long as JavaScript doesn’t get replaced by WebAsm, means other languages in the frontend and degrades to just a kind of web assembly. But I think not.Julia, Dart, well, plays no part in real programming, Rust tries to beat C and while the language is nice, it’s coming 50 years too late to the market to make any difference to C, in my humble opinion. So all you do in Rust will be rusty in 5–6 years and gone.Nice language, though, it just can’t compete and the only thing it does is making the language babel worse than it is. It doesn’t solve anything new.Kotlin, Elixir, well these languages just will be replaced with the next FotM language that someone jams into the LLVM compiler package, which is mostly the reason we have such a 90’s like babel of languages these days. I have lived through the last language babel in the 90’s and they are all gone. Nobody’s talking about Modula, Modula-2, Pascal, Occam or Eiffel. And I can add like twenty others that died during that time and I had to learn all of them at some time.It’s all gone and the latest folly will be gone, too. If in five years or in ten is hard to say, but they will be gone.I wish Oracle goes bankrupt, if they do, Java bites the dust. But I think Java will survive for a long time, like the black death was killing off most of Europe through all the middle ages and before we don’t have a cure for Java or kill the plaque of rats that spreads this disease this will live on for a looong time to all our unlucky pleasure.It is just nice, that we have languages like C and C++ (which I dislike, still…), the Assembly languages will stay, OpenCL will stay, Verilog will stay and hopefully Lua will stay, because it’s easy to learn and to keep active even if you don’t program it for a longer time and because it goes hand in hand with C as a perfect partner and team that doesn’t want to do C’s job and C doesn’t want to do Lua’s.And with that it is unique on the market, still the community is too small and maybe that’s my little pet language, but I worked with that thing with great pleasure and success and it did everything a HLL needs to do.BASIC will stay in the one or other form, maybe in Gambas or inside the LibreOffice project.And I personally think, that Brainfuck will live for longer, more intensive than most of those new corporate venus-fly-trap languages that the companies invent lately to bind their own programming-pool.There are old languages that will not go away, like Perl and M4 and others, and which I love very much and a lot of specialized languages, like LaTeX, which has started to revive through LuaLaTeX and many other projects that started to use Lua as a scripting language as many games do now.My advice is, don’t go with fashion, go with the tool aspect. If the thing is open and well combining with other languages, it will survive forever and it will serve you as a tool for much longer than you think. My C programs from 1985 still compile and run without any problem, so C is not a fashion language, it is eternal.And everyone fluent in C knows that.There are more legacy languages than I could count up and that didn’t survive. And many that I hope die soon. If I could cast a necromancy “take life” spell, that will feast on the flesh of a language like the Ark of the Covenant did on the soldiers in Indiana Jones, and I had to name three languages: it would be: Java, Python, Ruby in that order.May this pest be cured and the world stop suffering.Die, Die, Die! And may the spirits of fashion claw your flesh in their bony and substanceless fingers and pull them down into the realm of Hel, into the eternal Ice of the viking hell, that is the incorporation of the Technocracy that is the mother of these stupid lifesucking things.Eternal slowness of corruption and the frozen flow of water in the ice coldness of the frozen desert that is the realm of Hel. With those three languages the abominations of languages that sacrifice the life of the living and fun of programming to the goddess of Technocracy and total order, sacrificing CPU speed with nothing to give back to you but endless lines of unreadable code and code-noise to the point your fingers condense the air to liquid while typing on the keyboard. The cold ice of a frozen screen on the one side and the technocratic monstrosity that is far away from anything that resembles our hardware, that comes with OOP on the other. The cold skin of the girl without life on the one side and the dead cogs of the machine on the other.These three are the unholy trinity of Hel. And I don’t say that lightly.Die! Die! Die!These three are to programming languages whatThat would be a happy life for me, a life and a death with meaning. And I’m sure with such monsters slain the valkyries would take me to Folkvangr, the heaven of good programmers that have managed to keep their souls through their job.","58,988 followers",Michael Wolfe,504,2K,17.3M,2020,https://www.quora.com/What-will-be-the-next-up-and-coming-programming-technologies
4,Honorable mention goes to languages that weren’t huge technological advances - but massively influential:,"68,793 followers",Kanthaswamy Balasubramaniam,898,17.3K,413.5M,2019,https://www.quora.com/What-will-be-the-next-up-and-coming-programming-technologies
5,"If you build a solid foundation in computer science (and math), you don’t really have to worry about which technologies will be in-demand next decade.In 40 years of “solving problems using computers” you will see many technologies catch hype and slowly die. Best way to shape your career is to pick the problems you are most passionate of solving. Tools that help you solve those problems are secondary and can easily be picked given your solid foundation.After a few years, when you feel tired and bored of whatever you are doing, transition smoothly to the next set of problems and relevant tools.If you build a solid foundation in computer science (and math), you don’t really have to worry about which technologies will be in-demand next decade.In 40 years of “solving problems using computers” you will see many technologies catch hype and slowly die. Best way to shape your career is to pick the problems you are most passionate of solving. Tools that help you solve those problems are secondary and can easily be picked given your solid foundation.After a few years, when you feel tired and bored of whatever you are doing, transition smoothly to the next set of problems and relevant tools. This technology of “reinventing yourself” every few years will keep you relevant and in-demand.",16 followers,Daniel Bourke,6.9K,178,7.7M,2019,https://www.quora.com/What-will-be-the-next-up-and-coming-programming-technologies
6,"The IT industry is one among the fields where frequent changes are being brought in. Adapting those changes is one among the key way to success indeed. Artificial Intelligence, Blockchain and Cloud computing are undoubtedly disruptive technologies that is already being used in a variety of fields. Among the three, I think AI will have slightly higher demand. Artificial Intelligence will be a total disruptor, it will find application in almost every field and will make our life much more simpler. Apples Siri, Google’s voice search etc are all a smaller version of the AI. It's full fledged versiThe IT industry is one among the fields where frequent changes are being brought in. Adapting those changes is one among the key way to success indeed. Artificial Intelligence, Blockchain and Cloud computing are undoubtedly disruptive technologies that is already being used in a variety of fields. Among the three, I think AI will have slightly higher demand. Artificial Intelligence will be a total disruptor, it will find application in almost every field and will make our life much more simpler. Apples Siri, Google’s voice search etc are all a smaller version of the AI. It's full fledged version will be revolutionary indeed. Second place goes to blockchain, which is relatively a newer technology compared to the others. But the growth it has attained in few years is really mind boggling. Indeed, blockchain was one among the top demanded field in LinkedIn for the year 2017. Cloud computing on the other hand has been already been in existence for few years and has shown it's dominance and Applications widely. Presently, more and more enterprises are currently moving their database and operation to the cloud seeing it's advantages. So I think, even though one may have slightly more weightage over the other, all three technologies are equally disruptive and will find a lot of Applications in the future.","1,203 followers",Abhishek Chandak,845,163,1.3M,2019,https://www.quora.com/What-will-be-the-next-up-and-coming-programming-technologies
7,“A picture is worth a 1000 words” so here’s 3000:The saying goes: “Many a truth is spoken in jest”. Though I tend to see the above as more like: “Many a farse is spoken in earnest”.“A picture is worth a 1000 words” so here’s 3000:The saying goes: “Many a truth is spoken in jest”. Though I tend to see the above as more like: “Many a farse is spoken in earnest”.,16 followers,Cory Hicks,572,0,0,2020,https://www.quora.com/What-will-be-the-next-up-and-coming-programming-technologies
8,"As per in demand , I'll say machine learning and rest what fascinates you, but first really be good , i mean so good at machine learning that the community around it can't ignore you.Don't just go with in demand technology, if you have a strong base and are creative enough, you can pull something awesome and literally be irreplaceable in the new economy (i.e. technology driven economy)Just master the basics of ML and rest let your curiosity guide you.Some useful prerequisites for ML :-As per in demand , I'll say machine learning and rest what fascinates you, but first really be good , i mean so good at machine learning that the community around it can't ignore you.Don't just go with in demand technology, if you have a strong base and are creative enough, you can pull something awesome and literally be irreplaceable in the new economy (i.e. technology driven economy)Just master the basics of ML and rest let your curiosity guide you.Some useful prerequisites for ML :-Once prerequisites are out of the picture , (ps: do learn them fast at least in 1 month it should be out of your to do list, as I'm assuming you're from maths background and hence in college or highschool you've done these maths subjects) and learn ML by building projects. There are a lot of resources out there, if you didn't find one do visit this :Its the best site for guidance to people for a career in ML / Software Engineering Field.Ps: I've found this amazing website this month only and find it extremely useful, if it helps anyone, please give it a try. :)-Be real",0 followers,Lyken Syu,595,0,0,2019,https://www.quora.com/What-will-be-the-next-up-and-coming-programming-technologies
9,"GPU instances on the cloud: With more computing power you have better results for many applications like machine vision. (Better accuracy, faster processing speed)GPU on device: When you run a model locally on the device one big advantage is not needing an internet connection. It’s possible to run a hybrid model where simple tasks are parsed on the device.2. The cloud also seems more conducive for creating SaaS business modelGPU instances on the cloud: With more computing power you have better results for many applications like machine vision. (Better accuracy, faster processing speed)GPU on device: When you run a model locally on the device one big advantage is not needing an internet connection. It’s possible to run a hybrid model where simple tasks are parsed on the device.2. The cloud also seems more conducive for creating SaaS business models out of AI. You can charge per API call. When your code is on your user’s devices it’s harder to price in a way that scales with use-volume. Additionally there’s obvious risks when your codebase is accessible on customer’s devices.","3,905 followers",Monica Anderson,806,148,1M,2017,https://www.quora.com/What-will-be-the-next-up-and-coming-programming-technologies
10,"Black Mirror is such an eye-opener for humans to realize the damage we are doing to ourselves in the name of technology. It urges you to dwell into complicated, yet comfort giving technical advancements while reminding us that it’s never too late to stop and turn back. Alas, there is no way back. Not in the 21st century, where the pace is uncontrolled.In maybe 2 decades, all the episodes might comBlack Mirror is such an eye-opener for humans to realize the damage we are doing to ourselves in the name of technology. It urges you to dwell into complicated, yet comfort giving technical advancements while reminding us that it’s never too late to stop and turn back. Alas, there is no way back. Not in the 21st century, where the pace is uncontrolled.In maybe 2 decades, all the episodes might come true. But I am pretty sure that some of these will be a reality in the coming 10 years. Like -Nosedive [ https://en.wikipedia.org/wiki/Nosedive_(Black_Mirror) ]Is there anything at all, that we did today, that did not demand a rating? Right from the toothpaste we use to start the day, until the comforter that puts us to sleep, from paying utility bills to shopping online, one thing we most definitely come across is this question - How did we do? Rate us online. Which means, we are pretty half way through Nosedive. The only remaining unrated entity being us.The Entire History of You [ https://en.wikipedia.org/wiki/The_Entire_History_of_You ]This episode would have been a joke for companies like Google and Facebook. This is not future for them, this is already current. Except, they probably don’t have an album of our private moments. And I don’t think it is far from near future.Playtest [ https://en.wikipedia.org/wiki/Playtest_(Black_Mirror) ]No matter how old people become or how ever many generations prosper, playing games is a ne...","90,345 followers",Alisha Talks,696,910,103.9M,2019,https://www.quora.com/What-will-be-the-next-up-and-coming-programming-technologies
11,"Black Mirror, is one of the terrifying sci-fi anthology series i have ever watched, that showcases, the dark side of the modern world, through the usage of technology and it’s worst consequences.many of those episodes featured larger-than-life, tech, like, Grains, cookies, MASS implants, but, i got really surprised and terrified at the same time, when some of those techs, were actually going to be become a reality. What more surprising is that some of them already exists.Nosedive - ‘social credit’ systemNosedive was an episode, which is set in a world where, everyone is obsessed with rating syBlack Mirror, is one of the terrifying sci-fi anthology series i have ever watched, that showcases, the dark side of the modern world, through the usage of technology and it’s worst consequences.many of those episodes featured larger-than-life, tech, like, Grains, cookies, MASS implants, but, i got really surprised and terrified at the same time, when some of those techs, were actually going to be become a reality. What more surprising is that some of them already exists.Nosedive - ‘social credit’ systemNosedive was an episode, which is set in a world where, everyone is obsessed with rating system, in their mobile devices. which allows people to rate a particular person from one to five stars, based on their daily activities and interactions. Also, these ratings influence one’s socioeconomic status.This was one of my favorite episodes of the series. as it had a very relatable protagonist Lacie Pound (played by Bryce Dallas Howard) didn’t shy away from showing, how people could become so mean to each other, for any simple reason and nobody’s gonna understand deep feelings of others.In this story, Lacie is shown to have 4.2 rating and struggling to get 4.5, but, in the end, due to the mean people and her over obsession with this technology, caused a severe reduction in her ratings, for which, she was incarcerated.I felt sad, for Lacie, but what made me surprise was the fact that, Chinese government is planning to create a ‘social credit’ system, that rates everyone's trustworthiness based on things like financial ratings and political beliefs.China sets up huge 'social credit' systemThe series creator Charlie Brooker, has commented on numerous occasions about links between the episode and the Social Credit System, he pointed out that a key difference between China's rating system and the one in ""Nosedive"" is ""that there's a central government assessing things. Being state-controlled, it feels even more sinister"".ADIs - Robot BeesIn the episode of ‘Hated In the Nation’, features, a bunch of, drone bees, called as ADIs, created by a company called as Granular, invented to counteract UK’s decreasing bee population.Hated in the Nation, can be considered as the best episode of the entire series and it has the potential to become a feature film of it’s own.This episode follows , a bunch of NCA investigators, finding the reason behind the mysterious death of few people, who died right after, they have been singled in a Twitter hashtag #DeathTo. After a through investigation, they found out, that, these deaths are caused by a bunch of hacked ADIs. In the end, the perpetrator, who was a former Granular employee, Garrett Scholes, decides to, orchestrate a genocide, to wipe out, every person who has used the hashtag.This was not a surprise for me, as , Harvard University, has been working on these, RoboBees, for decades. but thankfully, it’s ability is only limited to only being able to fly to being able to dive in and out of water. It doesn’t look anywhere close to the ones that are shown in this episode, which looks and acts exactly like an actual bee.Robot Dogs - AIBOs and BigDogThe episode, Metalhead, can be described as a , Black Mirror’s version of Terminator. which features, a bunch of robot dogs, that might have wiped out humanity, due to it’s, feature, to kill someone.For me, Metalhead was the weakest episode of the series, yet, i was quite impressed by it’s darker nature and it’s ending.Metalhead is set in a post-apocalyptic world, where, the protagonist, Bella. is trying to escape from a killer robot dog, in end, having no other choice, she have to commit suicide, rather than, getting killed brutally by, that robot.SONY has developed, a series of, robotic pet dogs, that is, meant to be a companion robot for adults. Also, NASA, developed a military robot, BigDog, but has been deemed too loud for combat.",0 followers,Ashok Bishnoi,12.8K,314,7.7M,2019,https://www.quora.com/What-will-be-the-next-up-and-coming-programming-technologies
