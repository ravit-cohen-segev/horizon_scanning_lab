,Topic,label,link,abstract,titles
0,-1,-1_new_said_study_people,http://jhr.uwpress.org/content/early/2022/10/03/jhr.0222-12169R2.abstract,"We study how ambient lead exposure impacts learning in elementary school by leveraging a natural experiment where a large national automotive racing organization switched from leaded to unleaded fuel. We find increased levels and duration of exposure to lead negatively affect academic performance, shift the entire academic performance distribution, and negatively impact both younger and older children. The average treated student in our setting has an expected income reduction of $5,200 in present value terms. Avoiding said treatment has an effect size similar to improving teacher value added by one-fourth of a standard deviation, reducing class size by 3 students, or increasing school spending per pupil by $750. The marginal impacts of lead are larger in impoverished, non-white counties, and among students with greater duration of exposure, even after controlling for the total quantity of exposure.",A Thousand Cuts: Cumulative Lead Exposure Reduces Academic Achievement
355,-1,-1_new_said_study_people,https://www.eurekalert.org/news-releases/967553,"RIVERSIDE, Calif. -- Thirdhand smoke, or THS, comprises the residual pollutants from tobacco smoke that remain on surfaces and in dust after tobacco has been smoked. It can remain on indoor surfaces indefinitely, causing potentially harmful exposure to both smokers and non-smokers.A team led by researchers at the University of California, Riverside, has found that acute exposure of the skin to THS elevates biomarkers associated with the initiation of skin diseases, such as contact dermatitis and psoriasis.“We found exposure of human skin to THS initiates mechanisms of inflammatory skin disease, and elevates urinary biomarkers of oxidative harm, which could lead to other diseases, such as cancer, heart disease, and atherosclerosis,” said Shane Sakamaki-Ching, a former graduate student at UC Riverside who graduated with a doctoral degree in cell, molecular, and developmental biology in March 2022. “Alarmingly, acute dermal exposure to THS mimics the harmful effects of cigarette smoking.”The study, published in eBioMedicine of The Lancet family of journals, is the first to be performed on humans exposed dermally to THS.The clinical investigation, which took place at UC San Francisco, involved the participation of 10 healthy, non-smokers who were 22 to 45 years old. For three hours, each participant wore clothing impregnated with THS and either walked or ran on a treadmill for at least 15 minutes each hour to induce perspiration and increase uptake of THS through the skin. The participants did not know the clothing had THS. Blood and urine samples were then collected from the participants at regular intervals to identify protein changes and markers of oxidative stress induced by the THS. Control exposure participants wore clean clothing.“We found acute THS exposure caused elevation of urinary biomarkers of oxidative damage to DNA, lipids, and proteins, and these biomarkers remained high after the exposure stopped,” said Sakamaki-Ching, now a research scientist at Kite Pharma in California, where he leads a stem cell team. “Cigarette smokers show the same elevation in these biomarkers. Our findings can help physicians in diagnosing patients exposed to THS and help develop regulatory policies dealing with remediation of indoor environments contaminated with THS.”Prue Talbot, a professor of cell biology in whose lab Sakamaki-Ching worked, explained that skin is the largest organ to contact THS and may thus receive the greatest exposure.“There is a general lack of knowledge of human health responses to THS exposure,” said Talbot, the paper’s corresponding author. “If you buy a used car previously owned by a smoker, you are putting yourself at some health risk. If you go to a casino that allows smoking, you are exposing your skin to THS. The same applies to staying in a hotel room that was previously occupied by a smoker.”The THS exposures that the 10 participants were subjected to were relatively brief and did not cause visible changes in the skin. Nevertheless, the molecular biomarkers in blood that are associated with early-stage activation of contact dermatitis, psoriasis and other skin conditions were elevated.“This underscores the idea that dermal exposure to THS could lead to molecular initiation of inflammation-induced skin diseases,” Sakamaki-Ching said.Next, the researchers plan to evaluate residues left by electronic cigarettes that can come into contact with human skin. They also plan to evaluate larger populations exposed to longer periods of dermal THS.Sakamaki-Ching and Talbot were joined in the study by Jun Li of UCR, Suzaynn Schick of UC San Francisco, and Gabriela Grigorean of UC Davis.The study was supported by grants to Talbot and Schick from the Tobacco Related Disease Research Program of California.The title of the research paper is “Dermal thirdhand smoke exposure induces oxidative damage, initiates skin inflammatory markers, and adversely alters the human plasma proteome.”The University of California, Riverside is a doctoral research university, a living laboratory for groundbreaking exploration of issues critical to Inland Southern California, the state and communities around the world. Reflecting California's diverse culture, UCR's enrollment is more than 26,000 students. The campus opened a medical school in 2013 and has reached the heart of the Coachella Valley by way of the UCR Palm Desert Center. The campus has an annual impact of more than $2.7 billion on the U.S. economy. To learn more, visit www.ucr.edu.",Thirdhand smoke can trigger skin diseases
353,-1,-1_new_said_study_people,https://www.eurekalert.org/news-releases/965575,"Deep Longevity bridges the gap between the concepts of biological and psychological aging. According to the new aging clock, vulnerable mental health has a stronger effect on the pace of aging compared to a number of health conditions and smokingMolecular damage accumulates and contributes to the development of aging-related frailty and serious diseases. In some people these molecular processes are more intense than in others, a condition commonly referred to as accelerated aging.Fortunately, the increased pace of aging may be detected before its disastrous consequences manifest by using digital models of aging (aging clocks). Such models can also be used to derive anti-aging therapies on individual and population levels.According to the latest article published in Aging-US , any anti-aging therapy needs to focus on one’s mental health as much as on one’s physical health. An international collaboration led by Deep Longevity with US and Chinese scientists have measured the effects of being lonely, having restless sleep, or feeling unhappy on the pace of aging and found it to be significant.The article features a new aging clock trained and verified with blood and biometric data of 11,914 Chinese adults. This is the first aging clock to be trained exclusively on a Chinese cohort of such volume.Aging acceleration was detected in people with a history of stroke, liver and lung diseases, smokers, and most interestingly, people in a vulnerable mental state. In fact, feeling hopeless, unhappy, and lonely was shown to increase one’s biological age more than smoking. Other factors linked to aging acceleration include being single and living in a rural area (due to the low availability of medical services).The authors of the article conclude that the psychological aspect of aging should not be neglected either in research or in practical anti-aging applications. According to Manuel Faria from Stanford University:“Mental and psychosocial states are some of the most robust predictors of health outcomes — and quality of life — yet they have largely been omitted from modern healthcare”.Alex Zhavoronkov, the CEO of Insilico Medicine, points out that the study provides a course of action to“slow down or even reverse psychological aging on a national scale.Earlier this year, Deep Longevity released an AI-guided mental health web service FuturSelf.AI that is based on a preceding publication in Aging-US. The service offers a free psychological assessment that is processed by an AI and provides a comprehensive report on a user’s psychological age as well as current and future mental well-being. Deepankar Nayak, the CEO of Deep longevity affirms,""FuturSelf.AI, in combination with the study of older Chinese adults, positions Deep Longevity at the forefront of biogerontological research"".About Deep LongevityDeep Longevity developed the Longevity as a Service (LaaS)© solution to integrate multiple deep biomarkers of aging dubbed “deep aging clocks” to provide a universal multifactorial measure of human biological age. Deep Longevity is owned by Hong Kong Stock Exchange listed Endurance Longevity (SEHK:0575.HK).",Being lonely and unhappy accelerates aging more than smoking
352,-1,-1_new_said_study_people,https://www.eurekalert.org/news-releases/958880,"By subjecting a quantum computer’s qubits to quasi-rhythmic laser pulses based on the Fibonacci sequence, physicists demonstrated a way of storing quantum information that is less prone to errorsBy shining a laser pulse sequence inspired by the Fibonacci numbers at atoms inside a quantum computer, physicists have created a remarkable, never-before-seen phase of matter. The phase has the benefits of two time dimensions despite there still being only one singular flow of time, the physicists report July 20 in Nature.This mind-bending property offers a sought-after benefit: Information stored in the phase is far more protected against errors than with alternative setups currently used in quantum computers. As a result, the information can exist without getting garbled for much longer, an important milestone for making quantum computing viable, says study lead author Philipp Dumitrescu.The approach’s use of an “extra” time dimension “is a completely different way of thinking about phases of matter,” says Dumitrescu, who worked on the project as a research fellow at the Flatiron Institute’s Center for Computational Quantum Physics in New York City. “I’ve been working on these theory ideas for over five years, and seeing them come actually to be realized in experiments is exciting.”Dumitrescu spearheaded the study’s theoretical component with Andrew Potter of the University of British Columbia in Vancouver, Romain Vasseur of the University of Massachusetts, Amherst, and Ajesh Kumar of the University of Texas at Austin. The experiments were carried out on a quantum computer at Quantinuum in Broomfield, Colorado, by a team led by Brian Neyenhuis.The workhorses of the team’s quantum computer are 10 atomic ions of an element called ytterbium. Each ion is individually held and controlled by electric fields produced by an ion trap, and can be manipulated or measured using laser pulses.Each of those atomic ions serves as what scientists dub a quantum bit, or ‘qubit.’ Whereas traditional computers quantify information in bits (each representing a 0 or a 1), the qubits used by quantum computers leverage the strangeness of quantum mechanics to store even more information. Just as Schrödinger’s cat is both dead and alive in its box, a qubit can be a 0, a 1 or a mashup — or ‘superposition’ — of both. That extra information density and the way qubits interact with one another promise to allow quantum computers to tackle computational problems far beyond the reach of conventional computers.There’s a big problem, though: Just as peeking in Schrödinger’s box seals the cat’s fate, so does interacting with a qubit. And that interaction doesn’t even have to be deliberate. “Even if you keep all the atoms under tight control, they can lose their quantumness by talking to their environment, heating up or interacting with things in ways you didn’t plan,” Dumitrescu says. “In practice, experimental devices have many sources of error that can degrade coherence after just a few laser pulses.”The challenge, therefore, is to make qubits more robust. To do that, physicists can use ‘symmetries,’ essentially properties that hold up to change. (A snowflake, for instance, has rotational symmetry because it looks the same when rotated by 60 degrees.) One method is adding time symmetry by blasting the atoms with rhythmic laser pulses. This approach helps, but Dumitrescu and his collaborators wondered if they could go further. So instead of just one time symmetry, they aimed to add two by using ordered but non-repeating laser pulses.The best way to understand their approach is by considering something else ordered yet non-repeating: ‘quasicrystals.’ A typical crystal has a regular, repeating structure, like the hexagons in a honeycomb. A quasicrystal still has order, but its patterns never repeat. (Penrose tiling is one example of this.) Even more mind-boggling is that quasicrystals are crystals from higher dimensions projected, or squished down, into lower dimensions. Those higher dimensions can even be beyond physical space’s three dimensions: A 2-D Penrose tiling, for instance, is a projected slice of a 5-D lattice.For the qubits, Dumitrescu, Vasseur and Potter proposed in 2018 the creation of a quasicrystal in time rather than space. Whereas a periodic laser pulse would alternate (A, B, A, B, A, B, etc.), the researchers created a quasi-periodic laser-pulse regimen based on the Fibonacci sequence. In such a sequence, each part of the sequence is the sum of the two previous parts (A, AB, ABA, ABAAB, ABAABABA, etc.). This arrangement, just like a quasicrystal, is ordered without repeating. And, akin to a quasicrystal, it’s a 2D pattern squashed into a single dimension. That dimensional flattening theoretically results in two time symmetries instead of just one: The system essentially gets a bonus symmetry from a nonexistent extra time dimension.Actual quantum computers are incredibly complex experimental systems, though, so whether the benefits promised by the theory would endure in real-world qubits remained unproven.Using Quantinuum’s quantum computer, the experientialists put the theory to the test. They pulsed laser light at the computer’s qubits both periodically and using the sequence based on the Fibonacci numbers. The focus was on the qubits at either end of the 10-atom lineup; that’s where the researchers expected to see the new phase of matter experiencing two time symmetries at once. In the periodic test, the edge qubits stayed quantum for around 1.5 seconds — already an impressive length given that the qubits were interacting strongly with one another. With the quasi-periodic pattern, the qubits stayed quantum for the entire length of the experiment, about 5.5 seconds. That’s because the extra time symmetry provided more protection, Dumitrescu says.“With this quasi-periodic sequence, there’s a complicated evolution that cancels out all the errors that live on the edge,” he says. “Because of that, the edge stays quantum-mechanically coherent much, much longer than you’d expect.”Though the findings demonstrate that the new phase of matter can act as long-term quantum information storage, the researchers still need to functionally integrate the phase with the computational side of quantum computing. “We have this direct, tantalizing application, but we need to find a way to hook it into the calculations,” Dumitrescu says. “That’s an open problem we’re working on.”ABOUT THE FLATIRON INSTITUTEThe Flatiron Institute is the research division of the Simons Foundation. The institute's mission is to advance scientific research through computational methods, including data analysis, theory, modeling and simulation. The institute's Center for Computational Quantum Physics aims to develop the concepts, theories, algorithms and codes needed to solve the quantum many-body problem and to use the solutions to predict the behavior of materials and molecules of scientific and technological interest.",Strange new phase of matter created in quantum computer acts like it has two time dimensions
351,-1,-1_new_said_study_people,https://www.engadget.com/zipline-drone-delivery-medicine-utah-114733625.html,"Zipline has teamed up with a healthcare provider servicing the Intermountain Region in the US to deliver medicine to customers using its drones. The company has started doing drone deliveries to select Intermountain Healthcare patients in the Salt Lake Valley area. For now, it can only do drops for local communities within several miles of its distribution center. Zipline intends to add more centers over the next five years, though, so it can eventually expand beyond Salt Lake Valley and deliver medicine throughout Utah.As TechCrunch notes, Zipline has long been deploying drones for delivery in Africa, and it wasn't until the pandemic that it started doing drops in the US. In 2020, it teamed up with Novant Health to ferry personal protective gear and other types of medical equipment to frontline healthcare workers tending to COVID-19 patients in North Carolina. Later that year, it signed a deal with Walmart to deliver health and wellness supplies to customers near the retailer's headquarters in northwest Arkansas.In June this year, the FAA authorized Zipline to conduct long range on-demand commercial drone deliveries in the US. The company said that the certification it received from the agency allows it to significantly expand its services in the country. That means we'll see it expand its covered areas with current partners and perhaps see it sign agreements with more partner companies in the future.Turn on browser notifications to receive breaking news alerts from Engadget You can disable notifications at any time in your settings menu. Not now Turned on Turn onIntermountain Healthcare patients in the Salt Lake Valley area can now sign up for Zipline deliveries. The company will then evaluate their eligibility based on their location, their yard size — its target delivery area must be at least two parking spaces big — and their surrounding airspace. Zipline's drones are six-foot gliders with a wingspan that's 10 feet long. These drones fly 300 to 400 feet above the ground, though they drop down to an altitude of around 60 to 80 feet to deliver packages outfitted with a parachute.Bijal Mehta, head of global fulfillment operations at Zipline, said in a statement:",Zipline drones will deliver medicine to communities in Utah
349,-1,-1_new_said_study_people,https://www.engadget.com/t-mobile-will-start-charging-a-35-fee-on-all-new-activations-and-upgrades-065518011.html,"T-Mobile may be joining rivals Verizon and AT&T by introducing an $35 charge for all new postpaid activations and upgrades, according to The T-Mo Report and some Redditors. According to T-Mobile internal documents, it's introducing a ""Device Connection Charge"" for ""all activations and upgrades for mobile, Beyond the Smartphone and broadband devices.""Before, the Uncarrier charged activation fees only if you received in-store customer support for new activations, with online orders exempt. Now, all new postpaid activations are charged, whether or not you were assisted. This includes updating to a new device, adding a Bring-Your-Own-Device line, or ordering a Home Internet line, according to The T-Mo Report.T-Mobile has always tried to separate itself from regular telecoms, but charging customers for essentially nothing doesn't sound very Uncarrier-like, if the reports are accurate. And you can't take your business to Sprint, as it no longer exists thanks to its merger with T-Mobile. When that deal was finalized, T-Mobile said things would be ""better for customers,"" but constant activation charges would definitely not be better.Turn on browser notifications to receive breaking news alerts from Engadget You can disable notifications at any time in your settings menu. Not now Turned on Turn onWorse, it appears to be justifying the new fee in a dubious way, saying it's ""simplifying"" the system to bring a ""more consistent and straightforward experience for customers."" In other words, you'll no longer need to wonder if you'll get soaked for the charge or not — you definitely will. Engadget has reached out to T-Mobile to confirm the report's accuracy.",T-Mobile will start charging a $35 fee on all new activations and upgrades
347,-1,-1_new_said_study_people,https://www.engadget.com/scientists-find-affordable-way-break-down-pfas-185743322.html,"A team of scientists may have found a safe and affordable way to destroy “forever chemicals.” PFAS, or perfluoroalkyl and polyfluoroalkyl substances, are found in many household items, including non-stick Teflon pans and dental floss. According to the US Environmental Protection Agency , at least 12,000 such substances exist today. They all share one common feature between them: a carbon-fluorine backbone that is one of the strongest known bonds in organic chemistry. It’s what gives PFAS-treated cookware its non-stick quality. However, that same characteristic can make those substances harmful to humans.Since they’re so durable from a molecular perspective, PFAS can stay in soil and water for generations. Scientists have shown that prolonged exposure to them can lead to an increased risk of some cancers, reduced immunity and developmental effects on children. Researchers have spent years trying to find a way to destroy the carbon-fluorine bond that makes PFAS so stubborn, but a breakthrough could be in sight.In a study published Thursday in the journal Science , a group of chemists from UCLA, Northwestern University and China found that a mixture of sodium hydroxide, a chemical used in lye, and an organic solvent called dimethyl sulfoxide was effective at breaking down a large subgroup of PFAS known as perfluoro carboxylic acids or PFCAs. When lead author Brittany Trang heated the mixture between 175 and 250 degrees Fahrenheit (about 79 to 121 degrees Celsius), it began breaking down the bonds between the PFAS molecules. After a few days, the mixture can even reduce any fluorine byproducts into harmless molecules. The sodium hydroxide is part of what makes the mixture so potent. It bonds with PFAS molecules after the dimethyl sulfoxide softens them and hastens their breakdown.Turn on browser notifications to receive breaking news alerts from Engadget You can disable notifications at any time in your settings menu. Not now Turned on Turn on",Scientists may have found an affordable way to destroy forever chemicals
343,-1,-1_new_said_study_people,https://www.eenews.net/articles/global-emissions-targets-spell-growth-for-co2-tech-sector/,"Companies that produce technologies to remove or reduce carbon emissions are “poised for strong continued growth,” reaching an expected value of $1.4 trillion by 2027, according to new market research.The research report, recently released by the financial data firm PitchBook, predicts that the sector will be worth $905 billion by the end of this year. That makes the global climate tech sector relatively small — collectively worth less than electric vehicle maker Tesla.But PitchBook predicts that the emerging sector will enjoy an 8.8 percent growth rate over the next five years, “thanks to increasing global focus on aggressive emissions targets and consumer interest in emissions reduction.” That rate could also increase if there were “dramatic regulatory change or technological innovation” during that time, the report for investors said.AdvertisementThe sector PitchBook analyzed is a broad one. It includes startups that capture or trade carbon dioxide, industrial and building firms with products that are less emissions-intensive than conventional ones, and land management companies that use or produce monitoring tools or low-emissions fertilizers.The most valuable group of companies in the space are ones working to reduce planet-warming emissions during construction and over the lifetimes of buildings. They are currently worth almost $459 billion and are expected to increase in value to $650 billion over the next five years, according to the report.That segment includes incumbents like building materials producers Holcim Group and HeidelbergCement, as well as startups like green construction firm Veev and energy efficiency company Resideo Technologies.The segment PitchBook calls “green industry” is focused on decarbonizing industrial production of chemicals and raw materials. It’s valued at more than $400 billion, according to the report. Notable firms include lithium battery recycler Redwood Materials and the mining company Lilac Solutions.PitchBook expects green industry to be the most valuable climate tech segment by 2027, with an estimated value of $657 billion. For those projections to play out, however, emissions pricing legislation like the European Union’s carbon border adjustment mechanism “will be critical to ensure that those providing green chemicals and materials are not at a disadvantage against foreign high-emissions products,” the report said.The value of low-carbon land management companies is expected to reach $30 billion this year, increasing to $49.5 billion in five years. The segment includes major monitoring firms like Honeywell and the alternative fertilizer company Pivot Bio.Venture capital investment in land management firms has recently slowed, the report noted. But the Inflation Reduction Act, which President Joe Biden signed into law in August, included more than $20 billion to support climate-smart agriculture practices and $5 billion in grants for fire-resilient forests, urban tree planting and forest conservation.The smallest but fastest-growing segment of the sector is made up of carbon technology firms like the direct air capture pioneer Climeworks and the carbon trading platform Xpansiv. They’re now valued at around $9 billion and estimated to be worth $20.9 billion by 2027.The carbon tech sector will also get a boost from Biden’s new climate law, the report said. The value of carbon captured in the United States has increased from $50 per ton to $85 for CO2 removed from smokestacks. For direct air capture projects, which suck CO2 from the atmosphere, the price they earn per ton of carbon has jumped 250 percent, to $180.But it’s not all smooth sailing for carbon capture and trading firms.“The core risk facing the carbon tech space is in the potential for the value of carbon to shift dramatically due to changes in policy and regulation,” PitchBook said. “The product of carbon tech (removing or reducing carbon) does not directly provide value — outside of some value as a feedstock for carbon utilization — and fiscal value from carbon tech activities is heavily driven by the value that legislative incentives and schemes place on carbon.”Despite that risk, investors continue to be very enthusiastic about the potential for carbon capture firms. There were 11 venture capital deals in the second quarter of 2022, raising $882.2 million for Climeworks, Carbon Clean and other firms, according to PitchBook.“This far surpasses any prior quarter, with the total invested over the previous four quarters totaling only $432.1 million,” the research firm said in another investor report published last week.",Global emissions targets spell growth for CO2 tech sector
342,-1,-1_new_said_study_people,https://www.economist.com/business/2022/10/24/the-end-of-apples-affair-with-china,"B y a dusty stretch of the deafening road from Chennai to Bengaluru lie three colossal, anonymous buildings. Inside, away from the din of traffic, is a high-tech facility operated by Foxconn, a Taiwanese manufacturer. A short drive away Pegatron, another Taiwanese tech firm, has erected a vast new factory of its own. Salcomp, a Finnish gadget-maker, has set one up not far away. Farther west is a 500-acre campus run by Tata, an Indian conglomerate. What these closely guarded facilities have in common is their client: a demanding and secretive American firm known locally as “the fruit company”.Listen to this story. Enjoy more audio and podcasts on iOS or Android Your browser does not support the <audio> element. Listen to this story Save time by listening to our audio articles as you multitask OKThe mushrooming of factories in southern India marks a new chapter for the world’s biggest technology company. Apple’s extraordinarily successful past two decades—revenue up 70-fold, share price up 600-fold, a market value of $2.4trn—is partly the result of a big bet on China. Apple banked on China-based factories, which now churn out more than 90% of its products, and wooed Chinese consumers, who in some years contributed up to a quarter of its revenue. Yet economic and geopolitical shifts are forcing the company to begin a hurried decoupling. Its turn away from China marks a big shift for Apple, and is emblematic of an even bigger one for the world economy.Apple’s packaging proclaims “Designed by Apple in California”, but its gadgets are assembled along a supply chain that stretches from Amazonas to Zhejiang. At the centre is China, where 150 of Apple’s biggest suppliers operate production facilities. Tim Cook, who was Apple’s head of operations before he became chief executive in 2011, pioneered the firm’s approach to contract manufacturing. A regular visitor to China, Mr Cook has maintained good relations with the Chinese government, obeying its requirements to remove apps and to hold Chinese users’ data locally, where it is available to the authorities.Now a change is under way. Big tech is showing strains. On October 25th Alphabet and Microsoft presented disappointing quarterly results. Meta, which lost another fifth of its value after reporting the second straight quarter of declining sales, is a shadow of its former self. Apple’s latest earnings, due out after The Economist went to press on October 27th, may be dented by creaky Chinese supply chains and softening demand from Chinese consumers. So Mr Cook, who has not been seen in China since 2019, is wooing new partners. In May he entertained Vietnam’s prime minister, Pham Minh Chinh, at Apple’s futuristic headquarters. Next year Apple is expected to open its first physical store in India (whose prime minister, Narendra Modi, is a fan of gold iPhones).The two countries are the main beneficiaries of Apple’s strategic shift. In 2017 Apple listed 18 large suppliers in India and Vietnam; last year it had 37. In September, to much local fanfare, Apple started making its new iPhone 14 in India, where it had previously made only older models. The previous month it was reported that Apple would soon start making its MacBook laptops in Vietnam. Some of Apple’s newer gadgets show the way things are going. Almost half its AirPod earphones are made in Vietnam and by 2025 two-thirds will be, forecasts JPMorgan Chase. The bank reckons that, whereas today less than 5% of Apple’s products are made outside China, by 2025 the figure will be 25% (see chart 1). As Apple’s production system is shifting, its suppliers are diversifying away from China, too. One crude measure of this is the share of long-term assets that Taiwanese tech-hardware and electronics firms have located in China. In 2017 the average figure was 43%. Last year that had fallen to 31%, according to our estimates using company and Bloomberg data. The most urgent reason for the scramble is the need to spread operational risk. Two decades ago the garment industry beefed up its operations outside China after the sars epidemic paralysed supply chains. “ sars made it very clear to everyone operating in China that you needed a ‘China+1’ strategy,” observes Dominic Scriven of Dragon Capital, an investment firm in Vietnam. Covid taught tech firms the same lesson. Lockdowns in Shanghai in the spring temporarily shut a factory run by Quanta, a Taiwanese firm, believed to be making most of Apple’s MacBooks. Avoiding this kind of chaos is the “primary driving force” for Apple’s supply-chain moves, says Gokul Hariharan of JPMorgan Chase. Another motive is containing costs. Average wages in China have doubled in the past decade. By 2020 a Chinese manufacturing worker typically earned $530 a month, about twice as much as one in India or Vietnam, according to a survey by JETRO , a Japanese industry body. India’s ropey infrastructure, with bad roads and an unreliable electrical grid, held the country back. But it has improved, and the Indian government has sweetened the deal with subsidies. Vietnam offers tax rebates and holidays, too, as well as free-trade deals, including one recently signed with the eu . Bureaucracy around visas and customs remains a pain. But the work ethic is similar to that in China: “Confucius still gets them out of bed in the morning,” says one foreign executive in Vietnam.Apple also increasingly sees locals as potential customers, particularly in India, the world’s second-largest smartphone market. Though iGadgets are too pricey for most Indians, that is changing. Apple said in July that its revenues in India had nearly doubled in the past quarter, year on year, driven by the “engine” of iPhone sales.This is diminishing China’s relative importance as a consumer market. At its high point in 2015, China accounted for 25% of Apple’s annual sales, more than Europe. Since then its share has steadily shrunk, to 19% so far this financial year (see chart 2). By the sounds of it Xi Jinping, China’s president, would like it to fall further. At a Communist Party shindig on October 16th he urged “self-reliance and strength in science and technology”, suggesting that foreign importers may face stiffer competition from Chinese national champions. He repeated the phrase five times.An iWire actThis points to perhaps the biggest reason for Apple’s shift: geopolitics. Rising Sino-American tensions are making China an awkward place to do business. Heightened Chinese sensitivity is adding friction. This summer Apple reportedly had to ask Taiwanese manufacturers to label their products “Made in Chinese Taipei” to appease newly finicky Chinese customs officials (at the risk of angering Taiwanese ones).America, for its part, has become more aggressive in its competition with China’s domestic tech industry. On October 7th America announced a ban on “ us persons” working for some Chinese chipmakers. On the same day it added 30 Chinese companies to a list of “unverified” firms its officials had been unable to inspect. Apple had reportedly been about to sign a deal to buy iPhone memory chips from one such company, ymtc , which can offer low prices thanks in part to Chinese government subsidies. Following America’s export controls that deal was put on ice, according to Nikkei, a Japanese newspaper.The question is whether shifting production out of China will be enough to avoid future crackdowns. Even as Apple makes more of its gadgets outside China, it is no less reliant on Chinese-owned companies to build them. Chinese manufacturers such as Luxshare, Goertek and Wingtech are taking an increasing share of Apple’s business beyond China’s borders.Luxshare and Goertek are reported to be making AirPods in Vietnam, helped by the fact that some Taiwanese rivals, like Inventec, have scaled back their work for Apple in recent years. In September press reports hinted that the Indian government might let some Chinese companies set up production facilities in India. Chinese companies’ share of iPhone electronics production will rise from 7% this year to 24% by 2025, believes JPMorgan Chase, which predicts that in the next three years Chinese companies will increase their share of production across Apple’s range of products.Could Chinese manufacturers outside China be targeted by American sanctions? For now this is unlikely, believes Nana Li of Impax, an asset manager. “There are no handy alternative [suppliers] available with the same level of experience, efficiency and cost-effectiveness,” so cutting them off would hurt American firms, she notes. In time, that may change. Countries like India and Vietnam are keen to cultivate their own suppliers. Tata is reportedly in talks with Wistron, a Taiwanese manufacturer, about making iPhones in India. Indian firms report that “the fruit company” is discreetly on the hunt for local suppliers.Given the growing rift between America and China, it is sensible for Apple to place some side-bets, before restrictions go any further. Chinese firms outside China are safe for now, says one Western investor in Asia. But “the noose is tightening”. ■To stay on top of the biggest stories in business and technology, sign up to the Bottom Line, our weekly subscriber-only newsletter.",The end of Appleâs affair with China
340,-1,-1_new_said_study_people,https://www.digitaltrends.com/computing/nvidia-says-falling-gpu-prices-are-over/,"Nvidia has just confirmed what many of us were already suspecting — GPUs are expensive, and Nvidia plans to keep it that way.During a Q&A session with the media, Nvidia CEO Jensen Huang lifted the veil of suspense on RTX 40-Series pricing, and the insights are not what we’ve been hoping to hear.Nvidia has only just announced the GeForce RTX 4090, RTX 4080 16GB, and RTX 4080 12GB, but not everyone was happy. It’s not the capabilities of these cards that were called into question, but their pricing. The RTX 4090 will arrive with a $1,599 price tag, followed by $1,199 for the RTX 4080 16GB and $899 for the RTX 4080 12GB. These prices are too steep, all things considered, but it now seems that this might be the new normal.“The idea that the chip is going to go down in price is a story of the past.”During the Q&A session, Jensen Huang was asked about GPU prices. His response was very telling.“Moore’s Law is dead. […] A 12-inch wafer is a lot more expensive today. The idea that the chip is going to go down in price is a story of the past,” said Nvidia CEO Jensen Huang in a response to PC World’s Gordon Ung.Moore’s Law is the idea that there’s a trend between PC performance and price, with roughly double the performance for half the price every two years. Huang cited the rising costs of components and slowing of additional power as driving forces behind high GPU prices.Instead of Moore’s Law, Huang focused on price points and generational improvement. “The performance of Nvidia’s $899 GPU or $1,599 GPU a year ago, two years ago, at the same price point, our performance with Ada Lovelace is monumentally better. Off the charts better.”The response seems to echo what many of us have already been suspecting. After the GPU shortage has subsided, Nvidia and its partners were left with an oversupply of graphics cards. Once terribly overpriced, these GPUs are now up for grabs at more reasonable prices, but they’re most likely not selling as quickly as the manufacturers might have hoped.“The 3080 was, and still is, great value, and it will continue to live on,” an Nvidia spokesperson said in another briefing, noting that it was far from dead.The introduction of the RTX 40-Series steals the thunder from RTX 30-Series, but Nvidia still wants to sell off these older (but still very good) cards. It makes sense to price the RTX 4090 and the two RTX 4080s so high because this might push more people to buy one of the RTX 30 GPUs instead. Be that as it may, it’s sad to hear a confirmation that the prices will continue following an upward trend.Editors' Recommendations",Nvidia says falling GPU prices are âa story of the pastâ
339,-1,-1_new_said_study_people,https://www.columbiapsychiatry.org/news/columbia-study-finds-mass-school-shootings-are-not-caused-mental-illness,"A research team at Columbia University Irving Medical Center and the New York State Psychiatric Institute (NYSPI) examining 82 mass murders that occurred at least partially in academic settings throughout the world found that most mass murderers and mass shooters did not have severe psychiatric illnesses.The study, led by Ragy R. Girgis, MD, found that 100% of the mass killings were initiated by males (mean age 28) of whom 66.7% were Caucasian. Sixty-three percent of the murders involved firearms. While severe mental illnesses, such as psychotic disorders, including schizophrenia, were not present in the perpetrators of these events, it is notable that almost half of these mass shooters took their own lives at the scene, leading the authors to hypothesize that these perpetrators viewed themselves as engaging in some form of “final act.”The research, published online Oct. 27 in the Journal of Forensic Sciences, according to study authors, is the largest analysis ever conducted on mass school shootings.“Our findings suggest that mass school shootings are different from other forms of mass murder and that they should be looked at as a distinct phenomenon,” said Dr. Girgis, director of the Center of Prevention and Evaluation (COPE), a research clinic at Columbia/NYSPI specializing in the study and treatment of young adults at high risk for schizophrenia and other psychoses. “To prevent future mass school shootings, we need to begin to focus on the cultural and social drivers of these types of events, such as the romanticization of guns and gun violence, rather than on individual predictors.”To conduct their study, the researchers analyzed data from the Columbia Mass Murder Database (CMMD), developed by the COPE team to gain much-needed insight into the relationship between serious mental illness and mass shootings. Creating the CMMD involved extensive review of 14,785 murders publicly described in English in print or online, occurring worldwide between 1900 and 2019.For the mass school shooting study, the researchers isolated cases of mass murder perpetrated at least in part at schools, colleges, and universities and categorized them by location (within or outside of the US), and whether firearms were used.Of the 82 incidents of mass murder involving academic settings:Nearly half (47.6%) were U.S.-based.Most involved firearms (63.2%), commonly semi- or fully-automatics.Consistent with previous reports, perpetrators of mass shootings involving academic settings are primarily Caucasian (66.7%) and male (100%).Severe mental illness (e.g., psychosis) was absent in the majority of perpetrators; when present, psychotic symptoms were more often associated with mass murders involving means other than firearms.About half (45.6%) of mass school shootings ended with the perpetrator's suicide.Coauthor Paul S. Appelbaum, MD, the Elizabeth K. Dollard Professor of Psychiatry, Medicine and Law at Columbia, said that identifying psychiatric illness as a primary cause of violence is misleading.“The findings strongly suggest that focusing on mental illness, particularly psychotic illness, when talking about mass school shootings risks is missing other factors that contribute to the vast majority of cases, as well as exacerbating the already widespread stigma surrounding severe mental illness,” said Dr. Appelbaum.The researchers hope that the findings will help lawmakers and law enforcement officials better understand the phenomenon of mass school shootings, as well as how mass school shootings differ from other forms of mass murder, and ways to identify youth who may be troubled though not necessarily schizophrenic or psychotic. The authors also emphasize that these data cannot be used to predict behavior on an individual level.",Columbia Study Finds Mass School Shootings Not Caused by Mental Illness
332,-1,-1_new_said_study_people,https://www.cnet.com/google-amp/news/nasas-webb-space-telescope-is-so-good-we-might-need-improved-planetary-models/,"This telescope is producing impeccable results, but do our models match its excellence?It has become exceedingly clear, over the past few months, that NASA's James Webb Space Telescope does exactly what it set out to do. Just as its creators had hoped, the multibillion-dollar machine is flawlessly ""unfolding the universe"" by revealing cosmic light we cannot see with our own eyes -- and its excellent results make even the most unlikely of stargazers feel alive.Because of this gold-plated telescope, Twitter went wild one day over a bleary red dot. For 48 hours, people worldwide were gawking at a galaxy born shortly after the birth of time itself. It would appear that, thanks to the technological prowess of the JWST, humanity stands united over stardust.Get the CNET Now newsletter Spice up your small talk with the latest tech news, products and reviews. Delivered on weekdays. Yes, I also want to receive the CNET Insider newsletter, keeping me up to date with all things CNET. Subscribe By signing up, you agree to our Terms of Use and acknowledge the data practices in our Privacy Policy. You may unsubscribe at any time. Thanks for signing up! Personalize my inbox An error occurred. Please check your email and try againBut here's the thing.Amid personal awe, scientists from the Massachusetts Institute of Technology warn that we ought to consider one crucial scientific consequence of having a superhero telescope.If the JWST is like a zero-to-100 'scope upgrade, they wonder, is it possible our science models need a zero-to-100 reboot, too? Are the datasets scientists have been using for decades unable to match the device's power and therefore falling short in revealing what it's trying to tell us?""The data we will be getting from the JWST will be incredible, but ... our insights will be limited if our models don't match it in quality,"" Clara Sousa-Silva, a quantum astrochemist at the Center for Astrophysics, Harvard & Smithsonian, told CNET.And, according to a new study of which she's a co-author, published Thursday in the journal Nature Astronomy, the answer is yes.More specifically, this paper suggests some of the light-parsing tools scientists normally use to understand exoplanet atmospheres aren't totally equipped to deal with the JWST's exceptional light data. In the long run, such a hindrance may impact the most massive JWST quest of all: the hunt for extraterrestrial life.""Currently, the model we use to decrypt spectral information is not up to par with the precision and quality of data we have from the James Webb telescope,"" Prajwal Niraula, graduate student at MIT's department of Earth, atmospheric and planetary sciences and co-author of the study, said in a statement. ""We need to up our game.""NASAHere's one way to think about the conundrum.Imagine pairing the newest, most powerful Xbox console with the very first iteration of a TV. (Yes, I know the extreme hypothetical nature of my scenario). The Xbox would be trying to give the TV awesome high-resolution, colorful, beautiful graphics to show us -- but the TV wouldn't have the capacity to compute any of it.I wouldn't be surprised if the TV straight up exploded. But the point is you wouldn't know what the Xbox is trying to provide for you, unless you get an equally high-res TV.Similarly, in the vein of exoplanet discoveries, scientists feed a bunch of deep-space light, or photon, data into models that test for ""opacity."" Opacity measures how easily photons pass through a material and differs depending on things like light wavelength, material temperature and pressure.This means every such interaction leaves behind a telltale signature of what the photon's properties are, and therefore, when it comes to exoplanets, what kind of chemical atmosphere those photons passed through to get to the light detector. That's how scientists sort of reverse-calculate, from light data, what an exoplanet's atmosphere is composed of.In this case, the detector liaison lies on the James Webb Space Telescope -- but in the team's new study, after putting the most commonly used opacity model to the test, the researchers saw JWST light data hitting what they call an ""accuracy wall.""The model wasn't sensitive enough to parse stuff like whether a planet has an atmospheric temperature of 300 or 600 Kelvin, the researchers say, or whether a certain gas takes up 5% or 25% of the atmosphere. Such a difference is not only statistically significant, but per Niraula, also ""matters in order for us to constrain planetary formation mechanisms and reliably identify biosignatures.""That is, evidence of alien life.""We need to work on our interpretive tools,"" Sousa-Silva said, ""so that we don't find ourselves seeing something amazing through JWST but not knowing how to interpret it.""T. Treu/GLASS-JWST/NASA/CSA/ESA/STScIFurther, the team also found its models kind of disguising its uncertain readings. A few adjustments can easily paper over uncertainty, deeming results a good fit when they're incorrect.""We found that there are enough parameters to tweak, even with a wrong model, to still get a good fit, meaning you wouldn't know that your model is wrong and what it's telling you is wrong,"" Julien de Wit, assistant professor at MIT's EAPS and study co-author, said in a statement.Going forward, the team urges opacity models be improved to accommodate our spectacular JWST revelations – especially calling for crossover studies between astronomy and spectroscopy.""There is so much that could be done if we knew perfectly how light and matter interact,"" Niraula says. ""We know that well enough around the Earth's conditions, but as soon as we move to different types of atmospheres, things change, and that's a lot of data, with increasing quality, that we risk misinterpreting.""De Wit compares the current opacity model to the ancient language translation tool the Rosetta Stone, explaining that so far, this Rosetta Stone has been doing OK, such as with the Hubble Space Telescope.""But now that we're going to the next level with Webb's precision,"" the researcher said, ""our translation process will prevent us from catching important subtleties, such as those making the difference between a planet being habitable or not.""As Sousa-Silva puts it, ""it's a call to improve our models, so that we will not miss the subtleties of data.""","NASA's Webb Space Telescope Is So Good, We Might Need Improved Planetary Models"
321,-1,-1_new_said_study_people,https://www.cnbc.com/2022/09/27/chipotle-mexican-grill-will-test-robotic-tortilla-chip-maker-.html,"Chipotle Mexican Grill is moving one step closer to having a robot make its tortilla chips.The burrito chain said Tuesday that it will test ""Chippy,"" an autonomous kitchen assistant made by Miso Robotics, next month in a restaurant in Fountain Valley, California. Chipotle has already tested Chippy's ability to make and season its tortilla chips with salt and lime at its headquarters' innovation hub in Irvine, California.Like the rest of its new tech and menu items, the company is relying on its ""stage-gate process"" to test and learn from workers and customers to decide how to roll out the technology nationwide. Today, workers at Chipotle restaurants fry and season the chips, which can be time consuming.Restaurants and retailers have been testing robotics and automation to speed up operations and reduce menial tasks for workers. Starbucks recently unveiled new systems for more efficiently making cold coffee drinks, brewing drip coffee and serving food. Elsewhere, Panera Bread and McDonald's have been testing automated drive-thru ordering to cut down service times, while White Castle and Buffalo Wild Wings also are testing Miso Robotics' technology.In addition to the Chippy restaurant test, Chipotle said it's piloting a new kitchen management system that uses machine learning to predict demand for its ingredients in order to improve freshness and minimize food waste. The system designed by PreciTaste is being tested at eight restaurants in Orange County.And in Cleveland, the company said 73 of its restaurants are piloting location-based technology to improve its mobile app. The program is meant to help customers and delivery drivers know when orders are ready, if they're at the wrong location and to scan loyalty QR codes. The technology created by Flybuy by Radius Networks is also being used by retailers like Harris Teeter, Albertsons and Vineyard Vines.",Chipotle Mexican Grill will test robotic tortilla chip maker 'Chippy' in California restaurant
320,-1,-1_new_said_study_people,https://www.cnbc.com/2022/09/26/feds-commit-50-million-to-for-profit-nuclear-fusion-companies.html,"A picture shows the winding facility for the construction of poloidal field coils which will be part of the magnetic system that will contribute to confine and model plasma during the launch of the assembly stage of nuclear fusion machine ""Tokamak"" of the International Thermonuclear Experimental Reactor (ITER) in Saint-Paul-les-Durance, southeastern France, on July 28, 2020. - Thirty-five nations are collaborating in the ITER energy project aimed at mastering energy production from hydrogen fusion, as in the heart of the sun, a potential new source of carbon-free and non-polluting energy.The United States government is putting a sizable amount of money behind private sector nuclear fusion companies for the first time in the latest sign of how momentum is building behind the ""holy grail"" of clean energy.At the Global Clean Energy Action Forum in Pittsburgh on Thursday, the Department of Energy officially announced $50 million will go toward private fusion companies in public-private partnerships.""This money signifies that the U.S. government is getting serious about building a fusion program that will have commercial significance on an accelerated timeframe,"" Andrew Holland, the CEO of the Fusion Industry Association, an industry trade group, told CNBC.""If the U.S. government puts its full weight behind accelerating fusion energy to the grid, it could bring a transformational new energy source to the U.S.,"" Holland told CNBC.Conventional nuclear reactors are based on nuclear fission, a process in which a neutron slams into a large atom and splits it, releasing energy. Nuclear fusion occurs when two heavier atoms slam together to form a heavier atom — the way stars are powered. It is often seen as the holy grail of clean energy, because it offers virtually unlimited energy, releases no greenhouse gasses and generates no long-lasting nuclear waste. But it's proven very difficult to duplicate the process safely in a way that can be scaled and commercialized.The U.S. government has put federal money into fusion research since the 1950s and today invests about $700 million per year. Holland told CNBC. But that money has mostly gone toward national labs and universities and toward the primary international research project in France, ITER.But the $50 million announced in Pittsburgh for private fusion companies ""is the first substantial investment by the U.S. government into private sector fusion-energy companies,"" Holland told CNBC.""This is not for pure science. This is a commercial development and deployment program,"" Holland told CNBC.The $50 million will help companies prepare detailed plans, but isn't sufficient funding to construct expensive fusion power plants. Nevertheless, it will help bolster and give U.S fusion companies credibility.""This is critical since fusion power is such an audacious but vital technology for the United States and our collective fight against climate change. We want a U.S. firm to be the first to reach net power,"" Matthew Moynihan, a nuclear fusion consultant, told CNBC.""Net power"" refers to a key threshold in the fusion industry whereby more power is generated than it takes to catalyze the reaction. ""This is also more than just a paycheck: Winning this funding will give firms the government's stamp of approval, something investors will want to see as they consider adding more money to the industry,"" Moynihan told CNBC.The private sector fusion industry has attracted almost $5 billion in venture capital and other funding, according to the Fusion Industry Association.Notable recent raises include $1.8 billion in funding from Commonwealth Fusion Systems, a spinoff from Massachusetts Institute of Technology research, from a slew of heavy-hitting investors including Bill Gates, John Doerr, Salesforce co-CEO Marc Benioff's Time Ventures, and Google . Another private fusion company, Helion, announced a $500 million raise led by Silicon Valley insider Sam Altman and which includes the potential for another $1.7 billion in funding depending on Helion meeting particular funding goals.While the program is currently funded at $50 million over the next 18 months, Congress has authorized spending as much as $415 million in future budgets. The public-private funding program was first authorized in the Energy Act of 2020.","Feds commit $50 million to for-profit nuclear fusion companies, chasing the 'holy grail' of clean energy"
318,-1,-1_new_said_study_people,https://www.cnbc.com/2022/09/17/china-testing-floating-car-that-uses-magnets-to-hover-at-143-mph.html,"If you've ever imagined a future filled with flying cars, your dream might be getting slightly closer to reality.Chinese researchers at Southwest Jiaotong University in Chengdu, Sichuan province, performed road tests last week for modified passenger cars that use magnets to float 35 millimeters above a conductor rail, according to Chinese state news agency Xinhua.The researchers outfitted the sedans with powerful magnets on the vehicle floors, allowing them to levitate over a conductor rail nearly five miles in length. Eight cars in total were tested, with one test reaching speeds of roughly 143 miles per hour, according to the report.A video posted to Twitter by a Chinese journalist shows the vehicles floating — albeit bumpily — along the track:Xinhua says the tests were run by government transportation authorities to study safety measures for high-speed driving. But Deng Zigang, one of the university professors who developed the vehicles, told the state news agency that using magnetic levitation for passenger vehicles has the potential to reduce energy usage and increase the vehicles' range.That could be useful for the electric vehicle industry's issues with ""range anxiety,"" or when consumers fear they won't be able to complete a trip in an electric vehicle without running out of power.",China is testing a magnet-powered floating car that goes up to 143 miles per hourâtake a look
313,-1,-1_new_said_study_people,https://www.cbsnews.com/miami/news/scientists-warn-south-florida-coastal-cities-will-be-affected-by-sea-level-rise/,"MIAMI - Sea level rise is increasing at a dramatic rate. Scientists at the University of Miami warn that if we don't act soon, coastal cities and towns will slowly diminish.Scientists say a few decades from now, downtown Miami will be underwater.The tide is coming in and eventually it's not going to go back out,"" says Dr. Harold Wanless, a Geologist and Professor of Geography and Sustainable Development at University of Miami""Climate change is real. This isn't something that might happen,"" he says. ""The problem is, sea level is rising at an accelerating rate now because of ice melting in Greenland and Antarctica. So for now what is just a high tide - a rare high tide.. is going to become a frequent high tide,"" he says.So what does that mean for us? According to Dr. Wanelss's research, by the year 2060, nearly 60% of Miami-Dade county will be underwater.""Now since the ice melt started we're up to a rate of almost two feet per century,"" he says.Scientists say greenhouse gases like methane, nitrous oxide and carbon dioxide are the root of the problem.""Carbon dioxide, that's the main one. It's the one that every time you burn oil, gas, coal, wood.. you're taking carbon that was stored in the earth and you're converting it to CO2. Over 90% of the extra heat we've created is transferred to the ocean. So global warming is really about warming the ocean,"" he explains.Wanless say not only do we have to stop putting greenhouse gases into the atmosphere, we have to remove the gases already out there. Otherwise, he says, our coastal paradise won't last forever.""It's a beautiful place to live right now but it is so vulnerable,"" he adds.Get Outlook for iOS",Scientists warn South Florida coastal cities will be affected by sea level rise
361,-1,-1_new_said_study_people,https://www.eurekalert.org/news-releases/968551,"Making policies fully liberal in all U.S. states could have saved hundreds of thousands of lives in recent yearsState policies in eight different policy domains, including gun safety, labor and tobacco, are associated with U.S. working-age mortality, according to a new study published this week in the open-access journal PLOS ONE by Jennifer Karas Montez of Syracuse University, US, and colleagues. They note that more conservative state policies were generally associated with higher mortality.Americans die younger than people in most other high-income countries and within the United States life expectancy differs markedly across geographic areas. In 2019, it ranged from 74.4 years in Mississippi to 80.9 years in Hawaii.In the new work, the researchers used data from the 1999-2019 National Vital Statistics System to calculate state-level age-adjusted mortality rates for deaths from all causes and from cardiovascular disease (CVD), alcohol, suicide and drug poisoning among adults ages 25 to 64. They merged that data with annual state-level data on eight policy domains, where each state’s policies were scored on a 0-to-1 conservative-to-liberal continuum.The analysis revealed that more liberal policies on the environment, gun safety, labor, economic taxes and tobacco taxes were associated with lower mortality in each state. However, for marijuana, more conservative policies were associated with lower mortality. Particularly strong associations were found between gun safety policies and suicide mortality among men; between labor policies and alcohol-induced mortality; and between economic and tobacco tax policies and cardiovascular disease mortality. Simulations suggested that changing all policies in all states to a fully liberal orientation could have saved 171,030 lives in 2019, while changing them to a fully conservative orientation may have cost 217,635 lives.The authors conclude that the emergence of more conservative state policies in several domains and shifts in the share of the population living in states with these policies provide a partial explanation for the high and rising mortality among working-age Americans and the overall mortality disadvantage of the US compared to other high-income countries.The authors add: “U.S. state policies in recent decades may have contributed to the high and rising mortality rates of working-age adults. Changing state policies could prevent thousands of deaths every year from cardiovascular disease, suicide, alcohol, and drug poisoning.”#####In your coverage please use this URL to provide access to the freely available article in PLOS ONE: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0275466Citation: Montez JK, Mehri N, Monnat SM, Beckfield J, Chapman D, Grumbach JM, et al. (2022) U.S. state policy contexts and mortality of working-age adults. PLoS ONE 17(10): e0275466. https://doi.org/10.1371/journal.pone.0275466Author Countries: USAFunding: This article was supported by a grant from the National Institute on Aging to JKM (grant R01AG05581). www.nih.nia.gov. The funder had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.",Conservative state policies generally associated with higher mortality
310,-1,-1_new_said_study_people,https://www.cambridge.org/core/journals/journal-of-public-policy/article/hidden-homeownership-welfare-state-an-international-longterm-perspective-on-the-tax-treatment-of-homeowners/6B29E4D84EEB28E9BEAC308ABAD3211B,"Tax treatment of the owner-occupied housing Most countries foster homeownership in one way or another. Traditionally, homeownership policies are rather found in the manifestos of conservative parties and are particularly pronounced in Anglophone countries (Schelkle Reference Schelkle2012). In German-speaking countries, by contrast, Social Democrats in particular were rather skeptical about homeownership subsidies and either introduced it quite late in their party manifestos or did not consider it as a central objective of their housing policy (Kohl Reference Kohl2020). The political parties that propose homeownership subsidies not only for reasons of housing provision but also for reasons of fostering equality, secure wealth and stable democracies (Arundel and Ronald Reference Arundel and Ronald2021). This translated into a number of different subsidy schemes. On the one hand, homeownership can be promoted through direct subsidies. In countries with a tradition of “socialised homeownership” such as Iceland or Ireland, future homeowners have been eligible to subsidised loans or government transfers (Sveinsson Reference Sveinsson2000). In Germany, for example, these are housing construction bonuses (Wohnungsbauprämie) and family housing grants (Baukindergeld) (Kohlhase Reference Kohlhase2011). On the other hand, homeownership can be stimulated through the taxation system. In this study, we will focus on this second element of subsidy policies. In the literature, the following four types of instruments are mainly considered: taxes on imputed rent, interest relief on mortgage repayments, capital gains tax on housing, and the VAT on new dwellings (Haffner Reference Haffner1992; van Weesep and van Velzen Reference van Weesep and van Velzen1995; MacLennan et al. Reference MacLennan, Muellbauer and Stephens1998; Stephens Reference Stephens2003; Wolswijk Reference Wolswijk2009; Figari et al. Reference Figari, Paulus, Sutherland, Tsakloglou, Verbist and Zantomio2012). Below, we introduce each tax and exemptions in turn. Footnote 1 Imputed rent tax Taxes on imputed rent must be paid by the owner for the dwelling he occupies. This is justified by the fact that homeowners, unlike renters, do not pay any rent and therefore have an additional source of income. Especially, if at the same time the mortgage interest can be deducted from the income tax (see below), a bias in favour of homeowners emerges. The tax on imputed rent is aimed at restoring tax neutrality. In order to evaluate the amount of unpaid rent, fiscal authorities estimate a monetary use value of owner-occupied dwelling. The tax is expected to reduce the formation of homeownership. On the other hand, since the use value of housing can be considered as an additional income, the failure to collect such a tax would mean an unequal treatment of other types of income and, hence, a stimulation of the homeownership. The collection of this tax is often complicated, because the use value is difficult to assess correctly. Moreover, the absence of a tax on imputed rent represents a subsidy, which does not discriminate between the newly built and existing housing. Thus, everyone occupying one’s own dwelling can benefit from it. Tax deductibility of mortgage payments The possibility to deduct mortgage interests goes often hand in hand with the tax on imputed rents. It follows from the logic that the cost incurred to obtain an additional income (nonpayment of rent) must be deductible. In some countries, the possibility of mortgage interest relief exists even in the absence of an imputed rent tax. The interest deductibility makes the purchase of a home more attractive. However, this can generate the risk of speculative price bubbles. Capital gains tax This tax is imposed in cases when the owner makes profits resulting from the positive difference between the selling price of a dwelling and its purchasing price, provided that this difference cannot be entirely related to the improvements made to the dwelling. The capital gains tax tends to make the purchase of housing less attractive. One of the disadvantages of homeownership compared to renting is its reduced flexibility and mobility. Typically, it takes more time to sell an owner-occupied home than to terminate a rental contract. The absence of a capital gains tax could compensate for such a disadvantage and eventually make it more attractive for renters to become homeowners. On the other hand, it could create incentives for speculating with housing, since the absence of the capital gains tax for housing would make it more attractive than other assets (e.g., shares), which are subject to such a tax. This could stimulate the formation of speculative house price bubbles and, hence, make it more difficult for the low- and middle-income households to purchase homes. Therefore, the capital gains tax on housing is sometimes conceived as a speculation tax from which the owners, who really occupy their dwellings, are exempted. Being a speculative tax, the capital gains tax imposes as a rule a minimum holding period. It means that the real estate must be kept by the owner for a certain time period until it is exempted from the taxation. Table A5 reports different exemptions from the capital gains tax for each country. The capital gains tax is assumed to be applicable regardless of the holding period, save for the cases, where the owner-occupier is explicitly exempted from the tax. VAT on the new dwellings The VAT on newly built dwellings is added to the purchasing price of a dwelling offered for sale. As a result, housing becomes more expensive and less attractive to buy. At the same time, exactly as in the case of the imputed rent tax, the VAT for new dwellings allows treating housing similar to other goods, which are subject to VAT. Hence, the absence of the VAT on housing can be considered as a subsidy. Unlike the absence of the tax on imputed rent, the absence of the VAT stimulates the construction of new dwellings.Quantification of taxation attractiveness and tenure neutrality In order to assess the impact of these forms of housing taxation, they have to be measured in numeric terms. The coding of regulations is a difficult task, since it has to strike a balance between capturing the essence of legal acts and producing interpretable and objective indices. Surely, the regulations are very complex and trying to mimic them in a detailed way would make their quantification infeasible. Therefore, certain simplifying assumptions must be made in order to render the task tractable. We therefore only account for the existence of taxes not for their rates or application sphere. Leximetric approach to taxation policies Here, we apply the methodology, which is known as leximetrics, used since at least the early 1990s to measure the intensity of governmental regulations. Leximetrics is employed in a large variety of areas of economics, such as labour markets, finance, shareholder protection, and housing. Footnote 2 There are already several studies examining homeownership taxation (e.g. Wolswijk Reference Wolswijk2009; Figari et al. Reference Figari, Paulus, Sutherland, Tsakloglou, Verbist and Zantomio2012). However, none of them intends to quantify the regulations. The first researcher to quantify the housing ownership policies was Atterhög (Reference Atterhög2005). Based on expert surveys conducted in 18 countries, he built six indices (direct grants for buying, other subsidies, mortgage deduction, grant tax deduction, low property tax, and homeownership allowances) covering the period between 1970 and 2000 at decade frequency. His indices vary between 0 (no support) and 5 (very generous support). Thus, our databases partly overlap (countries, periods, and policies). However, our data have annual frequency, are based on regulation and not expert opinion, and cover a much longer period. In addition, our database and that of Atterhög share only one common policy index – the mortgage deduction. Barrios et al. (Reference Barrios, Denis, Ivaskaite-Tamosiune, Reut and Vazquez Torres2019), on the other hand, consider five homeownership policy indicators (transfer taxes, recurrent property taxes, capital gains taxes, imputed rent taxation, and mortgage interest deduction) to show the distortions for households decisions by computing the cost of owner-occupied housing. Their policy indicators are similar to the ones that we use in this study, except for their additional implicit recurrent property taxes. Another difference is that they measure transfer and capital gains taxation in absolute terms rather than as a dummy variable. Their sample comprises 28 EU countries between 1995 and 2017. While these approaches examine the existence or magnitude of housing taxation policies, Seelkopf et al. (Reference Seelkopf, Bubek, Eihmanis, Ganderson, Limberg, Mnaili, Zuluaga and Genschel2021) take another dimension of taxation policies into account, which is the year of introduction of the corresponding tax. They achieve this by constructing a new data set containing the year and mode of introduction of six key modern taxes (personal income tax, corporate income tax, social security contributions, inheritance tax, general sales tax, and VAT) in 220 countries between 1750 and 2018. While this is a useful database for the introduction of general tax codes, it is not specific enough for the subdomain of housing and homeownership. Our approach In this study, we follow the approach suggested by Kholodilin (Reference Kholodilin2020), who measures the intensity of rental market regulations worldwide over a long period. First, we conduct an overview of the relevant legislation pieces in order to extract information concerning the tax treatment of owner-occupied dwellings. Footnote 3 Second, for each of the four taxation types discussed above, a binary index is constructed that equals one, if regulation is more favourable with respect to homeowners, and zero, otherwise: (1) $${I_{jt}} = \left\{ {\matrix{ {1,\quad {\rm{if}}\;{\rm{taxation}}\;{\rm{of}}\;{\rm{type}}\;j\;{\rm{is}}\;{\rm{favorable}}\;{\rm{to}}\;{\rm{homeowners}}\;{\rm{in}}\;{\rm{period}}{\mkern 1mu} {\mkern 1mu} t} \hfill \\ {0,\quad {\rm{otherwise}}} \hfill } } \right.$$ Thus, the binary indices for the imputed rent tax, capital gains tax, and VAT are equal to 1, when homeowners are not subject to these taxes, while the binary index for interest deductibility is equal to 1, when such an option is provided to homeowners. When a regulation exists (such as capital gains taxation), but subject to major exemptions (e.g. tax exemptions for certain holding periods), we consider the regulation to not be in place (cf. Table A5 in Supplementary material for a detailed list of exemptions). The resulting binary indices are plotted in Figures 1, 2, 3, and 4 as shaded areas. Each horizontal bar corresponds to an individual country. The darker shades of grey correspond to regulations that are more beneficial for homeowners. Yellow colour denotes missing observations. In addition and to reduce descriptive complexity, we compute a composite homeowneship taxation attractiveness index as a simple average of binary variables: (2) $$HOT{A_t} = {1 \over J}\sum\limits_{j = 1}^J {{I_{jt}}} $$ where $J = 4$ is the number of individual binary taxation indices. Hence, the index can vary between 0 and 1. The higher its values, the more favourable the housing taxation for homeowners. The indices of homeownership tax attractiveness cover 37 countries in total which reflects our attempt to cover the economically most important OECD and a dozen of non-OECD countries, where data were available and accessible. Figure 5 shows their geographical distribution in 2020. Again, the shades of grey depict the degree of attractiveness of taxation, while yellow denotes countries for which no such information is available. The composite indices for individual countries are displayed in Figure 6. Finally, we also compute the degree of neutrality of homeownership taxation with respect to the housing tenure. If the tax treatment is more favourable towards homeowners, then ceteris paribus it can create an additional incentive for people to choose owning over renting. The taxation neutrality is defined through the following two cases: either the imputed rent tax is absent and mortgage payments are not deductible or the imputed rent tax is levied and mortgage deductions are allowed. (3) $$TN{I_t} = {{I\,_t^{\rm imputed\ rent\ tax} + I\, _t^{\rm mortgage\ deductibility} - 1} \over 2}$$ Thus, the value of this index corresponding to the taxation neutrality will be equal 0. When it is below zero, taxation is biased towards renters, while when it is positive, it is biased towards homeowners.",The hidden homeownership welfare state: an international long-term perspective on the tax treatment of homeowners
362,-1,-1_new_said_study_people,https://www.eurekalert.org/news-releases/969038,"Two minute bursts of vigorous activity totalling 15 minutes a week are associated with a reduced risk of death, according to research published today in European Heart Journal, a journal of the European Society of CardiologySophia Antipolis, 28 October 2022: Two minute bursts of vigorous activity totalling 15 minutes a week are associated with a reduced risk of death, according to research published today in European Heart Journal, a journal of the European Society of Cardiology (ESC).1“The results indicate that accumulating vigorous activity in short bouts across the week can help us live longer,” said study author Dr. Matthew N. Ahmadi of the University of Sydney, Australia. “Given that lack of time is the most commonly reported barrier to regular physical activity, accruing small amounts sporadically during the day may be a particularly attractive option for busy people.”A second study, also published today in EHJ, found that for a given amount of physical activity, increasing the intensity was associated with a reduced likelihood of cardiovascular disease.2 “Our study shows that it’s not just the amount of activity, but also the intensity, that is important for cardiovascular health,” said study author Dr. Paddy C. Dempsey of the University of Leicester and University of Cambridge, UK, and the Baker Heart and Diabetes Institute, Melbourne, Australia.Both studies included adults aged 40 to 69 years from the UK Biobank. Participants wore an activity tracker on their wrist for seven consecutive days. This is an objective way to measure motion, and particularly sporadic activity of different intensities during the day.The first study enrolled 71,893 adults without cardiovascular disease or cancer. The median age was 62.5 years and 56% were women. The investigators measured the total amount of weekly vigorous activity and the frequency of bouts lasting two minutes or less. Participants were followed for an average of 6.9 years. The investigators analysed the associations of volume and frequency of vigorous activity with death (all-cause, cardiovascular disease and cancer) and incidence of cardiovascular disease and cancer after excluding events occurring in the first year.The risk of all five adverse outcomes reduced as the volume and frequency of vigorous activity increased, with benefits seen even with small amounts. For example, participants with no vigorous activity had a 4% risk of dying within five years. Risk was halved to 2% with less than 10 minutes of weekly vigorous activity, and fell to 1% with 60 minutes or more.Compared with just two minutes of vigorous activity per week, 15 minutes was associated with an 18% lower risk of death and a 15% lower likelihood of cardiovascular disease, while 12 minutes was associated with a 17% reduced risk of cancer. Further gains were observed with greater amounts of vigorous activity. For instance, approximately 53 minutes a week was associated with a 36% lower risk of death from any cause.Regarding frequency, accumulating short bouts (up to two minutes) of vigorous activity on average four times a day was associated with a 27% lower risk of death. But health benefits were observed at even lower frequencies: 10 short bouts a week was associated with 16% and 17% lower risks of cardiovascular disease and cancer, respectively.The second study included 88,412 adults free of cardiovascular disease. The average age was 62 years and 58% were women. The investigators estimated the volume and intensity of physical activity, then analysed their associations with incident cardiovascular disease (ischaemic heart disease or cerebrovascular disease). Participants were followed for a median 6.8 years.The researchers found that both higher amounts and greater intensity were associated with lower rates of incident cardiovascular disease. Increasing the intensity led to greater reductions in cardiovascular disease for the same volume of exercise. For example, the rate of cardiovascular disease was 14% lower when moderate-to-vigorous activity accounted for 20% rather than 10% of activity, the equivalent of converting a 14 minute stroll into a brisk seven minute walk.Dr. Dempsey said: “Our results suggest that increasing the total volume of physical activity is not the only way to reduce the likelihood of developing cardiovascular disease. Raising the intensity was also particularly important, while increasing both was optimal. This indicates that boosting the intensity of activities you already do is good for heart health. For example, picking up the pace on your daily walk to the bus stop or completing household chores more quickly.”ENDSAuthors: ESC Press OfficeMobile: +33 (0)7 8531 2036Email: press@escardio.orgFollow us on Twitter @ESCardioNewsFunding: Please see the papers.Disclosures: Please see the papers.NotesReferences1Ahmadi MN, Clare PJ, Katzmarzyk PT, et al. Vigorous physical activity, incident heart disease, and cancer: how little is enough? Eur Heart J. 2022. doi:10.1093/eurheartj/ehac572.Link will go live on publication:https://academic.oup.com/eurheartj/article-lookup/doi/10.1093/eurheartj/ehac5722Dempsey PC, Rowlands AV, Strain T, et al. Physical activity volume, intensity and incident cardiovascular disease. Eur Heart J. 2022. doi:10.1093/eurheartj/ehac613.Link will go live on publication:https://academic.oup.com/eurheartj/article-lookup/doi/10.1093/eurheartj/ehac613Joint editorial:Matthews CE, Saint-Maurice PF. The hare and the tortoise: physical activity intensity and scientific translation. Eur Heart J. 2022. doi:10.1093/eurheartj/ehac626.Link will go live on publication:https://academic.oup.com/eurheartj/article-lookup/doi/10.1093/eurheartj/ehac626About the European Society of CardiologyThe European Society of Cardiology brings together health care professionals from more than 150 countries, working to advance cardiovascular medicine and help people lead longer, healthier lives.About European Heart JournalEuropean Heart Journal (EHJ) is the flagship journal of the European Society of Cardiology. It is the world’s leading publication in general cardiology. Please acknowledge the journal as a source in any articles.",Short bursts of vigorous activity linked with increased longevity
365,-1,-1_new_said_study_people,https://www.eurekalert.org/news-releases/969866,"URBANA, Ill. – Bats help keep forests growing. Without bats to hold their populations in check, insects that munch on tree seedlings go wild, doing three to nine times more damage than when bats are on the scene. That’s according to a groundbreaking new study from the University of Illinois.“A lot of folks associate bats with caves. But as it turns out, the habitat you could really associate with almost every bat species in North America is forest. And this is true globally. Forests are just really important to bats,” says Joy O’Keefe, study co-author and assistant professor and wildlife extension specialist in the Department of Natural Resources and Environmental Sciences at Illinois. “We wanted to ask the question: Are bats important to forests? And in this study, we've demonstrated they are.”Other researchers have demonstrated bats’ insect-control services in crop fields and tropical forest systems, but no one has shown their benefits in temperate forests until now.“It's especially important for us to learn how bats affect forests, given that bats are declining due to diseases like white-nose syndrome or collisions with wind turbines. This type of work can reveal the long-term consequences of bat declines,” says Elizabeth Beilke, postdoctoral researcher and lead author on the study.The research team built giant mesh-enclosed structures in an Indiana forest to exclude the eight bat species that frequent the area, including two federally threatened or endangered species. The mesh openings were large enough to allow insects free movement in and out, but not flying bats. Every morning and evening for three summers, Beilke opened and closed the mesh sides and tops of the structures to ensure birds had daytime access to the plots. That way, she could be sure she was isolating the impacts of bats.Beilke then measured the number of insects on oak and hickory seedlings in the forest understory, as well as the amount of defoliation per seedling. Because she erected an equal number of box frames without mesh, Beilke was able to compare insect density and defoliation with and without bats.Overall, the researchers found three times as many insects and five times more defoliation on the seedlings when bats were excluded than in control plots that allowed bats in each night. When analyzed separately, oaks experienced nine times more defoliation and hickories three times more without bats.“We know from other research that oaks and hickories are ecologically important, with acorns and hickory nuts providing food sources for wildlife and the trees acting as hosts to native insects. Bats use both oaks and hickories as roosts, and now we see they may be using them as sources of prey insects, as well. Our data suggest bats and oaks have a mutually beneficial relationship,” Beilke says.While insect pressure was intense in plots without bat predation, the seedlings didn’t succumb to their injuries. But the researchers say long-term bat declines could prove fatal for the baby trees.“We were observing sublethal levels of defoliation, but we know defoliation makes seedlings more vulnerable to death from other factors such as drought or fungal diseases. It would be hard to track the fate of these trees over 90 years, but I think a natural next step is to examine the impact of persistent low levels of defoliation on these seedlings,” Beilke says. “To what extent does repeated defoliation reduce their competitive ability and contribute to oak declines?”The researchers point out that birds, many of which share the same insect diets as bats, are also declining. While they specifically sought to isolate bats’ impact on forest trees, the researchers are confident insect density and defoliation rates would have been higher if they had excluded both birds and bats in their study. In fact, similar exclusion studies focusing on birds failed to account for bats in their study designs, leaving mesh enclosures up all night.“When we were initially working on the proposal for this research, we looked at 37 different bird exclusion studies, across agriculture and forest systems. We found nearly all of them had made this mistake. Most of them had not opened or removed their treatment plots to bats,” Beilke says.In other words, before Beilke’s study, birds were getting at least partial credit for work bats were doing in the shadows.Clearly, both types of winged predators are important for forest health in temperate systems. And, according to O’Keefe, that makes these studies even more critical to inform forest management.“I think it’s important to stress the value of this type of experimental work with bats, to really try to dig into what their ecosystem services are in a deliberate manner. While we can probably extrapolate out and say bats are important in other types of forest, I wouldn't discount the value of doing the same kind of work in other systems, especially if there are questions about certain insect or tree species and how bats affect them. So rather than extrapolating out across the board, let's do the work to try to figure out how bats are benefiting plants,” she says. “And before they're gone, hopefully.”The article, “Bats reduce insect density and defoliation in temperate forests: an exclusion experiment,” is published in Ecology [DOI: 10.1002/ecy.3903]. The research was supported by USDA’s National Institute of Food and Agriculture, the Indiana State Department of Natural Resources, the Indiana Space Grant Consortium, the Department of Biology at Indiana State University, and the Department of Natural Resources and Environmental Sciences at Illinois.The Department of Natural Resources and Environmental Sciences is in the College of Agricultural, Consumer and Environmental Sciences at the University of Illinois Urbana-Champaign.","Bats protect young trees from insect damage, with three times fewer bugs"
399,-1,-1_new_said_study_people,https://www.laptopmag.com/features/video-game-graphics-are-a-ticking-time-bomb-the-industry-needs-to-focus-on-art-over-tech,"It’s no secret that video games are inherently linked to the technology that allows us to experience them. However, as the industry continues to evolve, I can’t help but wonder if we’ve gone too far. It’s gotten to the point where graphical complexity is seen as the ultimate indicator of quality, making the game development process infinitely harder and longer. It seems like people will completely disregard an experience if the game’s graphics don’t hold up, or just absolutely demolish a game if it looks like it could have come out 10 years ago. To me, this seems absurd: Graphics are immensely overrated.Of course, graphics aren’t actually “overrated” in a literal sense. They are the visual gateway into the video games we play. Without them, we’d have nothing to look at. But when I say graphics are overrated, I’m referring to the overwhelming focus video game communities and marketing puts on graphical prowess and modernity. Sometimes it feels like the quality of a game’s art and style is tossed aside just to analyze how “new” and “shiny” something looks.Some genuinely believe that any game released more than a decade ago is “outdated and ugly,” and it truly makes me wonder, how did we get here? Why are people so obsessed with graphics?The technology raceThere’s plenty of reason why these advancements are such a fundamental part of the industry. Perhaps the single largest is because, at the end of the day, gaming is a technological medium: You need hardware to play the games.No hardware company will survive selling you the same specs if games aren’t evolving hand-in-hand with tech. If I purchased an RTX 3080 and the industry decided that we’ve hit the roof in hardware requirements, with the RTX 3080 being the final piece of the puzzle, then what would Nvidia sell us?(Image credit: Nvidia)Yes, they could continue manufacturing RTX 3080s. But whenever someone purchases one, they will have lost a customer for many many years, as you can make a GPU last a long time with proper care. When a PC gamer decides to upgrade their tech, it’s usually because their old tech is having trouble running modern software, not because it’s broken.My Nvidia GeForce GTX 970, a graphics card that launched in 2014, still works. It’s a functioning GPU, but the reason I upgraded to an RTX 3080 was because I was sick of games moving on without me. If my GTX 970 ran every piece of software I threw at it perfectly, I wouldn’t need to upgrade. Essentially, Nvidia needs to innovate to sell new hardware. It’s how they turn profits and continue to be a financially successful company.Console manufacturers have the same goal. How would PlayStation sell a PS5 to you if the hardware specs were the same as the PS4? What good reason could they have to convince you to move over? The only way they could do that is by forcing exclusivity on games, but in this imaginary world, that would be immensely problematic.(Image credit: Future)And of course, when the industry has that technology in front of them, they’re going to evolve their game engines accordingly. This is a technological medium and many of the brilliant engineers who work within it find themselves able to do more when better hardware is in their hands.It’s also important to note how damaging this obsession with progress is to game developers and their work cycle. Games are getting more and more complicated to make with every passing year. It is currently harder to make games than it has ever been; development times are longer than they were 10 years ago and the amount of crunch we’ve seen developers endure to get the biggest titles released is alarming. Whether we’re talking about The Last of Us Part II or Cyberpunk 2077 , so much overtime has been needed to put these enormous games together. How much worse do things need to get before we realize it’s not worth it?And when the players themselves are so accustomed to this cycle, they’ll expect it and get upset when it doesn’t continue. It’s common for players to be critical of a lack of technological modernity. If character models, animations, textures, or other types of assets look remotely outdated, it will be pointed out and criticized. This is a trend I absolutely despise.There’s more to art than just technologyThe gaming industry’s obsession with graphical fidelity has only made me more aware of the flaws of technology. The more advanced a game tries to be, the more cognizant I am of how lacking it actually appears. The jarring contrast between an engine’s best looking moments and the parts that just don’t look right result in a rude awakening, making me realize how much progress the industry still needs to reach the goals it's striving for.(Image credit: Laptop Mag)For example, in Ghost of Tsushima , there are visual setbacks that make it harder for me to latch onto its world. Soaring through a gorgeous field of flowers on horseback is an inspiring sight, but when I stop next to a collection of rocks that look blurry or the sun is reflecting light against mud in an unrealistic manner, it quickly takes me away from that moment.When a game attempts to mimic reality, my brain often processes the moments where it fails to look “realistic” with uncanniness. It just ends up seeming wrong, and whenever I think back to the game, my brain is hyper fixated on that flaw. On the other hand, my brain does not process older games in the same way, as there was never room for me to think of it as “realistic” to begin with.(Image credit: Nintendo)There is very little potential to mistake Super Mario 64 as an attempt at realism. When I think back on it, I don’t process its technological flaws, because that technology was used to present a certain style that cannot be assumed to be something akin to reality. Your brain can process those “flaws” as artistic intent, whereas if you’re trying to render a realistic glass of milk, your brain will latch onto the flaws that clearly make it look different from what a bottle of milk should actually look like.This isn’t to say modern games don’t have visual styles, of course they do. I’m specifically referring to how my brain processes it. And if you feel your brain is similar, then you probably have the same issue I do.This also isn’t to say games look ugly nowadays. Obviously, modern games can look absolutely breathtaking, just as much as they could have twenty years ago. I simply do not believe in the notion that graphical complexity or modernity determines whether or not a game is visually compelling.(Image credit: New Blood Interactive)You don’t decide whether an illustration looks good based on the tools used to make it come to life; different tools merely offer alternate styles. Dusk utilizes a graphical style that mimics first-person-shooters from the late 1990s, and that style has no bearing on the quality of the art itself. That style is merely a tool to express the intentions of the artist.This isn’t to say you can’t have preferences with style. Everyone does. I cannot blame someone for preferring the modern, realistic look of AAA games launched in the last few years. It’s a perfectly understandable preference to have, but it’s important to keep in mind that a game doesn’t look good just because it has high quality textures. There is so much more to the artform than that, and I wish people allowed games more leeway when they’re not as graphically complex as others.(Image credit: FromSoftware)Elden Ring, an experience that boasts some of the most breathtaking visual setpieces in gaming this year, was (and still is) frequently criticized for its outdated graphics by players who believe technology is the end all be all in determining the quality of visuals. Yes, Elden Ring suffered from pop-in and its foliage and textures look nowhere near as advanced as Demon’s Souls (2020). But I genuinely believe Demon’s Souls 2009 looks infinitely better than its remake, yet that game came out 13 years ago.In fact, the best looking game ever made was and still is 2005’s Shadow of the Colossus. Technological prowess can’t make up for a vapid aesthetic or a world lacking inspiration. There is so much more to art than just progress, and the industry needs to reconsider this gluttonous obsession.Bottom lineAt what point do we realize that constant advancement is not worth it? How much longer do development times need to get? How much harder does it need to be to create a game? AAA development is a ticking time bomb waiting to blow, and as the industry continues its obsession with progress, the artform will suffer.I hope I’m wrong. But I truly do believe that this cycle has a time limit. There will be a point where expectations are so unbelievably high that it’s not financially viable to continue making games of that complexity anymore.",Video game graphics are a ticking time bomb â the industry needs to focus on art over tech
396,-1,-1_new_said_study_people,https://www.japantimes.co.jp/news/2022/10/04/world/politics-diplomacy-world/us-limits-tech-to-chinese/,"The administration of U.S. President Joe Biden is expected to announce new measures to restrict Chinese companies from getting access to technologies that enable high-performance computing, according to several people familiar with the matter, the latest in a series of moves aimed at hobbling Beijing’s ambitions to craft next-generation weapons and automate large-scale surveillance systems.The measures, which could be announced as soon as this week, would be some of the most significant steps taken by the Biden administration to cut off China’s access to advanced semiconductor technology. They would build on a Trump-era rule that struck a blow to Chinese telecom giant Huawei by prohibiting companies around the world from sending it products made with the use of American technology, machinery or software.",U.S. plans new limits on tech sent to Chinese firms
395,-1,-1_new_said_study_people,https://www.inverse.com/science/nasa-ufo-uap-study-team-members,"A 16-person team — including an astronaut, a space-treaty drafter, a boxer, and several astrobiologists — will soon begin its review of unexplained aerial phenomena (UAP) for NASA.The space agency announced Friday the members of the team, who will labor over the course of nine months starting on Monday to analyze unclassified data on UAPs, peculiar sightings of objects behaving unlike anything we’re familiar with. But until the full report is released to the public in mid-2023, NASA says everything will be kept a secret.UAPs get their classification due to their puzzling behavior in the sky, which doesn’t fit into the known behavior of aircraft or known natural phenomena. NASA will unpack the data to come up with a way to study the unknown.NASA says their work will “lay the groundwork” for future UAP studies. This first phase is a brainstorm, to see how observations that civilian government entities and commercial data have gathered could be analyzed. And then, they’ll look at how future data can be collected.NASA will hold a public meeting after the report is released to discuss the study’s findings, an event that curious onlookers might want to earmark.The space agency says officials are excited to see what the team uncovers. “NASA is going in with an open mind,” the space agency writes in a Frequently Asked Questions webpage devoted to UAPs. “And we expect to find that explanations will apply to some events and different explanations will apply to others. We will not underestimate what the natural world contains, and we believe there is a lot to learn.”Is it aliens? The short answer is, NASA doesn’t know. The space agency chooses to highlight its search for extraterrestrial life when it publishes new information about the new UAP study. But agency officials have also been candid about where the data stands. They explicitly stated back in June that, “there is no evidence UAPs are extra-terrestrial in origin.”Meet the UAP teamDaniel Evans, the assistant deputy associate administrator for research at NASA’s Science Mission Directorate, orchestrated the study.The 16-member team includes:",NASA announces its unidentified aerial phenomena research team to examine mysterious sightings
392,-1,-1_new_said_study_people,https://www.independent.co.uk/tech/solar-panel-world-record-window-b2211057.html,"For free real time breaking news alerts sent straight to your inbox sign up to our breaking news emails Sign up to our free breaking news emails Please enter a valid email address Please enter a valid email address SIGN UP I would like to be emailed about offers, events and updates from The Independent. Read our privacy notice Thanks for signing up to theBreaking News email {{ #verifyErrors }}{{ message }}{{ /verifyErrors }}{{ ^verifyErrors }}Something went wrong. Please try again later{{ /verifyErrors }}Scientists have achieved a new efficiency record for dye-sensitized solar cells (DSCs), opening up new commercial possibilities for transparent solar panels.A team from École Polytechnique Fédérale de Lausanne in Switzerland made the breakthrough using specially designed photosensitizer dye molecules that when combined are capable of harvesting light from across the entire visible light spectrum.The transparent properties of DSCs make them suitable for use in windows, greenhouses and glass facades, the researchers said, as well as in the screens of portable electronic devices.They are also flexible, relatively low-cost and can be made using conventional roll-printing techniques. Theoretically, the price/performance ratio is also good enough to allow them to compete with fossil fuel electrical generation.The first commercial applications are already being realised, with dye-sensitized solar windows installed in the SwissTech Convention Center, however their capacity for generating electricity has so far been restricted by their lack of efficiency compared to traditional solar cells.The latest development pushes the power conversion efficiency to between 28.4-30.2 per cent, while still maintaining long-term operational stability over 500 hours of testing.“Our findings pave the way for facile access to high performance DSCs and offer promising prospects for applications as power supply and battery replacement for low-power electronic devices that use ambient light as their energy source,” wrote the authors of a study detailing the technology.The study, titled ‘Hydroxamic acid preadsorption raises efficiency of cosensitized solar cells’, was published in the scientific journal Nature on Wednesday.",Record-breaking transparent solar panels pave way for electricity-generating windows
389,-1,-1_new_said_study_people,https://www.igorslab.de/en/new-drivers-for-the-geforce-rtx-4090-also-eliminate-the-dx12-bottle-neck-of-the-geforce-3000-series-so-it-can-be-launched/,"1 - Introduction and Performance Comparison in WQHD 2 - Performance Comparison in Ultra-HD and ConclusionI have been asked by some readers why our test results from the launch review were better for the GeForce 3000 series cards, especially in WQHD, than on quite a few other sites. Others, on the other hand, said that everything would have to be re-benchmarked now because the new drivers, which were released at the same time as the GeForce RTX 4090, would make the old GeForce cards look better. The joke is that even before the first benchmark, I decided to completely retest everything as usual and use only one driver for all GeForce cards. This not only saves work in retrospect, but also shows that, although the review includes fewer cards for the reasons mentioned, it was coherent from day one. I also wanted to test the performance of the GeForce RTX 4090 first, so that I could adjust the benchmarks if necessary or even replace them…Because the 512.90 press driver we used already contains exactly these features for the older cards and it also confirms the hint of some sources in the run-up that data recycling and preparatory work would not be worthwhile this time. That’s exactly why I turned the tables and today, for a change, I’m showing what would have happened if I had given in to convenience or looser time management. I at least ran the GeForce RTX 3090 Ti as the fastest 3000 card through all games again with the older WHQL driver 517.48. On the other hand, since the two new drivers (521.90 Press and 522.25 WQHL) are almost binary-compatible, a plausibility test did not show anything to the contrary either: namely, the results agree completely within the scope of possible measurement tolerances.Performance losses of the GeForce RTX 3090 Ti with the old WQHL 517.48 in WQHD (2560 x 1440 pixels)Let’s first take a look at the cumulative FPS and P1 values of the tested cards in direct comparison to the two measurements with GeForce 3090 Ti, where the different drivers were used. The green bars represent the new driver and the benchmark results from my run test, the red bars represent what would have happened if I had used the old drivers. However, not all games from my benchmark suite benefit. If it were up to Far Cry 6 (a bug was also fixed here) and Cyberpunk 2077, it would have probably even lost up to 10 percentage points. I tested the majority of the games with the respective RT options anyway, so I’ll refrain from splitting them into “with” and “without” here.On average, there is a disadvantage of 5.2 percentage points for the older driver, so the difference to the GeForce RTX 4090 would also increase to a whopping 146.3 percentage points. This would really do the Ampere cards an injustice. The reader will therefore have to take a better look at the drivers in some reviews in order to avoid being caught up in the hypetrain.The P1 Low, i.e. the one percent of the worst frames per second, has also changed quite a bit here. A disadvantage of 5.4 percentage points would really do the GeForce RTX 3090 Ti a great injustice. Especially since the advantage of this card over the Radeon RX 6950XT would also disappear and both cards would render de facto equally fast if the old drivers had been used. It also shows very nicely here that the CPU load has dropped significantly. This in turn indicates that the disadvantage of the CPU limit for very fast cards in small resolutions has clearly become none with the new drivers!The downside of more gaming performance is of course the slightly increased power consumption. Speed almost never comes for free, and Jensen is not God either. The average power consumption of the new drivers is 6.5 watts, i.e. 1.6 percentage points higher – but this is almost compensated by the CPU’s lower power consumption of 4 watts on average. However, you get a whopping 5.4 percentage points more performance……which is then also reflected in the efficiency. This is where the old driver has to suffer a lot. The system with the new drivers is also much more efficient not only in terms of graphics cards, but also in the sum of GPU and CPU. Again, the GeForce RTX 3090 Ti has not done anything good with the old drivers.We can see especially in WQHD that NVIDIA has neatly tweaked the DX12 performance and that the games always benefit when the CPU load also drops because it was previously above average. But what about Ultra HD? Please turn the page once!",New drivers for the GeForce RTX 4090 also eliminate the DX12 bottleneck of the GeForce 3000 series â so our launch test is right
388,-1,-1_new_said_study_people,https://www.igorslab.de/en/evga-pulls-the-plug-with-loud-bang-yet-it-has-long-been-editorial/,"If you want to (or have to) separate, then there are two ways to announce this: one emotionless and one where you once again seek the big stage. EVGA chose, what a coincidence, the time shortly before NVIDIA’s in-house exhibition GTC (after that, the whole thing would have gone down a bit) to drop the bombshell that wasn’t really a bombshell anymore. This media uprising and aftermath is really just the long overdue flashback after a smoldering fire that lasted for months and in which the green fuse was even shorter than Jensen’s wood screws in his old Fermi mock up.I thought long and hard about whether to write something about it (and about what), because I also have my own private opinion about the circumstances in general and EVGA’s appearance in particular, which has been formed in the last few years also due to my own experiences. That’s why I also used the weekend to chat or talk on the phone with one or the other competitor and colleague, in addition to all the benchmarks. Because the way in which and where the reason is now located and the way in which a suitable culprit is presented at the same time has caused quite a stir among many people. It’s not about defending NVIDIA, but using Jensen as the ideal object of hatred is a bit short-sighted and only distracts from one’s own failures. But you have to throw something down the throat of the investors and brand-affine customers. And that’s where the green leather jacket Hulk is best suited.My colleague Stephen Burke from Gamersnexus has actually already said most of it, I don’t need to dissect and regurgitate that again. I’ll just post the short quote from EVGA CEO Andrew Han again now before I get my own thoughts on it.We are not going to be on Jensen’s lap on stage, so I don’t want people to speculate what’s going on….EVGA has decided to nat carry the next gen. (Andrew Han, CEO)The business model: Let others do itIn principle, EVGA is nothing more than a brand without its own production. Of course, the company has its own R&D department for the engineering, but the level of creativity is not particularly high here either, if you only have to rely on the abilities of third parties in the end, be it the circuit boards, the coolers and the complete production as the final implementation. Unfortunately, having it done instead of doing it yourself costs money. And while this very concept mitigates risk and effort in manufacturing, it also reduces the margin that can still be achieved with such a highly complex product. Therefore, I asked several competitors and found out that the currently achievable margins in the worst case (like EVGA) are still 5%, and up to 10% for the do-it-yourself manufacturers, who can manage an own production.This is not less than in 2021 or 2020, only this year merchandise sales have dropped significantly due to orders staying away and a change in demand. However, that one produces at a loss because of NVIDIA’s money-grubbing claws, as EVGA colports, is considered by many to be an urban legend. These are rather homemade problems with faulty designs and a RMA rate that was beyond good and evil. I don’t want to quote myself again, but the disaster with Amazon’s New World and the scorched circuit boards was expensive, really expensive. To attach this to every card as a loss is definitely too short-sighted.Let’s say they made 10x profit than normal business last year, if you are the boss, will you quit now or waste another 10 years just to make the same profit amount last year? (Competitor)The business of brands is actually like the good old stock market business, because you should sell at the peak and best take what is still to be taken. If you have to buy almost all services at a high price, there is not much left in the graphics card business. A large manufacturer (not EVGA) easily produces around 200,000 graphics cards per week, which puts the 5 to 10 percent in a different light. In the system gastronomy, business also only works via sheer mass, which is no different with graphics cards. However, EVGA lacks exactly this mass (which can also be seen as a buffer), which can cushion smaller outliers.If you ask the competitors why they haven’t copied EVGA’s warranty, upgrade and exchange model as well, you actually only see grinning faces or shaking heads. Statements such as “economic suicide by default” are still the most polite thing to say. I wrote that the cards have become more and more complex and that the risk of failure has increased dramatically as a result. Therefore, the RMA processes do not become more favorable, on the contrary. Goodwill and generosity, as EVGA has made its trademark, must also be afforded and what was manageable 10 years ago can end in collapse any day today. House of cards and all. Here, EVGA simply lacks the critical mass to easily pull off something like this financially. Being the US market leader is all well and good, but how big is the DIY market there?If it were profitable, we would have done it long ago (Competitor)Peripherals and power supplies are much easier to keep track of and now offer much higher margins of up to 30 percent. Power supplies are just the new thermal paste and the run is still properly fueled by ATX 3.0. Speaking of power supplies, I remember how EVGA at the time (this was back in the Kepler days) made the sampling of graphics cards dependent on a positive review of the newly introduced power supplies. Back then, I was still writing for Tom’s Hardware and very stubbornly resisted the “reward for award” system. At that time, however, the “Just buy it!” philosophy did not yet exist there and one was still allowed to do such things as a reviewer.As a result, I was then excluded from sampling for a few years. By the way, I’m still alive, which shows once again that you don’t have to go along with something like that if you don’t want to. I also don’t want to repeat how EVGA products later failed my tests and the engineering used my findings (pad mod, area increase on the cooling frame, RAM monitoring in the ICX design), but the PR kicked nasty in my direction. I never asked for money for my support, but would probably have been happy to receive a thank you now and then. By the way, this is exactly where you notice the difference between a US company like EVGA and a German medium-sized company. EVGA is extremely profit-oriented, and when things don’t go so well, they just part with a division. By the way, this is not reprehensible, but the explanations should then also be more honest.The Green Light Program as a buzzkillAnd what does NVIDIA have to do with it now? Yes, even the competitors are properly pissed off that they still don’t know NVIDIA’s prices and don’t know at which dumping or scalper rate Master Jensen will beat his new Ada into the market as a playmate (depending). That’s why you won’t see so many completely new designs, and what I had already called “Playground” in the GeForce RTX 3090 Ti will mostly be continued. This is then economic prudence and not even stupid. EVGA could have thought of that as well.However, you also have to know that NVIDIA strives for total control and also mercilessly enforces these policies. There isn’t much room left for technical gimmicks á la EVGA’s special models and both sides have clashed hard not only once. EVGA always explores how far you can go and NVIDIA then intervenes to correct the situation. I have already explained this in detail in the article linked above and I don’t want to repeat myself. However, it is also a fact that NVIDIA only provides replacements if you follow “NVIDIA’s rules”. You can approve of this or not, but it leads (at least in theory) to more durable products.Things like EVGA’s Kingpin models break those rules, but that’s where they’ve found a clever loophole because it’s effectively declared a mod. Galax does the same with the HoF series. Various overclockers give their face for it (for good money) and it is not a consumer product. That’s why you don’t get a real HoF including unlimited OC tool as a normal customer, but only the “roadworthy” version under the same name. The rest is then expensive cannon fodder for the LN2 artillery. Such marketing escapades also cost money, of course, even if they can boost the company’s image. The good Kingpin must now look for another place in the money rainforest, but the choice is rather manageable there.I didn’t really want to go that far, but once you’re in such a nice writing flow, the shore you’re aiming for gets further and further away. In order to come to some kind of conclusion at the end, I’ll make it simple for myself, because I can perhaps also question some things differently: It is certainly not a loss for me (and many others), because it became apparent that the model practiced for years would no longer have been financially viable in this way. And before one admits this publicly and meekly, one looks for the last big performance and says goodbye to the shocked audience with a big bang. I only hope that all previous customers will still be dealt with gallantly and treated fairly. Then it will surely work out with the power supplies, housings and other stuff.After all, I haven’t heard from any other small Jensen exclusive customers that they now have to throw in the towel for the reasons I mentioned. But they can probably calculate better and also produce themselves. It is a pity about a colorful facet on the graphics card market, which I will also miss, but the customer will get over it.","EVGA pulls the plug with a loud bang, but it has been stewing for a long time | Editorial"
387,-1,-1_new_said_study_people,https://www.igorslab.de/en/adapter-of-the-gray-analyzed-nvidias-brand-hot-12vhpwr-adapter-with-built-in-breakpoint/,"Those who are now beating up on the new 12VHPWR (although I don’t really like the part either) may generate nice traffic with it, but they simply haven’t recognized the actual problem with the supposedly fire-hazardous and melting connections or cables. Even if certain YouTube celebrities are of a different opinion because they seem to have found a willing object of hate in the 12VHPWR once again: This connection is actually quite safe, even if there are understandable concerns regarding the handling.However, the “safe” is only valid if e.g. the used supply lines from the power supply with “native” 12VHPWR connector have a good quality and 16AWG lines or at least the used 12VHPWR to 4x 6+2 pin adapter also offers what it promises. Which brings us directly to the real cause of the cases that occurred: It’s the adapter solution exclusively provided by NVIDIA to all board partners, which has fire-dangerous flaws in its inner construction!Our investigation has now also put NVIDIA on the map, but not officially yet, only in their own board partner circles.Let’s first take a quick look at the typical error pattern that occurs when such an adapter triggers a defect, before we start searching for clues inside the accessories distributed by NVIDIA. And before you start beating up the PCIe 5.0 connector, even a layman can see from the error pattern and the load distribution what or where the actual trigger must have been.As a reminder, I’ll now also briefly show you the block diagram of the special NVIDIA adapter, whereby we are only interested in the upper six yellow connections of the connector including the bridge here. Electrically, all 12V pins are still connected to each other and then also once again to the four feeding wires:The sad truth lies in the plug, not in the form factorI’m pretty sure that NVIDIA itself doesn’t know or didn’t know exactly what kind of contact party the supplier is having inside the molded connector. Otherwise, this part should not have been approved in this form. When you remove layer after layer, something gradually emerges that you would have been better off not seeing in the first place:A total of four thick 14AWG wires are distributed over a total of six contacts, with the two outer leads soldered to one pin each and the two middle leads soldered to two pins each. The solder base is a mere 0.2 mm thin copper base with a width of 2 mm per incoming wire, which then results in 4 mm per pair for the middle connections. Soldering one or even two 14AWG wires to it is very sporty. Confucius says succinctly, “What is cast in cannot be seen, and what cannot be seen cannot be broken.”But just carefully lifting off the enveloping layer causes the thin plate to tear immediately. With which we also see that bending the cables at the connector in the housing can cause such damage. Wobbly contacts or unsafe bridges, as well as increased resistances as a result of such actions are highly dangerous at these current strengths and very quickly lead to exactly the errors that we have already seen above. And you can now also see why especially the two outer contacts of the connector are affected. The scorched socket on the card is then only the consequential damage.However, you can cut the plug open even further and see what exactly is inside. Because this detail is also highly interesting for the cause research. We see that the contacts in the connector are once again interconnected. If, in the worst case, the outer two wires break off, the entire current in the middle flows through the remaining two wires and is only then distributed in the plug again. But even this “foil” is thin and does not replace a real 14AWG. The fact that this then becomes really hot does not have to be explained separately….That’s the right way to do 12VHPWR!You can like the connection or not, but you also have to stick to the facts and the truth. The native 12VHPWR cables of the better power supplies show that it can be done differently. Incidentally, I also measured the 12VHPWR using resistance sensors and was able to determine absolutely unsuspicious temperatures (picture above). At plenty of 530 watts it was under 60 °C and even the maximum temperature of just under 68 °C I could only prove after an hour with 600 watts in the stress test. But definitely nothing melts there yet!A good example of a functioning connection are, for example, the two 12VHPWR cables of the new be quiet! Dark Power Pro 13, which I still show here as an example. Because there you don’t have to do the balancing act with the voltage bridges, but spend each pin its own 16AWG line. Sure, 12 thick wires in one cable is not that sexy now either, but it is at least an accurate and clean solution. I also snapped these cables right at the connector several times and did much of my testing for the GeForce RTX 4090 as well as the Intel Core i9-13900K in the lab with them on the redundant test system.Summary and conclusionThe overall build quality of the included adapter for the GeForce RTX 4090, which is distributed by NVIDIA itself, is extremely poor and the internal construction should never have been approved like this. NVIDIA has to take its own supplier to task here, and replacing the adapters in circulation would actually be the least they could do. I will therefore summarize once again what has struck those involved (myself included) so far:The problem is not the 12VHPWR connection as such, nor the repeated plugging or unplugging.Standard compliant power supply cables from brand manufacturers are NOT affected by this so far.The current trigger is NVIDIA’s own adapter to 4x 8-pin in the accessories, whose inferior quality can lead to failures and has already caused damage in single cases.Splitting each of the four 14AWG leads onto each of the 6 pins in the 12VHPWR connector of the adapter by soldering them onto bridges that are much too thin is dangerous because the ends of the leads can break off at the solder joint (e.g., when kinked or bent several times).Bending or kinking the wires directly at the connector of the adapter puts too much pressure on the solder joints and bridges, so that they can break off.The inner bridge between the pins is too thin (resulting cross section) to compensate the current flow on two or three instead of four connected 12V lines.NVIDIA has already been informed in advance and the data and pictures were also provided by be quiet! directly to the R&D department.Actually, I wanted to do something completely different today, but this correction was more important to me. Blanket panic and gloating are really bad advisors here when it comes to introducing new standards. That AMD has not (yet) joined the plug change was shown in my news about one of the upcoming board partners. But if you, like NVIDIA, take such a radical step, then at least the included accessories should work properly over after a little bending and ensure a safe, stable operation of the graphics cards.My thanks also go to Christian Rex from be quiet! whose pocket knife was faster than my Dremel.Source: Own, be quiet!",The horror has a face â NVIDIAâs hot 12VHPWR adapter for the GeForce RTX 4090 with a built-in breaking point
385,-1,-1_new_said_study_people,https://www.guru3d.com/news-story/the-future-of-8k-tvs-is-in-danger-because-the-eu-new-consumption-limits-are-too-low.html,"While I still do not see the need for an 8k screen, we also know that blogger resolutions often require more energy. And that's about to come to a halt due to new EU legislation.On March 1, 2023, all new TVs will have to meet stricter rules, including a lower maximum level for how much power they can use. The European Union most recently updated its Energy Label in March 2021 which resulted in many TV models being moved to the lowest energy class (G). On March 1, 2023, even stricter requirements including a lower maximum level for power consumption will be imposed on all new TVs produced reports flatpanelshd.Representatives from Samsung Electronics told FlatpanelsHD that they think 8K TVs can meet the new rules, but it will not be easy. In either case, it is bad news for 8K TVs because the rule will force TV makers to give up or make compromises in the short term.- ""With the 8K EU Regulatory Ruling, March 2023 will be a bad time for the new 8K industry if something does not change. That is when new EU rules about how much power you use will start to take effect. The limits on how much power 8K TVs and microLED displays can use are so low that almost none of these devices will pass ""in a post, the 8K Association wrote. Today, OLED TVs are allowed to consume a bit more power than LCD TVs (""LED"", ""QLED"", ""miniLED"" etc.) while microLED and all types of 8K TVs are exempt from having to comply with the maximum power limit, which is why they can be sold in Europe. Starting March 1, 2023, there will no longer be exceptions for any display technology or 8K TVs. ""The EEI of an electronic display shall not exceed the maximum EEI (EEImax) according to the limits in Table 1"", says EU.Table 1 EEImax for electronic displays with resolution up to 2 138 400 pixels (HD) EEImax for electronic displays with resolution above 2 138 400 pixels (HD) and up to 8 294 400 pixels (UHD-4k) EEImax for electronic displays with resolution above 8 294 400 pixels (UHD-4k) and for MicroLED displays March 1, 2021 0,90 1,10 n.a. March 1, 2023 0,75 0,90 0,90FlatpanelsHD has done the calculations for EEImax (Energy Efficiency Index) for various popular TV size classes. Here is what it means for LCD, OLED and microLED TVs, both 4K and 8K.Size class : Maximum power for 4K and 8K TVs 40"" 48 W 42"" 53 W 48"" 66 W 55"" 84 W 65"" 112 W 75"" 141 W 77"" 148 W 83"" 164 W 85"" 169 W 88"" 178 W EU energy labels for existing 4K and 8K TVs (see table below) provide some context as to why it will be so hard for 8K TVs, both LCD and OLED, to comply. Standard 4K LCD TVs and 4K OLED TVs should pass without changes but some high-end 4K LCD TVs with advanced zone-dimming could also be in trouble. The requirements relate to the default picture mode that a TV ships with. TVs can still offer other picture modes that consume more power as manual selections that must present on-screen a warning notification making the user aware of the increased energy consumption. The EU has scheduled a review of the 2023 Energy Efficiency Index (EEI) by the end of 2022 so it remains to be seen if anything changes before March 2023. EU's energy labelling and ecodesign legislation is designed ""to eliminate the least performing products from the market"", the European Commission said in 2021.",The future of 8K TVs is in danger because the EU new consumption limits are too low.
382,-1,-1_new_said_study_people,https://www.geekwire.com/2022/eviation-all-electric-alice-airplane-first-flight-test/,"Eviation’s all-electric Alice airplane streaks over the runway at Moses Lake, Wash. (Eviation Photo)MOSES LAKE, Wash. — After years of on-the-ground development, Eviation’s all-electric Alice airplane quietly took to the air here this morning for its first test flight.Test pilot Steve Crane guided the nine-passenger aircraft, powered by two 640-kilowatt electric motors, through its takeoff from Grant County International Airport in Moses Lake, a facility in Eastern Washington’s high desert that’s often used for testing innovations in aviation.When the motors revved up, they sounded like electric grass trimmers. And when the plane flew overhead, the noise was more like a hum than a roar.Alice flew for eight minutes and reached a maximum altitude of 3,500 feet before landing safely back at the airport.So how was the ride? “It was wonderful,” Crane said. “It handled just like we thought it would. Very responsive, very quick to the throttle, and it came on in for a wonderful landing. I couldn’t be happier.”Crane explained that the relatively short flight was intended to be the first in a series of “baby steps” for the test program. “Today was just about the initial envelope,” he told reporters. “For future tests, we’ll expand that envelope.”After the first flight of the all-electric Alice airplane, test pilot Steve Crane shakes hands with Eviation CEO Gregory Davis while Clermont Group Chairman Richard Chandler looks on. (Eviation Photo)Arlington, Wash.-based Eviation is on a growing list of ventures that are aiming to make aviation more efficient and less expensive by taking advantage of advances in electric propulsion and battery technology.“What we have just done is made aviation history,” Gregory Davis, Eviation’s president and CEO, said after the flight. “This is about changing the way that we fly. It’s about connecting communities in a sustainable way, and we are obviously beaming with pride on this beautiful sunny day here at Moses Lake.”The Alice aircraft — whose name was inspired by the book “Alice in Wonderland” and the Jefferson Airplane song “White Rabbit” — will come in different variants for commuter, cargo and executive flights. Davis said the initial goal is to build a plane with a maximum range of 200 to 300 nautical miles. According to Eviation’s stats, Alice’s maximum useful load would be 2,500 to 2,600 pounds, and its maximum operating speed would be 260 knots (300 mph).Davis acknowledged that the design specifications and capabilities of the production version of the plane may be something of a moving target, due to Eviation’s dependence on improvements in battery technology. “It’s going to be carbon fiber, it’s going to be fly-by-wire, it’s going to be electric — so in that respect, it’s the same plane,” Davis said. “As far as the actual design of the aircraft, I think everything’s going to be evolved.”Eviation’s Alice airplane passes overhead during its first test flight. (GeekWire Photo / Alan Boyle)Eviation’s majority owner is the Clermont Group, a privately held conglomerate based in Singapore.The plane’s electric propulsion system is provided by MagniX, a Clermont corporate cousin that has its headquarters in Everett, Wash. Eviation’s partner in the flight test program is AeroTEC, which operates the Moses Lake Flight Test Center. For what it’s worth, MagniX and AeroTEC have been working together since 2020 to conduct flight tests for a Cessna Grand Caravan airplane that was converted to use MagniX’s electric motors.Alice’s inaugural flight test came more than three years after the first prototype was unveiled at the Paris Air Show. Since then, Eviation has moved its base of operations from Israel to Arlington, and has gone through a complicated executive transition that brought Davis in as CEO.The company has been putting its flight-ready prototype through more than a year of assembly and ground testing — with some setbacks along the way.If all goes according to plan, the airplane will win certification from the Federal Aviation Administration and hit the market by 2027 — which is later than the 2024 time frame that Eviation was listing a year ago.“What we’ve learned is a lot, and one of the key things that’s driving the development of our program is the advancement of battery technology, right?” Davis told GeekWire. “So we’re being, I will say, entirely honest with ourselves about what we’re going to be able to achieve. … It’s all going to be based on getting the batteries to converge to the development cycle for the aircraft.”Three years ago, Eviation said the list price for an Alice airplane would be $4 million — but today Davis declined to provide an updated price tag. “I wouldn’t rely on anything that was mentioned a few years ago,” he said.In any case, the list price for an airplane doesn’t always reflect what customers pay. Actual pricing is typically decided privately on a deal-by-deal basis, and several deals have already been announced. DHL Express ordered 12 Alice eCargo planes last year, and Massachusetts-based Cape Air ordered 75 all-electric passenger planes in April. This month, the GlobalX charter airline said it intended to order 50 passenger planes, with deliveries due to begin in 2027.More photos from the Moses Lake flight test:Photographers wait for Alice’s first flight – and watch the sunrise. (GeekWire Photo / Alan Boyle)Eviation’s Alice airplane passes by on the runway. (GeekWire Photo / Alan Boyle)Alice lifts off. (GeekWire Photo / Alan Boyle)Alice taxis toward the airport terminal after its first test flight. (GeekWire Photo / Alan Boyle)",âIt was wonderfulâ: Eviationâs Alice electric airplane wins praise after its first flight test
381,-1,-1_new_said_study_people,https://www.futurity.org/amazon-discounts-consumers-2811182/,"Share thisArticle FacebookTwitterEmail You are free to share this article under the Attribution 4.0 International license. University University of FloridaMore than a quarter of vacuum cleaners sold on Amazon have at some point pretended to offer a discount when they had actually just increased the price, according to new research.By pairing a price increase with the introduction of a previously unadvertised “list price” for a product, Amazon signals to shoppers that they are receiving a discount when they actually pay 23% more, on average, for their new vacuum than they would have just a day earlier. Days after the price hike, the price drops and both the list price and misleading discount claim disappear.Sellers of digital cameras, blenders, drones, and even books use the same misleading practice, although less frequently. The false discounts drive higher sales despite charging more money, causing the products to improve in Amazon’s sales rankings.“When you see this list-price comparison, you naturally assume you are getting a discount. It’s not just that you didn’t get a discount. You actually paid a higher price than before the seller displayed the discount claim,” says Jinhong Xie, a professor in the Warrington College of Business at the University of Florida.Currently, regulations prohibiting deceptive pricing require that sellers use truthful price comparisons. Consumers have won class-action lawsuits against retailers like JC Penny and Ann Taylor for making discount claims using illegitimate values in price comparisons.In the pricing practice that Xie and her colleagues uncovered, the list price can be truthful yet still misleading. That’s because retailers advertise a price discount by displaying the list price when they actually raise prices and give the impression of a deal. But most of the time, the product is sold at a cheaper price without any comparison to a list price. It is the timing of the price comparison that misleads shoppers.“Current regulations are all about the value of the list price, and they don’t say anything about misleading consumers by manipulating the timing of the list price’s introduction,” Xie says.For the study, published in Marketing Science, researchers looked at the pricing of household products on Amazon from 2016 to 2017. Xie and colleagues followed more than 1,700 vacuums and gathered nearly half a million individual observations of prices. While most introductions of a new list price were associated with a price drop or no price change, 22% were instead accompanied by a price increase.Because shoppers perceive they are getting a deal, these misleading discounts actually improved the products’ sales rankings on Amazon, a proxy for sales volume.“We found that by increasing the price by 23% on average, the seller achieves an 11% advantage in their sales rank among all products in the home and kitchen category,” Xie says. “This allows firms to achieve the impossible: increasing margins and increasing sales simultaneously.”Other products used this practice anywhere from 3% of the time for books to more than 13% of the time for blenders, digital cameras, and drones.Consumers can protect themselves by questioning ubiquitous “discounts” advertised in online stores, Xie says. Shoppers should not assume a discount claim means the price is lower than usual. Instead, shoppers should comparison-shop across multiple websites. They can also use online tools that provide price histories to learn if the advertised price they are seeing is really a deal or not.“We think consumers need to be aware so they can protect themselves,” Xie says. “And we think that consumer organizations and regulators should evaluate this new marketing practice to determine whether and how to manage it.”Additional coauthors are from the University of South Carolina and Arizona State University.Source: University of Florida",You may actually pay more with an Amazon discount
380,-1,-1_new_said_study_people,https://www.ft.com/content/c08642f5-3aa3-447b-9028-c1c84b8e10fe,"Airbus and Boeing have competed neck and neck for five decades in one of the world’s great commercial rivalries. But, as the contest resumes in earnest in the wake of the pandemic, the old order has changed. Airbus now has a wide lead in the single-aisle market — the hottest area of aviation — leaving Boeing to grapple with how to bridge the gap.The European company has grabbed new orders and market share with its popular A320 family of narrow-body jets, which predominantly serve shorter-haul destinations, to which travel has rebounded faster, compared with long-haul flights.Latest figures to the end of June show that Airbus had close to 17,000 orders of its A320 and A220 aircraft, and has so far delivered just over 10,600. The A320 is now the bestselling aircraft ever, overtaking US-based Boeing’s 737 jet.Recent fleet data from aviation consultancy Cirium showed Airbus’s A320 family with a “firm order backlog” market share of 59 per cent compared with Boeing’s 737 family of jets.The bestseller within the A320 family is its largest model, the A321neo, which is also offered with extra fuel tanks in long-range and extra-long range variants. Some airlines — such as America’s JetBlue — now use these for long-haul flights that were traditionally flown by more expensive wide-body jets.Airbus’s market share is only likely to grow as the group moves ahead with aggressive plans to lift output of the jets to 75 a month by 2025, despite industry-wide concerns about constraints in the supply chain. Production had reached a record 60 a month in 2019 before dropping to 40 a month when Covid hit.Some analysts have questioned when Boeing will be able to regain some of that lost market share, as the company has continued to deal with fallout from the grounding of its 737 Max 8 after two fatal crashes within five months.Production problems have prevented customer deliveries of Boeing’s wide-body 787 aircraft © Richard Ellis/AlamyGlobal regulators have since cleared the jet to fly again after Boeing implemented software and design changes and the plane is notching up new orders.Another challenge looming, however, is the recertification of the shorter 737 Max 7 and longer 737 Max 10 variants. Boeing is racing to complete those processes before the end of the year to avoid having to develop a new flight deck under rules introduced by US Congress following the Max crashes.I have a lot of time for Dave Calhoun . . . but he’s not in Seattle and the problems in Boeing lie in SeattleBoeing has also suffered production issues that have prevented customer deliveries of its wide-body 787 aircraft. In addition, development of its 777X aircraft has been delayed, while problems on a number of big defence contracts have only added to the challenges.Union representatives, meanwhile, have criticised the company’s decision to move its global headquarters from Chicago to Washington, DC — seen as taking it further away from its spiritual home, the commercial aircraft factories of Seattle, and adding to persistent questions over the quality of its engineering.Repeated delays on the commercial side have frustrated some of Boeing’s largest customers, including Ryanair. The low-cost carrier’s chief executive, Michael O’Leary, has said sweeping changes are needed to Boeing’s senior management led by chief executive Dave Calhoun.In a recent interview, O’Leary told the Financial Times that the US aviation group has “lost huge market share and existing customers are being lost to Airbus, which is why I don’t understand why they sit there and do nothing”.“I have a lot of time for Dave Calhoun . . . but he’s not in Seattle and the problems in Boeing lie in Seattle,” he said.But others, including Lufthansa chief executive Carsten Spohr, have continued to back the US company. Boeing has insisted that things have changed since the Max accidents. It is also ramping up the hiring of engineers as it works with the US aviation regulator to certify its upcoming 777X aircraft.Boeing’s chief executive Dave Calhoun has sought to resist discussion on market share © Christopher Goodney/BloombergHowever, in a further blow, Boeing last month lost out to Airbus on a $37bn order from China’s three big state airlines. They chose 290 A320neo jets — the biggest order by Chinese airlines since the start of the pandemic. The country is a crucial market for both Airbus and Boeing and the orders come at a time of rising political tensions between the US and China.Calhoun has previously cited uncertainty over orders from China as one of the three main risks facing the company, along with securely boosting production of the Max and resolving issues around the 787. China was the first country to ground the Max and, although its aviation regulator last year approved its return to service, Chinese carriers have not started flying the plane commercially again.The multibillion-dollar question facing Boeing, therefore, is how to recover its lead against Airbus, at the same time as China’s first self-developed aircraft — Comac’s narrow-body C919 — is finally nearing completion.Boeing abandoned a plan for a new midsize aeroplane — a twin-aisle aircraft that would have sat between the 737 Max and the larger 787 — and it remains unclear what its next step will be. Its balance sheet also remains constrained, with some $45bn of net debt.$45bn Net debt on Boeing balance sheetThe company needs to “launch an answer to the A321neo,” says Kevin Michaels, managing director of Michigan-based AeroDynamic Advisory. “Can they afford it? I think they can. Their balance sheet isn’t great but you’ve got to invest in your core business to stay around in the long run.”“The other problem they face is knowing what to do — if you do press the button, how do you do that plane? The world is about to change massively but no one yet knows what that technology will be and you could end up spending $20bn on something that will be immediately obsolete,” says Nick Cunningham, analyst at research firm Agency Partners in London, who believes the company needs to raise equity to fund a new programme.But Calhoun has recently played down any concerns about the company’s market share position vis-à-vis Airbus. “At this moment in time, it’s less important,” he told an investor conference in June. Boeing, he added, had to concentrate on getting planes back in the air.“We have to stay focused on doing that incredibly well. One aeroplane at a time. If I jump to a market share discussion immediately and say, ‘Let’s get above 50 and let’s do it next year’, what happens? The whole system gets crammed down in every way.”He also indicated at the same conference that the company might not launch anything new for at least another couple of years. He does not believe that the performance improvement to justify an all new aircraft currently exists.Boeing, he said, was sure it had the ability to compete. “I’m confident, my sales team is confident,” said Calhoun. “Our customers express confidence to me, so why would I rush? No good reason.”Additional reporting by Philip Georgiadis",Airbus climbs past Boeing in single-aisle market share
379,-1,-1_new_said_study_people,https://www.freethink.com/technology/minimally-invasive-surgery,"Once the stuff of science fiction, robotic surgery is now used in hospitals across the world for a wide variety of procedures. Across several common procedures, some research has found that robotic surgery can improve outcomes in areas such as blood loss, complications, and recovery times, compared to open and some laparoscopic surgeries.Robotic procedures have been game changers in minimally invasive care, and innovative surgical systems have been crucial. Robotic surgery isn’t performed by a machine; a specially trained surgeon uses advanced medical technology to streamline the surgical process. The end goal is to increase safety, reduce complications, and improve patient outcomes. By reducing surgical trauma and increasing precision, robotic surgery can help pave the way for predictable, reproducible results.Although robotic surgery has made impressive advances over the past two decades, there is much more to be done. As minimally invasive care continues to advance, researchers are finding new ways to help surgeons do more for patients.How robotic surgery worksThe first commercially viable robotic surgical system was developed in the late 1990s by Intuitive. Intuitive’s systems have gone on to become the most advanced and widely used in the world. Four generations of its pioneering da Vinci system have so far been used in more than 10 million procedures.Intuitive’s robotic surgical systems enable surgeons to use a console, precisely guiding surgical instruments inside the patient’s body. The surgeon’s hand movements are translated in real time to the instruments, which are situated on a cart beside the patient. The instruments are small, nimble, and have a greater range of motion than the human wrist and hand, helping to enable surgical precision in moments that matter.In contrast to open surgery, which typically requires a large incision, robotic surgery involves one or more very small incisions to insert the robotic arms. One of the arms has a camera that feeds high-quality 3D video to the surgeon’s console.“I remember distinctly when I first sat at the console,” says Tanuja Damani, a general surgeon at New York University who first began using the da Vinci system in 2011. “I realized right away that this was the future of surgery. But there was tremendous skepticism early on.”Dr. Damani and other early adopters saw the potential of this approach. Their vision, perseverance, and mentoring have smoothed the way for those who have followed.Take Amanda Pysher, a young bariatric and general surgeon with Surgical Consultants of Northern Virginia, who was first introduced to robotics during her residency and fellowship. Unlike Dr. Damani, Dr. Pysher has benefited from a robust network of mentors, who’ve helped her as she’s learned the ropes of using the da Vinci system. She’s become so comfortable with the system that three years ago she focused her practice on robotic surgery. She now mentors other surgeons who are in the process of learning robotic surgery.“I think it has been easier for me to transition to robotics than the first adopters years ago,” she says. “It’s one of the reasons I feel so strongly about mentoring. I want to be able to give back and help others the way that the mentors above me helped me. We’re helping people live longer for their families and their friends. I just can’t think of any other surgery which has a bigger impact than that.”Robotic surgery’s impact on patientsFor both Dr. Pysher and Dr. Damani, the goal is to do more for their patients. One such patient is Jennifer Luebke. Last year, after struggling with her weight for decades, Luebke decided to have a robotic bariatric surgery with Dr. Pysher.That decision stemmed largely from her sense that her weight increased her long-term health risks. “I want to be able to grow old and be a grandmother, and I really didn’t see that in my future,” Luebke says. “I really was concerned. This is the one thing that has completely changed my life.”As an emergency room nurse, Luebke was reluctant to undergo traditional open bariatric surgery – she’d seen patients who’d had to deal with complications. But as her weight increased and her health problems grew, she decided that robotic surgery might reduce the risk of complications.Her decision paid off. After the procedure, she spent the night in the hospital and was discharged the next morning.As the technology continues to advance, more and more surgeons and hospitals are coming on board.The number of worldwide robotic procedures more than doubled between 2012 to 2020. A study published two years ago in JAMA Network Open analyzed records from 73 hospitals in Michigan, finding that 8.7 percent of surgeons performed robotic general surgery in 2012; by 2018, that number had jumped to over 35 percent.“We are turning the skeptics into believers,” Dr. Damani says. “The shared sense of purpose of the robotics surgery community now is something that is so refreshing.”Total da Vinci Practice**Total da Vinci Practice refers to the transferable value of robotic-assisted surgery with a da Vinci system across procedures in a surgeon’s minimally invasive practice. It is at the surgeon’s discretion to determine when a patient is a candidate for minimally invasive surgery and whether robotic-assisted surgery with a da Vinci system is an option.",Robotic surgery is a game changer for minimally invasive surgery
376,-1,-1_new_said_study_people,https://www.france24.com/en/live-news/20221013-neanderthals-humans-co-existed-in-europe-for-over-2-000-years-study,"Paris (AFP) – Neanderthals and humans lived alongside each other in France and northern Spain for up to 2,900 years, modelling research suggested Thursday, giving them plenty of time to potentially learn from or even breed with each other.Advertising Read moreWhile the study, published in the journal Scientific Reports, did not provide evidence that humans directly interacted with Neanderthals around 42,000 years ago, previous genetic research has shown that they must have at some point.Research by Swedish paleogeneticist Svante Paabo, who won the medicine Nobel prize last week, helped reveal that people of European descent -- and almost everyone worldwide -- have a small percentage of Neanderthal DNA.Igor Djakovic, a PhD student at Leiden University in the Netherlands and lead author of the new study, said we know that humans and Neanderthals ""met and integrated in Europe, but we have no idea in which specific regions this actually happened"".Exactly when this happened has also proved elusive, though previous fossil evidence has suggested that modern humans and Neanderthals walked the Earth at the same time for thousands of years.To find out more, the Leiden-led team looked at radiocarbon dating for 56 artefacts -- 28 each for Neanderthals and humans -- from 17 sites across France and northern Spain.The artefacts included bones as well as distinctive stone knives thought to have been made by some of the last Neanderthals in the region.The researchers then used Bayesian modelling to narrow down the potential date ranges.'Never really went extinct'Then they used optimal linear estimation, a new modelling technique they adapted from biological conservation sciences, to get the best estimate for when the region's last Neanderthals lived.Djakovic said the ""underlying assumption"" of this technique is that we are unlikely to ever discover the first or last members of an extinct species.""For example, we'll never find the last woolly Rhino,"" he told AFP, adding that ""our understanding is always broken up into fragments"".The modelling found that Neanderthals in the region went extinct between 40,870 and 40,457 years ago, while modern humans first appeared around 42,500 years ago.This means the two species lived alongside each other in the region for between 1,400 and 2,900 years, the study said.Humans, neanderthals, Denisovan and mystery hominins AFPDuring this time there are indications of a great ""diffusion of ideas"" by both humans and Neanderthals, Djakovic said.The period is ""associated with substantial transformations in the way that people are producing material culture,"" such as tools and ornaments, he said.There was also a ""quite severe"" change in the artefacts produced by Neanderthals, which started to look much more like those made by humans, he added.Given the changes in culture and the evidence in our own genes, the new timeline could further bolster a leading theory for the end of the Neanderthals: mating with humans.Breeding with the larger human population could have meant that, over time, Neanderthals were ""effectively swallowed into our gene pool,"" Djakovic said.""When you combine that with what we know now -- that most people living on Earth have Neanderthal DNA -- you could make the argument that they never really went extinct, in a certain sense.""© 2022 AFP","Neanderthals, humans co-existed in Europe for over 2,000 years: study"
367,-1,-1_new_said_study_people,https://www.eurekalert.org/news-releases/969904,"Brazilian researchers interviewed 1,905 children twice in seven years and analyzed 22 risk factors that can influence human development. An article on the study is published in Scientific Reports.A significant reduction in childhood poverty could cut criminal convictions by almost a quarter, according to a study conducted in Brazil. An article on the study is published in Scientific Reports. The researchers used an innovative approach involving an analysis of 22 risk factors that affect human development and interviews with 1,905 children at two points – a first interview to form a baseline (mean age 10.3) and a follow-up interview seven years later (mean age 17.8).The scientists concluded that poverty – measured broadly as a combination of little schooling for the head of household, low purchasing power and limited access to basic services – was the only crime-related factor that could be prevented. They used estimates of the population-attributable risk fraction (PARF) to predict the possible reduction in criminal convictions assuming successful early anti-poverty intervention in the lives of the children.In a scenario without poverty, 22.5% of criminal convictions involving these young people could have been prevented. On the other hand, factors such as unplanned pregnancy, prematurity, breastfeeding and prenatal maternal smoking or drinking showed no correlation with future criminal convictions.“A holistic view of young people who commit crimes is necessary in order to understand the circumstances that lead to this situation and a range of preventable factors need to be considered,” said Carolina Ziebold, a researcher in the Department of Psychiatry at the Federal University of São Paulo’s Medical School (EPM-UNIFESP) and first author of the article.Ziebold was supported by FAPESP during her PhD research. She also received a Talented Young Investigator scholarship from CAPES, the Ministry of Education’s Coordination for the Improvement of Higher Education Personnel, via its Internationalization Program.For Ary Gadelha, last author of the article, the use of a complex measure of poverty including many more factors than household income is a groundbreaking aspect of the study. Gadelha is a professor of psychiatry at EPM-UNIFESP and was Ziebold’s thesis advisor.“The study took into account housing conditions and access to public services such as healthcare or sanitation, for example, in order to understand poverty more comprehensively. This led us to advocate broader solutions than merely improving income. The many adversities faced by these children become difficulties in adulthood, such as low educational attainment and unemployment, among others,” Gadelha told Agência FAPESP.The approach used in the study is based on an epidemiological method called exposure-wide association, which is similar to the method used in genome-wide association studies (GWAS). “Exposure-wide association studies explore a broad array of potential exposures relating to a single outcome (using a hypothesis-free approach)”, the authors write.In this case, they add, the analysis encompassed “multiple modifiable perinatal, individual, family and school-related exposures associated with youth criminal conviction to identify new potential targets for the prevention of this complex phenomenon”. Moreover, they argue, “when a significant risk factor [such as poverty] is identified, the magnitude of its effect on criminal conviction should be explained to inform and guide public measures for crime prevention”.Another study led by Ziebold involving the same cohort and published in December 2021 had already found correlations between childhood poverty and a heightened propensity to develop externalizing disorders during adolescence and early adulthood, especially among girls. The researchers concluded that multidimensional poverty and exposure to stressful life events, including frequent deaths and family conflicts, were avoidable risk factors that should be addressed in childhood in order to reduce the impact of mental health problems in adult life (more at: agencia.fapesp.br/37879/).ResultsIn the recent Scientific Reports article, the researchers stress that although baseline poverty was the only modifiable risk factor significantly associated with crime as far as the children in the study sample were concerned, most of them (89%) did not have any criminal convictions.“We wanted to avoid criminalizing poverty and show that it’s a complex phenomenon. Exposure to this situation during a life can lead to social tragedy. Crime is a social question, and punishment alone may not be appropriate in the case of young people. It would be more useful to create real opportunities for rehabilitation – life opportunities,” Gadelha said.Only a small proportion (4.3%) of the 1,905 participants interviewed reported any history of criminal convictions, mainly involving theft, violent robbery, drug dealing and other violent crimes, including a homicide and an attempted homicide.The participants were from the Brazilian High-Risk Cohort Study for Psychiatric Disorders (BHRC), a major community-based survey involving 2,511 families with children aged 6-10 when it began in 2010. They were all students at public schools in two large Brazilian state capitals, São Paulo and Porto Alegre (Rio Grande do Sul). Three follow-up surveys have been completed so far, the last in 2018-19. A fourth has begun this year and is scheduled for completion in 2024.Considered one of the most ambitious childhood mental health surveys ever conducted in Brazil, the BHRC, also known as Project Connection – Minds of the Future, is led by the National Institute of Developmental Psychiatry (INPD), which is supported by FAPESP and the National Council for Scientific and Technological Development (CNPq), an arm of the Ministry for Science, Technology and Innovation (MCTI).More than 20 universities in Brazil and elsewhere are involved in INPD’s activities. Its principal investigator is Eurípedes Constantino Miguel Filho, a professor in the Department of Psychiatry at the University of São Paulo’s Medical School (FM-USP).ImpactAccording to a report published by the United Nations Children’s Fund (UNICEF) in March 2022, “children and adolescents have always been – and continue to be – the most affected by poverty. By the beginning of 2020, the percentage of children and adolescents living in monetary poverty and extreme monetary poverty in Brazil was, proportionally, twice that of adults”.Between 35% and 45%, depending on the age group, lived on less than USD 5.50 per day in 2020. The proportion living on less than USD 1.90 per day – the extreme monetary poverty line – was 12%.Furthermore, according to Getúlio Vargas Foundation’s Center for Research on Social Policies (FGV Social), food insecurity reached a record level in Brazil at the end of 2021, surpassing the global average and affecting mainly women, poor families and people aged 30-49. The proportion of the overall population suffering from food security reached 36%, compared with 17% in 2014. The global average for 2021 was 35%.“We know people have yet to feel the full economic impact of the pandemic, including food insecurity and lack of access to schooling. The consequences of children’s exposure will become clear in the future,” Ziebold said, adding that more research is needed to understand how the vulnerabilities of the places where children live can influence juvenile crime rates. “This type of factor has been observed in research conducted in other countries, such as the United States, where young people are more likely to commit crimes if they live in areas without infrastructure or with gangs. This is a topic for further research.”About 46,000 young people in conflict with the law were processed in 2019 by SINASE, Brazil’s special justice system for juvenile offenders.About São Paulo Research Foundation (FAPESP)The São Paulo Research Foundation (FAPESP) is a public institution with the mission of supporting scientific research in all fields of knowledge by awarding scholarships, fellowships and grants to investigators linked with higher education and research institutions in the State of São Paulo, Brazil. FAPESP is aware that the very best research can only be done by working with the best researchers internationally. Therefore, it has established partnerships with funding agencies, higher education, private companies, and research organizations in other countries known for the quality of their research and has been encouraging scientists funded by its grants to further develop their international collaboration. You can learn more about FAPESP at www.fapesp.br/en and visit FAPESP news agency at www.agencia.fapesp.br/en to keep updated with the latest scientific breakthroughs FAPESP helps achieve through its many programs, awards and research centers. You may also subscribe to FAPESP news agency at http://agencia.fapesp.br/subscribe","Reducing childhood poverty could cut criminal convictions by almost a quarter, study shows"
366,-1,-1_new_said_study_people,https://www.eurekalert.org/news-releases/969890,"New research from the University of Vermont finds the most viewed content on TikTok relating to food, nutrition and weight perpetuates a toxic diet culture among teens and young adults and that expert voices are largely missing from the conversation.Published today in PLOS One, the study found weight-normative messaging, the idea that weight is the most important measure of a person’s health, largely predominates on TikTok with the most popular videos glorifying weight loss and positioning food as a means to achieve health and thinness. The findings are particularly concerning given existing research indicating social media usage in adolescents and young adults is associated with disordered eating and negative body image.“Each day, millions of teens and young adults are being fed content on TikTok that paints a very unrealistic and inaccurate picture of food, nutrition and health,” said senior researcher Lizzy Pope, associate professor and director of the Didactic Program in Dietetics at UVM. “Getting stuck in weight loss TikTok can be a really tough environment, especially for the main users of the platform, which are young people.""The study is the first to examine nutrition and body-image related content at scale on TikTok. The findings are based on a comprehensive analysis of the top 100 videos from 10 popular nutrition, food and weight-related hashtags, which were then coded for key themes. Each of the 10 hashtags had over a billion views when the study began in 2020; the selected hashtags have grown significantly as TikTok’s user base has expanded.“We were continuously surprised by how prevalent the topic of weight was on TikTok. The fact that billions of people were viewing content about weight on the internet says a lot about the role diet culture plays in our society,” said co-author Marisa Minadeo ’21, who conducted the research as part of her undergraduate thesis at UVM.Over the past few years, the Nutrition and Food Sciences Department at UVM has shifted away from a weight-normative mindset, adopting a weight-inclusive approach to teaching dietetics. The approach centers on using non-weight markers of health and wellbeing to evaluate a person’s health and rejects the idea that there is a “normal” weight that is achievable or realistic for everyone. If society continues to perpetuate weight normativity, says Pope, we’re perpetuating fat bias.“Just like people are different heights, we all have different weights,” said Pope. “Weight-inclusive nutrition is really the only just way to look at humanity.”Weight-inclusive nutrition is becoming popular as a more holistic evaluation of a person’s health. As TikTok users, UVM health and society major Minadeo and her advisor Pope were interested in better understanding the role of TikTok as a source for information about nutrition and healthy eating behaviors. They were surprised to find that TikTok creators considered to be influencers in the academic nutrition space were not making a dent in the overall landscape of nutrition content.White, female adolescents and young adults accounted for the majority of creators of content analyzed in the study. Very few creators were considered expert voices, defined by the researchers as someone who self-identified with credentials such as a registered dietitian, doctor, or certified trainer.“We have to help young people develop critical thinking skills and their own body image outside of social media,” said Pope. “But what we really need is a radical rethinking of how we relate to our bodies, to food and to health. This is truly about changing the systems around us so that people can live productive, happy and healthy lives,” said Pope.",TikTok perpetuates toxic diet culture among  teens and young adults
364,-1,-1_new_said_study_people,https://www.eurekalert.org/news-releases/969469,"Researchers Jennifer Dill, Jiahui Ma, Nathan McNeil, Joseph Broach and John MacArthur of Portland State University have published a new article in the November 2022 issue of Transportation Part D: Transport and Environment. The open-access article, ""Factors influencing bike share among underserved populations: Evidence from three U.S. cities,"" examines bike share use and interest among lower-income residents and people of color in New York, Chicago, and Philadelphia.There is evidence that lower-income and people of color (POC) in the U.S. do not use bike share as much as higher-income and white people. Using data from residents living near bike share stations in New York, Chicago, and Philadelphia, the paper examines reasons for these disparities. Researchers looked at many factors that might explain bike share use and interest in lower-income, racially diverse, traditionally underserved neighborhoods. They focused on residents who live near bike share stations, so that proximity would not be a barrier.A few key findings:People who are not members, but are interested in using bike share, including POC, are motivated to use bike share for fun, recreation, and social reasons (as opposed to utility). Knowledge of bike share and receiving information from interactive sources (for example, bike share ambassadors) are associated with bike share use. Cost is a barrier for people who are interested in using bike share, but are not members. Discounted memberships are one solution, but survey results indicate that many people do know know about them.Some reasons for not using bike share among people of color and lower-income people may also be related to reasons for not bicycling, generally. These include concerns about traffic safety as well as personal safety.This paper is an analysis of data collected in a ""Breaking Barriers to Bike Share"" project funded by the National Institute for Transportation and Communities (NITC) and the Better Bike Share Partnership (BBSP). Read more about the original study and explore some of the products to come out of this research, including a set of ten bike share equity briefs to help operators establish equity programs based on what's been shown to work.Portland State University's Transportation Research and Education Center (TREC) is home to the U.S. DOT funded National Institute for Transportation and Communities (NITC), the Initiative for Bicycle and Pedestrian Innovation (IBPI), PORTAL, BikePed Portal and other transportation grants and programs. We produce impactful research and tools for transportation decision makers, expand the diversity and capacity of the workforce, and engage students and professionals through education and participation in research.",Factors influencing bike share among underserved populations: evidence from three US cities
401,-1,-1_new_said_study_people,https://www.mcgill.ca/newsroom/channels/news/flatworm-inspired-medical-adhesives-stop-blood-loss-342730,"Every year around 2 million people die worldwide from hemorrhaging or blood loss. Uncontrolled hemorrhaging accounts for more than 30% of trauma deaths. To stop the bleeding, doctors often apply pressure to the wound and seal the site with medical glue. But what happens when applying pressure is difficult or could make things worse? Or the surface of the wound is too bloody for glue? Drawing inspiration from nature, researchers from McGill University have developed a medical adhesive that could save lives, modeled after structures found in marine animals like mussels and flatworms.“When applied to the bleeding site, the new adhesive uses suction to absorb blood, clear the surface for adhesion, and bond to the tissue providing a physical seal. The entire application process is quick and pressure-free, which is suitable for non-compressible hemorrhage situations, which are often life-threatening,” says lead author Guangyu Bao, a recently graduated PhD student under the supervision of Professor Jianyu Li of Department of Mechanical Engineering.In putting the new technology to the test, the researchers found that the adhesive promotes blood coagulation. The adhesive can also be removed without causing re-bleeding or even left inside the body to be absorbed. “Our material showed much better-improved safety and bleeding control efficiency than other commercial products. Beyond bleeding control, our material could one day replace wound sutures or deliver drugs to provide therapeutic effects,” says senior author Professor Jianyu Li.About this study“Liquid-infused microstructured bioadhesives halt non-compressible hemorrhage” by Guangyu Bao et al. was published in Nature Communications.About McGill UniversityFounded in Montreal, Quebec, in 1821, McGill University is Canada’s top ranked medical doctoral university. McGill is consistently ranked as one of the top universities, both nationally and internationally. It is a world-renowned institution of higher learning with research activities spanning three campuses, 11 faculties, 13 professional schools, 300 programs of study and over 39,000 students, including more than 10,400 graduate students. McGill attracts students from over 150 countries around the world, its 12,000 international students making up 30% of the student body. Over half of McGill students claim a first language other than English, including approximately 20% of our students who say French is their mother tongue.",Flatworm-inspired medical adhesives stop blood loss
301,-1,-1_new_said_study_people,https://www.businessinsider.com/great-labor-shortage-looming-population-decline-disaster-global-economy-2022-10?utm_source=reddit.com,"The world has seen a population boom over the past few decades. The globe's 4 billionth person was born in 1975 and now, in about a month, the 8 billionth person will probably be born. This ""population bomb"" has been cited by experts as an unprecedented challenge to the ecological balance of our planet — when 50 Nobel laureates were asked in 2017 what the greatest threat to humanity is, more than a third of them named the overpopulation of the planet. And there is no doubt that slowing population growth is an important tool to combat the climate crisis.But what those Nobel laureates overlooked is the fact that the forces leading to a population drop are already in motion. In fact, in 40 years or so, the global population will begin to decline.It will not be a virus, a war, or a natural disaster that will cause this population decline. Instead, it will be an increase in living standards. Progress in living standards since the birth of the Industrial Revolution has been accompanied not only by rising life expectancy, but also by falling birth rates. People are healthier, richer, better educated, living longer, and having fewer children. As a result, the number of children born in rich countries like the US, in Europe, and China is not sufficient anymore to keep those populations stable. But this decrease in humanity is not a reason to cheer, but rather a looming disaster for our economy. The great labor shortage caused by the declining population will cripple our global economy unless we find innovative ways to keep things running.The population is going to shrinkThe economic and demographic forces that will lead to a global population drop by the end of the 21st century have been at work in major economies for a long time. Year after year, the birth rates of more rich and middle-income countries fall below the critical ""replacement level"" — the level at which people have enough children to maintain current population levels. A society needs 2.1 children per woman for the population to remain stable. In the US, the current rate is 1.6 children per woman — and it's even lower in most European countries as well as Japan (1.3), China (1.2), and South Korea (0.8). Soon, almost every country in the world will fall below this breakeven point.Research has shown that across countries and regions, as living standards improve, the number of children per family starts to decline. The reasons are varied — more economic opportunity for women, better access to education for children, lower infant mortality — but eventually, what was once a poor, young country becomes wealthier and must face the challenge of an aging population that does not have enough young workers to support it.By the end of this century, the global population will have decreased by 1 billion people from its peak, according to a 2020 analysis by researchers at the Gates Foundation, and in the most extreme scenario, the population could decline by almost 2 billion from where it is today, to just over 6 billion. The German working population will have declined by a third, based on the average scenario from the researchers, and in Italy, Spain, and Greece it will have declined by more than half. Poland, Portugal, Romania, Japan, and China will all lose up to two-thirds of their labor force, according to the projections. The looming population decline is a wake-up call: Instead of the ""population bomb"" that some have feared for decades, we will face a population drop, and it will have enormous consequences for the world's prosperity.The labor shortage has already startedWhile a global population drop is good news for the planet, it will be a huge challenge for our economic and social systems. The most important fuel of economic growth in the past several centuries has been people. And with fewer people, less work can get done.We are already experiencing the beginning of this great labor shortage in industries ranging from airlines to day cares to military service. In the coming years, many more sectors and occupational fields will be affected. With fewer train drivers, teachers, engineers, doctors, care workers, and programmers, many companies will produce or perform less. And as the population drops, the amount of money being spent at these businesses will also shrink. Less consumption leads to fewer sales and fewer sales lead to lower profits and, thus, less economic growth.Not only will the number of workers decline with the population, but how much each of these workers can produce will slip as well. Global per capita productivity, the formal measure of how much a worker can produce in an hour and the most important indicator of progress, has recently stagnated. The combination of a declining labor force and stagnating productivity is toxic. It could lead to a drop in economic output or, in the worst-case scenario, decades of stagnant growth. Some 250 years after the Industrial Revolution, we could enter an era of stasis.Our growing economy has also allowed people to retire in their later years, with a social safety net and younger relatives to take care of them. But we could soon see the reverse: The number of people that need to be taken care of will far exceed the number of working people who can support them. In the US, the increase in the number of retirees compared with the working population is already straining the system. In 2020, there were 3.5 people of working age for each retired person. In 2050, this ratio will drop to 2.6. This will put enormous pressure on the working population of the US to become more productive in order to support an increasing number of older people while also sustaining economic growth.This is a stunning reversal of the world's historical growth patterns. For centuries, millions of people poured into factories and offices and boosted consumption with their incomes. With their tax money, they ensured that investments could be made in education, health, research, infrastructure, and in a social system that finances an ever-growing number of pensions. This engine is now beginning to sputter as more people retire and there aren't enough young workers to replace them.More people or more productivityThere are two solutions to combat labor shortages: We can have more people work, and/or we can work more productively. The US and its effective migration policies are a visible example of how to get more people working despite low birth rates. Japan is a successful example of how to deal with both a shrinking and aging population by becoming more productive through automation and digitization.To get out of a labor shortage, economies will need a productivity boost on par with the industrialization miracle that led the world out of relative widespread poverty 250 years ago. If fewer working people are going to finance an ever-expanding welfare state, more investment in innovation and progress is needed. This is especially true for the service sector, where a majority of the labor force works in developed countries, and where productivity has been stagnating for almost 20 years now. There is plenty of room for improvement. Business models based on low-skilled labor at minimal wages are not characteristic of a high-tech country. They are a sign of regression. Therefore, it's necessary to create incentives through raising minimum wages to automate simple work wherever possible.Change also means letting go of creaky processes, outdated business models, and obsolete industries. That means investing in cutting-edge companies: Among the world's top-10 companies when it comes to market capitalization, eight are tech companies whose success is based on products and business models that have adapted to quickly changing markets and even created markets we never knew existed. That also means making a more dynamic labor market that motivates people to find the right job. The Great Resignation has shown that we can allow workers to more dynamically switch to jobs with higher salaries that better fit their skills. And finally, that means reforming our education system. As long as one-fifth of 15-year-old students cannot read at a basic level, we won't be able to combat the challenges of population decline. The next era of work will require skills other than diligence and obedience — which is what schools primarily teach today. Creativity, resilience, and the ability to solve complex problems will be imperative skills to keep our economy running.And a new economy will not emerge without the help of immigrants from around the world. As the population declines, countries will shift from trying to keep immigrants out to fighting over the dwindling supply of in-demand workers. To continue to attract people in the future, countries will need to start shifting their immigration strategies now. For instance, a lack of language support excludes many children of immigrants from the labor market, and high real-estate prices lead to increasing segregation, especially in large cities. Countries like Canada have tried to overcome these problems by proactively welcoming refugees and supporting new residents. This rejection of unequal treatment and discrimination will pay off in the long run as the world's most important resource, human beings, becomes scarce.If history has taught us anything, it's that progress is always accompanied by change and the courage to try something new. To combat the coming population bust, the world will need nothing less than a revolution of our minds. We need innovation and new ideas: robots and artificial intelligence that do our work for us and let everyone get the chance for good education and training. Along the way, we will have to find solutions to make this continued progress climate neutral by investing in sustainable-energy production and low-emission technologies. By doing so, we will make a better world for our children.Sebastian Dettmers is the CEO of StepStone, which is owned by Axel Springer, the parent company of Insider. He is also the author of a new book on the future of the world's population.",The Great People Shortage is coming â and it's going to cause global economic chaos
289,-1,-1_new_said_study_people,https://videocardz.com/press-release/noctua-unveils-thermal-paste-guard-for-amd-am5-ryzen-7000-processors,"« press release »Noctua presents NA-TPG1 thermal paste guard for AMD AM5Vienna, October 6th 2022 – Noctua today announced its new NA-TPG1 thermal paste guard for AMD’s latest AM5 based Ryzen processors. When the mounting pressure of the cooling solution is applied, excess thermal paste will be squeezed outwards. With AM5 CPUs, this excess paste tends to accumulate in the cut-outs at the sides of the heat-spreader and may become difficult to remove. Simple and risk-free to apply, the NA-TPG1 prevents this undesired phenomenon.“While there’s no denying that AMD’s new Ryzen 7000 processors perform fantastic, we found that the cut-outs at the side of the heat-spreader tend to attract thermal paste that can be challenging to clean off” says Roland Mossig (Noctua CEO). “This is where our new thermal paste guard steps in: it’s a simple yet highly effective tool for keeping your new Ryzen 7000 series CPU nice and clean.”Made from highly heat-resistant polycarbonate and forming a tight seal around the edges of the CPU’s heat-spreader , the NA-TPG1 is simple and risk free to apply and remove. Despite its simplicity, it is highly efficient at preventing thermal paste from accumulating in the cut-outs at the sides of AM5 CPUs.The NA-TPG1 will be available in a separate set with ten NA-CW1 cleaning wipes (NA-STPG1) as well as with new AM5 editions of Noctua’s award-winning NT-H1 and NT-H2 thermal pastes. All three products are scheduled to become available in December.The manufacturer’s suggested retail price will be EUR/USD 7.90 for the NA-STPG1, EUR/USD 9.90 for the NT-H1 3.5g AM5 Edition and EUR/USD 13.90 for the NT-H2 3.5g AM5 Edition.Links« end of the press release »",Noctua unveils thermal paste guard for AMD AM5 Ryzen 7000 processors
228,-1,-1_new_said_study_people,https://techcrunch.com/2022/08/04/uae-aims-to-convert-oil-wealth-into-tech-prowess/,"The Middle East has long been thought of as an oil region, but the United Arab Emirates aims to change this with an intense focus on growing the country’s technology and startup scene.For the first half of 2022, the Middle East region brought in $1.73 billion in investments across 354 deals, up from more than $1.2 billion in the first half of 2021 — a 64% year over year growth. The UAE took in 46% of the total venture capital received in the Middle East and Africa in 2021, according to the country’s Ministry of Economy.The UAE began focusing on its tech and startup hub goal in 2016 by establishing the Sharjah Research Technology and Innovation Park to incubate companies in a variety of industries, including water management, renewable energy, transportation, manufacturing and agriculture.TechCrunch highlighted some of the more recent technology activity coming out of the United Arab Emirates, including that the country was going to pour $800 million into a fund to invest in space initiatives, that the region is now home to the “world’s largest vertical farm” and a global investment in local proptech startup Huspy.In 2017, the UAE created an artificial intelligence ministry position, which it filled with H.E. Omar bin Sultan Al Olama, who had previously worked in the banking and telecommunications sectors.H.E. Al Olama recently spoke with me about the burgeoning Emerati startup and venture capital ecosystem, and the country’s approaches to attracting U.S. VC investment. What follows are highlights from our conversation, lightly edited for clarity and length.TechCrunch: Is the UAE’s venture capital presence fairly new?H.E. Al Olama: If you look at the geography, you will see that the UAE attracts over 50% of all venture capital investments of this whole region. That is interesting, but when you actually look at the size of the population, it becomes a lot more interesting because you’re talking about a very high concentration of very high-quality talent, as well as an ecosystem that allows for thriving startups and startups that don’t just get started but actually go through different scale up phases.In terms of venture capital and investment into the region, I saw it was over a billion dollars in the last year. Do you see that increasing this year or on par with last year?For the first half of this year, the investments have been much more than we expected. Of all the investments in the first six months, there has been $1.73 billion invested in the Middle East, out of which 37.2% were invested in the UAE. So it is actually quite substantial. If we look at the comparison from 2022 to 2021, January was 2.5 times as was February, and March was 1.5 times, April was 1.5 times, May was 1.4 times and June was 1.2 times. That is for the whole region. What you can see from that is the interest that the global investors are having in the region. And, the theory that the UAE is still getting the biggest portion of that compared to other countries in the region that have a bigger population or seemingly a bigger market size, shows that the snowball started rolling a few years back with the startups that we’ve had and it’s actually just getting bigger. I think we’re just getting started.How have you been able to attract tech companies to the UAE?Being tax free is definitely one incentive, but the UAE today is also a financial hub for our region and one of the top financial hubs globally. There’s a lot of capital here ready to be deployed. One great advantage is a lot of investors feel more comfortable investing in a company situated in the UAE because of the transparency of the court system. Government legislations are friendly to the private sector. It’s an environment that allows people to be able to thrive because they do not feel marginalized or disadvantaged because they have a certain ethnicity or sex or nationality. It is known to be able to be a place where anyone from anywhere in the world can actually succeed. In addition, the infrastructure is also quite advanced in terms of the quality of the roads and penetration of smartphones — we have the highest smartphone penetration in the world.The government has been rolling out a lot of different incentives over the years, including startup-friendly policies. If someone were to move to the UAE, what would they need to know?We looked at all of the different sectors that are supportive of the startup landscape and tried to put in incentives to ensure that people actually prefer something up in the UAE as opposed to anywhere else. In most countries, it’s very, very hard to get a visa. If you are a talent and work specifically in a digital economy that we’re very focused on, you can get a permanent residency or a long-term residency right off the bat. Another thing, you can start the company within a day. Third, there are a lot of different programs, for example, incubators and accelerators and government contracts that are very appealing.TC: How did the UAE’s AI mandate come about, and what was your plan to get it started?We asked what are we really trying to decipher? What exactly is the potential for the UAE, whether it’s positive potential or negative potential, and how can we ensure that we deploy AI effectively across the country in a way that improves quality of life. Quality of life is actually the main driver for AI. It’s not economic gain, as it is in many countries. Second, it is hard to ensure that our policies or legislations actually give us an advantage with regards to the negative consequences of deploying AI, whether it’s locally or globally. If AI goes wrong somewhere else, how do we ensure we are less likely to get burdened by it? Our motto from day one was building a responsible artificial intelligence nation that is useful for the current, but also for the future.Do you expect the UAE to have challenges attracting U.S. talent and investment due the region being known for its human rights violations?The Middle East does have a reputation as being something of a ‘tough neighborhood’ for a wide range of reasons, from conflict to failures of governance. We do see the Emirates as differentiated, with a highly tolerant, multinational community consisting of people from over 195 nationalities living, working, learning and playing together in an atmosphere of peace, stability and security.Why has supporting startups and venture capital been such an important push for the region?A few reasons: First, if you look at our history, the UAE has always been a country of merchants and always looking to support business and unleash opportunities. The second thing is, we have been very adamant and very vocal about our ambition to diversify away from oil and that can be seen through investments in renewables and other parts of our economy, namely the digital economy, to ensure that we are competitive and comparative to advanced and developed countries from around the world. Finally, we are not a large country, so we will not be able to compete in certain sectors with other countries that have a cheaper cost of labor. But, if you look at the digital economy and sectors that are emerging right now, because of the advancement of technology, you’re able to get incredible returns from talent that — even if they are quite expensive — are able to create output, and that is what we’re actually aiming towards.",UAE aims to convert oil wealth into tech prowess
227,-1,-1_new_said_study_people,https://techcrunch.com/2022/08/04/bill-gates-breakthrough-energy-terabase-robot-solar-farms/,"Breakthrough Energy Ventures, a climate-focused VC firm linked to some of Earth’s wealthiest individuals, has joined a $44 million bet on solar startup Terabase Energy.Terabase aims to rapidly build new solar farms “at the terawatt scale,” CEO Matt Campbell said in a statement. The startup claims its automated, on-site factory can already speed up plant construction and cut costs by employing robotic arms that lift and connect heavy solar panels to sun trackers. When asked for photos of the insides of its factory, Campbell pointed TechCrunch to previously published aerial pics and declined to share more, “for competitive reasons.”Terabase also makes software tools to manage the design and construction of solar farms. The startup recently wrapped its first commercial project, where its robots reportedly installed 10 megawatts worth of panels. There are one million megawatts in a terawatt, so the startup still has a long way to go to reach its aspirations.Breakthrough Energy Ventures was founded by Bill Gates, and its board members include Jeff Bezos and Masayoshi Son. The VC firm co-led the Terabase deal alongside Lime and Amp Robotics investor Prelude Ventures.Their investment comes as rich folks face scrutiny for their outsized climate pollution. Gates’ private jet might not be as active as Taylor Swift’s, yet the Microsoft co-founder reportedly owns several and has called private flying his “guilty pleasure.”Other recent deals for solar energy startups include panel installer Zolar ($105 million) and solar network developer Okra ($2.1 million).",Bill Gatesâ Breakthrough Energy backs Terabaseâs robot-built solar farms
221,-1,-1_new_said_study_people,https://techcrunch.com/2022/07/21/luminar-lidar-ecarx-geely-china-ev/,"Luminar, the Florida-based lidar company that went public via SPAC in 2020, has formed a close alliance with an auto behemoth in China. It’s making a strategic investment of an undisclosed amount in Ecarx, an auto tech startup co-founded by Eric Li, founder of China’s largest private automaker Geely, Ecarx said on Thursday.The funding will be part of the pair’s wider collaboration on automotive-grade technologies, which aims to “enable advanced safety and automated driving capabilities in the production of consumer vehicles and commercial trucks,” a plan that Luminar unveiled in May.Luminar’s sensing technology can potentially reach millions of vehicles through the Geely/Ecarx auto empire. Ecarx is building a comprehensive platform for the future of cars, focusing on the likes of auto chips and smart vehicles. Its customers include, unsurprisingly, Geely-owned brands like Lotus and Volvo.Shen Ziyu, Ecarx’s other co-founder and a former General Motors executive, told Reuters in March 2021 that the company had already supplied 2.5 million vehicles.The lidar industry in China is enjoying a boom as the country’s electric car makers — which themselves have benefited from government support for renewable energy — woo picky consumers with advanced driving technology, in-car entertainment and other novel auto features.Luminar’s tie-up with Ecarx will be critical in helping it confront domestic lidar players, from BYD-backed RoboSense and Bosch-backed Hesai to Temasek-funded Innovusion and Livox, which sprang out of DJI.“The collaboration will help Luminar to accelerate deployment of its industry-leading long-range lidar and software in the [Chinese] market and beyond through Ecar’s deep connection with Geely and the Geely ecosystem, comprising some of the world’s most reputable automotive brands,” Luminar said in May.The same month, the American lidar maker said it had brought on Jackie Chen, a Harman veteran, to head its China business.Ecarx’s empire is ever expanding. On Thursday, the company announced another funding boost along with the Luminar partnership. Siengine, an automotive system on chip maker it co-founded with Arm China, has completed a Series A funding round of nearly 1 billion yuan or $15 million. Sequoia Capital China led the round, with Bosch’s China venture capital arm Boyuan Capital and others participating.Earlier this month, Li and Shen bought Chinese smartphone maker Meizu, once a Xiaomi archrival, to work toward a future of “multi-device, scenario-agnostic, and immersive” digital experience, which will no doubt include system integration between vehicles and handsets.In May, Ecarx announced plans to go public through a merger with a blank-check firm in a $3.8 billion deal.","Luminar to invest in Geely-affiliated Ecarx, eyes China market"
218,-1,-1_new_said_study_people,https://techcrunch.com/2022/07/18/united-arab-emirates-launches-820m-fund-to-boost-domestic-space-economy/,"The United Arab Emirates will pour more than $800 million into space initiatives through a massive new fund, with the first investment going toward the establishment of a remote sensing satellite constellation.That constellation, called “Sirb” (the Arabic word for a flock of birds), will use synthetic aperture radar technology to capture high-resolution images. The country listed border control, oil spill detection and ship detection and tracking among the practical uses for the satellites. The UAE aims to launch the first satellite in three years, with the entire satellite program to last around six years. It did not specify the total amount it plans to invest in the constellation.The 3 billion AED ($820 million) National Space Fund, announced Monday, is the latest signal that the country is looking to the space sector to help diversify its oil-dependent economy. As of 2020, oil exports made up nearly 30% of total gross domestic product. However, the UAE’s Ministry of Economy counts space among its “promising economic sectors,” and its space program has undertaken a number of major programs in the past five years.The UAE established an astronaut program in 2017; one of its two astronauts will fly to the International Space Station in 2023 as part of a long-duration mission with Axiom Space. That astronaut, Hazza Al Mansoori, was the first person from the Emirates to go to space in a 2019 mission to the ISS.KhalifaSat, the first satellite that was entirely designed and manufactured in the Emirates, was launched to orbit in 2018. Since, the UAE sent an orbiter to Mars in 2020, announced plans to send a 22-pound rover to the moon with Japanese startup ispace and said it would send a probe to the asteroid belt between Mars and Jupiter, with the aim of ultimately landing on an asteroid in the early 2030s.The National Space Fund will “actively encourage partnerships between international and local enterprises,” according to a statement. The country will also seek solicitation from local and international companies for the Sirb satellite constellation.",United Arab Emirates launches $820M fund to boost domestic space economy
215,-1,-1_new_said_study_people,https://techcrunch.com/2022/07/13/xfuel-hopes-to-sail-past-biofuels-troubled-past-with-modular-reactor-design/,"Biofuels are the sirens of renewable energy, luring startups with enchanting promises of enormous markets, from aviation to trucking and shipping. Companies just can’t help throwing themselves at the problem, and more than a few have been dashed to pieces in the process.One new startup, though, hopes it’s Ulysses in this tale, able to sail past the rocky shores that have sunk more than a few of its predecessors.XFuel is pinning its hopes on a modular refinery design that it says can produce replacement fuels for marine shipping and diesel vehicles with a carbon footprint that’s up to 85% lower now and possibly carbon-neutral in the future.Today, the Dublin-based startup announced an €8.2 million investment round led by AENU, a new German VC firm, and joined by Union Square Ventures and HAX/SOSV.XFuel is currently making marine- and diesel-grade fuels and developing aviation fuels.“It is a direct replacement for the fossil counterparts,” co-founder and CEO Nicholas Ball said. “The focus here was to develop a technology to be able to produce fuel at a comparable or lower price point than fossil fuels. We went down this R&D path for a number of years, a lot of trial- and-error-type work.”The result, he said, is a demonstration plant in Spain that “proves out the technology that we have and the modularity that we’re trying to encompass here.”The heart of XFuel’s work revolves around what the company calls mechanical catalytic conversion, which takes plant material and uses heat, chemical reactants and friction to help break it down. (Ball, when pressed, wouldn’t be more specific than that.)",XFuel hopes to sail past biofuelsâ troubled past with modular reactor design
203,-1,-1_new_said_study_people,https://techcrunch.com/2021/09/29/energize-ventures-raises-330m-to-fund-energy-mobility-and-climate-resiliency-technology/,"Energize Ventures, an early and growth-stage venture fund, has announced the closing of its second fund with total capital commitments of $330 million. Fund II will be used to help scale and commercialize software across renewable energy, mobility, cybersecurity, battery storage, critical infrastructure and climate resiliency.The fund, which is worth exactly double what Energize raised for its first fund, is backed by anchor investors such as Invenergy, CDPQ, SE Ventures, GE Renewable Energy and Hannon Armstrong. Credit Suisse, Xcel Energy, American Electric Power and Equinor Ventures also participated.“Since we first launched Energize five years ago, we have seen the energy and industrial sectors undergo a massive digital transformation,” said John Tough, managing partner of Energize Ventures, in a statement. “The transition towards a more renewable and sustainable future is outpacing all expectations, and market participants are digitizing operations to address this new, emerging scale.”To date, Energize has deployed capital from Fund II into three investments, including Munich-based predictive battery analytics software TWAICE, Columbus, Ohio-based IoT device company Finite State and New York-based critical infrastructure safety AI company Urbint, according to the firm. Fund II is targeting 15 or more early-stage digital-first startups that are raising Series A, B or C rounds in the energy and sustainable industry sectors. Typically, Energize Ventures invests around $10 million to $20 million and prefers to lead the round. The firm’s first round invested in 14 software-based companies, including the now-public EV charging solution Volta, cloud manufacturing startup Fast Radius and solar software company Aurora Solar.“Energize exclusively invests in digital solutions,” according to the company. “That means no hardware, no moonshots that require untold amounts of capex to get off the ground — just technologies at the software layer at commercialization. Part of that comes from the firm’s investment strategy; the team leans on its LPs (including well-known corporates such as GE and Schneider) to identify the challenges facing the sectors today, and then finds solutions that overcome those barriers to decarbonization.”","Energize Ventures raises $330M to fund energy, mobility and climate resiliency technology"
201,-1,-1_new_said_study_people,https://techcrunch.com/2021/08/25/zeits-early-warning-wearable-for-sleep-strokes-could-save-hours-and-lives/,"Those at risk are always vigilant for the signs of a stroke in progress, but no one can be vigilant when they’re sleeping, meaning thousands of people suffer “wake-up strokes” that are only identified hours after the fact. Zeit Medical’s brain-monitoring wearable could help raise the alarm and get people to the hospital fast enough to mitigate the stroke’s damage and potentially save lives.A few decades ago, there wasn’t much anyone could do to help a stroke victim. But an effective medication entered use in the ’90s, and a little later a surgical procedure was also pioneered — but both need to be administered within a few hours of the stroke’s onset.Orestis Vardoulis and Urs Naber started Zeit (“time”) after seeing the resources being put toward reducing the delay between a 911 call regarding a stroke and the victim getting the therapy needed. The company is part of Y Combinator’s Summer 2021 cohort.“It used to be that you couldn’t do anything, but suddenly it really mattered how fast you got to the hospital,” said Naber. “As soon as the stroke hits you, your brain starts dying, so time is the most crucial thing. People have spent millions shrinking the time between the 911 call and transport, and from the hospital door to treatment. but no one is addressing those hours that happen before the 911 call — so we realized that’s where we need to innovate.”If only the stroke could be identified before the person even realizes it’s happening, they and others could be alerted and off to the hospital long before an ambulance would normally be called. As it turns out, there’s another situation where this needs to happen: in the OR.Surgeons and nurses performing operations obviously monitor the patient’s vitals closely and have learned to identify the signs of an impending stroke from the EEG monitoring their brainwaves.“There are specific patterns that people are trained to catch with their eyes. We learned from the best neurologists out there how they process this data visually, and we built a tool to detect that automatically,” said Vardoulis. “This clinical experience really helped, because they assisted in defining features within the signal that helped us accelerate the process of deciding what is important and what is not.”The team created a soft, wearable headband with a compact EEG built in that monitors the relevant signals from the brain. This data is sent to a smartphone app for analysis by a machine learning model trained on the aforementioned patterns, and if anything is detected, an alarm is sent to the user and pre-specified caregivers. It can also be set to automatically call 911.“The vast majority of the data we have analyzed comes out of the OR,” said Vardoulis, where it can immediately be checked against the ground truth. “We saw that we have an algorithm that can robustly capture the onset of events in the OR with zero false positives.”That should translate well to the home, they say, where there are actually fewer complicating variables. To test that, they’re working with a group of high-risk people who have already had one stroke; the months immediately following a stroke or related event (there are various clinically differentiated categories) is a dangerous one when second events are common.“Right now we have a research kit that we’re shipping to individuals involved in our studies that has the headband and phone. Users are wearing it every night,” said Vardoulis. “We’re preparing for a path that will allow us to go commercial at some point in 2023. We’re working with he FDA to define the clinical proof needed to get this clear.”They’ve earned a “Breakthrough Device” classification, which (like stroke rehabilitation company BrainQ) puts them in position to move forward quickly with testing and certification.“We’re going to start in the U.S., but we see a need globally,” said Naber. “There are countries where aging is even more prevalent and the support structure for disability care is even less.” The device could significantly lower the risk and cost of at-home and disability care for many people who might otherwise have to regularly visit the hospital.The plan for now is to continue to gather data and partners until they can set up a large-scale study, which will almost certainly be required to move the device from direct-to-consumer to reimbursable (i.e., covered by insurance). And although they are totally focused on strokes for the present, the method could be adapted to watching for other neurological conditions.“We hope to see a future where everyone with a stroke risk is issued this device,” said Vardoulis. “We really do see this as the missing puzzle piece in the stroke care continuum.”",Zeitâs early warning wearable for sleep strokes could save hours and lives
200,-1,-1_new_said_study_people,https://techcrunch.com/2021/08/16/brainq-raises-40m-to-transform-stroke-patient-rehabilitation-with-its-home-therapy-device/,"If you injure your elbow, surgery can help. If you lose a leg, prostheses are available. But problems within the brain are more difficult to treat, and for stroke victims, rehabilitation is largely left to the body’s own repair mechanisms. BrainQ aims to change that with a device that stimulates the damaged part of the brain and promotes self-repair, showing enough improvement in studies to warrant a Breakthrough Device certification from the FDA — and the company has just raised $40 million to take it to market.It should be said at the outset that doubting the efficacy of some brainwave-emitting miracle device is natural. And in fact when I spoke with BrainQ’s founder Yotam Drechsler, he reminded me of the last time we’d talked — back in 2017, at which time I “expressed strong skepticism.”No hard feelings — the tech was largely notional then, he admitted — but since that time the team has continued its work, raised some money, and what was a promising if not well-supported thesis then has turned into one backed by firsthand data and clinical outcomes. The resulting system could be the biggest improvement to stroke therapy in decades or more.Strokes can result in various obvious impairments, such as grip strength or coordination, but of course the injury is not to the hand or leg itself, it is to the networks in the brain that govern those parts. But medical science has no method for directly rebuilding those networks — the brain must do so on its own, in its own time.To aid this, regular physical therapy and brain health checkups, sometimes for years on end, are used to, in essence, make sure the brain is still working on it and that the parts of the body don’t themselves fall into disrepair.The most interesting improvements to this process in recent years have added tech into the loop to provide immediate feedback, such as that one’s balance is skewed to one side, and providing stimuli that aim to counteract that. But ultimately it’s still targeted physical therapy.Drechsler and BrainQ see the problem a little differently. It’s not simply an injury but a disturbance to the brain’s carefully cultivated homeostasis, one which it has no means to counteract. He compared a stroke not to an analogous injury but to a baby born prematurely and whose body is not up to the task of heating itself. What do you do in such a case? You don’t attempt to “fix” the body so it can operate at lower temperatures, or supercharge the heat output — you just put the kid in an incubator, and everything proceeds as it should.BrainQ’s device does something similar, making the brain operate better by changing its local environment.“We map the channels of healthy brains and non-healthy brains and compare them. Once we find these, we use a low-intensity magnetic field therapy to resonate in the brain and facilitate its endogenous recovery mechanisms,” explained Drechsler.It’s been shown in other contexts that this type of stimulation can produce improved neuroplasticity — the capability of the central nervous system to reprogram itself. By narrowly targeting stroke-affected areas, BrainQ’s device promotes neuroplasticity in them, leading to expedited recovery.But it’s not simply a matter of saying “the stroke affected the ventral half of the right occipital lobe, aim the magnets there.” The brain is a complicated system, and strokes affect networks, not just a given cubic centimeter. BrainQ has deployed machine learning and a large collection of data to better understand how to target those networks.Without diving too deeply into how the brain operates, let it suffice to say that certain networks operate locally at very specific spectral signatures or frequencies as detected by EEG readings. The left hand and left foot may occupy the same region of the motor cortex, but the hand might operate at 22 Hz, while the foot operates at 24 Hz, for example.“The question is, how do you find these signatures?” asked Drechsler. As it’s somewhat difficult to explain, I asked him to put it in his own words after we spoke:The novelty of BrainQ’s investigational treatment lies in the data-driven method we have deployed in order to inform the ELF-EMF frequency parameters. In choosing these parameters, our aim is to select frequencies that characterize motor-related neural networks in the CNS, and are related to the disability a person experiences following a stroke or other neurological trauma. To achieve this, we have analyzed a large-scale amount of healthy and non-healthy individuals’ brainwaves (electrophysiology data). Our technology uses explanatory machine learning algorithms to observe the natural spectral characteristics and derive unique therapeutic insights. These are used by BrainQ’s technology to target the recovery of impaired networks.The device they’ve created to administer the treatment is unusual. Because it’s a whole-brain magnetic field generator, it has a rather bulky cylindrical headpiece, but the rest of it fits into a sort of back brace and hip pack. That’s because, unlike the more common magnetic brain imaging tech, MRI, the fields and currents involved are extremely small.“We use very, very low intensity, about the same level as normal brain activity,” said Drechsler. “It’s not about creating an action potential or a jump in activity, it’s about creating the right conditions for the recovery mechanisms.”The results of this stimulation were borne out in a small (25 patients) but decisive study due to be reviewed and published soon (preprint abstract here). Patients given the BrainQ treatment in addition to normal therapy saw hugely improved recovery evaluations, which look at metrics like improvements to balance and strength; 92% saw major improvements over just therapy and 80% achieved what could be called recovery (though this term is inexact).Generally speaking the therapy would last for about an hour at a time, during which the patient would do various physical exercises while wearing the device, and they would need to be repeated five days a week for two months or so. The headset feeds the patient’s own patterns into BrainQ’s cloud-based service, which does the crunching and matching necessary to produce a tailored treatment pattern. It’s all run via tablet app, which can be operated by a caregiver (such as an outpatient nurse) or by using a built-in telemedicine platform.Drechsler said that this approach was poorly received early on, and not just by this reporter.“In 2017, we started to set the ground for a cloud-connected therapeutic device that can treat the patient wherever she or he is,” he said. “Back then no one was willing to even talk about treating patients outside the controlled environment of the hospital. Then in 2020 COVID came and everything changed.”He noted that during the pandemic, many of those recovering from a stroke who would normally visit the hospital for regular care were (and some remain) unable to do so. A home-based therapy with low risk and potentially great outcomes would be of enormous benefit for thousands and thousands of people currently recovering from a stroke. And importantly, he notes, it doesn’t shift resources away from existing treatment plans, just improves their outcomes. (“We don’t move anybody’s cheese.”)Here is where you would normally read something along the lines of “but it may be five years before the FDA approves it for insurance and use.” But BrainQ recently received Breakthrough Device certification, an expedited approval process that, since just the beginning of this year, also confers qualification for coverage under Medicare. This means that conceivably, BrainQ could be shipping devices very soon — though still a year or two out.Its next step, very prudently, is a larger-scale study, toward which the company intends to devote a large portion of its recent fundraise, $40 million led by Hanaco Ventures, with Dexcel Pharma and Peregrine Ventures participating.“The reason why we raised all this money is we are on the verge of a unique study with 12 sites,” Drechsler said. While he could not yet name the hospitals or research organizations they partnered with, he said they were basically the cream of the stroke rehabilitation crop and “really we couldn’t aspire for better than getting all these top sites in the same study. There’s this excitement that maybe something new is coming — in stroke recovery there has been almost no progress in the last two or three decades, and physical therapy has been the standard for two hundred years.”Without making any promises, he suggested that this line of inquiry could move medicine toward not just mitigating but reversing some disabilities, a feat the value of which can hardly be enumerated.“I was looking over my pitch decks from 2016,” Drechsler mused. “Early on as a CEO, you have big dreams. We heard a lot of skepticism early on in the process, but I was proud to see that many of those dreams have materialized.”",BrainQ raises $40M to transform stroke patient rehabilitation with its home therapy device
195,-1,-1_new_said_study_people,https://techcrunch.com/2021/02/17/astra-hires-longtime-apple-veteran-benjamin-lyon-as-chief-engineer/,"New space startup Astra, which is currently focused on commercial rockets, but which plans to eventually build satellites, too, has hired one of Apple’s key engineering leaders to head its own engineering efforts. Benjamin Lyon spent more than two decades at Apple, where he worked on everything from the iPhone to input devices and sensor hardware to special projects: the department at Apple working on autonomous vehicle technology.“When I’ve looked at what to do next at Apple, it has always been this combination of ‘What is the most impactful thing that I can do for humanity?’ — the iPhone was very much one of these,” Lyon told me in an interview. “Phones were awful [at the time], and if we could fundamentally come up with a new interface, that would completely change how people interact with devices.”Creating a mobile device with an interface that was “completely flexible and completely customizable to the application” was what seemed so transformative to Lyon about the iPhone, and he sees a direct parallel in the work that Astra is doing to lower the barrier of access to space through cheap, scalable and highly efficient rocketry.“Astra to me feels very, very much like redefining what it means for a phone to be smart,” Lyon said. “I think the Astra vision is this magical combination of fundamentally taking the rocket science out of space. How do you do that? Well, you better have a great foundation of a team, and a great foundation of core technologies that you can bring together in order to make a compelling series of products.”Foundations are the key ingredient according not only to Lyon, but also to Astra co-founder and CEO Chris Kemp, who explained why an experienced Apple engineer made the most sense to him to lead a rocket startup’s engineering efforts.“We did not want anyone from aerospace — I’ll just say that out of the gate,” Kemp told me. “Aerospace has not figured out how to build rockets at scale, or do anything profitably — ever. So I found no inspiration from anyone I talked to who had anything to do with any of the other space-related companies. We do feel that there are people that are at SpaceX and Blue Origin who are really good at what they do. But in terms of the culture that we’re trying to establish at Astra, if you look back at Apple, and the things that Benjamin worked on there over many decades, he really took on not only designing the thing, but also designing the thing that makes the thing, which was more important than the thing itself.”Kemp’s alluding to Apple’s lauded ability to work very closely with suppliers and move fundamental component engineering in-house, crafting unique designs for things like the system-on-a-chip that now powers everything from the iPhone to Macs. Apple often designs the processes involved in making those fundamental components, and then helps its suppliers stand up the factories required to build those to its exacting specifications. Astra’s approach to the space industry centers around a similar approach, with a focus on optimizing the output of its Alameda-based rocket factory, and iterating its products quickly to match the needs of the market while keeping pricing accessible.And Astra’s definition of “iteration” matches up much more closely with the one used by Silicon Valley than that typically espoused by legacy aerospace companies — going further still in questioning the industry’s fundamentals than even watershed space tech innovators like SpaceX, which in many ways still adheres to accepted rocket industry methods.“You don’t do the iPhone X at iPhone 1 — you start with the iPhone 1 and you work your way to the iPhone X,” Lyon told me. “You’re going to see that with Astra as well, there’s going to be this amazing evolution, but it’s going to be tech company-rate evolution, as opposed to an ‘every 20 years’ evolution.”That sentiment lines up with Astra and Kemp’s approach to date: The company reached space for the first time late last year, with a rocket that was the second of three planned launches in a rapid iteration cycle designed to achieve that milestone. After the first of these launches (Rocket 3.1 if you’re keeping track) failed to make space last September, Astra quickly went back to the drawing board and tweaked the design to come back for its successful attempt in December (Rocket 3.2) — an extremely fast turnaround for an aerospace company by any measure. The company is now focused on its Rocket 3.3 launch, which should only require software changes to achieve a successful orbit, and put it on track to begin delivering commercial payloads for paying customers.Astra’s rocket is tiny compared to the mammoth Starship that SpaceX is currently developing, but that’s part of the appeal that drew Lyon to the startup in the first place. He says the goal of “design[ing] a rocket to match the application,” rather than simply “design[ing] a rocket to end all rockets” makes vastly more sense to serve the bourgeoning market.“And that’s just the beginning,” he added. “Then you’ll take the next step, which is if you look at the technology that’s in a satellite, and a bunch of the smart technology that’s in a rocket, there’s a tremendous amount of duplication there. So, get rid of the duplication — design the rocket and the satellite together as one system.”Eventually, that means contemplating not only launch and satellite as a single challenge, but also managing “the entire experience of getting to space and managing a constellation” as “a single design problem,” according to Lyon, which is the level of ambition at Astra that he views as on par with that of Steve Jobs at Apple at the outset of the iPhone project.Ultimately, Astra hopes to be able to provide aspiring space technology companies with everything they need so that the actual space component of their business is fully handled. The idea is that startups and innovators can then focus on bringing new models and sensing technologies to Astra, worrying only about payload — leaving launch, integration and eventually constellation management to the experts. It’s not unlike what the App Store unlocked for the software industry, Lyon said.“We’re trying to do something that’s never been done before in aerospace, which is to really scale the production of rockets, and also focus on the overall economics of the business,” Kemp explained about additional advantages of having Lyon on board. “As we become a public company, in particular, we have very aggressive EBITDA targets, and very aggressive production targets, much the same way Apple does. We also want to have a new rocket every year, just like [the iPhone] and so to some degree, we found every aspect of Benjamin’s ethos aligned with our values, and the culture that we’re creating here at Astra of relentless, constant innovation and iteration.”",Astra hires longtime Apple veteran Benjamin Lyon as chief engineer
194,-1,-1_new_said_study_people,https://techcrunch.com/2020/12/21/the-us-wants-startups-to-get-a-piece-of-the-16-billion-spent-on-space-tech/,"The US wants startups to get a piece of the $16 billion spent on space techThe U.S. government is one of the biggest spenders in the nascent space industry, and the man who handles the money for the Air Force’s $16 billion checkbook wants startups to know that his door is open for them.In all, Will Roper, the Assistant Secretary of the Air Force for Acquisition, Technology and Logistics, handles about $60 billion worth of budget for the Air Force — a mandate that includes spending money on the new tech initiatives the Air Force deems important.Historically, the Department of Defense hasn’t been the greatest at working with startups — and many tech companies have been loath to work with the DoD. However, since much of modern civilian infrastructure is based on global positioning systems and other satellite technologies that fall under the Defense Department’s purview, those views on cooperation are changing on both sides.“Space isn’t a quiet domain of communication and navigation and exploration anymore,” Roper told the audience at TechCrunch’s latest Sessions event, TC Sessions: Space 2020. “It’s increasingly becoming a hostile place… So we’re gearing up a new kind of competition on the military side that could extend to space and that’s creating a lot of new space programs.”Roper emphasized that the interest from the Air Force and the government more broadly extends well beyond offensive capabilities and military priorities. As space becomes an economic opportunity, Roper sees the Air Force as an engine for driving technology development forward in ways that have commercial benefits.“It’s a great, great time for innovation in new technologies that could help the military, but we want to do more than just help the military. That’s the old thinking in the Pentagon. That’s all that would help us win the Cold War in the 20th Century, but it’s not going to help us in the 21st, where technology is globalized and accelerating,” Roper said.“We want to find ways where our military mission and our funding can help accelerate commercial markets too, so it’s competing on a much bigger stage. But we think it’s where we need to aspire to be, so that we’re playing the right catalyst role in this nation and with our partners around the world,” Roper said.There are several programs that startups can tap to get those federal dollars. Two of the easiest points of entry are through the AFWERX and its recently announced SpaceWERX arm focused entirely on space technology.“These look like any tech company,” Roper told the audience at the TechCrunch event. “They’re outside our fence lines. They’re easy to walk into… Now you don’t have to know the mission, we will help you find the mission and the customer — the warfighter associated with it. It’s a great model because it keeps the company focused on what they know best, which is their tech.”Over the last three years, Roper estimated that the AFWERX program had brought 2,300 companies into the Air Force and Space Force programs, and most of them had never worked with the military before, he said.Within AFWERX there are three programs that particularly relate to integrating startups into the procurement process, Roper said. One is the Spark program, which pairs military with private industry; one is the AFVentures program, which is designed to finance new innovations coming from private industry; and finally there’s the Prime program, which helps commercialize and certify technologies.Roper pointed to the recent certification the Air Force gave to Joby Aviation for its flying cars. “So there’s a new military market that will hopefully generate a new commercial market,” Roper said.In 2021, the Prime program will expand to space technologies, according to Roper.As the demand for new tech grows, there’s no shortage of innovations Roper would like to see from private industry. From new autonomous innovations that could help co-pilot spacecraft to technology for refueling and in-space maneuverability, and reusable equipment from boosters to other components that can bring costs down.Roper also acknowledged that the Pentagon has a long way to go to “hack the acquisition system” when it comes to dual-use technologies.Entrepreneurs have pointed out that one of the biggest obstacles to the growth of the commercial space industry has been the inability of the U.S. government to open up the technology for use by private industry.Roper hopes to change that. “We want to use our military dollars, our mission, and potentially our certifications to help get you there without changing your core product,” he said. “If you succeed as a commercial success, then we succeed as well, because now we’ve got a great tech partner, that hopefully we can continue to come to to solve problems in future. The thing that we’ll want to understand early on is how our military market and all those benefits I just mentioned, how can they help you get to commercial success? And what is it that we not need to do to pull you off that trajectory?”Contracts with AFWERX are fixed-price and progress as companies hit certain milestones on the product roadmap. These orders increase incrementally as the technology proves itself, so a contract could start with the delivery of a prototype, then experimental usage, then a commercial contract, then broad adoption. “What we’re looking to do is see if you can move the ball forward on your technology, and if you do, then we do another contract. We step you up our process,” Roper said.Roper sees the project as nothing less than the evolution of the aerospace and defense industry.“We have a lot of amazing companies today that helped build stealth bombers and space planes and all sorts of awesome stuff. They’re defense companies and we still need them,” Roper said. “What we’re hoping to help build in this century is a set of new companies that are just tech companies. They’re not defense, purely, and they’re not commercial purely. They’re just technology companies and they do a bit of business on both sides.”",The US wants startups to get a piece of the $16 billion spent on space tech
192,-1,-1_new_said_study_people,https://techcrunch.com/2020/11/20/onit-acquires-legal-startup-mccarthyfinch-to-inject-ai-into-legal-workflows/,"Onit, a workflow software company based in Houston, announced this week that it has acquired 2018 TechCrunch Disrupt Battlefield alum McCarthyFinch. Onit intends to use the startup’s AI skills to beef up its legal workflow software offerings.The companies did not share the purchase price.After evaluating a number of companies in the space, Onit focused on McCarthyFinch, which gives it an artificial intelligence component the company’s legal workflow software had been lacking. “We evaluated about a dozen companies in the AI space and dug in deep on six of them. McCarthyFinch stood out from the pack. They had the strongest technology and the strongest team,” Eric M. Elfman, CEO and co-founder of Onit told TechCrunch.The company intends to inject that AI into its existing Aptitude workflow platform. “Part of what really got me excited about McCarthyFinch was the very first conversation I had with their CEO, Nick Whitehouse. They considered themselves an AI platform, which complemented our approach and our workflow automation platform, Aptitude,” Elfman said.McCarthyFinch CEO and co-founder Whitehouse says the startup was considering whether to raise more money or look at being acquired earlier this year when Onit made its interest known. At first, he wasn’t really interested in being acquired and was hoping to go the partner route, but over time that changed.“I was very much on the partner track, and was probably quite dismissive to begin with because I was quite focused on that partner strategy. But as we talked, all egos aside, it just made sense [to move to acquisition talks],” Whitehouse said.The talks heated up in May and the deal officially closed last week. With Onit headquartered in Houston and McCarthyFinch in New Zealand the negotiations and meetings all happened on Zoom. The two companies’ principals have never met in person. The plan is for McCarthyFinch to stay in place, even after the pandemic ends. Whitehouse expects to make a trip to Houston whenever it is safe to do so.Whitehouse says his experience with Battlefield has had a huge influence on him. “Just the insights that we got through Battlefield, the coaching that we got, those things have stuck with me and they’ll stick with me for the rest of my life,” he said.The company had 45 customers and 17 employees at the time of the acquisition. It raised US$5 million along the way. Now it becomes part of Onit as the journey continues.",Onit acquires legal startup McCarthyFinch to inject AI into legal workflows
181,-1,-1_new_said_study_people,https://techcrunch.com/2020/01/07/union-square-ventures-leads-legal-tech-startup-juros-5m-series-a/,"Juro, a UK startup that’s using machine learning tech and user-centric design to do for contracts what Typeform does for online forms, has caught the eye of Union Square Ventures. The New York-based fund leads a $5 million Series A investment that’s being announced this morning.Also participating in the Series A are existing investors Point Nine Capital, Taavet Hinrikus (co-founder of TransferWise) and Paul Forster (co-founder of Indeed). The round takes Juro’s total raised to-date to $8M, including a $2M seed which we covered back in 2018.London is turning into a bit of a hub for legal tech, per Juro CEO and co-founder Richard Mabey — who cites “strong legal services industry” and “strong engineering talent” as explainers for that.It was also, he reckons, “a bit of a draw” for Union Square Ventures — making what Juro couches as a “rare” US-to-Europe investment in legal tech in the city via the startup.“Having brand name customers in the US certainly helped. But ultimately, they look for product-led companies with strong cross-functional teams wherever they find them,” he adds.Juro’s business is focused on taking the tedium out of negotiating and drawing up contracts by making contract-building more interactive and trackable. It also handles e-signing, and follows on with contract management services, using machine learning tech to power features such as automatic contract tagging and for flagging up unusual language.All of that sums to being a “contract collaboration platform”, as Juro’s marketing puts it. Think of it like Google Docs but with baked in legal smarts. There’s also support for visual garnish like animated GIFs to spice up offer letters and engage new hires.“We have a data model underlying our editor that transforms every contract into actionable data,” says Mabey. “Juro contracts look like contracts, smell like contracts but ultimately they are written in code. And that code structures the data within them. This makes a contract manager’s life 10x easier than using an unstructured format like Word/pdf.”“Still our main competitor is MS Word,” he adds. “Our challenge is to bring lawyers (and other users of contracts) out of Word, which is a significant task. Fortunately, Word was never designed for legal workflows, so we can add lots of value through our custom-built editor.”Part of Juro’s Series A funds will be put towards beefing up its machine learning/data science capabilities, per Mabey — who says the overall plan at this point is to “double down on product”, including by tripling the size of the product team.“That means hiring more designers, data scientists and engineers — building our engineering team in the Baltics,” he tells us. “There’s so much more we are excited to do, especially on the ML/data side and the funding unlocks our ability to do this. We will also be building our commercial team (marketing, sales, cs) in London to serve the EU market and expand further into the US, where we already have some customers on the ground.”The 2016-founded startup still isn’t breaking out customer numbers but says it’s processed more than 50,000 contracts for its clients so far, noting too that those contracts have been agreed in 50+ countries. (“Everywhere from Estonia to Japan to Kazakhstan,” as Mabey puts it.)In terms of who Juro users are, it’s still mostly “mid-market tech companies” — with Mabey citing the likes of marketplaces (Deliveroo), SaaS (Envoy) and fintechs (Luno), saying it’s especially companies processing “high volumes of contracts”.Another vertical it’s recently expanded into is media, he notes.“E-signature giants have grown massively in the last few years, and some are gradually encroaching into the contract lifecycle — but again, they deal with files (pdfs mostly) rather than dynamic, browser-based documentation,” he argues, adding: “In terms of new legal tech entrants — I’m excited by Kira Systems especially, who are working on unpicking pdf contracts post-signature.”As part of the Series A, Union Square Ventures parter, John Buttrick, is joining Juro’s board.Commenting in a supporting statement, Buttrick said: “We look for founders with products equipped to change an industry. While contract management might not be new, Juro’s transformative vision for it certainly is. There’s no greater proof of the product’s ease of use than the fact that we negotiated and closed the funding round in it. We’re delighted to support Juro’s team in making their vision a reality.”",Union Square Ventures leads legal tech startup Juroâs $5M Series A
180,-1,-1_new_said_study_people,https://techcrunch.com/2019/11/07/elon-musk-says-building-the-first-sustainable-city-on-mars-will-take-1000-starships-and-20-years/,"SpaceX CEO Elon Musk went into a bit more detail about the timelines and vehicle requirements to not only reach Mars, but to set up a sustainable base on the Red Planet that can serve as an actual city, supporting a local population. That’s the long-term vision for Musk and his space technology company, after all — making humans an interplanetary species. The timeline that Musk discussed today, replying to fans on Twitter, might be incredibly impressive or incredibly ambitious, depending on your perspective.Addressing a question about comments he made earlier this week at the U.S. Air Force startup pitch day event in California, Musk said that his stated launch cost of only around $2 million per Starship flight are essentially required, should the final goal be to set up a “self-sustaining city on Mars.” In order to make that city a reality, he added, SpaceX will need to build and fly around 1,000 Starships according to his estimates, which will need to transport cargo, infrastructure and crew to Mars over the course of around 20 years, since planetary alignment only really allows for a realistically achievable Mars flight once every two years.Musk addressed more near-term potential for Starship as well, including how much payload capacity Starship will provide for Earth orbital transportation. Starship’s design is intended to maximize re-use, and in fact Musk noted that ideally it can fly up to three times per day. That amounts to more than 1,000 flights per year per Starship, which means that if they end up with as many Starships as they currently have built Falcon rockets (around 100) and those can each transport as much as 100 tons to orbit, then on an annual basis, SpaceX will be able to launch upwards of 10 million tons to orbit per year.To put that in perspective, Musk points out that if you take all cargo-bearing spacecraft currently in operation into account, the total payload capacity is just 500 tons per year — with Falcon series rockets from SpaceX itself making up around half of that.That’s a lot of payload; in fact, it’s probably more than there will be demand for in any near-term time scale. But it’s also true that Musk envisions a future where orbital space is a much busier place, and a staging ground for orbital cargo transfer and refueling as Moon and Mars-bound spacecraft ready themselves for the outward journey.Of course, to set up a permanent, sustainable city on Mars, we first have to get there with a crewed flight. There’s another step between now and then, which is landing astronauts back on the Moon. NASA has set 2024 as its goal for that milestone, and SpaceX has said it hopes to land Starship there by as early as 2022 to help with staging in preparation for that landing. In the past, Musk has discussed crewed Mars mission also taking place as early as 2024, but that goal seems mighty aspirational (as do most of his timelines) from where we sit today.","Elon Musk says building the first sustainable city on Mars will take 1,000 Starships and 20 years"
172,-1,-1_new_said_study_people,https://techcrunch.com/2019/07/15/cambridge-uni-graphene-spin-out-bags-16m-to-get-its-first-product-to-market/,"Cambridge Uni graphene spin-out bags $16M to get its first product to marketCambridge, UK based graphene startup, Paragraf, has closed a £12.8 million (~$16M) Series A round of funding led by early stage VC Parkwalk. Also investing this round: IQ Capital Partners, Amadeus Capital Partners and Cambridge Enterprise, the commercialisation arm of the University of Cambridge, plus several unnamed angel investors.The funding will be used to bring the 2015-founded Cambridge University spin out’s first graphene-based electronics products to market — transitioning the startup into a commercial, revenue-generating phase.When we covered Paragraf’s $3.9M seed raise just over a year ago CEO and co-founder Dr Simon Thomas told us it was looking to raise a Series A ahead of Q3 2019 so the business looks to be right on track at this stage.During the seed phase Paragraf says it was able to deliver a manufacturing facility, graphene layer production and first device prototypes “significantly” ahead of plan.It’s now switching focus to products — with strategic volume device production partners, and commercialisation of its first device: A super-high sensitivity magnetic field detector which it says operates over temperature, field and power ranges “that no other device can currently achieve”.Commenting in a statement, Thomas added: “I am extremely proud of the young team at Paragraf who have collectively delivered the early strategy milestones with great skill. This next phase will allow Paragraf to make these truly game-changing technologies a reality. Paragraf is continually seeking like-minded collaborative development, production and commercial partners to accelerate the delivery of the many exciting electronics technology opportunities graphene has to offer.”In terms of the touted benefits of graphene, the atom-layer-thick 2D material has long been exciting scientists as a potential replacement for silicon in computer chips — thanks to a raft of key properties including high conductivity, strength and flexibility and thermal integrity. Researchers suggest it could deliver a performance speed increase of up to 1000x, while reducing energy use by up to 50x.But while excitement about how graphene could transform electronics has been plentiful in the more than a decade since it was discovered, those seeking to commercialize the wonder material have found it challenging to manufacture at commercial grade and scale.This is where Paragraf aims to come in — claiming to be the first company to deliver IP-protected graphene technology using what it bills as “standard, mass production scale manufacturing approaches”.It also says its first sensor products have demonstrated “order of magnitude operational improvements over today’s incumbents”.Such claims of course remain to be tested in the wild but Paragraf isn’t dialling down the hype vis-a-vis the transformative potential of baking graphene into next-gen electronics.“Achieving large-scale, graphene-based production technology will enable next generation electronics, including vastly increased computing speeds, significantly improved medical diagnostics and higher efficiency renewable energy generation as well as currently unachievable products such as instant charging batteries and very low power, flexible electronics,” it writes.A year ago Thomas told us Paragraf expected high-tech applications of graphene in consumer technologies to appear in the general market within the next 2-3 years — a timeline that should now have shrunk to just a year or two out.",Cambridge Uni graphene spin-out bags $16M to get its first product to market
164,-1,-1_new_said_study_people,https://techcrunch.com/2019/02/12/darpa-wants-smart-bandages-for-wounded-warriors/,"Nowhere is prompt and effective medical treatment more important than on the battlefield, where injuries are severe and conditions dangerous. DARPA thinks that outcomes can be improved by the use of intelligent bandages and other systems that predict and automatically react to the patient’s needs.Ordinary cuts and scrapes just need a bit of shelter and time and your amazing immune system takes care of things. But soldiers not only receive far graver wounds, but under complex conditions that are not just a barrier to healing but unpredictably so.DARPA’s Bioelectronics for Tissue Regeneration program, or BETR, will help fund new treatments and devices that “closely track the progress of the wound and then stimulate healing processes in real time to optimize tissue repair and regeneration.”“Wounds are living environments and the conditions change quickly as cells and tissues communicate and attempt to repair,” said Paul Sheehan, BETR program manager, in a DARPA news release. “An ideal treatment would sense, process, and respond to these changes in the wound state and intervene to correct and speed recovery. For example, we anticipate interventions that modulate immune response, recruit necessary cell types to the wound, or direct how stem cells differentiate to expedite healing.”It’s not hard to imagine what these interventions might comprise. Smart watches are capable of monitoring several vital signs already, and in fact have alerted users to such things as heart-rate irregularities. A smart bandage would use any signal it can collect — “optical, biochemical, bioelectronic, or mechanical” — to monitor the patient and either recommend or automatically adjust treatment.A simple example might be a wound that the bandage detects from certain chemical signals is becoming infected with a given kind of bacteria. It can then administer the correct antibiotic in the correct dose and stop when necessary rather than wait for a prescription. Or if the bandage detects shearing force and then an increase in heart rate, it’s likely the patient has been moved and is in pain — out come the painkillers. Of course, all this information would be relayed to the caregiver.This system may require some degree of artificial intelligence, although of course it would have to be pretty limited. But biological signals can be noisy and machine learning is a powerful tool for sorting through that kind of data.BETR is a four-year program, during which DARPA hopes that it can spur innovation in the space and create a “closed-loop, adaptive system” that improves outcomes significantly. There’s a further ask to have a system that addresses osseointegration surgery for prosthetics fitting — a sad necessity for many serious injuries incurred during combat.One hopes that the technology will trickle down, of course, but let’s not get ahead of ourselves. It’s all largely theoretical for now, though it seems more than possible that the pieces could come together well ahead of the deadline.",DARPA wants smart bandages for wounded warriors
233,-1,-1_new_said_study_people,https://techcrunch.com/2022/08/09/how-this-founder-is-saas-ifying-air-quality-tracking/,"Welcome back to Found, where we get the stories behind the startups. This week, Darrell and Jordan talked with Davida Herzl the co-founder and CEO of Aclima.We all have a right to clean air, but chances are you aren’t getting accurate air quality data — Davida and the Aclima team are looking to change that. In this episode, she talks with Jordan and Darrell about the struggles she faced trying to start a climate company right after the clean tech bubble burst, how she’s stayed laser-focused on her mission, and how working with state governments is paramount for her company and measuring air-quality at scale.Subscribe to Found to hear more stories from founders each week.Connect with us:",How this founder is SaaS-ifying air-quality tracking
297,-1,-1_new_said_study_people,https://www.bbc.com/news/science-environment-63407459?at_medium=RSS&at_campaign=KARANGA,"The report also finds that a raft of new policies in countries like the US, Japan, Korea and the EU will likely see clean energy investments of around $2 trillion by 2030, a rise of more than 50% from today.",Climate change: UN warns key warming threshold slipping from sight
239,-1,-1_new_said_study_people,https://techcrunch.com/2022/08/14/twitter-crypto-privacy-sanctions-tornado-chain-reaction/,"Welcome back to Chain Reaction.Last week, we talked about a hack that gave new, ironic meaning to the word “trustless.” This week, we’ll get into one of the most polarizing aspects of crypto — privacy.If someone forwarded you this message, you can subscribe on TechCrunch’s newsletter page.all mixed upA weekly window into the thoughts of senior crypto reporter Anita Ramaswamy:Tornado Cash has been the talk of the town this week in crypto circles. The U.S. government’s Office of Foreign Asset Control (OFAC), a watchdog within the Treasury, leveled sanctions against the cryptocurrency mixer for its role in helping facilitate money laundering. North Korean-backed hackers, among others, have used the Tornado Cash platform to mask stolen crypto associated with some of the highest-profile hacks in web3 to date, including last week’s Nomad heist and the hack of play-to-earn video game Axie Infinity earlier this year.But in imposing sanctions, OFAC was essentially using a sledgehammer to crack a nut. The agency’s official notice on the topic said that the platform had facilitated $7 billion worth of money laundering — which happens to be the total value of crypto assets that have been sent through Tornado Cash since it was created in 2019. Meanwhile, blockchain analytics provider Elliptic says only ~$1.5 billion of funds on Tornado are actually linked to crime, including ransomware attacks and fraud. The rest, Elliptic argues, could include “legitimate uses of mixers such as Tornado, such as to preserve financial privacy.”So what are some of those legitimate uses? One example came from Ethereum co-founder Vitalik Buterin, who confessed on Twitter that he has used the service to send donations to aid Ukraine securely without the knowledge of the Russian government.I'll out myself as someone who has used TC to donate to this exact cause. — vitalik.eth (@VitalikButerin) August 9, 2022The OFAC’s dictum does not differentiate between criminal and legitimate use cases, though. As a result, many law-abiding crypto users are likely suffering. Two major crypto infrastructure providers, Alchemy and Infura, blocked access to their API from any wallets that used Tornado Cash. Circle has reportedly frozen ~$75,000 worth of its USDC stablecoins that were connected to Tornado through a shared wallet, according to Dune Analytics data.Of course, internet pranksters got in on the fun, as is usually the case in the crypto world. Some have been sending crypto through Tornado Cash to known wallets held by celebrities such as Jimmy Fallon and Shaquille O’Neal in an attempt to troll them by getting their wallets banned under the sanction rules.OFAC’s heavy-handed action comes across as a bungled approach that raises more questions than it resolves when it comes to enforcement. Only time will tell how the latter plays out, but in the meantime, the crypto community is, understandably, pretty upset.the latest podThis week on Chain Reaction, Jacquelyn and Anita ran the show while Lucas was on vacation. Jacquelyn was coming off of an exciting Friday night call with Vitalik himself, so she shared some of his comments on where crypto is headed.We then dove into the news of Tornado Cash getting sanctioned in the U.S., Coinbase’s disappointing second-quarter earnings and the beef between Binance and India’s largest crypto exchange, WazirX, over a transaction that supposedly took place two and a half years ago (or did it)?Be sure to give it a listen to get up to speed on the latest tea in crypto and tune in next Tuesday for Anita and Lucas’s conversation with Li Jin, a web3 investor focused on the creator economy at Variant Fund.Subscribe to Chain Reaction on Apple, Spotify or your alternative podcast platform of choice to keep up with us every week.follow the moneyWhere startup money is moving in the crypto world:Jump Crypto led Injective‘s $40 million round to help expand DeFi applications. Pinata raised $21.5 million in a newly announced Series A and seed round from investors, including Greylock and Pantera. CreatorDAO, a decentralized platform for content creators, raised $20 million in an a16z and Initialized Capital-led round with participation from celebrities including Paris Hilton and Liam Payne. Blockchain gaming company Lysto raised $12 million in a round led by Hashed, Square Peg and Beenext. Unstoppable Finance snagged $12.8 million in a round led by Lightspeed for its DeFi wallet. Kurtosis, a crypto-focused developer tool system, brought in $20 million in a Series A round led by Coatue. Blockchain payments platform Ansible Labs raised a $7 million seed round led by Archetype. Zero-knowledge cryptography startup RISC Zero scooped up $12 million in a seed round led by Bain Capital Crypto. Fair.xyz landed $4.5 million from investors including OpenSea for its NFT minting platform. Cashmere raised $3 million at a $30 million valuation from investors including Coinbase Ventures to build a Solana enterprise wallet.TC+ analysisHere’s some of this week’s crypto analysis available on our subscription service TC+ from senior reporter Jacquelyn Melinek:5 takeaways from Coinbase’s disappointing Q2 resultsCoinbase, once hugely profitable in the wake of its 2021 direct listing thanks to a run in crypto-related trading activities, is now working to limit costs and brave the ongoing “winter” in its market and stick to prior profitability targets for the full year. What follows are five takeaways from Coinbase’s report that stood out to TC’s Alex Wilhelm and Ram Iyer.As Telegram grows in size, so does crypto traders’ dependence on the appThe crypto community has relied on social media sites like Twitter or messaging apps like Discord and Telegram to interact. But some say Telegram is the ultimate hub for communication and information — an imperative place to be in the crypto community. “Telegram usage is the bedrock of the crypto community,” the founder of Telegram channel unfolded, who goes by the username nakamotocat, said to TechCrunch. “Projects have come and gone, players have risen and fallen, but much of the discourse between various projects and market participants resides on Telegram, and that remains a constant.”Ethereum co-founder sees role diminishing as blockchain becomes increasingly decentralizedAs the layer-1 blockchain Ethereum continues to focus on a road map toward greater decentralization, its co-founder, Vitalik Buterin, thinks that moment might come sooner than expected. Also looking to the future, Buterin thinks the next decade will be pivotal for crypto. “I think in general, the next 10 years, crypto has to transform into something that is not based on promises of being useful in the future but is actually useful.”Solana co-founder says NFTs have ‘50 different use cases’ that can onboard millions this yearIt feels like yesterday that the NFT boom captured the attention of the crypto community, making waves even outside the web3 world. But a year or so down the line, the NFT hype has somewhat died down. But that isn’t stopping some in the crypto world from staying optimistic about non-fungible tokens. “I think within NFTs, everything is just really scratching the surface,” Raj Gokal, co-founder of Solana, told TechCrunch. “I think NFTs have 50 different use cases that seem to be lumped into one. I think we expect the majority of the [crypto] projects to make use of NFTs.”Thanks for reading! And — again — to get this in your inbox every Thursday, you can subscribe on TechCrunch’s newsletter page.",Why Twitter anons are sending crypto to celebrities
250,-1,-1_new_said_study_people,https://techcrunch.com/2022/09/06/one-startups-solution-to-the-carbon-offsetting-mess-cut-out-the-middle-men-resellers/,"As well as the traditional carbon offset resellers and exchanges such as Climate Partner or Climate Impact X, the tech space has also produced a few, including Patch (U.S.-based, raised $26.5 million) and Lune (U.K.-based, raised $4 million).Now, Ceezer, a B2B marketplace for carbon credits, has closed a €4.2 million round, led by Carbon Removal Partners with participation of impact-VC Norrsken VC and with existing investor Picus Capital.Ceezer’s pitch is that companies have to deal with a lot of complexity when considering how they address carbon removal and reduction associated with their businesses. While they can buy offsetting credits, the market remains pretty ‘wild-west”, and has multiple competing standards running in parallel. For instance, the price range of $5 to $500 per ton is clearly all over the place, and sometimes carbon offset resellers make buyers pay high prices for low-quality carbon credits, pulling in extra revenues from a very opaque market.The startup’s offering is for corporates to integrate both carbon removal and avoidance credits in one package. It does this by mining the offsetting market for lots of data points, enabling carbon offset sellers to reach buyers without having to use these middle-men resellers.The startup claims that sellers no longer waste time and money on bespoke contracts with corporates but instead use Ceezer’s legal framework for all transactions. Simultaneously, buyers can access credits at a primary market level, maximizing the effect of the dollars they spend on carbon offsets.Ceezer says it now has more than 50 corporate customers and has 200,000 tons of carbon credits to sell across a variety of categories, and will use the funds to expand its impact and sourcing team, the idea being to make carbon removal technologies more accessible to corporate buyers, plus widen the product offering for credit sellers and buyers.",One startupâs solution to the carbon-offsetting mess: Downgrade the âmiddle-menâ resellers
287,-1,-1_new_said_study_people,https://today.ku.edu/2022/10/10/media-coverage-hurricanes-reinforces-images-minorities-victims-racial-hierarchies-study,"LAWRENCE — As Hurricane Ian made landfall, devastating parts of Florida, South Carolina and the Caribbean, readers saw media images of destruction, rescues and recovery. How images from such disasters are presented often cast people in certain roles. A new study from the University of Kansas shows newspaper images from Hurricane Harvey in 2017 continued patterns of presenting people of color as victims and white people as rescuers bringing order back to the chaos. While those presentations may not have been conscious or ill-intentioned decisions, they reflect patterns in journalism and cultural values, according to the study's author.Hurricane Harvey made landfall in Houston in 2017, and newspapers from across the country devoted extensive coverage to the disaster. Ever Josue Figueroa, assistant professor of journalism & mass communications at KU, watched the news like many others, but the Houston native noticed something about the presentation.“I was watching the coverage and at the time was taking a course on visual communication. I thought, ‘I want to do something with what just happened,’” Figueroa said. “The main crux of this paper is using an approach for analyzing images called semiotics. Basically, the premise is the way we create and interpret visual images is drawn from things we understand and have seen before.”For the study, Figueroa conducted a visual textual analysis of 106 front page images from Aug. 28, 2017, to Sept. 4, 2017. The results show media coverage presented people of color as displaced migrants, women as damsels in distress and white men as saviors and caretakers. The paper was published in the journal Critical Studies in Media Communication.Analysis of the front-page images showed coverage fell into four main themes.People in floodwatersShelters and salvationMasculine heroesRepair and maintenance of homes.Among those themes, Figueroa noted that most photos of people in floodwaters were presented as people of color wading through the waters, attempting to escape the devastation. Those images are similar to common media photos of immigrants wading through the waters of the Rio Grande River to cross into the United States, Figueroa said. But while people were forced from their homes in both cases, they emphasized how people were moving toward rescue in the hurricane and toward a hopefully better life in terms of immigration.The theme of shelter and salvation tended to show people of color receiving assistance from volunteers and shelter staff. White people in shelters were shown expressing grief or emotional trauma but not receiving material aid. Such images reinforce narratives of welfare and arguments about who is entitled to assistance and who abuses such aid.“Everyone in those shelters largely experienced the same things, but when we look at the coverage patterns, there was a clear, racialized component to them,” Figueroa said.The masculine heroes theme was reflected in images of men, mostly white men, rescuing people, coming to the aid of victims and being in positions of authority, such as first responders or government officials. One image showed a white man carrying a woman of color in his arms through floodwaters as the woman carried an infant in her own arms. The repeated use of these types of images reinforces patriarchal gender hierarchy to readers, Figueroa said.The final theme was presentation of people repairing their homes, cleaning up from damages or otherwise rebuilding and beginning recovery from the destruction. However, most people in such images were white homeowners. Houston is a very diverse city, but that was not necessarily reflected in how images were presented, the study found.“Within those themes, I argue there were racialized and gendered ways of how people were presented,” Figueroa said. “That reifies who is depicted as a homeowner or who is presented as a victim. All of that reiterates ideas of who controls or owns land. Who has self-agency or is dependent on others. It should be understood, the news photos also tell a story, and over time, they can perpetuate harmful cultural narratives and stereotypes of marginalized people and communities.”Figueroa emphasized that he was not blaming photographers for covering storms and recovery, nor first responders or government officials for helping people who need it. Rather, he said that such representations have happened for many years in American media and that they reflect our cultural values. Such racialized and gendered presentations, however, are not only a continuation of what we are familiar with as a nation but likely result from normalized work routines in traditional journalism. Journalists have long worked closely with government officials. That relationship results in media often seeking out police, fire departments and first responders as well as federal government agents working in response to situations such as natural disasters, similar to how embedded war correspondents regularly present news from the perspective of the officials they are closest to.There are potential ways to address ingrained patterns, however, such as looking to the community response in such disasters, he said. Aligning with community members to find examples and present coverage of neighbors helping neighbors or covering civilian response could help dilute dominant themes. The idea of “solidarity journalism,” or covering the lived experiences of people most affected by issues such as natural disasters, labor strife or the pandemic, to name a few examples, could help address dominant themes of government response, economic effects and other presentations of such topics. Journalists should center news coverage from the perspective of people suffering from unjust conditions, rather than center coverage from the perspective of authority figures, Figueroa said.“The findings from this study suggest that people of color are still the spectacle and their displacement served as the underlying metaphor of Hurricane Harvey. Marginalized people are represented as victims whose lives were ruined by natural disasters and go on journeys to seek salvation from authoritative figures,” Figueroa wrote in the article. “Marginalized groups should not be defined by their victimhood and instead should be given the opportunity to showcase their resilience and communal power. It is time for news media to provide a better representation of the communities they cover, one that is more truthful of the human experience during moments of crisis.”Image: Excerpt from a Dallas Morning News front page following Hurricane Harvey in 2017.","Media coverage of hurricanes reinforces images of people of color as victims, study finds"
283,-1,-1_new_said_study_people,https://themarijuanaherald.com/2022/10/study-cbd-might-be-beneficial-in-alleviating-uvb-induced-skin-damage-in-humans/,"The findings of a new study “indicate that CBD might be beneficial in alleviating UVB-induced skin damage in humans.”The study was published in the peer-reviewed journal Molecules, and was epublished by the U.S. National Institute of Health. It was conducted by researchers at Sichuan University in China.“Cannabidiol (CBD) has emerged as a phytocannabinoid with various beneficial effects for the skin, including anti-photoaging effects, but its mechanisms of action are not fully elucidated”, states the study’s abstract. “The study assessed CBD’s photoprotective effects against acute ultraviolet B (UVB)-induced damage in HaCaT human keratinocyte cells and murine skin tissue.”Researchers found that “CBD (8 μM) alleviated UVB-induced cytotoxicity, apoptosis, and G2/M cell cycle arrest in HaCaT cells.”In addition, “The contents of γH2AX and cyclobutane pyrimidine dimers were decreased after CBD treatment”, and “CBD reduced the production of reactive oxygen species and modulated the expression of antioxidant-related proteins such as nuclear factor erythroid 2-related factor 2 in UVB-stimulated HaCaT cells.”Furthermore, “CBD mitigated the UVB-induced cytotoxicity by activating autophagy. In addition, a cream containing 5% CBD showed effectiveness against UVB-induced photodamage in a murine model.The study states that “CBD cream improved the skin’s condition by lowering the photodamage scores, reducing abnormal skin proliferation, and decreasing expression of the inflammation-related protein cyclooxygenase-2 in UVB-irradiated skin tissue.”In conclusion, researchers state that “These findings indicate that CBD might be beneficial in alleviating UVB-induced skin damage in humans. The photoprotective effects of CBD might be attributed to its modulatory effects on redox homeostasis and autophagy.”For further info on this study, click here. The study’s full abstract can be found below:Cannabidiol (CBD) has emerged as a phytocannabinoid with various beneficial effects for the skin, including anti-photoaging effects, but its mechanisms of action are not fully elucidated. The study assessed CBD’s photoprotective effects against acute ultraviolet B (UVB)-induced damage in HaCaT human keratinocyte cells and murine skin tissue. CBD (8 μM) alleviated UVB-induced cytotoxicity, apoptosis, and G2/M cell cycle arrest in HaCaT cells. The contents of γH2AX and cyclobutane pyrimidine dimers were decreased after CBD treatment. CBD reduced the production of reactive oxygen species and modulated the expression of antioxidant-related proteins such as nuclear factor erythroid 2-related factor 2 in UVB-stimulated HaCaT cells. Furthermore, CBD mitigated the UVB-induced cytotoxicity by activating autophagy. In addition, a cream containing 5% CBD showed effectiveness against UVB-induced photodamage in a murine model. The CBD cream improved the skin’s condition by lowering the photodamage scores, reducing abnormal skin proliferation, and decreasing expression of the inflammation-related protein cyclooxygenase-2 in UVB-irradiated skin tissue. These findings indicate that CBD might be beneficial in alleviating UVB-induced skin damage in humans. The photoprotective effects of CBD might be attributed to its modulatory effects on redox homeostasis and autophagy.",Study: âCBD Might be Beneficial in Alleviating UVB-Induced Skin Damage in Humansâ
282,-1,-1_new_said_study_people,https://theintercept.com/2022/09/28/cia-extinction-woolly-mammoth-dna/,"As a rapidly advancing climate emergency turns the planet ever hotter, the Dallas-based biotechnology company Colossal Biosciences has a vision: “To see the Woolly Mammoth thunder upon the tundra once again.” Founders George Church and Ben Lamm have already racked up an impressive list of high-profile funders and investors, including Peter Thiel, Tony Robbins, Paris Hilton, Winklevoss Capital — and, according to the public portfolio its venture capital arm released this month, the CIA. Colossal says it hopes to use advanced genetic sequencing to resurrect two extinct mammals — not just the giant, ice age mammoth, but also a mid-sized marsupial known as the thylacine, or Tasmanian tiger, that died out less than a century ago. On its website, the company vows: “Combining the science of genetics with the business of discovery, we endeavor to jumpstart nature’s ancestral heartbeat.” In-Q-Tel, its new investor, is registered as a nonprofit venture capital firm funded by the CIA. On its surface, the group funds technology startups with the potential to safeguard national security. In addition to its long-standing pursuit of intelligence and weapons technologies, the CIA outfit has lately displayed an increased interest in biotechnology and particularly DNA sequencing. “Why the interest in a company like Colossal, which was founded with a mission to “de-extinct” the wooly mammoth and other species?” reads an In-Q-Tel blog post published on September 22. “Strategically, it’s less about the mammoths and more about the capability.”“Biotechnology and the broader bioeconomy are critical for humanity to further develop. It is important for all facets of our government to develop them and have an understanding of what is possible,” Colossal co-founder Ben Lamm wrote in an email to The Intercept. (A spokesperson for Lamm stressed that while Thiel provided Church with $100,000 in funding to launch the woolly mammoth project that became Colossal, he is not a stakeholder like Robbins, Hilton, Winklevoss Capital, and In-Q-Tel.) Colossal uses CRISPR gene editing, a method of genetic engineering based on a naturally occurring type of DNA sequence. CRISPR sequences present on their own in some bacterial cells and act as an immune defense system, allowing the cell to detect and excise viral material that tries to invade. The eponymous gene editing technique was developed to function the same way, allowing users to snip unwanted genes and program a more ideal version of the genetic code. “CRISPR is the use of genetic scissors,” Robert Klitzman, a bioethicist at Columbia University and a prominent voice of caution on genetic engineering, told The Intercept. “You’re going into DNA, which is a 3-billion-molecule-long chain, and clipping some of it out and replacing it. You can clip out bad mutations and put in good genes, but these editing scissors can also take out too much.” The embrace of this technology, according to In-Q-Tel’s blog post, will help allow U.S. government agencies to read, write, and edit genetic material, and, importantly, to steer global biological phenomena that impact “nation-to-nation competition” while enabling the United States “to help set the ethical, as well as the technological, standards” for its use. In-Q-Tel did not respond to The Intercept’s requests for comment.",The CIA Just Invested in Woolly Mammoth Resurrection Technology
281,-1,-1_new_said_study_people,https://techxplore.com/news/2022-11-window-coating-cool-energy.html,"This window film (held in fingers at top left) keeps rooms bright and cool by allowing visible light to pass in while reflecting invisible infrared and ultraviolet sunlight and radiating heat into outer space. Credit: Adapted from ACS Energy Letters 2022, DOI: 10.1021/acsenergylett.2c01969As climate change intensifies summer heat, demand is growing for technologies to cool buildings. Now, researchers report in ACS Energy Letters that they have used advanced computing technology and artificial intelligence to design a transparent window coating that could lower the temperature inside buildings, without expending a single watt of energy.Studies have estimated that cooling accounts for about 15% of global energy consumption. That demand could be lowered with a window coating that could block the sun's ultraviolet and near-infrared light—the parts of the solar spectrum that typically pass through glass to heat an enclosed room. Energy use could be reduced even further if the coating radiates heat from the window's surface at a wavelength that passes through the atmosphere into outer space. However, it's difficult to design materials that can meet these criteria simultaneously and can also transmit visible light, meaning they don't interfere with the view. Eungkyu Lee, Tengfei Luo and colleagues set out to design a ""transparent radiative cooler"" (TRC) that could do just that.The team constructed computer models of TRCs consisting of alternating thin layers of common materials like silicon dioxide, silicon nitride, aluminum oxide or titanium dioxide on a glass base, topped with a film of polydimethylsiloxane. They optimized the type, order and combination of layers using an iterative approach guided by machine learning and quantum computing, which stores data using subatomic particles. This computing method carries out optimization faster and better than conventional computers because it can efficiently test all possible combinations in a fraction of a second. This produced a coating design that, when fabricated, beat the performance of conventionally designed TRCs in addition to one of the best commercial heat-reduction glasses on the market.In hot, dry cities, the researchers say, the optimized TRC could potentially reduce cooling energy consumption by 31% compared with conventional windows. They note their findings could be applied to other applications, since TRCs could also be used on car and truck windows. In addition, the group's quantum computing-enabled optimization technique could be used to design other types of composite materials.More information: High-Performance Transparent Radiative Cooler Designed by Quantum Computing, ACS Energy Letters (2022). Journal information: ACS Energy Letters High-Performance Transparent Radiative Cooler Designed by Quantum Computing,(2022). DOI: 10.1021/acsenergylett.2c01969",Clear window coating could cool buildings without using energy
279,-1,-1_new_said_study_people,https://techxplore.com/news/2022-10-laser-autonomous-vehicles-deleting-pedestrians.html,"A schematic of the attack, which can delete lidar data from a region in front of a vehicle, leading to unsafe vehicle movement. Below, showing the deletion of lidar data for a pedestrian in front of a vehicle, visible below left but invisible below right. Credit: Sara Rampazzi/University of FloridaSelf-driving cars, like the human drivers that preceded them, need to see what's around them to avoid obstacles and drive safely.The most sophisticated autonomous vehicles typically use lidar, a spinning radar-type device that acts as the eyes of the car. Lidar provides constant information about the distance to objects so the car can decide what actions are safe to take.But these eyes, it turns out, can be tricked.New research reveals that expertly timed lasers shined at an approaching lidar system can create a blind spot in front of the vehicle large enough to completely hide moving pedestrians and other obstacles. The deleted data causes the cars to think the road is safe to continue moving along, endangering whatever may be in the attack's blind spot.The attack deletes data in a cone in front of the vehicle, making a moving pedestrian invisible to the lidar system within that range. Credit: Sara Rampazzi/University of FloridaThis is the first time that lidar sensors have been tricked into deleting data about obstacles.The vulnerability was uncovered by researchers from the University of Florida, the University of Michigan and the University of Electro-Communications in Japan. The scientists also provide upgrades that could eliminate this weakness to protect people from malicious attacks.The findings will be presented at the 2023 USENIX Security Symposium and are currently published on arXiv.The real-world experiment showing the effect of the attack on a pedestrian moving in front of a lidar-equipped vehicle. Credit: Sara Rampazzi/University of FloridaLidar works by emitting laser light and capturing the reflections to calculate distances, much like how a bat's echolocation uses sound echoes. The attack creates fake reflections to scramble the sensor.""We mimic the lidar reflections with our laser to make the sensor discount other reflections that are coming in from genuine obstacles,"" said Sara Rampazzi, a UF professor of computer and information science and engineering who led the study. ""The lidar is still receiving genuine data from the obstacle, but the data are automatically discarded because our fake reflections are the only one perceived by the sensor.""The scientists demonstrated the attack on moving vehicles and robots with the attacker placed about 15 feet away on the side of the road. But in theory in could be accomplished from farther away with upgraded equipment. The tech required is all fairly basic, but the laser must be perfectly timed to the lidar sensor and moving vehicles must be carefully tracked to keep the laser pointing in the right direction.An animated GIF showing how the attack uses a laser to inject spoofed data points to the lidar sensor, which causes it to discard genuine data about an obstacle in front of the sensor. Credit: Sara Rampazzi/University of Florida""It's primarily a matter of synchronization of the laser with the lidar device. The information you need is usually publicly available from the manufacturer,"" said S. Hrushikesh Bhupathiraj, a UF doctoral student in Rampazzi's lab and one of the lead authors of the study.Using this technique, the scientists were able to delete data for static obstacles and moving pedestrians. They also demonstrated with real-world experiments that the attack could follow a slow-moving vehicle using basic camera tracking equipment. In simulations of autonomous vehicle decision making, this deletion of data caused a car to continue accelerating toward a pedestrian it could no longer see instead of stopping as it should.The attack deletes data in a cone in front of the vehicle, making a moving pedestrian invisible to the lidar system within that range. Credit: Sara Rampazzi/University of FloridaUpdates to the lidar sensors or the software that interprets the raw data could address this vulnerability. For example, manufacturers could teach the software to look for the telltale signatures of the spoofed reflections added by the laser attack.""Revealing this liability allows us to build a more reliable system,"" said Yulong Cao, a Michigan doctoral student and primary author of the study. ""In our paper, we demonstrate that previous defense strategies aren't enough, and we propose modifications that should address this weakness.""More information: Yulong Cao et al, You Can't See Me: Physical Removal Attacks on LiDAR-based Autonomous Vehicles Driving Frameworks, arXiv (2022). arxiv.org/abs/2210.09482 Journal information: arXiv Yulong Cao et al, You Can't See Me: Physical Removal Attacks on LiDAR-based Autonomous Vehicles Driving Frameworks,(2022). DOI: 10.48550/arxiv.2210.09482","Laser attack blinds autonomous vehicles, deleting pedestrians and confusing cars"
275,-1,-1_new_said_study_people,https://techcrunch.com/2022/11/05/laid-off-climate-tech-is-looking-for-talent-and-founders/,"As rumors rumbled that the U.S. Federal Reserve would hike rates once more — and when it followed through earlier this week — another round of layoffs hit the tech sector. Stripe, Opendoor, Chime, Zillow, Cerebral, Brex, and of course Twitter, among others, have already cut or are about to eliminate thousands of jobs.That’s bad news for employees today, but it might be good news for the climate in the near future.Before we get too far, let me say up front that getting laid off is terrible and not something I wish to happen to anyone. Not knowing where your paychecks will come from or what benefits you’ll receive is difficult in the best of times, and it’s far worse when economic signs are mixed or major life changes are looming. I am not at all trying to minimize what people go through when they’ve been laid off. It’s happened to me, and it sucks.But layoffs also offer a chance at a new beginning. Even before the recent waves of layoffs started washing over the tech industry, people were leaving their old jobs for new opportunities in climate tech.While this is a TechCrunch+ story, we made sure the paywall is below the key links in case you are job-hunting. Hugs — The TC+ team“One thing we’re seeing is really, really strong talent leaving larger companies,” Erin Price-Wright, a partner at Index Ventures, said at TechCrunch Disrupt, “because some of the financial upside for public tech companies or maybe even late-stage tech companies has sort of vaporized in the last few months. And people are like, ‘Well, I had these golden handcuffs, and that was preventing me from working on what I really care about. And I don’t have that anymore. So I’m going to take a risk and I’m going to do something.’”Climate tech has been booming relative to the rest of the market, with startups in the sector raising $5.6 billion in the first half of this year, short of 2021’s crazy hauls but still well ahead of 2020, the next previous record, according to PitchBook. Five years from now, PitchBook expects the climate tech market to be worth $1.4 trillion, a compound annual growth rate of 8.8%.All those companies are in desperate need of talent. Nearly every early-stage founder in the climate tech space I’ve spoken with in recent months went out of their way to mention that they’re hiring. Climatebase has thousands of jobs listed right now, and that’s just a portion of the climate tech companies with active listings.Shaun Abrahamson, co-founder of climate-focused Third Sphere, pointed out that his firm’s portfolio companies are currently hiring for over 400 positions. Breakthrough Energy Ventures’ portfolio companies are hiring for nearly 1,200 positions.Elsewhere, around 100 companies are using the climate career platform Terra.do to directly connect with applicants, chief business officer Nishant Mani told TechCrunch. The startup frequently runs virtual job fairs to match employees with employers, and business is booming. The platform’s user base is growing 50% month on month, and Mani is aiming to get 1,000 companies actively using the platform in the next six months.",Laid off? Climate tech is looking for talent and founders
274,-1,-1_new_said_study_people,https://techcrunch.com/2022/11/04/how-to-land-investors-who-fund-game-changing-companies/,"A lot of problems worth solving aren’t ones that you can solve in a year or two or even 10.For founders and investors alike, such long timelines can seem daunting. But for Gene Berdichevsky, co-founder and CEO of battery tech startup Sila, hard tech problems are also some of the most tantalizing.“It’s always a good time to be a hard tech startup,” Berdichevsky said at TechCrunch Disrupt. “One of the reasons is that the world doesn’t change just because it should. It changes because someone goes after something insanely hard and actually succeeds at it.”Such hard.tech startups run the gamut from advanced batteries like those made by Sila to nuclear fusion, quantum computing, automation and robotics. Any tech that has the potential for such broad impact also has a massive potential market, and that means a certain class of investors are willing to be in it for the long haul.“Hire people to do the technical stuff. Keep an eye on it, but then go learn the other pieces.” Gene Berdichevsky, co-founder and CEO, Sila“We look for real step-change, game-changing technologies that are going to benefit everyone and we think that will drive a huge [total addressable market],” said Milo Werner, a general partner at The Engine.When Berdichevsky founded Sila, he believed his company’s technology, a silicon-based anode that promises to improve lithium-ion battery energy density by 20%–40%, would be a significant enough advance that it would have no problem finding a market.What he didn’t expect was how long it would take. When Sila’s first product debuted inside the Whoop 4.0 wearable last year, the path to market had been twice as long as Berdichevsky had expected.",How to land investors who fund game-changing companies
272,-1,-1_new_said_study_people,https://techcrunch.com/2022/10/26/genzeros-frederick-teo-on-limitless-opportunities-in-climate-tech/,"2050 is an important year for climate tech, with the Paris Agreement calling for emissions to reach net zero by then. In a conversation with GenZero’s Frederick Teo for SOSV’s Climate Tech Summit, we talked about realistic paths to hitting that goal and how startups can tackle what Teo called one of the most existentialist challenges of our generation.GenZero is a $3.6 billion investment company that is backed by Temasek, already known for its climate investing. Teo talked about how it gauges companies before investing, supporting nascent technologies and solutions in the space and what startups can tackle in the next two decades. This Q&A was edited for length, and you can watch the full conversation here or at the bottom of the article.TC: GenZero’s initial commit is from Temasek, which was already a leader in global investing when it announced GenZero in June. It’s a wholly owned company of Temasek, so why did Temasek decide to start GenZero and what is GenZero doing that Temasek isn’t already?FT: Temasek, as you know, has already taken a lot of steps in the past few years into making investments into sustainability, as well as clean energy and climate-related spaces. It is important for us to think about how to deploy capital in this space because obviously all of us are aware of the climate emergency, the fact that this is actually likely to be one of the most existentialist challenges of our generation. It is important for us to be able to find solutions that can actually address many of these things like global warming, sea level rises, the challenges of food production in a sustainable way. So we wanted to be able to have a dedicated capability to access some of these decarbonization opportunities, and Temasek decided to park aside a sizable amount of capital to be able to develop a team that would be able to focus on issues like carbon markets, decarbonization technologies as well as nature solutions. So that is the reason why we established GenZero as a separate investment platform company.In our work we have been looking at technology solutions such as low carbon materials and carbon capture capabilities, nature solutions that seek to protect and restore natural ecosystems, often with a view to generate carbon credits on top of that, as well as to invest into ecosystem enablers in the carbon market space. The reason for that is because we think that in the near term, energy transition would require some form of participation from carbon markets to allow people to gradually execute this transition. But we do need carbon markets to be credible, effective, transparent, high quality, and therefore there is still investments needed in order to be able to improve capabilities and technologies and solutions in that space.For companies that are curious about trying to pitch themselves to you, what are some examples of your current portfolio companies?In the technology space, we have invested into both funds as well as companies, so a major fund investment is Decarbonization Partners, and that is basically a climate-focused fund that is a joint venture between Temasek and BlackRock. We are an LP invested in that, and they are very focused on late-venture, early growth opportunities across different areas in the decarbonization space.We have also invested into a technology company called Newlight, which seeks to be able to produce bio plastics from captured methane. On the nature side, we have been investing into a few forestry projects that generate carbon credits, and then on the carbon market side, we count among our portfolio companies things like South Pole, which is a global leader in providing project advisory, technical advisory solutions and project development for companies seeking to embark on a net zero decarbonization journey, as well as a carbon exchange called Climate Impact X, which is headquartered here in Singapore.For companies that are curious about potentially getting investment from you, what investment stage does GenZero typically look at?We are kind of flexible. For very early-stage companies, say around the Series A or just before, we will work with different partners to be able to evaluate and deploy capital to support early-stage companies, but I think it’s important to understand why we need to do this. If we think about the broader net zero decarbonization challenge, everybody talks about this 2050 timeline to get to net zero. But the reality is that if we want to create significant climate impact by 2050, we are looking at new solutions that must already somewhat exist today or are starting to come into being today, because we will need another 10 to 15 years for the technologies and solutions to mature and get to a stage where they could be commercialized, and then probably another 10 to 15 years for it to actually be able to be deployed and create some kind of impact. That basically means that this current cohort of young companies are going to make a difference to the 2050 agenda. That is the reason why we are very excited to participate in this space right now, because the action must take place now in order to have any meaningful difference by 2050.Considering that, with technology not coming to fruition by them until then, or making actionable results by then, in light of that, what kind of metrics or milestones do you like to see companies bring to the table before you consider them for your portfolio?I think it goes back to the way we evaluate our performance at GenZero. We have a double bottom line, so our shareholder expects us to be able to obviously achieve some level of financial returns. That’s a given. But we also take the idea around measuring climate impact rather seriously. We try and understand, for example, the kind of climate impact that a solution would be able to achieve if successfully deployed. We also look at the kind of carbon yield that the company or solution would be able to deliver. For example, for every dollar invested in capital, how much carbon bang for the buck can we actually get, because obviously many solutions could be practical, cost effective and great.But for every dollar of invested capital, how much carbon impact can we actually achieve on a per annum or cumulative basis? So that is actually one metric that we think about because capital is going to be finite. We also have a very limited time to be able to achieve a significant amount of climate impact to be able to address the climate change challenge. So it is vitally important for us to understand how to deploy capital into the areas that will make the most meaningful difference.One of the questions I want to ask in terms of working with startups, especially for the long-term, because I think it’s fair to describe GenZero as an active investor that works closely with startups. Some of their work might take a while to come into fruition, so what kind of value add are you able to bring to startups?We work with a range of different companies, whether they are very, very early-stage startups or they are slightly further along on the journey. But I think there are a few things where we hope to be able to bring value to our partners, but the first one is a more considered view around how the carbon markets and developments around carbon are taking place around the world. Because at the end of the day, we are solving for a decarbonization challenge or climate change challenge, that understanding how we are underwriting investments, how we are thinking about the movement of carbon price, and how people are thinking about decarbonization strategies, policies, are being introduced will be important, and I think GenZero hopefully will be able to provide a useful perspective on that front.The second aspect is that GenZero is not alone. We do not profess to be the only game in town. There are many others who are doing great work in this space and we often want to think about how we are able to foster a sense of partnership, a kind of open architecture type of ecosystem in a way that we are able to partner each other dynamically in order to find new and interesting solutions and, more importantly, ways of actually applying them. It is not good enough for us to come up with great ideas. It is much more important to think about how we can actually get those ideas deployed, used and scaled.Being part of a broader Temasek ecosystem here in Singapore, and globally, we have a network of relationships and contacts that might be quite useful for startups, to either try out solutions, bounce ideas, get some of the solutions implemented and also be able to find other sources of financing and support on their own journey of growth. We have a Temasek portfolio in a range of different industries that could certainly use innovative decarbonization solutions, whether it’s, for example, airlines making sustainable aviation fuel, or some of our utility companies looking at carbon capture capabilities, so there is that opportunity to be able to deploy some of these solution across the network or to make those introductions and get actual practitioners and operating companies to provide feedback on what would be needed for some of these solutions to scale and be effective. Oftentimes, I think we are also talking to a broader network of fund managers and fellow investors, and therefore through that collective understanding of issues, we hope to be able to value add to some of our partners.For founders that are listening into this, what kind of opportunities do you think there are for startups in climate tech, or what in particular are you excited about?I think its limitless. The ones we are really watching for in the technology space includes some of the carbon capture and carbon removal technologies, low carbon fuels, low carbon materials and, in particular in the near term, probably things to do with a hydrogen transition that I think would be quite meaningful in terms of being able to push the envelope by around 2030 or so. I think there are also a lot of opportunities in supporting nature solutions. It is a class of investments and opportunities that many investors and corporates might be less familiar with, but nature is as important as technology in trying to solve for near-term decarbonization.Then finally into the carbon market space, I think we do need a lot more capabilities in the MRV space. These are the ones that are doing monitoring, reporting and verification of carbon project rating capabilities, to be able to improve the transparency, credibility and quality assurance in the carbon market space. But let me end off by just saying one thing. There is always a tendency for us to think about digital solutions and software solutions to be able to solve this and they are vitally important because they seek to optimize, and if you do not digitalize, you cannot optimize energy efficiency and many of those are critical to this decarbonization space. But I will certainly have a shout out and encourage many of our founders in the room to also think about the core engineering, the tougher kinds of solution sets that we need, because at the end of the day, even as we optimize, somebody has to fundamentally do something about taking the carbon out, improving the core underlying engineering efficiency of some of our solution. So that’s tougher to do, I think sometimes maybe depending on your point of view, as sexy or not as sexy. But it is vitally important, and I think therefore, there is space for many of us with varied interests to be able to tackled this climate crisis.",GenZeroâs Frederick Teo on âlimitlessâ opportunities in climate tech
269,-1,-1_new_said_study_people,https://techcrunch.com/2022/10/16/top-climate-tech-deals-net-nearly-4b-in-q3-outpacing-other-industries/,"Climate tech wrapped a strong Q3, landing three of the top four equity deals, including the whopping $1 billion Series A raised by fleet-charging startup TeraWatt.While the broader market might be cooling, climate tech continues to be a hot ticket, with investment figures for top deals in Q3 outpacing the two previous quarters this year. In total, five climate tech startups made CB Insights’ top 10 equity deals list in Q3, pulling in a combined $3.7 billion. That far exceeds last quarter’s $2.5 billion across eight top startups and Q1’s $1.4 billion across five top startups.Top climate tech investments continued to diversify, too, showing just how deeply it’s becoming embedded into the economy.","Top climate tech deals net nearly $4B in Q3, outpacing other industries"
266,-1,-1_new_said_study_people,https://techcrunch.com/2022/10/12/2422480/,"Spherics, a U.K.-based carbon accounting platform for SMEs to understand and reduce their environmental impact, has been acquired by accounting giant Sage. Terms of the deal were not disclosed but it’s understood Spherics had raised £1.25 million in equity financing from angel investors and £300,000 in grants.Spherics was a smaller startup playing in a similar space to larger ones, which include Normative, Plan A, Klimametrix.global, Persefoni and Planetly.com (other carbon accounting players, like Watershed and Climatiq, operate more like consultants).Sage previously stated it plans to support SMEs to get to net zero, and this acquisition appears to be part of their strategy.Spherics automates the process of calculating emissions by ingesting data from a company’s accounting software and matching transactions to gauge an estimate of their carbon footprint. It can also apply carbon emission factors to procurement categories (such as delivery, accommodation, electricity and travel).“We know that SMBs care about the impact they have on the environment, and our research shows that they want to work with suppliers and partners that can help them understand and address it,” said Amaya Souarez, EVP Cloud Operations, Sage, in a statement. “By combining Spherics’ innovative software with Sage’s digital network, we are connecting businesses with their customer and supplier emissions data, enabling easy and collaborative climate action across value chains which helps to reduce carbon.”George Sandilands, CEO and co-founder of Spherics, added: “Our vision and mission align very much with Sage’s core values, and we are excited to embark on this new journey to help SMBs knock down barriers to a more sustainable future. Global emissions are still rising fast, and we need immediate and meaningful climate action across the world.”Headquartered in Bristol, United Kingdom, Spherics is the second Bristol startup to be acquired by Sage in the last year, after Brightpearl was picked up in 2021.Bristol seems to be making a habit of climate tech, also producing Ecologi Zero, real-time carbon footprinting software for businesses.",Carbon accounting platform acquired by Sage as climate tech heats up
265,-1,-1_new_said_study_people,https://techcrunch.com/2022/10/11/earthmover-to-bring-petascale-data-tools-to-climate-tech/,"Shortly after the COVID pandemic hit, inspiration struck Ryan Abernathey and Joe Hamman. The pair had been watching the “mass mobilization” of the epidemiology research community, Abernathey said.“Joe and I were both thinking at the time, ‘We wanted to be part of something with that level of intensity and urgency around climate change.’”Abernathey and Hamman met while working on open-source projects, including Pangeo and Xarray, both of which gave them a taste of where the field was heading. Abernathey, who is an associate professor of Earth and environmental sciences at Columbia University, said he saw his lab’s work on tools having a greater impact than the results of their research projects.Hamman, who previously worked at the National Center for Atmospheric Research, also foresaw how data tools could begin to change the field. The business world was benefiting from a slew of new tools that worked well for their data types. “But none of those really exist for scientific data,” he said.The pair eventually connected with Tony Liu, a partner at Costanoa Ventures who specializes in data infrastructure.“We’ve seen the transformation of the business analytics world over the last few years,” Liu said, “where you have this ecosystem of cloud-native tooling that’s emerged that really reduces the complexity for someone less specialized to set up data infrastructure in their company. We believe that a similar pattern will emerge here.“We’re seeing use of climate data increase, even among our portfolio — there’s several companies that are making use of climate data at scale. And also we expect a continued, massive investment into climate tech companies,” he added.The scale of the data, and of the problem, is why Abernathey and Hamman’s new company, Earthmover, raised a $1.7 million pre-seed round from Liu and Costanoa, TechCrunch learned exclusively.",Earthmover to bring petascale data tools to climate tech with $1.7M pre-seed
259,-1,-1_new_said_study_people,https://techcrunch.com/2022/10/06/form-energys-iron-air-battery-on-pace-for-2024-launch-with-450m-series-e/,"Form Energy closed a $450 million Series E fundraising round yesterday to continue its work turning iron into rust.The round was led by TPG Rise Climate and joined by new investors GIC and Canada Pension Plan Investment Board. Existing investors ArcelorMittal, Breakthrough Energy Ventures, Capricorn Investment Group, Coatue, Energy Impact Partners, The Engine, NGP ETP, Temasek, Prelude Ventures and VamosVentures provided follow-on capital.Such large rounds are becoming commonplace in the battery industry as startups transition from research and development to manufacturing and commercialization.Form is one of many battery tech companies that have popped up over the past decade, though it’s taking a different tack from many others. Where most companies are focusing on lithium ion-related chemistries, the startup has been pursuing iron-air batteries. That means its cells don’t rely on expensive and supply-constrained minerals like lithium, cobalt or nickel. Form will start commercial production in late 2024, and it’s aiming to eventually produce its battery packs for less than $20 per kWh.If the company can hit its target, it would hasten the shift toward renewable power. Already, solar and wind are price-competitive with fossil fuel plants, and in some cases they’re cheaper than running existing natural gas-fired plants. Adding inexpensive storage to contain cheap power would make renewables competitive in more locations.Form already has a contract with Great River Energy in Minnesota to install a 150-MWh battery, and it’s working with Southern Company to explore a similar pilot in Georgia.It’s one of a growing number of battery companies that are ignoring EVs in favor of other markets that are key to the energy transition. Form’s batteries are heavy and large, but also inexpensive and long-lasting, all qualities that are ideal for storing excess renewable energy.",Form Energyâs iron-air battery on pace for 2024 launch with $450M Series E
258,-1,-1_new_said_study_people,https://techcrunch.com/2022/10/05/7-investors-discuss-how-agtech-can-solve-agricultures-biggest-problems/,"Climate change and geopolitical instability are wreaking havoc on agriculture. To gauge how VCs are responding to these issues, we spoke with seven investors.For starters, rising greenhouse gas emissions are driving punishing droughts and storms, which are harming crops, exacerbating food insecurity and threatening countless livelihoods. At the same time, Russia’s invasion of Ukraine is rattling the world’s grain supply, driving up costs and further aggravating supply chains.Even as these and other crises hammer the multitrillion-dollar industry, startup investors see potential for huge returns with tech that could boost yields, slash emissions and mitigate waste.“There are opportunities to develop [and] adopt new technologies all along the food value chain that will impact key issues like food security and emissions,” Adam Anders, a managing partner at Anterra Capital, told TechCrunch. Among the areas where he sees the biggest potential impact, the investor cited improving plant genetics, boosting the shelf life of more products and putting digital tools in the hands of farmers.Consumer behavior is another piece of the proverbial puzzle as climate literacy increasingly alters how folks shop.“Over the last few years, we have seen skyrocketing interest in sustainability from consumers and food brands, and awareness over the negative impacts of agriculture continues to grow,” said Ting-Ting Liu, investor at Prosus Ventures. “People are not only paying more attention to agricultural-related emissions but also how much land and water is required to support the world’s food supply and the amount of runoff being generated,” she said.Liu argued that this demand is creating strong tailwinds for businesses that strive to address agriculture’s environmental impact, ultimately driving more capital into everything from cellular agriculture to methane reduction solutions for livestock.Still, agtech is not immune to some of the broader trends in venture.While the value of agtech VC deals rose to $11.4 billion in 2021 from $6.5 billion in 2020, several investors told TechCrunch they’ve noticed a slowdown in agtech deals this year amid the wider tech downturn of 2022.“2021 was a record year for VC across the board. In 2022, VC investments across the board are about 30% lower year on year, and I would expect a similar slowdown for agtech,” Monica Varman, a partner at G2 Venture Partners, told TechCrunch. “Over the medium to long term, however, I do expect agtech VC funding to grow, given supply chain challenges, traceability concerns and advancements in enabling technologies in synbio and robotics,” she added.Agtech investors are also still largely funding men. Out of the nearly $11 billion dispensed into agtech in 2021, 78% went to firms with all-male founders, according to PitchBook. The disparity has only worsened so far in 2022, rising to 81% (out of nearly $7.3 billion) as of September 14, per the data firm.To gauge whether (and how) VCs are responding to these issues and more, we reached out to:Brett Brohl, managing director, Techstars Farm to Fork, and managing partner, Bread and Butter VenturesAgtech VC deal value rocketed from $6.5 billion in 2020 to $11.4 billion in 2021. Will this sort of growth continue?It’s not going to continue in the short run largely because of macroeconomic factors you’re just not seeing — for example, many late-stage deals are going through recently — so in the short term, definitely not.In the long run, the sector has a tremendous amount of opportunity and room for innovation, so with time, you will see continued growth and investor focus on agtech.Agriculture is responsible for about a quarter of global GhG emissions. How has the climate crisis changed how you invest?It is a huge reason deal value skyrocketed in 2020 and 2021. Investors understand that this challenge creates an opportunity. Agtech is not as mainstream as many other sectors, so we need more eyeballs and capital. If you are making the food system more effective and efficient, you are making it more sustainable.We aren’t a big enough fund to finance a startup forever, and we depend on later-stage investors, so this attention and resulting influx of capital helps remove some risk from our portfolio.Which emerging technologies, such as cellular agriculture and AI-powered robots, have the greatest potential to impact key issues like food security and emissions in the next decade?We 100% believe in cellular agriculture and are also huge fans of the robotics space, especially robotics that solve very specific pain points and have low BOMs.“Automation and computer vision will be transformative for agriculture over the next decade, particularly as food production is moved closer to the point of consumption due to food security concerns.” Monica Varman, partner, G2 Venture PartnersWe also love the packaging space — lots of packaging goes into the transportation and movement of food. We’re also excited about anything to do with logistics, manufacturing or transportation that makes the food chain more sustainable.When investing in an agtech startup, which green flags do you look for? Are you open to backing founders who don’t have experience in the industry?Investing in agtech startups is no different from any other company. A great team can take a C- idea, pivot, iterate and make it work. But a C- founder will run any idea into the ground, regardless of how good it is.While founder-market fit can be a benefit to a company, great entrepreneurs are smart, have a great work ethic, are coachable and know how to surround themselves with people who make up for their weaknesses. So industry experience isn’t a requirement for us.Which areas of agtech have received the most attention from early-stage founders in recent years? In which areas would you like to see more work done or investments?The obvious answer is alternative proteins. So much capital has been invested and so many founders are building cool things in the space.I’d love to see more attention paid to things that are a bit downstream, such as manufacturing, logistics and the future of food retail. Over the last few years, you have seen traditional agtech investors move their thesis further downstream, so it is happening.I’m also really interested in fintech applications in the agriculture space, like what Traive and Milk Moovement are doing.What are you doing to fund underrepresented founders in agtech?We actively seek out investors, forums and networks that support underrepresented founders and invest or work with entrepreneurs that are a stage earlier than where we invest. We also maintain a diverse investment team — 75% of our fund are women.Finally, we hold open office hours for anyone every week and provide free public education through multiple channels to help founders level up.Before the invasion, Russia and Ukraine accounted for about 28% of wheat and 15% of corn exports globally. How has the Russian invasion of Ukraine affected agtech VC deal-making given its impact on the global supply chain and the world’s grain supply?I don’t think it’s done much to early-stage agtech founders or venture capital. The macroeconomic effect of the war has at least, in part, been a tightening of monetary supply, which will trickle down to early-stage startups. However, the impact has not been significant at early stages yet.Bayer bought Monsanto for $63 billion in 2018, and a year earlier, ChemChina acquired Syngenta for $43 billion. Today, Bayer’s market cap is less than that deal’s value, and China’s ambassador to Switzerland has called the Syngenta acquisition a bad deal for Beijing. Have the outcomes of these deals affected investors’ hopes for blowout late-stage exits?I wouldn’t call these acquisitions of “modern” agtech companies. Monsanto has been around for 100+ years, and Syngenta was formed over 20 years ago, and even then it was a spin-off. Additionally, these happened in 2017 and 2018. Investment in agtech has exploded since then, indicating that the market does not think these two acquisitions are indicative of underperforming venture investments.The outcomes of companies like Upside Foods, FBN and Indigo Ag will be far more important to the agtech ecosystem. Unfortunately, it’s a very tough market for late-stage companies right now, and that will slow exits and depress ROI on many venture investments, not just agtech deals.How do you prefer to receive pitches? What’s the most important thing a founder should know before they get on a call with you?I’m open to warm intros, thoughtful cold emails or pitches during my open office hours. If you’re pitching me on a call, the number one thing is to be yourself.Anything else you’d like to comment on?I think the blurred lines between food tech and agtech are really interesting. What is agtech? It’s not just farm inputs; there is a lot more to it and that, to me, is exciting.Monica Varman, partner, G2 Venture PartnersAgtech VC deal value rocketed from $6.5 billion in 2020 to $11.4 billion in 2021. Will this sort of growth continue?2021 was a record year for VC. In 2022, VC investments across the board are about 30% lower, and I would expect a similar slowdown for agtech.Over the medium and long term, however, I do expect agtech VC funding to rise given supply chain challenges, traceability concerns and advancements in enabling technologies in synbio and robotics.",7 investors discuss how agtech can solve agricultureâs biggest problems
254,-1,-1_new_said_study_people,https://techcrunch.com/2022/09/13/latams-gen-t-is-working-to-enrich-biotech-with-brazilian-genomic-data/,"Improving the diversity of global genomic data could hasten medical breakthroughs — and that’s the goal of Gen-t, a Brazilian startup working to infuse biotech with genetic data from the country’s population.For Lygia da Veiga Pereira, founder of the company, she said it’s more than just building a company, it’s about advancing science and medical technology.“The field keeps saying that we need diversity, but most of the diversity in the world is in countries with [the] least developed health systems,” said Pereira in an interview with TechCrunch. “Lack of diversity became sort of the mantra for the field, and I saw an opportunity, because if Brazil had anything to contribute, it would be with our diversity.”Today, less than 1% of genomic data is collected from people of Latin American and Hispanic origin; 87% comes from people of European descent. With so much data coming from a European background, this population gains earlier benefits from precision medicine.Without diversifying that data, Pereira said it can’t lead to greater medical advancements.“When you study a population with an ancestry different than European, the chances to make novel discoveries of genes associated with different phenotypes is increased, just because it’s uncharted territory,” she said.Gent-t has a partnership with Dr. Consulta, a network of medical centers in São Paulo and Rio de Janeiro, to recruit participants for its initiative. Currently, the company has efforts across four clinics in São Paulo and over 200 active participants, despite being fairly new, with its soft launch back in June. Patients must give informed consent before any testing is done.Gen-t is looking to have over 200,000 participants and maintain contact with each of them across a five-year period, if not longer. Currently the company is looking for participants 45 years of age or older.Assuming the company reaches its enterprising goal of 200,000 participants, the notion of making genomic data more inclusive is ambitious. Companies like 54gene and Nucleus Genomics are also fighting this uphill battle.In addition to private efforts, the Pan American Health Organization has been conducting genomic and health research within Latin America. But recent efforts within the PAHO have been focused on the COVID-19 pandemic.Gen-t has been able to garner support and raised 10 million Brazilian Real ($2 million USD) in a pre-seed funding round led by Eduardo Mufarej, with participation from Armínio Fraga, Daniel Gold from QVT Financial LP and Roivant Sciences.This round’s funds will go toward building out the startup’s technical teams and beginning clinical research.The road to more diverse genomic data may be a long one, but Pereira hopes this initiative will inspire others to find ways of contributing.“We’re working on building a model to also make the platform available to academia … because the more people doing research on the genetics of our population, the better it is for the population and for science in general,” she said.",LatAmâs Gen-t is working to enrich biotech with Brazilian genomic data
251,-1,-1_new_said_study_people,https://techcrunch.com/2022/09/09/climate-tech-is-a-hot-investment-in-2022-next-five-years-could-be-even-hotter/,"Climate tech is a hot investment in 2022 — next five years could be even hotterDespite whispers of a downturn earlier this year, investors continue to express confidence in climate tech. Though numbers are down compared with 2021, a year that many agree is an outlier in the VC world, they’re on track to beat 2020 as the second hottest year for investment.What’s more, deal counts and values were up in the second quarter of this year compared with the first, suggesting that the slowdown has more or less skipped climate tech.Though deal count is down nearly 19% compared with last year, it was up 15.4% in the second quarter, according to a PitchBook analysis. Total market deal value, down year over year, was up significantly in Q2, and the average value per deal has held steady at $23.6 million, more than triple what it was five years ago.In some ways, those modest numbers could be interpreted as a slight cooldown. But the sector is probably just taking a breather given its near-term potential. Just five years from now, PitchBook estimates the climate tech market will near $1.4 trillion, representing a compound annual growth rate of 8.8%.With that kind of growth coming down the pike, there are a lot of different bets to place in the climate tech sector, but a few stand out for their early-stage potential and favorable tailwinds.",Climate tech is a hot investment in 2022 â next five years could be even hotter
242,-1,-1_new_said_study_people,https://techcrunch.com/2022/08/16/levidian-loop/,"The U.K. water processing industry produces a godawful amount of biogas annually. The gasses are primarily used to generate operational heat and power on-site, or they can be turned into biomethane and injected back into the national gas grid. New research funding is going to see if United Utilities can use Levidian’s Loop system to turn these waste gases into carbon-negative hydrogen (which can be easily stored for later use) along with graphene, which has a number of interesting use cases, including medicine, electronics and energy.“This is an exciting project that will lead the way to utilize Loop to decarbonize biogas at scale,” comments Levidian CEO John Hartley. “The consortium has a vast amount of knowledge and experience, which we are leveraging to produce carbon-negative hydrogen — there is no better goal to be working on right now.”The U.K. government’s Department for Business, Energy, and Industrial Strategy has awarded the project around $250,000 (£212,000, to be exact) through the Net Zero Innovation Portfolio for the first phase of a project. The hope is that the project will turn out to be commercially viable as well as an environmental win.The phase one feasibility study will allow the consortium to assess the performance of various biogas samples in a small-scale Loop system located at the Levidian Technology Centre in Cambridge. Although the primary goal of the work is to produce hydrogen, the Levidian Loop doubles as a carbon capture technology. The carbon extracted from the biogas is locked permanently into high-quality graphene, which can then go on to decarbonize a wide variety of other products.The company claims that the hydrogen produced by Loop will be carbon negative — if the system is powered by renewable electricity, that is.Founded in 2012, Levidian is a British climate-tech business whose Loop technology cracks methane into hydrogen and carbon, locking the carbon into high-quality green graphene. The device uses a low-temperature, low-pressure process to crack methane into its constituent atoms, hydrogen and carbon, without needing catalysts or additives.","Getting power from poop, with Levidianâs Loop"
406,-1,-1_new_said_study_people,https://www.nationalgeographic.co.uk/science-and-technology/2022/10/many-scientists-see-fusion-as-the-future-of-energy-and-theyre-betting-big,"Even with massive investment, there are very high hurdles still to overcome: technical challenges such as fuel performance and reactor maintenance; political challenges, too, although the Americans, the Europeans, the Russians, the Chinese, the Japanese and the Australians have all warmed to the idea.As have Britons. In October 2021, the Department for Business, Energy & Industrial Strategy published its strategy on nuclear fusion. This form of energy, it notes, will be abundant, efficient, carbon-free, safe, and will produce radioactive waste much shorter-lived than that of current nuclear power stations.Arthur Turrell is a former plasma physicist at Imperial College London, and author of a 2021 book, The Star Builders: Nuclear Fusion & the Race to Power the Planet. He says that “controlling fusion to produce energy is the biggest technological challenge we’ve ever taken on as a species”. He explains how fusion reactors, or “star machines”, are indescribably complex, with tens of millions of individual parts.The science bitSo just how does nuclear fusion work? It is the fusing of light nuclei to form a heavier nucleus, at the same time releasing huge amounts of energy. It’s what happens in the middle of stars like our Sun, providing the power that drives the universe. Crucially, it’s the opposite of nuclear fission, the process used in nuclear power stations whereby huge amounts of energy are released when nuclei are split apart to form smaller nuclei.The Sun notwithstanding, humans are currently experimenting with two main methods of fusion. JET, for example, uses what’s known as magnetic confinement fusion: two isotopes of hydrogen – deuterium and tritium – are heated to temperatures up to 150 million degrees Celsius, becoming an electrically-charged gas called plasma, which is confined in the doughnut-shaped tokamak, and controlled with strong magnetic fields. The deuterium and tritium fuses together to produce helium and high-speed neutrons, releasing vast amounts of energy in the process – 10 million times more energy per kg of fuel than that released by burning fossil fuels. As Turrell neatly explains, the mass of deuterium-tritium fuel equivalent to an Olympic swimming pool of water would contain more energy than the entire planet uses in a year.",Many scientists see fusion as the future of energy â and they're betting big.
407,-1,-1_new_said_study_people,https://www.nature.com/articles/d41586-022-02947-7,"Artificial-intelligence tools are helping to scientists to come up with proteins that are shaped unlike anything in nature.Credit: Ian C Haydon/UW Institute for Protein DesignIn June, South Korean regulators authorized the first-ever medicine, a COVID-19 vaccine, to be made from a novel protein designed by humans. The vaccine is based on a spherical protein ‘nanoparticle’ that was created by researchers nearly a decade ago, through a labour-intensive trial-and error-process1.Now, thanks to gargantuan advances in artificial intelligence (AI), a team led by David Baker, a biochemist at the University of Washington (UW) in Seattle, reports in Science2,3 that it can design such molecules in seconds instead of months.‘The entire protein universe’: AI predicts shape of nearly every known proteinSuch efforts are a part of a scientific sea change, as AI tools such as DeepMind’s protein-structure-prediction software AlphaFold are embraced by life scientists. In July, DeepMind revealed that the latest version of AlphaFold had predicted structures for every protein known to science. And recent months have seen an explosive growth in AI tools — some based on AlphaFold — that can quickly dream up completely new proteins. Previously, this had been a painstaking pursuit with high failure rates.“Since AlphaFold, there’s been a shift in the way we work with protein design,” says Noelia Ferruz, a computational biologist at the University of Girona, Spain. “We are witnessing very exciting times.”Most efforts are focused on tools that can help to make original proteins, shaped unlike anything in nature, without much focus on what these molecules can do. But researchers — and a growing number of companies that are applying AI to protein design — would like to design proteins that can do useful things, from cleaning up toxic waste to treating diseases. Among the companies that are working towards this goal are DeepMind in London and Meta (formerly Facebook) in Menlo Park, California.“The methods are already really powerful. They’re going to get more powerful,” says Baker. “The question is what problems are you going to solve with them.”From scratchBaker’s laboratory has spent the past three decades making new proteins. Software called Rosetta, which his lab started developing in the 1990s, splits the process into steps. Initially, researchers conceived a shape for a novel protein — often by cobbling together bits of other proteins — and the software deduced a sequence of amino acids that corresponded to this shape.But these ‘first draft’ proteins rarely folded into the desired shape when made in the lab, and instead ended up stuck in different confirmations. So another step was needed to tweak the protein sequence such that it folded only into a single desired structure. This step, which involved simulating all the ways in which different sequences might fold, was computationally expensive, says Sergey Ovchinnikov, an evolutionary biologist at Harvard University in Cambridge, Massachusetts, who used to work in Baker’s lab. “You would literally have, like, 10,000 computers running for weeks doing this.”What's next for AlphaFold and the AI protein-folding revolutionBy tweaking AlphaFold and other AI programmes, that time-consuming step has become instantaneous, says Ovchinnikov. In one approach developed by Baker’s team, called hallucination, researchers feed random amino-acid sequences into a structure-prediction network; this alters the structure so that it becomes ever-more protein-like, as judged by the network’s predictions. In a 2021 paper, Baker’s team created more than 100 small, ‘hallucinated’ proteins in the lab and found signs that about one-fifth resembled the predicted shape4.AlphaFold, and a similar tool developed by Baker’s lab called RoseTTAFold, were trained to predict the structure of individual protein chains. But researchers soon discovered that such networks could also model assemblies of multiple interacting proteins. On this basis, Baker and his team were confident they could hallucinate proteins that would self-assemble into nanoparticles of different shapes and sizes; these would be made up of numerous copies of a single protein and would be similar to those on which the COVID-19 vaccine is based.Nik Spencer/Nature; Source: Adapted from N. Ferruz et al. Preprint at bioRxiv https://doi.org/10.1101/2022.08.31.505981 (2022); and J. Wang et al. Science 377, 387–394 (2022).But when they instructed microorganisms to make their creations in the labs, none of the 150 designs worked. “They didn’t fold at all: they were just gunk at the bottom of the test tube,” says Baker.Around the same time, another researcher in the lab, machine-learning scientist Justas Dauparas, was developing a deep-learning tool to address what is known as the inverse folding problem — determining a protein sequence that corresponds to a given protein’s overall shape3. The network, called ProteinMPNN, can act as a ‘spellcheck’ for designer proteins created using AlphaFold and other tools, says Ovchinnikov, by tweaking sequences while maintaining the molecules’ overall shape.When Baker and his team applied this second network to their hallucinated protein nanoparticles, it had much greater success making the molecules experimentally. The researchers checked 30 of their new proteins using cryo-electron microscopy and other experimental techniques, and 27 of them matched the AI-led designs2. The team’s creations included giant rings with complex symmetries, unlike anything found in nature. In theory, the approach could be used to design nanoparticles corresponding to almost any symmetric shape, says Lukas Milles, a biophysicist who co-led the effort. “It is electrifying to see what these networks can do.”Deep-learning revolutionDeep-learning tools such as proteinMPNN have been a game changer in protein design, says Arne Elofsson, a computational biologist at Stockholm University. “You draw your protein, push a button, and you get something that one in ten times works.” Even higher success rates can be achieved by combining multiple neural networks to tackle different parts of the design process, as Baker’s team did in designing the nanoparticles. “Now we have full control over the shape of the protein,” says Ovchinnikov.Baker’s isn’t the only lab applying AI to protein design. In a review paper posted to the bioRxiv this month, Ferruz and her colleagues counted more than 40 AI protein-design tools that have been developed in recent years, using various approaches5 (see ‘How to design a protein’).Many of these tools, including proteinMPNN, tackle the inverse folding problem: they specify a sequence that corresponds to a particular structure, often using approaches borrowed from image-recognition tools. Some others are based on an architecture similar to that of language neural networks such as GPT-3, which produces human-like text; but, instead, the tools are capable of producing novel protein sequences. “These networks are able to ‘speak’ proteins,” says Ferruz, who has co-developed one such network6.With so many protein-design tools available, it’s not always clear how best to compare them, says Chloe Hsu, a machine-learning researcher at the University of California, Berkeley, who developed an inverse folding network with researchers from Meta7.Four examples of protein ‘hallucination’. In each case, AlphaFold is presented with a random amino-acid sequence, predicts the structure, and changes the sequence until the software confidently predicts that it will fold into a protein with a well-defined 3D shape. Colours show prediction confidence (from red for very low confidence, through yellow and light blue to dark blue for very high confidence). Initial frames have been slowed down for clarity. Credit: Sergey OvchinnikovMany teams gauge their network’s ability to accurately determine the sequence of an existing protein from its structure. But this doesn’t apply for all methods, and it’s not clear how this metric, known as recovery rate, applies to the design of novel proteins, say scientists. Ferruz would like to see a protein-design competition, analogous to the biennial Critical Assessment of protein Structure Prediction (CASP) experiment, in which AlphaFold first demonstrated its superiority over other networks. “It’s a dream. Something like CASP would really move the field forward,” she says.To the wet labBaker and his colleagues are adamant that making a novel protein in the lab is the ultimate test of their methods. Their initial failure to make hallucinated protein assemblies shows this. “AlphaFold thought they were fantastic proteins, but they clearly didn’t work in the wet lab,” says Basile Wicky, a biophysicist in Baker’s lab who co-led the effort, along with Baker, Milles and UW biochemist Alexis Courbet.But not all scientists developing AI tools for protein design have easy access to experimental set-ups, notes Jinbo Xu, a computational biologist at the Toyota Technological Institute at Chicago in Illinois. Finding a lab to collaborate with can take time, so Xu is establishing his own wet lab to put his team’s creations to the test.Experiments will also be essential when it comes to designing proteins with specific tasks in mind, says Baker. In July, his team described a pair of AI methods that allow researchers to embed a specific sequence or structure in a novel protein8. They used these approaches to design enzymes that catalyse particular reactions; proteins capable of binding to other molecules; and a protein that could be used in a vaccine against a respiratory virus that is a leading cause of infant hospitalizations.Last year, DeepMind launched a spin-off company called Isomorphic Labs in London that intends to apply AI tools such as AlphaFold to drug discovery. DeepMind’s chief executive, Demis Hassabis, says that he sees protein design as an obvious and promising application for deep-learning technology, and for AlphaFold in particular. “We’re working quite a lot in the protein design space. It’s pretty early days.”",Scientists are using AI to dream up revolutionary new proteins
410,-1,-1_new_said_study_people,https://www.nature.com/articles/s41591-022-02012-w,"We examined the association between step count volume and intensity across the entire spectrum of human disease using commercial activity monitors linked to an individual’s EHR. We identified consistent and statistically significant associations between activity levels and incident diabetes, hypertension, GERD, MDD, obesity and sleep apnea. Taking more steps each day was related to lower risk of developing these chronic diseases. Higher step counts were associated with protection from obesity in a high-risk population (BMI 25–29 kg m–2). Step count was positively correlated with step intensity, regardless of the bout cadence definition. The relation of step counts with disease risk persisted for diabetes, GERD, MDD and sleep apnea even when adjusting for step intensity. Step intensity was also significantly associated with these outcomes. These data provide new, empiric evidence of activity levels associated with chronic disease risk and suggest that integration of commercial wearables data into the EHR may be valuable to support clinical care.Our findings are consistent with previous literature describing associations between step counts and adverse events10,11. A systematic review by Hall et al.10 found that taking more steps per day was related to lower risk of all-cause mortality, cardiovascular events and incident diabetes. The National Health and Nutrition Examination Survey study, which quantified steps over a 7-day monitoring period and assessed mortality over an average of 10.1 years, found a 51% lower mortality at 8,000 steps per day compared with 4,000 steps per day1. Similar results were reported from a middle-aged, biracial cohort with 7 days of monitoring and over 10 years of follow-up time5. A prospective cohort study conducted in 3,055 community-dwelling adults aged over 70 years found a similar nonlinear relation between daily steps and risk of developing diabetes, where the risk leveled off at 8,000 steps per day12. It is notable that step count thresholds associated with risk of mortality and cardiometabolic disease in prior studies are similar to step count thresholds associated with a wide variety of previously unreported phenotypes in our study. These results suggest that a single step count target of approximately 8,000–9,000 steps per day may be suitable to reduce risk of many common conditions.Our study design and analytic approach differed from prior studies in important ways that make our results new and clinically relevant. First, prior studies assessed step counts over a single, short (usually 7 days) monitoring period with activity data between the baseline monitoring period and outcomes assessment, often many years later. Short monitoring periods are prone to an observer effect and may not accurately reflect true short- and long-term activity behavior13. In contrast, our models accounted for changes in steps over the entirety of an individual’s monitoring period (median of 4 years) rather than a brief snapshot. Second, prior studies have focused on a narrow set of outcomes (for example, mortality, diabetes and cardiovascular disease) ascertained at a single timepoint remote from the initial monitoring period. Our study used a hypothesis-generating phenome-wide association study approach, examining the association between step counts and the human phenome. In this manner, several new associations emerged including GERD, sleep apnea and MDD, which would likely go unidentified if disease phenotypes were selected a priori. Lastly, our analysis permitted incident disease to emerge at any point during clinical care rather than a prespecified follow-up time as performed in most cohort studies. One may speculate that this approach is more accurate with respect to the timing of incident disease and refines the temporal association between longitudinal activity and incident disease.The findings of this study should be viewed in the context of several limitations. We were not able to account for daily step variations between different types of Fitbit models14 and seasonal differences15 as well as the occurrence of the COVID-19 pandemic because device data were not available at the time of analyses and data were date-shifted to protect privacy of participants. The characteristics of our study sample may limit the generalizability of our findings to more diverse populations. The majority of our cohort was relatively young, female, white and college-educated, and only included participants who owned Fitbit devices. Further, participants engaged in more steps per day (median 7,731 steps per day) than the average steps per day values reported for adults in the USA aged over 60 years16, suggesting that the analytical cohort in this study was more active. The fact that we were able to detect robust associations between steps and incident disease in this active sample suggests even stronger associations may exist in a more sedentary population. Therefore, further studies are needed including participants who are historically under-represented in biomedical research and those with activity levels that more closely mirror the general community.Our data do not account for nonstepping activity such as swimming or cycling, such nonstepping movement is better captured via waveform or raw accelerometry and may provide additional insight into the association between physical activity and clinical diagnoses. Further, this study was observational in nature; therefore, causation should not be inferred. We acknowledge the potential for reverse causation in which the existence of a condition leads to taking fewer steps rather than the reverse. We attempted to mitigate this concern by focusing only on incident conditions and excluding any incident disease that emerged in the first 6 months of the monitoring period. Further, there is a potential for unmeasured confounding in our analyses because we were not able to account for an exhaustive list of potential confounders such as job status, environmental factors and differences in the usage patterns between participants over time17. Future studies are needed to investigate the impact of user behavior on health outcomes. Additionally, findings from exploratory logistic regression that did not find an association between steps per day and other outcomes such as cardiovascular diseases should be viewed with caution given that the analytical sample was relatively young, reported fewer outcomes and had limited follow-up. We excluded 15.4–16.0% (varies based on the outcome) of months due to fewer than 15 valid days of data in the Cox models. This missingness seems acceptable in comparison with prior studies which considered data to be valid if activity was captured on at least 3 out of 7 days (that is, up to 57% missing data)18. Lastly, we also acknowledge the limitations of using EHR data for outcomes ascertainment and the potential lack of specificity of diagnostic codes. It is possible that conditions are coded improperly, not coded at all or not recognized in the clinic. Nonetheless, our results reflect use of diagnostic codes in clinical practice across various medical systems, including large regional medical centers and federally qualified health centers.Despite these limitations, the sources of data for our study are unique and offer an example of the potential clinical value of linking wearables data to the EHR. Published activity studies almost exclusively used research-grade actigraphs to measure steps and/or activity counts. In contrast, our data derive from commercially available devices. Although some fidelity is lost between research-grade and commercial devices, data from the latter are highly generalizable to a large portion of the public who own such devices. Activity data in this study date to the creation of a Fitbit account by the user. Therefore, the risk of an observer effect in this cohort is negligible because much of the activity data was collected before the participant consented to All of Us.These findings may have important clinical and public health implications. We were unable to identify any published studies that investigated the association of physical activity data from a wearable device to health outcomes, defined using an individual’s EHR. Therefore, this study provides important new evidence that integration of these data sources is feasible and may provide valuable and actionable information for clinicians. Clinicians could monitor activity trends and provide evidence-based anticipatory guidance for activity tailored to an individual’s clinical characteristics and risk profile. For example, our data suggest that an individual with a BMI of 28 kg m–2 (can lower their risk of obesity 64% (95% CI 51, 80) by increasing steps from approximately 6,000 steps to 11,000 steps per day (Fig. 3). Although validation of these results is important, such data provide a necessary first step toward the development of personalized activity prescriptions. Further, wearables can also be used as an adjunct tool to encourage patients to engage in physical activity by allowing them to set, measure and track goals19. Finally, self-reported physical activity or exercise interventions may have potential beneficial effects to lower the incidence of depression20 and lower the severity of obstructive sleep apnea and associated comorbidities21. Therefore, these results provide support for the need for further research to examine the effect of real-world, unstructured physical activity to prevent or mitigate the effects of such conditions, including some previously unidentified activity-disease associations (for example, GERD).In summary, using the data from AoURP, higher daily step counts were associated with reduced risk of several common, chronic diseases, including diabetes, hypertension, GERD, MDD, obesity and sleep apnea. This association between step counts over time and incident chronic diseases was consistent even after adjusting for potential covariates, including baseline steps per day and step intensity. Step intensity was also significantly associated with these incident diseases, although the relationships were less consistent than with step counts. These findings provide a new, robust source of evidence in support of the physical activity guidelines to prevent the risk of developing chronic diseases. If validated, these results may offer an evidence-base for refining activity recommendations based on an individual’s risk profile. This study also provides an example of the potential clinical value of linking data from commercially available wearables to the EHR.",Association of step counts over time with the risk of chronic disease in the
544,-1,-1_new_said_study_people,https://www.tomshardware.com/news/lithograhy-tool-russia-7nm-2028,"A Russian institute is developing its own lithography scanner that could produce chips using 7nm-class fabrication technologies. The machine is under development, with the plan to build it by 2028. When it is ready, it should be more efficient than ASML's Twinscan NXT:2000i tool, whose development took over a decade.After Russia unleashed its bloody war against Ukraine on February 24, Taiwan was quick to ban shipments of advanced chips to the nation. The U.S., the U.K., and the E.U. then followed up with sanctions that effectively prohibit virtually all contract chipmakers with advanced fabs from working with Russian entities. In addition, companies like Arm cannot license their technologies to Russia-based chip designers. As a result, the Russian government rolled out a national program to develop the country's own 28nm-class fabrication technology by 2030, reverse engineer as many foreign chips as possible, and educate local talent to work on domestic chips.However, there is a problem with a 28nm-class production node by 2030. Russia's most advanced fab can produce chips using a 65nm fabrication technology. Meanwhile, American and European makers of fab tools cannot supply their equipment to Russia due to sanctions, so the country has to design and build domestic wafer production equipment if it wants to adopt a 28nm node. Essentially, what has taken companies like ASML and Applied Materials decades to develop and iterate has to be done in about eight years.Apparently, the Russian Institute of Applied Physics of the Russian Academy of Sciences intends to beat all expectations and produce a 7nm-capable lithography scanner by 2028, according to its plans published on Nizhy Novgorod Strategy Development website (via CNews).A modern lithography scanner capable of processing wafers using a 7nm-class process technology is a highly complex apparatus that involves a high-performance light source, sophisticated optics, and precise metrology, just to name a few critical parts. However, as a leading applied physics university in Russia, IAP believes that it can develop such a tool in a relatively short amount of time.The tool will be somewhat different from scanners produced by companies like ASML or Nikon. For example, IAP plans to use a >600W light source (total power, not intermediate focus power) with an 11.3nm exposure wavelength (EUV wavelength is 13.5nm), which will require considerably more sophisticated optics than exists today. Because the light source of the device will be relatively low power, it will make the tool more compact and easier to build. Yet, it also means that its production of the scanner will be considerably lower than that of modern deep ultraviolet (DUV) tools. That might not be a problem, according to IAP.When it comes to timing, IAP may be slightly too optimistic. For everything below 32nm, chipmakers use the so-called immersion lithography (which is essentially a booster to DUV tools). ASML introduced its first immersion lithography system — the Twinscan XT:1250i — in late 2003 with a plan to deliver one in Q3 2004 to produce 65nm logic chips and 70nm half-pitch DRAMs. It took the company about five years and another generation of tools to announce its 32nm-capable Twinscan NXT:1950i in late 2008, with customer deliveries starting in 2009.Then, it took the market leader some nine years to deliver its 7nm and 5nm-capable Twinscan NXT:2000i DUV tool in 2018. TSMC used less advanced tools with multi-patterning for its first-generation N7 fabrication technology, but the timing of ASML’s introductions demonstrate how hard it is to transition from 65nm to 7nm. It took ASML 14 years to go from 65nm to 7nm. Now, IAP, which does not have any experience in chip production or ties with chipmakers, intends to build a 7nm-capable machine for volume production from scratch in about six years. While the plan does not sound feasible, it looks like IAP is full of enthusiasm.""ASML, the global lithography leader, has been developing its EUV lithography system for almost 20 years and the technology has turned out to be incredibly complex,"" said Nikolai Chkhalo, Deputy Director of the Institute of Physics of Microstructures of the Russian Academy of Sciences for scientific and technological development. ""The main objective of ASML in this case was to maintain the extremely high productivity that is needed only at the world's largest factories. In Russia, no one needs such high productivity. In our work, we start from the needs and tasks faced by domestic microelectronics — and this is not so much about quantity, but about quality. First of all, we need to transit to our own fabrication processes, develop our own design standards, our own tools, engineering, materials, so our own path is inevitable here. In fact, we need to balance between simplicity and performance.""IAP plans to build a fully functional alpha scanner by 2024. This one will not have to offer high productivity or maximum resolution but will have to work and be attractive to potential investors. IAP intends to build a beta version of the scanner with higher productivity and resolution by 2026. This machine should be mass production ready, but its productivity is not expected to be at its maximum. The final iteration of the litho scanner is said to emerge in 2028. It should get a high-performance light source (hence better productivity), better metrology and overall capabilities. There is no word how many of such machines IAP and/or its production partners will be able to produce by 2028.It should be noted that fab equipment is not limited to lithography scanners. There are other types of machines performing etching, deposition, resist removal, metrology, and inspection operations that are not made in Russia. Furthermore, there is somewhat less advanced machinery like ultrapure air and water generators which also are not produced in Russia. Even if IAP RAS manages to build a lithography tool, Russia will still be a few hundred tools short of building a modern fab. Also, fabs need ultrapure raw materials produced in countries that will not supply to Russia.",Russian University Vows to Build 7nm Chipmaking Tools
543,-1,-1_new_said_study_people,https://www.tomshardware.com/news/intel-confirms-6gb-alder-lake-bios-source-code-leak-new-details-emerge,"We recently broke the news that Intel's Alder Lake BIOS source code had been leaked to 4chan and Github, with the 6GB file containing tools and code for building and optimizing BIOS/UEFI images. We reported the leak within hours of the initial occurrence, so we didn't yet have confirmation from Intel that the leak was genuine. Intel has now issued a statement to Tom's Hardware confirming the incident:""Our proprietary UEFI code appears to have been leaked by a third party. We do not believe this exposes any new security vulnerabilities as we do not rely on obfuscation of information as a security measure. This code is covered under our bug bounty program within the Project Circuit Breaker campaign, and we encourage any researchers who may identify potential vulnerabilities to bring them our attention through this program. We are reaching out to both customers and the security research community to keep them informed of this situation."" — Intel spokesperson.The BIOS/UEFI of a computer initializes the hardware before the operating system has loaded. Among its many responsibilities, the BIOS establishes connections to certain security mechanisms, like the TPM (Trusted Platform Module). Now that the BIOS/UEFI code is in the wild and Intel has confirmed it as legitimate, both nefarious actors and security researchers alike will undoubtedly probe it to search for potential backdoors and security vulnerabilities.In fact, famed security researcher Mark Ermolov has already been hard at work analyzing the code. His early reports indicate that he has found secret MSRs (Model Specific Registers) that are typically reserved for privileged code and thus can present a security problem, along with the private signing key used for Intel's Boot Guard, thus potentially invalidating the feature. In addition, there are also signs of ACMs (Authenticated Code Modules) for BootGuard and TXT (Trusted Execution Technology), portending potential future issues with the root of trust.I can't believe: NDA-ed MSRs, for the newest CPU, what a good day... pic.twitter.com/bNitVJlkkLOctober 8, 2022 See moreThe impact and breadth of discoveries could be limited, though. Most motherboard vendors and OEMs would have similar tools and information available to build firmware for Intel platforms. Moreover, Intel's statement that it doesn't rely upon information obfuscation as a security measure means it has likely scrubbed the most overly-sensitive material before releasing it to external vendors.Intel is being proactive, though, and encouraging researchers to submit any vulnerabilities they find to its Project Circuit Breaker bug bounty program, which awards between $500 to $100,000 per bug, depending on the reported issue's severity. It's unclear if the code can indirectly benefit open-source groups like Coreboot.Intel hasn't confirmed who leaked the code or where and how it was exfiltrated. However, we do know that the GitHub repository, now taken down but already replicated widely, was created by an apparent LC Future Center employee, a China-based ODM that manufactures laptops for several OEMs, including Lenovo. Additionally, one of the leaked documents refers to ""Lenovo Feature Tag Test Information,"" furthering the theories of the link between the company and the leak. There are also a plethora of files labeled 'Insyde,' referring to Insyde Software, a company that provides BIOS/UEFI firmware to OEMs and is known to work with Lenovo.We aren't aware of any attempts at ransom yet, but Intel or the affected parties might not have made those attempts public. Conversely, this could simply be the case of an employee inadvertently posting the source code to a public repository.However, recent hacks have targeted outside vendors to indirectly steal information from semiconductor manufacturers, thus enabling ransom attempts, and this leak could follow that model. A spate of recent attacks includes an attempt by RansomHouse to extort AMD after it obtained 56GB of data. AMD partner Gigabyte also had 112 GB of sensitive data stolen in the infamous 'Gigabyte Hack,' but AMD refused to pay the ransom for the latter hack. As a result, information about AMD's forthcoming Zen 4 processors was divulged before launch, which later proved genuine.Nvidia also suffered a recent attack that resulted in the theft of 1TB of its data, but the GPU-making giant retaliated with its own operations to render the stolen data useless.We'll update this article if any new details emerge.","Intel Confirms Alder Lake BIOS Source Code Leak, New Details Emerge"
542,-1,-1_new_said_study_people,https://www.timesofisrael.com/cloned-cannabis-cells-with-12-times-more-potency-are-grown-in-israeli-bioreactor/,"An Israeli company has cloned hemp cells and used a bioreactor to grow them into a substance with all the active compounds of cannabis — and 12 times the potency.BioHarvest Sciences says the breakthrough could make the medical benefits of cannabis available in cheaper, cleaner and greener form. It has started applying for the necessary licenses to manufacture and sell its product for medical use in Israel and the United States.“We don’t grow the plant at all,” BioHarvest CEO Ilan Sobel told The Times of Israel. Instead, the process involves “replicating” cells taken from a hemp plant in big tanks called bioreactors to produce huge numbers of identical cells.“We grow them in huge bioreactors in just three weeks — while regular cannabis takes 14 to 23 weeks,” Sobel said. “Our tech can also significantly increase the levels of active ingredients, as a percent of the weight, versus what is found normally in the plant.”And so, every gram emerging from the bioreactor in Rehovot requires less water and other resources to grow than a gram from a cannabis plant— and will stretch considerably further in terms of treating patients, Sobel said.Sign up for the Tech Israel Daily and never miss Israel's top tech stories Newsletter email address Subscribe By signing up, you agree to the termsHe expects the impact of his innovation on the cannabis market to be “breathtaking.” He declined to detail the proprietary technology that is used, but said that the potency has been increased 12-fold purely by changing the environment inside the bioreactor.Sobel said: “By adjusting specific conditions to which the cells are exposed, we can create different desired compositions of active ingredients, meaning we can dial up and down the various cannabinoids [compounds].”The biomass that comes out of the bioreactor consists of cells that are identical to those found in cannabis derived from plants, and there is no genetic modification, Sobel said. Patients can receive it in smokable form or as pills, drops, chewing gum, and a range of other formats.AdvertisementAs well as containing high levels of the active compounds, the biomass is full-spectrum, which means it has the full variety of chemical compounds found in the cannabis plant, and not just some of them.“Our composition has significant amounts of both major cannabinoids such as CBD and THC as well as significant amounts of what have been termed minor cannabinoids,” Sobel said.The controlled environment of the bioreactor could overcome two challenges of cannabis farming: contamination and crops with varying levels of active compounds. Sobel said that the protected environment of the bioreactor keeps out contaminants like fungi, and the controlled nature of the method delivers a product that has consistent compound levels.BioHarvest claims that if its biomass takes off, it will deliver the benefits of medical cannabis more cheaply and with less environmental impact. This is because, according to company calculations, each kilowatt of electricity produces eight times more material from the bioreactor than what is produced from plants. When it comes to water, each gallon produces 54 times more bioreactor material than plant material. Land requirements are reduced by more than 90%.The company’s resource-saving growing techniques are already in use for growing grapes, olives and pomegranates.“The bottom line is that we can make cannabis and hemp far more useful than before, at lower cost to our planetary resources,” Sober commented. “It is a wellness and sustainability solution from Israel that can provide a truly inspiring contribution to the world.”",Cloned cannabis cells with 12 times more potency are grown in Israeli bioreactor
541,-1,-1_new_said_study_people,https://www.theverge.com/c/23307867/human-composting-process-return-home,"Rachel Gerberding has a green thumb. So when her mother died this April, Gerberding decided to compost her. Gerberding, who lives in Washington state in a house surrounded by flowers, had heard about a newly legal method to turn human remains into soil. “I was like, ‘Mom, it would be such a wonderful thing for me — to be able to just walk through [my garden] and be like, ‘Oh, hi, Mom,’” Gerberding, 48, said, recounting their conversation. Sharon Gerberding, who had previously planned on a simple cremation, agreed: “I’m going to be dead,” she told Rachel. “Do whatever you want!”That’s why Sharon, who died from complications of multiple sclerosis, was laid to rest in an industrial park 30 minutes south of Seattle. On a chilly spring day, her family gathered in a nondescript, hangar-style building tucked between a belt rubber warehouse, recycling facilities, and an air quality testing company. Staff had placed Sharon’s body in a vessel filled with alfalfa, straw, sawdust, and notes written in biodegradable ink. Hymns played over the speaker system, a tribute to Sharon’s membership in The Church of Jesus Christ of Latter-day Saints. By early summer, all that would be left of their matriarch was a few hundred pounds of rich, dark soil.This is natural organic reduction — better known as human composting — the first truly new form of final disposition developed in decades. First legalized in Washington state in 2019, the process has proved popular with certain of the Pacific Northwest’s eco-conscious consumers, who are eager to have their last act on earth be a positive one. They know that traditional burials involve literal tons of steel, concrete, and toxic chemicals and that the heat of a cremation retort emits several hundred pounds of carbon into the atmosphere. For families like the Gerberdings, natural organic reduction, or NOR, promises a more gentle way out.Rachel Gerberding at Point Defiance Park, the last public garden she was able to visit with her mother before she died. — Tacoma, WA, July 11th 2022Today, there are four NOR companies in a roughly 150-mile radius of western Washington state, including Return Home, the company in charge of Sharon’s remains. New markets are opening up all the time. NOR is now legal in California, Oregon, Colorado, and Vermont. Boosters hope New York is next. But death industry disruptors face opposition, including from the Catholic Church, which has deemed NOR “more appropriate for vegetable trimmings and eggshells than for human bodies.” They aren’t the only ones: plenty of people find the whole process a little creepy.Despite these challenges, it’s clear a deathcare revolution is underway. New methods of body processing, from NOR to alkaline hydrolysis, are on the rise. So is the home funeral movement, which seeks to return the care of the dead to their families. New companies aim to be cost-competitive; at Return Home, NOR costs about $5,500 with a laying-in ceremony. That’s about twice as much as the average cremation but about half the cost of a traditional funeral and vault burial. But even as new options proliferate, after so many generations of viewing the corpse at a distance, few of us know what we really want to happen when we die — or how to ask for it.That’s where Return Home comes in. Its staff has worked to demystify the NOR process on the company TikTok, racking up over 5 million likes on videos of cartoon bones and pose-n-stay skeletons. They’ve opened up their facility to families, some of whom visit daily during their loved one’s composting. And they’ve slowed the funeral process down: at Return Home, turning a body into soil takes at least two months — much longer than the few chaotic days most families have to make arrangements, attend a service, and lay a body to rest. While people may think NOR is all “magic” and “Mother Earth,” Return Home founder and CEO Micah Truman says he’s out to prove “it’s just so much cooler than that.”Composting food dates back thousands of years, but the notion of actively turning human remains into a usable soil product is only a decade old. In 2013, Katrina Spade, an architecture student, proposed the idea in her graduate thesis. “Our bodies have nutrients,” Spade has said. “What if we could grow new life after we’ve died?”In her thesis, Spade envisioned a “dark, quiet, and safe” space where the natural work of decomposition could be scaled for an urban population and managed in an industrial setting. Think: green burial meets vertical farming. After graduation, Spade worked with soil scientists, lobbyists, and investors (to the tune of more than $15 million) to make the case for legalization in Washington state — and get her own NOR company up and running. It worked, and in May 2019, Governor Jay Inslee signed the bill into law.By early 2021, Spade’s firm, Recompose, was open for business. (Disclosure: my sister-in-law runs the front desk at Recompose’s Seattle facility.) By that point, Spade was hardly the only NOR entrepreneur. In Washington state, funeral consumers can also turn to Return Home, Herland Forest, and Earth.Truman, the founder of Return Home, wears the usual startup uniform: nondescript pants, a button-up shirt, and a fleece vest. But he speaks in zany constructions unlike anything you’d expect from a finance guy, let alone a de facto funeral home owner: We shouldn’t get “airy fairy” about composting, he says. Bodies are just “squishies and hards.” The process at Return Home is “like a Disney ride, only weirder.”Truman is often crying — an occupational hazard, he’s learned, in this new line of work.Initially, the plan for Return Home was something like a ghost kitchen but for human remains. People would ship bodies in, and Truman would send back soil, similar to the way a crematorium returns ashes. That informed the early look of the place — the vessels look like freezer chests in which hunters stock game in their garage. They’re stacked up by the dozen on enormous steel shelves, like bulk pallets of toilet paper in Costco.To make his vision a reality, Truman teamed up with John Paul of Transform Compost Systems just over the border in Canada. Together, they brought on a manufacturer to construct bespoke machinery for every stage process (and that, for proprietary interests, they refuse to name). For 18 months, the international team experimented relentlessly, including with pig carcasses — a good proxy, as any MythBusters fan knows, for the human body.This is, more or less, what they came up with:",Inside one of the worldâs first human composting facilities
540,-1,-1_new_said_study_people,https://www.theverge.com/2022/9/30/23378231/usb-rebranding-2022-logos-gbps-wattage-charging-transfer-speeds-simplification-usb4-superspeed,"The SuperSpeed USB branding is no more thanks to a new set of guidelines currently being rolled out by the USB Implementers Forum (USB-IF), the body that manages and maintains the USB standard.It’s part of a rebranding initiative that the organization kicked off last year with the introduction of a new series of packaging, port, and cable logos. But with its latest set of branding and logo guidelines it’s going even further, simplifying its legacy branding and signaling the end of the decade-old SuperSpeed branding. If the name doesn’t ring any bells, then that’s probably because you (like most other people) simply referred to it by its USB 3 version number. Alongside it, the USB-IF is also ditching USB4 as a consumer-facing brand name.An old Certified SuperSpeed logo. Photo by The VergeThe changes came into effect this quarter, and could start appearing on products and packaging as early as by the end of the year, according to the USB-IF’s president and chief operating officer Jeff Ravencraft. But any products that were certified prior to the shift will still be able to use the old brand names.In a Zoom call, Ravencraft explains that the new branding is designed to prioritize what the standards can actually do, rather than the USB version they’re based on. “As we started to update our branding we did a lot of focus group studies with many different types of consumers,” he tells The Verge, “and none of those people understood the messaging and the branding, and they don’t understand revision control or spec names.”The new range of USB packaging and device logos, no “SuperSpeed” or “USB4” branding in sight. Image: USB-IF“What consumers want to know — and what we learned — is they want to know two things: What’s the highest data performance level the product can achieve? And what’s the highest power level I can get or drive from this product,” he says. “That’s all they want to know.”So, instead of referring to USB devices by a version number or vague name like “SuperSpeed,” the USB-IF wants companies to use branding that reflects these all-important specs. “SuperSpeed USB 5Gbps” and “SuperSpeed USB 10Gbps” are now just “USB 5Gbps” and “USB 10Gbps” respectively, while “USB4 40Gbps” and “USB4 20Gbps” are becoming “USB 40Gbps” and “USB 20Gbps.”Branding for certified USB Type-C cables is also being updated. Rather than simply listing their data transfer speeds, cables will also (for the most part) have to list the charging wattage they’re capable of carrying. So a cable can’t just be branded as being a 40Gbps cable as with last year’s guidelines, it’ll now also have to list a charging speed like 60W or 240W.It’s important to note that using these brandnames is far from mandatory. They apply to USB devices certified by the USB-IF, but these only cover a fraction of the total number of USB products out on the market. That’s because unlike specifications like Thunderbolt 4, which manufacturers have to license directly from Intel, USB is an open standard that anyone is free to use. That’s allowed it to become as ubiquitous as it is, but means the USB-IF is all-but-powerless to stop companies from building USB products that don’t use the specification properly. And no one’s going to stop them from branding a device as USB4 Version 2, or offer no branding at all.Certified cables will now need to show maximum wattage alongside data transfer speeds, unless they’re using the older Hi-Speed standard. Image: USB-IFThe underlying USB version numbers aren’t going anywhere. But while manufacturers and OEMs still have to wrap their heads around specs like USB4 Gen3x2 and USB4 Version 2, the USB-IF doesn’t want customers to care what version number their devices or cables are using. Yes, USB4 Version 2.0 is a terrible name, but the idea is that most consumers should end up seeing it branded as USB 80Gbps.This branding applies regardless of the specific USB port being used, whether it’s old fashioned USB Type-A, microUSB, or USB Type-C. But naturally the higher performance logos will only be applicable to USB Type-C, which is the only connector to support transfer speeds over 10Gbps.Putting the two most important specs front-and-centerA key exception to all these rules is USB Hi-Speed, more commonly known by its version number USB 2.0, which maxes out at a now achingly slow 480Mbps. But the USB-IF’s reasoning is that if it were to be completely consistent, and brand USB 2.0 ports as “USB 480Mbps,” then it’d risk confusing customers who might see the branding next to a “USB 5Gbps” device and wrongly assume it’s faster because of the higher number. Original USB 1.0 branding is also unaffected by this year’s changes.As my former colleague Chaim Gartenberg wrote last year, even with new simplified branding the situation is still far from perfect. Ideally we wouldn’t need half a dozen different logos to describe all the variations of a single connector. But Ravencraft argues that it wouldn’t make sense to force every manufacturer to support the highest specs.“A USB printer will never be USB4 Version 2,” he says. “There’s no need for it, and no one’s going to put the cost of this higher end technology into a printer or a keyboard or a mouse.” Instead, the “billions” of devices on the market that only need the transfer speeds of older standards like USB 2.0 (ahem, sorry, USB Hi-Speed) can continue to use them.There’ll still be lots of different USB versions out thereThe new branding guidelines also don’t cover absolutely everything that a USB cable can do. There’s no information on resolutions or refresh rates if you need a cable that’s going to carry a DisplayPort video signal, nor is there anything to say how a cable might handle taking a PCIe signal. The logos are focused on the USB-IF’s own standards like USB Power Delivery. That also means they don’t offer the same guarantees if you need support for a competing fast-charging standard like Qualcomm’s Quick Charge.",USB kills off SuperSpeed branding as it tries to simplify its ubiquitous connector
536,-1,-1_new_said_study_people,https://www.theverge.com/2022/11/1/23435516/tumblr-porn-ban-modified-nudity-allowed-sexually-explicit-depictions-banned,"Tumblr has made an update it hinted at in September, changing its rules to allow nudity — but not sexually explicit images — on the platform.The company updated its community guidelines earlier today, laying out a set of rules that stops short of its earlier permissive attitude toward sexuality but that formally allows a wider range of imagery. “We now welcome a broader range of expression, creativity, and art on Tumblr, including content depicting the human form (yes, that includes the naked human form). So, even if your creations contain nudity, mature subject matter, or sexual themes, you can now share them on Tumblr using the appropriate Community Label,” the post says. “Visual depictions of sexually explicit acts remain off-limits on Tumblr.”A help center post and the community guidelines offer a little more detail. They say that “text, images, and videos that contain nudity, offensive language, sexual themes, or mature subject matter” is allowed on Tumblr, but “visual depictions of sexually explicit acts (or content with an overt focus on genitalia)” aren’t. There’s an exception for “historically significant art that you may find in a mainstream museum and which depicts sex acts — such as from India’s Śuṅga Empire,” although it must be labeled with a mature content or “sexual themes” tag so that users can filter it from their dashboards.“Nudity and other kinds of adult material are generally welcome. We’re not here to judge your art, we just ask that you add a Community Label to your mature content so that people can choose to filter it out of their Dashboard if they prefer,” say the community guidelines. However, users can’t post links or ads to “adult-oriented affiliate networks,” they can’t advertise “escort or erotic services,” and they can’t post content that “promotes pedophilia,” including “sexually suggestive” content with images of children.",Tumblr will now allow nudity but not explicit sex
529,-1,-1_new_said_study_people,https://www.theregister.com/2022/10/28/microsoft_nadella_2022_pay_karma/,"Relying on the karmic forces of the universe is clearly working out for Microsoft chairman and CEO Satya Nadella, judging by his expanding compensation package.The exec, who landed the role in 2014 and has presided over some monumental growth figures, particularly during the pandemic, scooped up a total of $54.946 million, up 10 percent on a year earlier, according to Microsoft's Proxy Statement 2022.Nadella's base salary was unchanged at $2.5 million, and stock awards sat at $42.269 million, up from $33 million the year before. Non-equity incentive plan compensation fell to $10.06 million from $14.2 million. The final element, classified as ""all other compensation"", was $110,250, of which $100,000 was charitable gifts.Some 96 percent of this near $55 million package was performance-based, versus 71 percent in 2021.Microsoft grew revenue 18 percent to $198.3 billion in the year ended June 30, 2022, including a 32 percent jump in its cloud division to $91.2 billion. Operating profit was up 19 percent to $83.4 billion and net income grew by the same percentage to $72.7 billion.When Nadella took over the organization, Microsoft turned over $86.8 billion in sales, net income was $22.1 billion, and earnings per share were $2.63. At the time the report was written, EPS was $9.65.""Shareholders expressed a desire to ensure the ongoing retention and motivation of Mr Nadella and the independent members of the Board continue to have high confidence in Mr Nadella's exceptional leadership of Microsoft,"" it said.Microsoft's growth has slowed in the past year as it has for many peers following a stall in economies around the world. Microsoft's share price has fallen 32 percent in the last 12 months.The median pay for Microsoft staffers was $190,302 in fiscal 2022. ""Based on this information, for fiscal year 2022 the ratio of the annual total compensation of our CEO to the annual total compensation of the median employee was 289 to 1.""Anyone among that workforce wanting a pay rise would do well to remember what Nadella said during his first months as CEO, when he spoke to an audience at the Grace Hopper Celebration of Women in Computing event.He said women should try to avoid asking for a raise and simply rely on ""karma."" Nadella added: ""It's not really about asking for a raise, but knowing and having faith that the system will give you the right raise.""Days later he admitted he was ""inarticulate"" about women asking for a raise and said closing the gender pay gap was important.According to the Microsoft 2022 Diversity and Inclusion report, out yesterday, women working for the company earned 90 cents for each dollar that men made, according to median unadjusted pay stats. ®",Microsoft boss Nadella's compensation pack swells 10% to $55m
528,-1,-1_new_said_study_people,https://www.theregister.com/2022/10/25/realpage_rent_lawsuit/,"The maker of an algorithm that calculates the optimum rent to charge for homes has been accused of causing an unfair and artificial hike in costs for tenants.Texas-based RealPage and nine real-estate giants have been hit with a lawsuit that claims the software developer formed a ""cartel"" with the landlords to drive up rent in cities where housing is in high demand in the US.Outside of things like rent control, setting rent is a balancing act involving supply and demand. Set it too low and you may not cover the mortgage and other costs (if applicable) as well as miss out on profit. Set it too high and you'll price yourself out of the market. Wouldn't it be handy if there was some kind of app that could automatically identify a sweet spot for rent on a per-home basis?The lawsuit, brought by five renters this month, claimed the nine property-managing giants competed against one another prior to 2016, but found a way to collude with one another via a third party: RealPage. It is claimed RealPage's software was used by the group to collectively set rent and ensure none of them undercut each other, thus inflating costs for tenants.The plaintiffs in the case, filed in San Diego, California, are Sherry Bason, Lois Winn, Georges Emmanuel Njong Diboki, Julia Sims, and Sophia Woodland. They are together suing, in which could turn into a class action suit, the managers of hundreds of thousands of apartments: Greystar, Lincoln Property Co, FPI Management, Mid-America Apartment Communities, Avenue5 Residential, Equity Residential, Essex Property Trust, Thrive Communities Management, and Securities Properties.""Beginning in approximately 2016, and potentially earlier, [the] lessors replaced their independent pricing and supply decisions with collusion,"" according to the federal district court complaint [PDF].""Lessors agreed to use a common third party that collected real-time pricing and supply levels, and then used that data to make unit-specific pricing and supply recommendations. Lessors also agreed to follow these recommendations, on the expectation that competing Lessors would do the same.""RealPage's algorithm, dubbed YieldStar, calculated how apartments should be priced depending on various factors including location and demand. Property managers using the software are free to accept or reject the algorithm's suggestions, though they are heavily encouraged in regular calls with RealPage to follow its pricing, it is claimed. Following an investigation that sparked the above lawsuit, ProPublica reported the code ingests data including rent for surrounding homes to figure out an optimal price.""Machines quickly learn the only way to win is to push prices above competitive levels,"" Maurice Stucke, a University of Tennessee law professor and a former prosecutor in the Justice Department's antitrust division, told the publication.YieldStar thus increases rental prices, it is claimed. Indeed, according to the lawsuit, RealPage boasted that its software drives ""rental rate improvements, year over year, between 5 percent and 12 percent in every market.""If numerous property owners use the same software to price apartments, it stifles competition and raises average rental costs in a city, the lawsuit stated. Renters suffer by having to pay more while landlords benefit by helping each other raise market prices, the plaintiffs alleged. They accused RealPage of breaking Section 1 of the Sherman Act, which states agreements that reasonably restrict trade are illegal; it's claimed RealPage and the real-estate giants were all knowingly on the same page when it came to using the software.And there's another facet to the lawsuit: RealPage, it is claimed, helped landlords, through a feed of information, keep an optimum number of properties vacant at any one time so as to avoid oversupply, and drive up demand and therefore prices.A spokesperson for RealPage denied the lawsuit's allegations, and in a statement to The Register claimed ProPublica's reporting was inaccurate and misleading.""RealPage's revenue management software is purposely built to be legally compliant,"" a representative told us.""RealPage is aware of the lawsuit. We strongly deny the allegations and will vigorously defend against the lawsuit. Beyond that, we do not comment on pending litigation.""A full statement from the biz can be found here, in which it argues its code ""eliminates"" the risk of collusion by recommend fair rates for the market. The property giants did not respond to earlier requests for comment. ®",Rent-calculating software biz accused of colluding with 'cartel' of landlords
527,-1,-1_new_said_study_people,https://www.theregister.com/2022/08/24/google_jupiter_network_mirrors/,"Google has scaled its network capacity from over one petabit per second to beyond six petabits per second since 2015, and some of that growth has come from switches that bounce optical signals off an array of mirrors to redirect traffic.As our sibling site The Next Platform reported in 2015, Google calls its datacenter networking tech Jupiter, which uses a mixture of merchant silicon and custom code to connect the kit that runs Search, YouTube, Gmail, the G-cloud, and plenty more besides.On Tuesday, the advertising giant published a paper and summary explaining the last decade or so of work on Jupiter.The headline figures in both documents detail 5x higher speed and capacity, 30 percent reduction in capex, and 41 percent reduction in power consumption.Plenty of those improvements are the result of Optical Circuit Switches (OCSes) that use mirrors mounted on Micro-ElectroMechanical Systems (MEMS) to map an optical fiber input port to an output port dynamically.And, yup, we're aware that MEMS-based, and non-MEMS, optical mirror switching has existed for computer networks for years and years. What's cool here is the density and throughput Google says it has developed, and now documented, for its own globe-spanning use.Well, we found it interesting, anyway.How Google's mirror-filled optical switch moves traffic ... Source: Google. Click to enlargeIn Google's switches, a signal reaches a ""fiber collimator array"" that offers 136 physical I/O paths, or individual fibers. An incoming signal emerges from one of those fibers, then bounces off a splitter before hitting a MEMS device that has 136 micro mirrors. The MEMS device moves in two dimensions and reflects the signal to one of the 136 fibers in the outgoing collimator array.The tech was needed because Google wanted its network to ""support heterogeneous network elements in a 'pay as you grow' model, adding network elements only when needed and supporting the latest generation of technology incrementally.""That means ""allowing the incremental addition of network capacity – even if of a different technology than previously deployed – to deliver a proportional capacity increase and native interoperability for the entire building of devices.""Achieving that vision is not easy because Google's datacenter networks need to be deployed ""at the scale of an entire building – perhaps 40MW or more of infrastructure.""""Further, the servers and storage devices deployed into the building are always evolving, for example moving from 40Gbit/sec to 100Gbit/sec to 200Gbit/sec and today 400Gbit/sec native network interconnects. Therefore, the datacenter network needs to evolve dynamically to keep pace with the new elements connecting to it.""Google also recognizes that its network is not a single entity.""Datacenter networks are inherently multi-tenant and continuously subject to maintenance and localized failures,"" Google's description, from infrastructure VP Amin Vahdat, explained. ""A single datacenter network hosts hundreds of individual services with varying levels of priority and sensitivity to bandwidth and latency variation.""""For example, serving web search results in real time might require real-time latency guarantees and bandwidth allocation, while a multi-hour batch analytics job may have more flexible bandwidth requirements for short periods of time,"" Vahdat stated. ""Given this, the datacenter network should allocate bandwidth and pathing for services based on real-time communication patterns and application-aware optimization of the network.""That kind of dynamic reconfiguration also helps resilience.""Ideally, if ten percent of network capacity needs to be temporarily taken down for an upgrade, then that ten percent should not be uniformly distributed across all tenants, but apportioned based on individual application requirements and priority,"" Vahdat explains.But networks are hardwired to do certain things well, and even Google's software-defined networking can only do so much to reconfigure them to adapt to new requirements.MEMS and OCS also make it possible to upgrade and reconfigure networks without having to rewire (or re-fiber) a datacenter.Vahdat concluded that its network ""delivers 50x less downtime than the best alternatives we are aware of.""So, take that, Cisco, Juniper, Arista, and pals. And for the rest of us, take comfort that this stuff usually trickles down over time – as happened with Kubernetes and cloud-inspired consumption based IaaS models. ®",How Google uses mirrors to dynamically reconfigure its networks
525,-1,-1_new_said_study_people,https://www.thelancet.com/journals/lanhl/article/PIIS2666-7568(22)00095-2/fulltext#%20,"1 Scott AJEllison MSinclair DA The economic value of targeting aging. 2 House of Lords Science and Technology Select CommitteeAgeing: Science, Technology, and Healthy Living. Improvements in health care, sanitation, diet, and education over the past century have markedly increased life expectancy, but this has not been paralleled by concomitant increases in healthy life expectancy. Increasing healthspan, which is the time spent free of major illness or disease, is therefore an important priority worldwide. The UK Government, for example, has pledged to increase the healthy life expectancy of the UK population by an extra 5 years by 2035 without increasing inequality. Achieving this goal would simultaneously improve individual quality of life, increase productivity, and boost national wealth. Notably, a gain of just one additional healthy year is predicted to be worth US$38 trillion to the USA.However, a 2021 review of national progress in the UK towards this goal revealed that policy makers have little confidence that healthy life expectancy can be markedly improved either through focusing on technological solutions (which promote independence but not health) or on diet and exercise regimes (which are often out of reach of those who would most benefit, because of socioeconomic inequalities).2 House of Lords Science and Technology Select CommitteeAgeing: Science, Technology, and Healthy Living. 3 López-Otín CBlasco MAPartridge LSerrano MKroemer G The hallmarks of aging. 4 Fraser HCKuan VJohnen Ret al. Biological mechanisms of aging predict age-related disease co-occurrence in patients. 5 Kulkarni ASGubbi SBarzilai N Benefits of metformin in attenuating the hallmarks of aging. , 6 Justice JNNambiar AMTchkonia Tet al. Senolytics in idiopathic pulmonary fibrosis: results from a first-in-human, open-label, pilot study. The same reviewrevealed a general lack of awareness and scrutiny of the potential offered for improving health in later life by ameliorating the ageing process itself. This unawareness is alarming when set against the backdrop of a quiet research revolution in biogerontology that has seen the identification of some key hallmarks of ageing,evidence that these hallmarks and underlying mechanisms independently predict the emergence of age-related diseases in real patients,and positive outcomes from preclinical and early-stage human clinical trials based on targeting ageing mechanisms.Humanity stands on the threshold of being able to prevent multimorbidity and age-associated diseases by addressing the underlying biology of ageing; parallels with the era of antibiotics do not seem unduly hyperbolic.7 WHODecade of healthy ageing: baseline report. 8 Cox LSBellantuono ILord JMet al. Tackling immunosenescence to improve COVID-19 outcomes and vaccine response in older adults. Ignorance of these developments carries appalling costs, which have been, and will continue to be, borne by older people unless the situation changes. For example, the UK's 2017 Healthy Ageing Grand Challenge specifically excluded biomedical science research from its funding remit, and the extensive Decade of Healthy Ageing: baseline report,by WHO (2021), fails to mention these advances, or even the word biology, focussing instead on socioeconomic determinants of health and overcoming ageism. Given that hallmark ageing mechanisms cause the progressive failure of innate and adaptive immunity, it is a fascinating counterfactual to consider how the ongoing COVID-19 pandemic would have been handled had it been preceded by a sustained international effort to target immune senescence and enhance later life immune function.The need to avoid such mistakes in the future is patent, as are the triple economic benefits (lower health and social care costs, greater economic activity of older adults, and investment into new industries) that will accompany efforts in this area.2 House of Lords Science and Technology Select CommitteeAgeing: Science, Technology, and Healthy Living. 9 Cox LS The economic and scientific case for therapeutic intervention in ageing. It is perhaps the perceived complexity of ageing biology that has dissuaded policy makers from directly tackling it. As a result, academic research on ageing in many parts of the globe is fragmented and siloed,and driven by funding models and professional metrics that reward specialisation and disease-specific focus. However, the biological mechanisms that cause ageing must be a central focus if integrational and intersectional approaches aimed at improving lifelong health figure ) are to be truly effective.Figure UK Research and Innovation-funded ageing networks addressing ageing processes across the life course Show full caption 8 Cox LSBellantuono ILord JMet al. Tackling immunosenescence to improve COVID-19 outcomes and vaccine response in older adults. , 9 Cox LS The economic and scientific case for therapeutic intervention in ageing. 4 Fraser HCKuan VJohnen Ret al. Biological mechanisms of aging predict age-related disease co-occurrence in patients. 11 networks (depicted in ovals) cover a broad but overlapping range of research topics, with intersections between socioeconomic determinants of health, diet, exercise, microbiome, extracellular matrix, and molecular and cellular mechanisms of ageing, through to cognition, immunity, frailty, therapeutics, and clinical trials. This breadth allows the networks to identify interventions appropriate for different stages of the life course, with a shift in focus from disease-preventive approaches in early life and mid-life through to more intense clinical monitoring and interventions in older adults, including development and testing of new geroprotective and gerotherapeutic medicines, such as senolytic drugs.A multiscale, multisystem, and whole of life course approach to improve healthy life expectancy can be achieved by drawing these broad research disciplines together through a core focus on hallmarks that underlie the biology of ageing and drive age-related disease.AGENTS=Ageing and Nutrient Sensing network. ART=Ageing Research Translation of Healthy Ageing Network. ATTAIN=Active and healthy ageing for all. BLAST=Building Links in Ageing Science and Translation. CARINA=CAtalyst Reducing ImmuNe Ageing. CELLO=CELLular metabolism Over a life-course in socioeconomically disadvantaged populations.CFIN=Cognitive Frailty Interdisciplinary Network. ECMage=Extracellular Matrix (ECM) ageing across the life course interdisciplinary research network. Food4Years=Food for-added-life-years: putting research into action. MyAGE=Muscle resilience across the life course: from cells to society. SMiHA=Skin Microbiome in Healthy Ageing.10 UKRIResearchers at 28 UK universities team up to tackle healthy ageing. It is therefore hugely exciting that the diversity and complexity of ageing across the life course has now been recognised in a national initiative by the Biotechnology and Biological Sciences Research Council and Medical Research Council of UK Research and Innovation (the national government-sponsored research funder). Launched in March, 2022,the UK Ageing Network comprises 11 new research networks addressing core aspects of ageing biology and health in a national macro network, drawing together ageing research across systems and scales ( figure ). These networks aim to enable knowledge exchange and research beyond the narrow focus of individual research projects and academic disciplines, drawing from the social sciences, humanities, economics, and biomedical and physical sciences, as well as bringing in expertise from industry, biotechnology, the third sector, and policy makers; membership is open and non-prescriptive, and interested individuals and organisations are encouraged to join . Importantly, the networks also include the voice of those with the lived experience of ageing, to enable relevant co-design of future research strategies, together with health practitioners and clinical trialists to ensure that promising interventions can be taken forward from the laboratory into patients. Through workshops, training programmes, grassroots events, and research pump priming, the networks will not only enable integration of expertise and cross-disciplinary working to generate new knowledge and provide solutions to ageing problems, they also aim to build ageing research capacity, enticing researchers from all career stages and disciplines in academia and beyond to embrace ageing research. Dissemination of new findings and development of new training materials for a range of audiences, including clinical practitioners and policy makers, is also key to the networks’ goals, to ensure that actionable findings from ageing research are rapidly adopted into policy and practice.Initially funded for 2 years, positive outcomes from the networks should drive investment to sustain and grow them, with expansion to include colleagues from around the world. We therefore welcome input, suggestions, and support from all who wish to contribute to improve health across the life course and work towards the goal of substantially extending healthy life expectancy.",Linking interdisciplinary and multiscale approaches to improve healthspanâa new UK model for collaborative research networks in ageing biology and clinical translation
524,-1,-1_new_said_study_people,https://www.thelancet.com/journals/lanchi/article/PIIS2352-4642(22)00254-1/fulltext,"BackgroundIn the Netherlands, treatment with puberty suppression is available to transgender adolescents younger than age 18 years. When gender dysphoria persists testosterone or oestradiol can be added as gender-affirming hormones in young people who go on to transition. We investigated the proportion of people who continued gender-affirming hormone treatment at follow-up after having started puberty suppression and gender-affirming hormone treatment in adolescence.MethodsIn this cohort study, we used data from the Amsterdam Cohort of Gender dysphoria (ACOG), which included people who visited the gender identity clinic of the Amsterdam UMC, location Vrije Universiteit Medisch Centrum, Netherlands, for gender dysphoria. People with disorders of sex development were not included in the ACOG. We included people who started medical treatment in adolescence with a gonadotropin-releasing hormone agonist (GnRHa) to suppress puberty before the age of 18 years and used GnRHa for a minimum duration of 3 months before addition of gender-affirming hormones. We linked this data to a nationwide prescription registry supplied by Statistics Netherlands (Centraal Bureau voor de Statistiek) to check for a prescription for gender-affirming hormones at follow-up. The main outcome of this study was a prescription for gender-affirming hormones at the end of data collection (Dec 31, 2018). Data were analysed using Cox regression to identify possible determinants associated with a higher risk of stopping gender-affirming hormone treatment.Findings720 people were included, of whom 220 (31%) were assigned male at birth and 500 (69%) were assigned female at birth. At the start of GnRHa treatment, the median age was 14·1 (IQR 13·0–16·3) years for people assigned male at birth and 16·0 (14·1–16·9) years for people assigned female at birth. Median age at end of data collection was 20·2 (17·9–24·8) years for people assigned male at birth and 19·2 (17·8–22·0) years for those assigned female at birth. 704 (98%) people who had started gender-affirming medical treatment in adolescence continued to use gender-affirming hormones at follow-up. Age at first visit, year of first visit, age and puberty stage at start of GnRHa treatment, age at start of gender-affirming hormone treatment, year of start of gender-affirming hormone treatment, and gonadectomy were not associated with discontinuing gender-affirming hormones.InterpretationMost participants who started gender-affirming hormones in adolescence continued this treatment into adulthood. The continuation of treatment is reassuring considering the worries that people who started treatment in adolescence might discontinue gender-affirming treatment.FundingNone.",Continuation of gender-affirming hormones in transgender people starting puberty suppression in adolescence: a cohort study in the Netherlands
523,-1,-1_new_said_study_people,https://www.theguardian.com/world/2022/oct/27/megalopolis-how-coastal-west-africa-will-shape-the-coming-century?CMP=longread_email,"It has long been said that no one knows with any certainty the population of Lagos, Nigeria. When I spent time there a decade ago, the United Nations conservatively put the number at 11.5 million, but other estimates ranged as high as 18 million. The one thing everyone agreed was that Lagos was growing very fast. The population was already 40 times bigger than it had been in 1960, when Nigeria gained independence. One local demographer told me that 5,000 people were migrating to Lagos every day, mostly from the Nigerian countryside. Since then, the city has continued to swell. By 2035, the UN projects that Lagos will be home to 24.5 million people.What is happening in Lagos is happening across the continent. Today, Africa has 1.4 billion people. By the middle of the century, experts such as Edward Paice, author of Youthquake: Why Africa’s Demography Should Matter to the World, believe that this number will have almost doubled. By the end of this century, the UN projects that Africa, which had less than one-tenth of the world’s population in 1950, will be home to 3.9 billion people, or 40% of humanity.These are staggering numbers, but they do not tell the full story. We need to zoom in closer. It is in cities where most of this astounding demographic growth will occur. Once we begin to think along these lines, what is at stake becomes even clearer. Much western commentary on Africa’s population growth has been alarmist and somewhat parochial, focusing on what this means for migration to Europe. The question of how African nations manage the fastest urbanisation in human history will certainly affect how many millions of its people seek to stay or leave. A recent continental survey by a South African foundation, for example, found that 73% of young Nigerians expressed an interest in emigrating within the next three years. But given its scale, this is a story with far larger implications than population movements alone, shaping everything from global economic prosperity to the future of the African nation state and the prospects for limiting climate crisis.There is one place above all that should been seen as the centre of this urban transformation. It is a stretch of coastal west Africa that begins in the west with Abidjan, the economic capital of Ivory Coast, and extends 600 miles east – passing through the countries of Ghana, Togo and Benin – before finally arriving at Lagos. Recently, this has come to be seen by many experts as the world’s most rapidly urbanising region, a “megalopolis” in the making – that is, a large and densely clustered group of metropolitan centres. When its population surpassed 10 million people in the 1950s, the New York metropolitan area became the anchor of one of the first urban zones to be described this way – a region of almost continuous dense habitation that stretches 400 miles from Washington DC to Boston. Other regions, such as Japan’s Tokyo-Osaka corridor, soon gained the same distinction, and were later joined by other gigantic clusters in India, China and Europe.But the Abidjan-Lagos stretch now stands to become the granddaddy of them all. In just over a decade from now, its major cities will contain 40 million people. Abidjan, with 8.3 million people, will be almost as large as New York City is today. The story of the region’s small cities is equally dramatic. They are either becoming major urban centres in their own right, or – as with places like Oyo in Nigeria, Takoradi in Ghana, and Bingerville in Ivory Coast – they are gradually being absorbed by bigger cities. Meanwhile, newborn cities are popping into existence in settings that were all but barren a generation ago. When one includes these sorts of places, the projected population for this coastal zone will reach 51 million people by 2035, roughly as many people as the north-eastern corridor of the US counted when it first came to be considered a megalopolis.But unlike that American super-region, whose population long ago plateaued, this part of west Africa will keep growing. By 2100, the Lagos-Abidjan stretch is projected to be the largest zone of continuous, dense habitation on earth, with something in the order of half a billion people.“I have worked in China and in India, and that is where most of the attention on cities has been until fairly recently, but Africa is unquestionably the continent that will drive the future of urbanisation. And it is that strip along the coast of west Africa where the biggest changes are coming,” said Daniel Hoornweg, a scholar of urbanisation at Ontario Tech University. “If it can develop efficiently, the region will become more than the sum of its parts – and the parts themselves are quite big. But if it develops badly, a tremendous amount of economic potential will be lost, and in the worst of cases, all hell could break loose.”The first time I travelled along this stretch of coast was in the late 1970s, on a long road trip to Nigeria from Ivory Coast, where my family then lived. My father, who ran a 20-country healthcare training programme for the World Health Organization, had a meeting to attend in Lagos, and he decided to invite my brothers and me along for the ride. At the time I was a first-year college student in the US, but it was the summer holidays, and I was excited to jump aboard a clattering old-school grey Land Rover for the trip.He followed a route traced out on a well-worn, foldable Michelin map. It did not take long to discover that many of the routes marked on the map in red – supposedly signifying national or international highways – were little more than two-lane asphalt roads, some of which had long ago been chewed to oblivion by heavy trucking traffic, or eroded by years of seasonal rains. The secondary or tertiary roads, traced more faintly in yellow and white, signalled far greater challenges: jarring dirt paths that were more like trails than highways. These would leave our bodies aching and caked with dust. For long stretches, the west African countryside was so empty that we had to carry our own fuel in jerry cans.One unfortunate artefact of this region’s imperial history is that, while the British and French built roads and railways to transport agricultural commodities and minerals from the hinterlands of their colonies to modern ports – where they could be shipped home for great profit – in their intense imperial rivalry, they did little to connect their respective possessions. But by 1992, when I took another long drive along this coast, a stretch of highway had been built on either side of the frontier between Ivory Coast and Ghana circumventing a coastal lagoon, and relegating the old picturesque but chancy border-crossing ferry to quaint memory. Back then, few could have imagined the full scope of the changes coming to this stretch of coast – though, in retrospect, some of the signs were already clear.Downtown Lagos, Nigeria. Photograph: peeterv/Getty Images/iStockphotoAs late as 1980, Lagos had still seemed like a series of modest towns barely stitched together by its highways and bridges. By the early 90s, though, it had exploded in size, and was already choking on itself. It had become notorious for some of the world’s worst traffic jams, known locally as go-slows. Abidjan, the region’s second-largest city, had also begun to morph. Its suburbs were expanding, thrusting toward the border with Ghana to the east. The other national and economic capitals of this region – Accra in Ghana, Lomé in Togo and Cotonou in Benin – were likewise starting to mushroom.But it was on more recent trips, in the 2010s, that I saw the urban revolution transforming west Africa coming into full view. By then, Ivory Coast had laid down a true highway all the way from Abidjan to its border with Ghana. Abidjan had gobbled up early colonial capitals like Bingerville and the postcard-pretty but long-stagnant beach town of Grand-Bassam, turning them into dormitory communities. The roadside scenery during a drive from border to border along the Ghanaian coast bore no resemblance to the lightly peopled landscapes of earlier decades. Towns and cities were strung together one after another along nearly the entire route. For long stretches, one scarcely ever left an urban environment.As always in this region, Lagos is where the most dramatic changes are visible. As it swells, the city is shooting thick urban tendrils west toward the border with Benin – the slender, francophone nation of 12 million people next door – rendering much of that country’s economy a satellite not so much of Nigeria, but of Lagos itself. (If Lagos state were an independent country, its economy would rank as the fourth-biggest in Africa.)Led by Lagos, as coastal west Africa’s urbanisation gathers pace, and populations and regional commerce begin to surge across old imperial borders, the lives of tens of millions of people along the coastal corridor are changing in ways that neither colonial designs nor six decades of independent government seem to have remotely anticipated.Earlier this year, I returned to the coast, this time not for one long road trip, but for a series of shorter forays by car in Ghana, Togo and Benin. Everywhere I went, the speed and scale of the historic transformations under way were evident. In Ghana, I visited a place I had encountered on previous trips, Takoradi, and its conjoined twin, the railroad town of Sekondi. In 1980, the two towns together counted 197,000 people. This year, their population surpassed a threshold that only 14 American cities have ever reached: 1 million people, a more than five-fold increase in little more than a generation.On the July morning I returned to Takoradi, it was the Islamic holiday of Eid al-Adha, or Tabaski, and the narrow downtown streets were packed with young celebrants from the local Muslim minority, all dressed colourfully in flowing laced robes. When the centre of Takoradi was built, more than a century ago, the city was Ghana’s lone port. It was here that Kwame Nkrumah famously returned by ship from England in 1947, emerging from obscurity to lead his country, then a British colony known as the Gold Coast, to independence 10 years later. In their fading pastel and dreary grey tones, the verandaed buildings of the old downtown looked like the set of a period drama. Just beyond here, though, the antiquated scene gave way to an enormous construction site, where a highway flyover was rising above dusty streets. Once complete, it will allow traffic to bypass the old, outgrown centre in favour of the much larger modern periphery, where most of the city’s people now live.Overpass construction in Sekondi-Takoradi. Photograph: Howard W FrenchAt Takoradi’s western edge, I stopped at a new shopping mall where, on the shelves of a busy supermarket, I found South African wines, Swiss chocolates, cellophane packs of the same brand of fresh blueberries I eat every day in New York, and – an even more surefire sign of disposable income – expensive canned dog food. There were also Portuguese and Chinese restaurants, a beauty salon, mobile phone shops and a bridal gown dealer doing brisk business.It is not immediately obvious where the income necessary to sustain this kind of commercial strip comes from. Some surely derives from work in the offshore oil business based nearby, some from a recently expanded regional port, some from a combination of old-line cocoa farming and new jobs in tech. And this points to the reality of what makes this megaregion so distinctive from earlier ones. Since at least the 18th century, as the writings of Hegel and Hume show, Africa has been widely regarded in the west as if it existed outside the flow of history – scarcely a participant in the global present, and even less relevant to the future. This has never been true, but those who cling on to such misapprehensions would do well to visit this stretch of coastline. In Lagos, Accra, Abidjan, or even in much smaller places like Takoradi, meanwhile, globalised enclaves with strong links to the rich world jostle with expanses of ragged urbanity, half hopefully striving, half congealed in poverty.On another morning, I drove from the heart of Ghana’s capital, Accra, to the city of Kasoa, less than 20 miles away. Kasoa is sometimes touted as one of the fastest-growing conurbations on the continent. When I made my first trips along this coast in the 70s, it was little more than a shambling collection of rural roadside traders’ stalls. In 1984, Kasoa had 3,000 people. Scarcely a decade ago, its population was just shy of 70,000. Now it is home to roughly half a million people, equal to Edinburgh or Tucson.The view from an overpass above Kasoa on the coastal highway is a reminder that cities throughout Africa have tended to sprawl outwards, rather than upward. There is little high-rise housing here, and few tall buildings of any kind. From up high, Kasoa has a rough-hewn, unfinished look. The newborn city lurches outward from the highway junction in all directions, its roads jammed with traffic. For many experts, this is a problematic feature of much of west Africa’s urbanisation: it is almost entirely unplanned.Kasoa’s streets are frenzied with jumbles of wooden stalls and incessant trading of all kinds. In the dusty byways beyond the highway, young people were everywhere: hawking sachets of cold water, running after cars to sell mobile phone credits and cheap plastic toys, crying out the prices of sweet, puffy bread or plantain chips from beneath beach umbrellas on street corners.Most noticeable of all were the schoolchildren walking the streets in their uniforms and backpacks. By 2050, about 40% of all the people under 18 in the world will be African, a proportion that will reach half by century’s end. On the streets of Kasoa, statistics like these come to life. Everywhere there were billboards for daycare centres, kindergartens and “international schools”. The only real competition for school ads came from church ads, which offer promises of success in this world as much as in the next.Most of the people who fill the streets of places like Kasoa are recent arrivals from the countryside, and live in ramshackle cinderblock dwellings. Julius Ackatiah, a 55-year-old, recently set himself up in business here after many years in Italy, where he had already realised the African dream of emigration, legally acquiring a new nationality in a rich European country. I met him as he peered out from the unfussy sidestreet storefront where he sells secondhand housewares he has shipped from Italy.Why had he chosen Kasoa, I asked him? Accra has recently become overbuilt and too expensive, Ackatiah said, but Kasoa was on the rise. “There are lots of people here, and they are trying to set up new homes for themselves and make new lives in this town. That makes for good business.” As Ackatiah spoke on the stairs of his shop, he was engulfed by his used-goods stock in trade: cheap plastic chairs, living room couches and tables, computer monitors and household appliances, small and large, from refrigerators and microwaves to laundry irons.One of the biggest challenges for Africa’s emerging megaregions remains its weak transport networks. In 2018, more than 40 nations agreed to create the African Continental Free Trade Area, an arrangement that economists say could boost African GDP by $450bn by 2035, mostly thanks to increased intra-African commerce. Since then, another 10 countries have joined, including Nigeria, making for a truly continent-wide agreement. “At its crux, outside the World Trade Organization, it is the biggest region of free trade in the world,” said Astrid Haas, a Ugandan independent economist based in Kampala. “What it is intended to do is unlock the benefits on the continental scale for African countries to be able to trade with each other; to eliminate both tariff and non-tariff barriers.”But realising its full potential will require much more intense cooperation between neighbours, and especially on improving physical infrastructure. Algiers and Cairo remain the only African cities with underground commuter lines. (In recent years, inspired citizen designers have carefully sketched out potential subway networks for cities such as Kigali and Port Harcourt, but these remain hopeful ideas for now.) Abidjan and Lagos are building surface light urban rail systems, but both are small-scale and behind schedule. Meanwhile, the lack of decent roads continues to hold this region back. The four-lane highway between Accra and Kasoa aside, almost the entire 600-mile stretch of coast consists of an undivided two-lane road that passes slowly through small towns and villages. Drivers sometimes find themselves having to dodge daring pedestrians and errant animals.Then there are the predatory police and soldiers who stop drivers in order to extort money under the pretext of traffic safety checks or the fight against crime. Last summer, on the outskirts of Takoradi, I was waved down by a portly, peanut-chewing police officer who asked, as if it was the most normal thing in the world: “What have you brought for me?” West African travellers face holdups like this, smiling or not, on a daily basis. On a trip in Ghana in the 90s, when I travelled 340 miles from the northern town of Bolgatanga to the central city of Kumasi, I counted 72 roadblocks. If anything, international borders in the region have long been even worse hotspots for this kind of predation.A development proposal in Accra, Ghana. Photograph: Howard W FrenchYet there is some reason for optimism. In May, the African Development Bank announced it had raised $15.6bn to fund the construction of a new coastal highway from Lagos to Abidjan. “We are talking about something like the road between Baltimore and New York – a toll road,” said Lydie Ehouman, a transportation economist at the bank, who told me the target for completion of the highway, which will be four to six lanes wide throughout, is 2026. “It will be free-flowing, with a chip in your licence plate so you don’t need to stop at toll gates. It will be a modern highway.” Economists at the African Development Bank argue that the West African Highway, as the new road will be called, will increase cross-border trade among the participating countries by 36%.“If people are confident in the availability of reliable, rapid transportation, other things will begin to change dramatically, too,” said Hoornweg, the Ontario Tech professor. “Property values will rise sharply along the major transportation axes, and that will encourage people to build upward, with highrises, rather than building out with more and more sprawl. The cities will also become much more efficient and environmentally friendly, and that makes their development more sustainable.”On the ground today, a vision like this isn’t so easily conjured. It’s true that in Lagos a collection of impressive modern highrises is slowly taking shape. And in downtown Accra, a dazzling new real estate scheme – high-end apartment towers, office buildings, fancy shopping plazas, luxury hotels – is planned for the waterfront. But such projects are catering to the needs of the already wealthy, and not to the growing millions of people in the region who will soon urgently need housing. Here, the contrast with China, where huge clusters of residential high-rises ring every large city, could not be more striking. Rather than avatars of the future, in fact, the easiest thing to conclude from projects like these is that the region’s governments are setting their sights far too low to address the sweeping demographic and social changes on their way. This may even be true about the coastal highway system.“The best thing that could happen to west Africa would be if someone could convince these countries to seriously consider the experience of Asia,” said Alain Bertaud, a senior fellow at the Marron Institute at New York University. As a former World Bank official who specialises in urbanisation, Bertaud advised China about developing one of the world’s most successful megaregions, in the Pearl River delta. “Density itself does not create prosperity,” Bertaud told me. “You will have to have lots more transportation, including new rail lines, new roads that link the coastal highway to the hinterlands and to small cities, where the cheaper land is.” He pointed out that this requires a lot of building across national borders, which is not easy in any part of the world. “In India, we have seen that even building a corridor that crosses several states within the same country is difficult. In Africa they will need much better coordination.”Haas, the Ugandan economist, agreed. “Africa faces a need for $20-25 billion annually in infrastructure investment, plus $20bn more each year for housing. Trying to convey the scale here is very hard. We are talking about ballooning numbers, and people need to be shocked into action.”Toward the end of my trip, I took a three-hour drive from Accra to the border with Togo. As we drove, Accra soon gave way to a grimy industrial zone that stretched for miles. From here all the way to the border, about 120 miles, the landscape was filled with peri-urban sprawl, its most distinctive feature being the ubiquitous roadside schools where children played sports or milled about.At the border, as soon as I climbed out of my car I was surrounded by touts eager to sell me taxi rides, exchange currency for me or help expedite my visa and vaccination checks. I proceeded alone, expecting complications, but was pleasantly surprised at how straightforward the procedures were on both sides of the border. My first question to the driver I hired on the Togolese side was how far it was to the capital, Lomé. He laughed. “You’re already in Lomé,” he said. “In 15 minutes, you will be at your hotel.”The next day, a Sunday, I drove 30 minutes east from Lomé to a small town with Royce Wells, a 30-year-old American IT professional who wanted to inspect the progress on a beachside house he is building. Togo is an unusually narrow country – wedged between Ghana and Benin, it runs about 430 miles north to south, but has only 31 miles of coastline. For this reason, local elites and foreign investors alike have long dreamed of building it into a kind of entrepot trading state that profits from various kinds of arbitrage, from sharp currency fluctuations in Nigeria and Ghana to varying levels of corruption and political risk among its neighbours.The Togo-Ghana border as seen from Lomé. Photograph: Joerg Boethling/AlamyTogo maintains a democratic facade through regular elections, but has been tightly controlled by one family since 1963. In contrast to Nigeria, though, the electricity works, the internet is fast, and everyday life is not plagued by insecurity. With its commercial future in mind, Togo has built a port with capacity much larger than its domestic needs, and also produces cement, steel and other industrial and consumer goods for its larger neighbours. On this basis, Wells sees the country as a good bet, and hopes to make money building hotels there. “The places that learn how to create the right tax incentives and legal protections [for investors] will basically be able to arbitrage on Lagos and its dysfunction,” he told me.Others are much more sceptical that this vision will ever be realised. After all, it relies on canny decision-making at the top of government. Bright Simons, a prominent Ghanaian political analyst and entrepreneur, called this five-nation megaregion “one of the most administratively broken landscapes on the planet”. Its governments are “unbelievably un-strategic”, he said. “I am always puzzled by the enthusiasm of elites for creating chambers of commerce with Mexico, or some other distant country, rather than with their own neighbours.”Here, the needs of west Africa’s booming population collide with the stubborn realities of the nation state, and specifically with contrasting colonial histories. Ivory Coast, Benin and Togo are former French colonies, and Nigeria and Ghana were colonised by Britain. This has left different official languages in place, whether English or French, as well as a currency in the French-speaking states, the CFA franc, that is a relic of colonisation – it was once tied to the French franc and is now attached to the euro. Perhaps the most important imperial legacy, though, is the insular national elites who, because of colonial history and the near-checkerboard way the countries alternate between English and French, pay scarce heed to each other. A Nigerian I met in Accra, for example, told me: “It wasn’t until I started spending time in Ghana recently that I realised Ghana isn’t our neighbour. Benin sits next to us, followed by Togo.”Cotonou, the economic capital of Benin (separate from but very close to its national capital, Porto-Novo), lies 20 miles from the border of Nigeria, and 76 miles from Lagos, but there is no immediate sense of the behemoth next door. The city of 700,000 (on its way to 5 million by 2100) clusters around a small and tidy administrative centre, complete with a modernist presidential palace built largely in glass, whose large size belies the diminutive nature of Benin itself, the corridor’s second-smallest state. With its low-rise buildings and heavy scooter traffic, much of Cotonou feels scarcely different from a big town or village. Whether Benin likes it or not, Lagos’s accelerating expansion seems destined to one day swamp this place.When I asked a longtime acquaintance, a successful businessman from Benin, whether people in his country, including its leaders, sustain close relations with Nigeria, the answer was no. “The elite here still flatters itself with talk about being the Latin Quarter of the region, due to our French chauvinism,” he said, referring to the pre-independence era when France made Benin a regional centre of colonial education. “Our leaders are very poor at thinking ahead … If you tell the president he has nice shoes, he’ll be swimming in happiness. With Nigeria next door, what we should have done long ago is make English a compulsory second language in school, but no one has ever thought of that.”This kind of pessimism, built upon a scornful assessment of governance at the national level in west Africa, is widespread. “We are going to need to have a functioning Ghanaian state, functioning states in Benin and Togo, and at least a minimally functional Nigerian government all at the same time in order to make this hugely urbanised future livable,” said E Gyimah-Boadi, the 70-year-old co-founder and former CEO of the Ghana Center for Democratic Development, a thinktank.“Part of me wants to believe that the youth of west Africa can be their own saviours, and that it is not because of the failures of my generation that they are necessarily doomed. The nation state has been a huge curse. It worked very well for some of us, but we have left very little behind for the young. Basically, we have cheated them.”Follow the Long Read on Twitter at @gdnlongread, listen to our podcasts here and sign up to the long read weekly email here.",Megalopolis: how coastal west Africa will shape the coming century
519,-1,-1_new_said_study_people,https://www.theguardian.com/society/2022/sep/07/woman-who-can-smell-parkinsons-helps-scientists-develop-test,"Scientists have harnessed the power of a woman’s hyper-sensitive sense of smell to develop a test to determine whether people have Parkinson’s disease.The test has been years in the making after academics realised that Joy Milne could smell the condition. The 72-year-old from Perth, Scotland, has a rare condition that gives her a heightened sense of smell.She noticed that her late husband, Les, developed a different odour when he was 33 – 12 years before he was diagnosed with the disease, which leads to parts of the brain become progressively damaged over many years.Milne, nicknamed “the woman who can smell Parkinson’s”, described a musky aroma, different from his normal scent.Her observation piqued the interest of scientists who decided to research what she could smell, and whether this could be harnessed to help identify people with the neurological condition.Years later, academics at the University of Manchester have made a breakthrough by developing a test that can identify people with Parkinson’s disease using a simple cotton bud run along the back of the neck.Researchers can examine the sample to identify molecules linked to the disease to help diagnose if someone has it.Though still in the early phases of research, scientists are excited about the prospect of the NHS being able to deploy a simple test for the disease.There is no definitive test for Parkinson’s and diagnosis is based on a patient’s symptoms and medical history.If the skin swab is successful outside laboratory conditions it could be rolled out to achieve faster diagnosis.Milne said it was not acceptable that people with Parkinson’s had such high degrees of neurological damage at the time of diagnosis, adding: “I think it has to be detected far earlier – the same as cancer and diabetes, earlier diagnosis means far more efficient treatment and a better lifestyle for people.“It has been found that exercise and change of diet can make a phenomenal difference.”She said her husband, a former doctor, was determined to find the right researcher to examine the link between odour and Parkinson’s and they sought out Dr Tilo Kunath at the University of Edinburgh in 2012.Kunath paired up with Prof Perdita Barran to examine Milne’s sense of smell.The scientists believed that the scent may be caused by a chemical change in skin oil, known as sebum, that is triggered by the disease.In their preliminary work they asked Milne to smell T-shirts worn by people who had Parkinson’s and those who did not. She correctly identified the T-shirts worn by Parkinson’s patients but also said that one from the group of people without Parkinson’s smelled like the disease – eight months later that individual was diagnosed with the disease.Researchers hoped the finding could lead to a test being developed to detect Parkinson’s, working under the assumption that if they were able to identify a unique chemical signature in the skin linked to the disease, they may eventually be able to diagnose it from simple skin swabs.In 2019, researchers at the University of Manchester, led by Barran, announced they had identified molecules linked to the disease found in skin swabs. The scientists have now developed a test using this information.The tests have been successfully conducted in research labs and scientists are assessing whether they can be used in hospital settings. If successful, the test could potentially be used in the NHS so GPs can refer patients for Parkinson’s tests.Sign up to First Edition Free daily newsletter Archie Bland and Nimo Omer take you through the top stories and what they mean, free every weekday morning Privacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy . We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply.The findings, which have been published in the Journal of the American Chemical Society, detail how sebum can be analysed with mass spectrometry – a method that weighs molecules – to identify the disease. Some molecules are present only in people who have Parkinson’s.Researchers compared swabs from 79 people with Parkinson’s with a healthy control group of 71 people.Barran said: “At the moment, there are no cures for Parkinson’s but a confirmatory diagnostic would allow them to get the right treatment and get the drugs that will help to alleviate their symptoms.“There would also be non-pharmaceutical interventions, including movement and also nutritional classes, which can really help. And I think most critically, it will allow them to have a confirmed diagnosis to actually know what’s wrong with them.”She added: “What we are now doing is seeing if (hospital laboratories) can do what we’ve done in a research lab in a hospital lab. Once that’s happened then we want to see if we can make this a confirmatory diagnostic that could be used along with the referral process from a GP to a consultant.“At the moment in Greater Manchester there are about 18,000 people waiting for a neurological consult and just to clear that list, without any new people joining it, will take up to two years. Of those 10-15% are suspect Parkinson’s.“Our test would be able to tell them whether they did or whether they didn’t (have Parkinson’s) and allow them to be referred to the right specialist. So at the moment, we’re talking about being able to refer people in a timely manner to the right specialism and that will be transformative.”Milne is working with scientists around the world to see if she can smell other diseases such as cancer and tuberculosis (TB).“I have to go shopping very early or very late because of people’s perfumes, I can’t go into the chemical aisle in the supermarket,” she said.“So yes, a curse sometimes but I have also been out to Tanzania and have done research on TB and research on cancer in the US – just preliminary work. So it is a curse and a benefit.”She said she can sometimes smell people who have Parkinson’s while in the supermarket or walking down the street but has been told by medical ethicists she cannot tell them.“Which GP would accept a man or a woman walking in saying ‘the woman who smells Parkinson’s has told me I have it’? Maybe in the future but not now.”",âWoman who can smell Parkinsonâsâ helps scientists develop test
517,-1,-1_new_said_study_people,https://www.theguardian.com/science/2022/oct/19/next-pandemic-may-come-from-melting-glaciers-new-data-shows,"The next pandemic may come not from bats or birds but from matter in melting ice, according to new data.Genetic analysis of soil and lake sediments from Lake Hazen, the largest high Arctic freshwater lake in the world, suggests the risk of viral spillover – where a virus infects a new host for the first time – may be higher close to melting glaciers.The findings imply that as global temperatures rise owing to climate change, it becomes more likely that viruses and bacteria locked up in glaciers and permafrost could reawaken and infect local wildlife, particularly as their range also shifts closer to the poles.For instance, in 2016 an outbreak of anthrax in northern Siberia that killed a child and infected at least seven other people was attributed to a heatwave that melted permafrost and exposed an infected reindeer carcass. Before this, the last outbreak in the region had been in 1941.To better understand the risk posed by frozen viruses, Dr Stéphane Aris-Brosou and his colleagues at the University of Ottawa in Canada collected soil and sediment samples from Lake Hazen, close to where small, medium and large amounts of meltwater from local glaciers flowed in.Next, they sequenced RNA and DNA in these samples to identify signatures closely matching those of known viruses, as well as potential animal, plant or fungal hosts, and ran an algorithm that assessed the chance of these viruses infecting unrelated groups of organisms.The research, published in Proceedings of the Royal Society B, suggested that the risk of viruses spilling over to new hosts was higher at locations close to where large amounts of glacial meltwater flowed in – a situation that becomes more likely as the climate warms.The team did not quantify how many of the viruses they identified were previously unknown – something they plan to do in the coming months – nor did they assess whether these viruses were capable of triggering an infection.However, other recent research has suggested that unknown viruses can, and do, loiter in glacier ice. For instance, last year, researchers at Ohio State University in the US announced they had found genetic material from 33 viruses – 28 of them novel – in ice samples taken from the Tibetan plateau in China. Based on their location, the viruses were estimated to be approximately 15,000 years old.In 2014, scientists at France’s National Centre for Scientific Research in Aix-Marseille managed to revive a giant virus they isolated from Siberian permafrost, making it infectious again for the first time in 30,000 years. The study’s author, Jean-Michel Claverie, told the BBC at the time that exposing such ice layers could be “a recipe for disaster”.Sign up to First Edition Free daily newsletter Archie Bland and Nimo Omer take you through the top stories and what they mean, free every weekday morning Privacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy . We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply.Even so, Aris-Brosou’s team cautioned that predicting a high risk of spillover was not the same as predicting actual spillovers or pandemics. “As long as viruses and their ‘bridge vectors’ are not simultaneously present in the environment, the likelihood of dramatic events probably remains low,” they wrote.On the other hand, climate change is predicted to alter the range of existing species, potentially bringing new hosts into contact with ancient viruses or bacteria.“The only take-home that we can confidently put forward is that as temperatures are rising, the risk of spillover in this particular environment is increasing,” said Aris-Brosou. “Will this lead to pandemics? We absolutely don’t know.”Also unclear is whether the potential for host switching identified in Lake Hazen is unique within lake sediments. “For all we know, it could be the same as the likelihood of host switching posed by viruses from the mud in your local pond,” said Arwyn Edwards, the director of the Interdisciplinary Centre for Environmental Microbiology at Aberystwyth University.However, “we do urgently need to explore the microbial worlds all over our planet to understand these risks in context,” he said. “Two things are very clear now. Firstly, that the Arctic is warming rapidly and the major risks to humanity are from its influence on our climate. Secondly, that diseases from elsewhere are finding their way into the vulnerable communities and ecosystems of the Arctic.”","Next pandemic may come from melting glaciers, new data shows"
516,-1,-1_new_said_study_people,https://www.theguardian.com/food/2022/sep/18/leading-the-whey-the-synthetic-milk-startups-shaking-up-the-dairy-industry,"In 1931, Winston Churchill predicted the rise of animal-free food. Then an opposition MP in his wilderness years, Churchill wrote an essay that imagined life in 50 years’ time. “Synthetic food will, of course, be used in the future,” he wrote.The artificial stuff would “be practically indistinguishable from the natural products, and any changes will be so gradual as to escape observation,” Churchill wrote. “Microbes, which at present convert the nitrogen of the air into proteins by which animals live, will be fostered and made to work under controlled conditions just as yeast is now.”Though several decades later than envisaged, Churchill’s prediction has been borne out by the development of lab-grown meat and, more recently, animal-free dairy products.Synthetic milk has emerged as a new potential alternative to cow’s milk, one that – unlike plant-based oat, nut and soy milks – purports to replicate its taste, appearance and mouthfeel. Described by experts as the future of milk, it has been touted as an environmentally friendly option that may shake up the dairy industry – and leave small-scale farmers in the lurch.“Lab-grown milk is considered the next food frontier,” says Dr Diana Bogueva, of Curtin University’s Sustainability Policy Institute, citing the growing popularity of dairy alternatives. Compared with dairy production, synthetic milk is likely to have a smaller carbon footprint and cause less pollution, and obviously eliminates animal welfare concerns, she says.The industry is expanding rapidly. In the US, cow-free dairy proteins produced by the firm Perfect Day are now widely used in products including ice-cream, cream cheese, chocolate and protein powders. Another American startup, New Culture, is commercialising a synthetic milk-based mozzarella, while the Israeli firm Remilk has set up a giant facility in Denmark to produce cheese, yoghurt and ice-cream.It will be some time before cow-free milk arrives in Australian supermarkets, but startups such as All G Foods and the CSIRO spin-off company Eden Brew are racing to bring products to market within the next two years.Yeast of EdenChemically, milk is mostly water – about 87%, to be precise. Milk solids comprise the rest: fats, proteins, sugars – primarily lactose – and minerals. By Australian law, at least 3.2% of the liquid in full-cream milk must be fat and another 3% protein.Most synthetic dairy companies are focusing on producing milk proteins using a process known as precision fermentation. It involves genetically programming yeast or other microorganisms using synthetic DNA to produce a specific protein. Jim Fader, the co-founder of Eden Brew, compares the process to beer brewing.“We use yeast to make a protein to make a drink. They use yeast to make alcohol to make a drink,” he says.There are at least 20 proteins in cow’s milk, about 80% of which are casein proteins, found in the curds; the rest are whey proteins, perhaps most familiar as a component of powdered protein shakes.Aggregates of casein, known as micelles, give milk its characteristic appearance and heat stability.“The micelle plays a fundamental role in many parts of milk,” Fader says. “For example, when it binds with calcium, it makes the milk look white. If you want to froth your milk and put it in your cappuccino, the ability for the milk to withstand that heat and the bubbles to be able to form … is also down to the micelle.”Synthetic milk technology is ‘less about trying to displace cows everywhere from dairy’ but ‘augmenting supply’, Jim Fader says. Photograph: ReutersEden Brew is producing six proteins that are most abundant in milk. Once brewed, these will be purified and dried.A key investor in the firm is the New South Wales dairy cooperative Norco, which will be responsible for rehydrating and blending the proteins. At this stage, other components such as minerals and coconut-based fat will be added. The end product will be lactose-free, with a small quantity of table sugar used to approximate the sweetness of cow’s milk.Fader says the firm will launch an ice-cream – simpler than milk because it can be made with just two proteins – around December next year. Milk will follow, likely in August 2024.All G Foods is focusing its efforts on whey proteins. The firm’s plant-based meat products are already served in commercial burger chains and sold in some supermarkets.The company’s chief scientific officer, Jared Raynes, says the ultimate goal is to produce yoghurts, cheese and fresh milk. But the firm’s focus for now is on beta-lactoglobulin, the main protein in whey.“We are going to be applying for regulatory approval with our protein powder,” he says.Parallels with synthetic fabricsMilena Bojovic, who is completing a PhD at Macquarie University, says while the promise of cow-free fresh milk has been widely trumpeted, the impact of synthetic dairy is likely to be greater on products such as milk powder.“Fresh milk consumption is on the decline,” she says and consumers may be wary of drinking a synthetic version of a natural product. She points out that traditional dairy production is also “very much technologically mitigated, from conception to the birth of calves to the milking process”.“If synthetic milk really takes off, the biggest disruption I think will be if it can be powdered and used in the ingredients space … as an additive like milk solids, which is in a lot of processed foods,” Bojovic says. “I don’t think most consumers are questioning where the milk solids in their KitKat came from.”“If and possibly when that happens, that will be one of the major disruptions for dairy industries that are producing exclusively for export in the form of powdered milk.”Bojovic, who has analysed global dairy trends as part of her research, is concerned that technological advances may leave farmers behind. Large dairy players such as Norco and Fonterra, a New Zealand multinational cooperative, have begun to invest in synthetic protein production.“Small-scale operations are really going to struggle in the context of global dairy consolidation,” she says. “There’s more pressure on farmers to innovate and also to invest in technology to make sure that they’re at the standard that bigger corporates are at.”Bojovic sees parallels with the rise of synthetic fabrics. “When synthetic fibres hit the market, that decimated the wool industry in a lot of regions,” she says. “It’s not the first time that farmers have been faced with the threat of synthetics, but they’ve adapted, they’ve innovated.”Melissa Cameron, Dairy Australia’s human health and nutrition policy manager, says it is yet to be seen how consumers will respond to a synthetic product. She points to statistics suggesting that 58% of Australian households exclusively buy fresh and long-life cow’s milk.“People are not abandoning dairy,” she says. “The commercialisation of synthetic proteins and products to a scale that makes these products available widely to consumers is yet quite a while off. As our populations grow around the world, synthetic products will deliver complementary protein and products. There will be room for all.”Dairy demand worldwide grew by 36% between 2007 and 2017 and is expected to keep rising as the world’s population increases and per-capita consumption grows.Fader says the technology that companies such as Eden Brew are developing is “less about trying to displace cows everywhere from dairy”. Rather, “it’s about augmenting the current supply because demand is forecast to go up by so much”.",Leading the whey: the synthetic milk startups shaking up the dairy industry
546,-1,-1_new_said_study_people,https://www.tue.nl/en/news-and-events/news-overview/27-09-2022-four-terminal-perovskite-silicon-pv-tandem-devices-hit-30-efficiency/,"Additionally, achieving high-power density will create more opportunities to integrate these solar cells into construction and building elements, so that more existing surface area can be covered with PV modules. Breaking the 30% barrier is therefore a big step in accelerating the energy transition and improving energy security by reducing the dependency on fossil fuels.The best of both worldsTandem devices can reach higher efficiencies than single junction solar cells because of a better utilization of the solar spectrum. The currently emerging tandems combine commercial silicon technology for the bottom device with perovskite technology: the latter featuring highly efficient conversion of ultraviolet and visible light and excellent transparency to near infrared light. In four-terminal (4T) tandem devices the top and bottom cells operate independently of each other which makes it possible to apply different bottom cells in this kind of devices. Commercial PERC technology as well as premium technologies like heterojunction or TOPCon or thin-film technology such as CIGS can be implemented in a 4T tandem device with hardly any modifications to the solar cells. Furthermore, the four-terminal architecture makes it straight forward to implement bifacial tandems to further boost the energy yield.Researchers from the Netherlands and Belgium have successfully improved the efficiency of the semi-transparent perovskite cells up to 19.7% with an area of 3x3 mm2 as certified by ESTI (Italy). “This type of solar cell features a highly transparent back contact that allows over 93% of the near infrared light to reach the bottom device. This performance was achieved by optimizing all layers of the semi-transparent perovskite solar cells using advanced optical and electrical simulations as a guide for the experimental work in the lab.” says Dr Mehrdad Najafi of TNO. “The silicon device is a 20x20-mm2 wide, heterojunction solar cell featuring optimized surface passivation, transparent conductive oxides and Cu-plated front contacts for state-of-the-art carrier extraction” says Yifeng Zhao, PhD student at TU Delft, whose results have been recently peer-reviewed. The silicon device optically stacked under the perovskite contributes with 10.4% efficiency points to the total solar energy conversion.Combined, 30.1% is the conversion efficiency of this non-area matched 4T tandem devices operating independently. This world’s best efficiency is measured according to generally accepted procedures.",Four terminal perovskite-silicon PV tandem devices hit 30% efficiency
514,-1,-1_new_said_study_people,https://www.theguardian.com/environment/2022/oct/07/forever-chemicals-found-insecticides-study,"Toxic PFAS chemicals have been detected in seven out of 10 insecticides tested in the US, according to new research. Six contained what the study’s lead author characterized as “screamingly high” levels of PFOS, one of the most dangerous PFAS compounds.The Environmental Protection Agency (EPA) has known about the findings for more than 18 months but appears to have not yet investigated the products or taken any action against the manufacturer.PFAS, also known as forever chemicals, can be taken up by crops. Such high levels in pesticides create a health risk if spread on fields where food is grown, public health advocates say.“We know PFOS is a carcinogen, we know it’s a deadly chemical and there’s no safe level in drinking water,” said Kyla Bennett, a former EPA official and science policy director with the non-profit Public Employees for Environmental Responsibility (PEER), which issued a press release on the study. “Our soil and water are now contaminated.”In a statement, the EPA told the Guardian it’s reviewing active ingredients used in pesticides – those which kill pests – to determine if any are PFAS. However, PFOS could be an inert ingredient.Per-and polyfluoroalkyl substances, or PFAS, are a class of about 12,000 chemicals typically used to make thousands of products water-, stain- and heat-resistant. They do not naturally break down and accumulate in humans and the environment. A growing body of evidence links them to serious health problems such as cancer, birth defects, liver disease, kidney disease, autoimmune disorders, high cholesterol and decreased immunity.Researchers from Texas Tech University checked 10 insecticides that were being used on cotton, but can also be used on food and other crops. The peer-reviewed study, published in Journal of Hazardous Materials Letters, found PFAS in seven of these “widely used” insecticides, said environmental toxicologist and lead author Steve Lasee, who was at Texas Tech University at the time of the study. He is now an independent consultant with Lasee Research and Consulting and a research fellow for the EPA.Testing revealed PFOS at a level as high as 19m parts per trillion (ppt) in one insecticide. The EPA hasn’t set limits for PFAS in pesticides, but in June it lowered its advisory health limit in drinking water to 0.02 ppt, a level so low as to suggest no amount of exposure to the compound is safe.Lasee said he presented his findings in March 2021 to staff members at the EPA’s Office of Research and Development and at a conference attended by environmental science professionals and EPA staff. He said he received an email from leadership in one of EPA’s divisions asking him to present his study to more EPA staff, but never heard anything beyond that.Lasee said he named the insecticide’s active ingredients but he never received requests for the brand names, meaning the EPA could not know which companies had sold tainted products.The EPA did not respond to direct questions about the study’s findings or about Lasee’s presentation to the agency.Lasee said the Massachusetts department of environmental protection (DEP) contacted him after his presentation to say it was interested in learning more about the research. A DEP spokesperson told the Guardian that the agency had been testing some pesticides for PFAS and had “discontinued use” of those that contain the chemicals. The agency is reviewing the information in the Texas Tech study and determining what’s next, the spokesperson said.It’s unclear what purpose PFAS in insecticides may serve, but Lasee said they could be used as a dispersing agent, to help the pesticide spread evenly.The study was published amid increased scrutiny of PFAS in pesticides because of the potential for widespread food and water contamination. Multiple studies have established that crops uptake PFAS and can be ingested by humans. The Food and Drug Administration (FDA) began monitoring PFAS in food in 2019 and has detected them in fruits and vegetables, but has not set any limits.The EPA earlier this year found PFAS added to plastic barrels and containers used to store pesticides can leach into the products. An EPA spokesperson said the agency alerted companies that they may be in violation of the law.However, Lasee said the type of PFAS compounds he found are different from those that leached from plastic containers, and the level of PFAS the Texas Tech study is several orders of magnitude higher, suggesting that the chemicals are from a different source.In September, the EPA proposed banning some PFAS that can be used as inert ingredients that were approved for use in pesticide products, but it said active ingredients are being reviewed. “EPA will share results of that investigation as soon as possible”, an agency spokesperson said.The agency also updated a webpage with information about PFAS in pesticides in September that claims PFOS is not used in the products. “The EPA Office of Pesticide Programs previously determined that there were no pesticide active or inert ingredients with structures similar to prominent PFAS such as PFOS,” it reads.That could be contradicted by Lasee’s research. The reason for the presence of PFOS in the insecticides is unclear. It could be the result of chemical companies illegally adding the compound, Bennett said. It could also be that a different PFAS compound is added to the fertilizer, then breaks down into PFOS after the pesticide is manufactured. The EPA did not reply to specific questions about the statement on its site.Bennett said there was little consumers can do to immediately protect themselves beyond eating organic food, but she noted that many people don’t have access to or can afford organic products.That leaves it up to the EPA to take swifter and more forceful action to get PFAS out of pesticides, Bennett added.“We have to get the EPA to stop allowing PFAS in pesticides,” she said. “We’ve got a toxic chemical in them that doesn’t need to be there, and pesticides are bad enough on their own without adding another carcinogen.”","Toxic âforever chemicalsâ detected in commonly used insecticides in US, study finds"
547,-1,-1_new_said_study_people,https://www.usatoday.com/story/money/2022/10/20/remote-workers-return-to-office-job-market-cools/10527961002/,"A cooling job market is leading to more than a slowdown in hiring, a pickup in layoffs and growing recession fears.It appears to be the one force capable of prodding America’s workers out of their homes and back to offices.The slowing labor market is starting to shift some bargaining power from employees to employers, allowing a growing number of companies to require workers to return to the office at least a few days a week, staffing officials and consultants say. Many businesses are still struggling to find workers and so the change is in its early stages, but it’s expected to accelerate as hiring pulls back further and layoffs spread in the months ahead, experts say.“Companies are a little less concerned that they’re not going to fill jobs if they lose people because of return-to-work policies,” says Jim McCoy, senior vice president of talent solutions for ManpowerGroup, a leading staffing firm. “There's starting to be less competition for talent, and employers can be a little more selective.”Price check:What will and won't become more affordable as the Fed hikes rates?Rising rates and your debt:Here's how the latest hike will hit your wallet and portfolioIn late September, 36% of organizations required workers to be in the office at least three days a week, up from 25% in August, according to a Gartner survey of 240 human resources leaders. And just 22% had no onsite work requirements, down from 31%.Employees 'more receptive' to working in officeWorkers are similarly becoming less demanding about remote work, if just marginally. Seventy-three percent of fully remote employees said they probably would find another remote or hybrid job if their company forced them to work from the office full-time, according to a Harris Poll survey last weekend for USA TODAY. That’s down from 78% in June.Ironside Human Resources, a Dallas-based health care staffing firm, has told its employees to work in the office five days a week since spring 2020, chiefly because of the collaboration that takes place when staffers are together, says CEO Doug Carter. But many job candidates insisted on working remotely. About six months ago, the company had to conduct 40 to 50 phone interviews to find one or two applicants willing to work on-site and interview in person, he says.Are I bonds still a good investment?:What to know if you're consider this inflation-flighting asset'I quit"" becomes harder:Workers could lose leverage as job openings fall.Now, he says, Ironside does about 25 phone interviews to find eight candidates who are open to on-site work. While the easing pandemic may be partly responsible, Carter mostly cites the slowing job market.“There are not as many remote jobs as there were. People are definitely more receptive"" to working on-site, he says. ""The challenge of getting people to work in-office had been difficult, but that landscape is rapidly changing.”Carter says he has seen a similar turnabout among his clients, such as hospitals. About 50% of their administrative employees are remote, he says, compared with 80% six months ago.Labor market cools down; job openings fallEarlier this year, with job openings at a record 11.9 million, workers enjoyed the leverage to demand higher wages and even more significantly, a continuation of the remote work set-ups that prevailed early in the pandemic. Even as companies such as Apple and General Motors announced return-to-office mandates, many had to walk them back after employee backlash.But the number of U.S. job openings fell from 11.2 million in July to a still historically high 10.1 million the following month. In September, net job growth slowed to 263,000, a solid total but the lowest since April 2021.In recent months, companies such as Intel, Oracle, Amazon, Apple, Netflix, and The Gap have cut hundreds or thousands of jobs.The job market, at least for now, remains sturdy, and employees in many industries still hold the cards because of pandemic-related worker shortages.“It’s still a very hot labor market,” and many job candidates still insist on working remotely, says Michael Steinitz, senior executive director of professional talent solutions for Robert Half staffing.Guide to big Social Security increase:Social Security COLA 2023: What retirees must knowBut “the tide is changing,” says Dustin York, a consultant for large corporations and associate professor of strategic communication and leadership at Maryville University in St. Louis.A softening economy and less robust hiring are beginning to alter the power balance between employers and workers. Soaring inflation is prompting the Federal Reserve to sharply raise interest rates, and higher prices and borrowing costs have clobbered the stock market. The troublesome dynamic is expected to trigger a recession by next year, according to the forecasts of economists surveyed by Wolters Kluwer Blue Chip Economic Indicators.'Barely making it':Americans face tough choices as prices soarBuy now, pay later takes a toll:Delinquencies could get 'dangerously' high.What will companies do about it?Am I more likely to get laid off if I resist a return to office?The slowdown initially hit technology, York says. The industry exploded as Americans worked from home and snapped up TVs, computers and appliances, but it’s downsizing as consumers shift their spending from goods to services, he says. Tech companies, in turn, increasingly are making workers return to the office, he says, and any layoffs are more likely to target employees who have resisted the directive.“Whom do you choose to lay off?” he says. “People who don’t match with the culture.”Many tech companies, in fact, are using return-to-office mandates instead of layoffs to get rid of workers, York and McCoy say. Employees who refuse to comply quit, which allows a company to trim its staff without the stain of job cuts.“The company can say ‘We didn’t lay anybody off’” and save the cost of severance payments, York says.As the job market downshifts in the next six to 12 months, York expects the back-to-office trend to ripple across all industries. Ultimately, he predicts, there will be slightly more fully remote workers than before the pandemic.Today, 25% of all workers are remote, 23% are hybrid and 52% are at the workplace full-time, according to a Harris Poll.Yet while more large companies are making employees return to the workplace, many of their smaller competitors are maintaining remote-work policies and using them as a competitive advantage to scoop up the skilled workers larger firms jettison, York says.Pulsar Products, which makes stationery and back-to-school items, asks workers who live near its headquarters in Cleveland to come in two days a week, CEO Eric Ludwig says. But some new hires can work fully remotely in other states.“The ability to be flexible and everyone’s knowledge of how to use (Zoom or Teams) has allowed us to hire the best,” he says.",Could refusing to return to office mean a layoff? Job market's shifting tide may change the rules.
549,-1,-1_new_said_study_people,https://www.vice.com/en/article/akek7g/this-startup-is-selling-tech-to-make-call-center-workers-sound-like-white-americans,"On the Clock On the Clock is Motherboard's reporting on the organized labor movement, gig work, automation, and the future of work. See More →Continuing Silicon Valley’s long and storied history of misreading dystopian satires as instruction manuals, a startup has created a tech product that makes call center workers' voices sound white.The startup, called Sanas and founded by three former Stanford students, was first reported on by Joshua Bote for SFGate on Monday. On Sanas' website, you can “hear the magic”: a simulated conversation between a call center worker with an Indian accent that can be modified with a slider that applies Sanas’s accent translation. After Sanas is applied, the voice sounds more like a text-to-voice reader than another human being, but it does sound typically American and white.AdvertisementIn a demonstration for Motherboard, company President Massih Sarimand and COO Sharath Keysheva Narayana called an employee in India who talked about his background and work. Then the Sanasa filter was applied. It removed the employee’s accent and created a passable white and American-sounding voice, albeit a bit robotic.Sanas describes its approach as ""accent matching,"" and advertises on its website that it can ""improve understanding by 31% and customer satisfaction by 21%."" Apparently, the software can offer multiple accents at the touch of a button—although its demo only features an Indian accent being turned into typically white and American—and the company frames its technology as ""empowering"" workers. According to materials offered by the company to SFGate, the company claims to have garnered about $132 million worth of funding thus far.Ironically, given its focus on empowerment, Sanas' software to turn call center workers' voices into white American voices mirrors the plot of Boots Riley's 2018 dystopian satire Sorry to Bother You. In the film, the ability to put on a ""white"" voice on the phone allows the film's Black protagonist to rise up in the company, but introduces tension in the workplace that undercuts a union drive and eventually pits him against his former co-workers.In an interview with Motherboard, Sarim and Narayana sketched out their business strategy and explained why call centers were their first choice as clients.Advertisement""Call centers have a very specific speech pattern. You don't have people laughing, crying, singing—those nuances of speech are not there so it is easier to build it. The next use case is on enterprise communications. As we started going live with enterprise call centers, they said 'Hey we have teams in Asia, we have teams in Africa,'"" Naryana told Motherboard. ""We want to leverage a tool like this so that we can give them a choice and we want everybody to be heard. We want to build a very inclusive work culture and we think this could be an extremely great product and technology to actually bring people closer.""Call centers are heavily surveilled workplaces—dominated by “employee monitoring” which some are eager to argue is somehow beneficial for the workers. These workers suffer the brunt of a customer’s anger when something goes wrong but lack the autonomy to go beyond a script or narrow guidelines laid out by management. Viewed as disposable, closely surveilled, experiencing little if any autonomy, and forced to deal with angry or racist customers, call center workers typically burn out in a few months—when they don’t, their mental health suffers. The core question here, then, is what deploying Sanas will actually do to working conditions for call center workers.AdvertisementIt’s not hard to imagine scenarios where the introduction of Sanas results in companies demanding more of their workers because they now have “accent matching"" that is supposed to increase their performance with customers—a typical outcome when workplaces with minimal autonomy implement performance-boosting software. Narayana said increased performance would be a side-effect of Sanas' software, but it mainly has the potential to improve every aspect of this industry: call center labor conditions, the mental health of these workers, and the experience of customers on the phone.""I don't care about metrics, I think about mental health, employee satisfaction, retention, and overall employee happiness. All of those metrics are checked out for me,"" Narayana told Motherboard. ""I strongly believe once you keep your employees happy, all the business metrics will get better. So that's a secondary result of what we're trying to achieve. But the primary result for me is actually improving the lives of all these agents.""Sanas’s product doesn’t address the structural issues with call center work nor racism from callers, which its product implicitly side steps. Sanas acknowledges the potential for misuse of its software and says nobody will be ""forced"" to use it because workers themselves activate it with a button—however, this doesn't acknowledge the possibility of being forced to use it by default due to performance quotas. Sanas also says it has a ""code of ethics"" with three values: individual choice (it's activated by the worker), personal control (effectively the same point), and flexibility (Sanas offers multiple accents).The desire to communicate clearly and seamlessly with one another is understandable—as Sarim and Narayana reiterated to Motherboard multiple times, and as the website says, 80 percent of this company’s workers were immigrants. Sarim and Narayana have both worked call center jobs where they dealt with racism. The two insist this informs their end goal: not to simply have an accent translation engine that turns anything into white, American English (“many-to-one”) but one day to develop a translator for any accent to any accent (“many-to-many”).",This Startup Is Selling Tech to Make Call Center Workers Sound Like White Americans
583,-1,-1_new_said_study_people,https://cleantechnica.com/2022/10/29/researchers-discover-substitutes-for-rare-earth-materials-in-magnets/,"Researchers at the University of Cambridge, in collaboration with colleagues in Austria, report that tetrataenite, a “cosmic magnet” that takes millions of years to develop naturally in meteorites, can potentially be used instead of rare earth materials in magnets.Researchers at the University of Cambridge, in collaboration with colleagues in Austria, report that tetrataenite, a “cosmic magnet” that takes millions of years to develop naturally in meteorites, can potentially be used instead of rare earth materials in magnets.Previously, attempts to make tetrataenite in the laboratory have depended on extreme and impractical methods, but the researchers say they have found a way to bypass those prior techniques by using phosphorus. In a research paper published in the journalPreviously, attempts to make tetrataenite in the laboratory have depended on extreme and impractical methods, but the researchers say they have found a way to bypass those prior techniques by using phosphorus. In a research paper published in the journal“Rare earth” is a misleading term that is sort of an inside joke among organic chemistry aficionados. It refers to a group of elements on the periodic table. “Noble gases” is another term that has little meaning except to organic chemists. In truth, “rare earth” elements aren’t all that rare in the grand scheme of things, but extracting them and purifying them is a challenge.“Rare earth” is a misleading term that is sort of an inside joke among organic chemistry aficionados. It refers to a group of elements on the periodic table. “Noble gases” is another term that has little meaning except to organic chemists. In truth, “rare earth” elements aren’t all that rare in the grand scheme of things, but extracting them and purifying them is a challenge.Rare Earth Materials And Permanent MagnetsRare Earth Materials And Permanent MagnetsThe real reason why this news is important is that rare earth materials are critical to making the permanent magnets that are an essential component of the electric motors that the transition to an emissions free economy depends upon.The real reason why this news is important is that rare earth materials are critical to making the permanent magnets that are an essential component of the electric motors that the transition to an emissions free economy depends upon.The sticking point is that China, with it predilection for dominating so many of the manufacturing processes for making electric vehicles, solar panels, and other critical technologies needed to address an overheating planet, controls over 80% of the world market for rare earth elements.The sticking point is that China, with it predilection for dominating so many of the manufacturing processes for making electric vehicles, solar panels, and other critical technologies needed to address an overheating planet, controls over 80% of the world market for rare earth elements.We know the danger of allowing tyrants in Saudi Arabia and Russia to control our access to fossil fuels. That experience suggests letting China be the gatekeeper for the new technologies we need to transfer away from relying on fossil fuels may be similarly fraught with danger in the future.We know the danger of allowing tyrants in Saudi Arabia and Russia to control our access to fossil fuels. That experience suggests letting China be the gatekeeper for the new technologies we need to transfer away from relying on fossil fuels may be similarly fraught with danger in the future.Professor Lindsay Greer of the materials science and metallurgy department at Cambridge University tellsProfessor Lindsay Greer of the materials science and metallurgy department at Cambridge University tellsOne of the most promising alternatives for permanent magnets is tetrataenite, an iron-nickel alloy with an ordered atomic structure. The material forms over millions of years as a meteorite slowly cools. This offers the iron and nickel atoms enough time to order themselves into a particular stacking sequence within the crystalline structure, resulting in a material with magnetic properties similar to those of rare earth magnets.One of the most promising alternatives for permanent magnets is tetrataenite, an iron-nickel alloy with an ordered atomic structure. The material forms over millions of years as a meteorite slowly cools. This offers the iron and nickel atoms enough time to order themselves into a particular stacking sequence within the crystalline structure, resulting in a material with magnetic properties similar to those of rare earth magnets.In the 1960s, tetrataenite was artificially formed by blasting iron-nickel alloys with neutrons, which allowed the atoms to form the desired ordered stacking. However, this technique is unsuitable for mass production. “Since then, scientists have been fascinated with getting that ordered structure, but it’s always felt like something that was very far away,” Greer says.In the 1960s, tetrataenite was artificially formed by blasting iron-nickel alloys with neutrons, which allowed the atoms to form the desired ordered stacking. However, this technique is unsuitable for mass production. “Since then, scientists have been fascinated with getting that ordered structure, but it’s always felt like something that was very far away,” Greer says.Over the years, many scientists have attempted to make tetrataenite on an industrial scale, but the results have been disappointing. Now Greer and his colleagues from the Austrian Academy of Sciences, and the Montanuniversität in Leoben, have found a potential alternative that avoids these extreme methods.Over the years, many scientists have attempted to make tetrataenite on an industrial scale, but the results have been disappointing. Now Greer and his colleagues from the Austrian Academy of Sciences, and the Montanuniversität in Leoben, have found a potential alternative that avoids these extreme methods.Taking A Closer LookTaking A Closer LookThe team studied the mechanical properties of iron-nickel alloys containing small amounts of phosphorus, which is present in meteorites. Inside these materials were a pattern of phases that indicated the expected tree-like growth structure called dendrites.The team studied the mechanical properties of iron-nickel alloys containing small amounts of phosphorus, which is present in meteorites. Inside these materials were a pattern of phases that indicated the expected tree-like growth structure called dendrites.“For most people, it would have ended there: nothing interesting to see in the dendrites, but when I looked closer, I saw an interesting diffraction pattern indicating an ordered atomic structure,” said first author Dr Yurii Ivanov, who completed the work while at Cambridge and is now based at the Italian Institute of Technology in Genoa.“For most people, it would have ended there: nothing interesting to see in the dendrites, but when I looked closer, I saw an interesting diffraction pattern indicating an ordered atomic structure,” said first author Dr Yurii Ivanov, who completed the work while at Cambridge and is now based at the Italian Institute of Technology in Genoa.Initially, the diffraction pattern of tetrataenite looks like the structure expected for iron-nickel alloys, namely a disordered crystal not of interest as a high-performance magnet. But when Ivanov looked closer, he identified the tetrataenite.Initially, the diffraction pattern of tetrataenite looks like the structure expected for iron-nickel alloys, namely a disordered crystal not of interest as a high-performance magnet. But when Ivanov looked closer, he identified the tetrataenite.According to the team, phosphorus allows the iron and nickel atoms to move faster, enabling them to form the necessary ordered stacking without waiting for millions of years. They were able to accelerate tetrataenite formation by between 11 and 15 orders of magnitude by mixing iron, nickel, and phosphorus in the right quantities. This meant the material was able to form over a few seconds in a simple casting.According to the team, phosphorus allows the iron and nickel atoms to move faster, enabling them to form the necessary ordered stacking without waiting for millions of years. They were able to accelerate tetrataenite formation by between 11 and 15 orders of magnitude by mixing iron, nickel, and phosphorus in the right quantities. This meant the material was able to form over a few seconds in a simple casting.“What was so astonishing was that no special treatment was needed. We just melted the alloy, poured it into a mold, and we had tetrataenite,” says Greer. “The previous view in the field was that you couldn’t get tetrataenite unless you did something extreme, because otherwise, you’d have to wait millions of years for it to form. This result represents a total change in how we think about this material.”“What was so astonishing was that no special treatment was needed. We just melted the alloy, poured it into a mold, and we had tetrataenite,” says Greer. “The previous view in the field was that you couldn’t get tetrataenite unless you did something extreme, because otherwise, you’d have to wait millions of years for it to form. This result represents a total change in how we think about this material.”Although the research is promising, more work is needed to decide whether it will be suitable for high performance magnets. The team is hoping to collaborate with major magnet manufacturers to determine this.Although the research is promising, more work is needed to decide whether it will be suitable for high performance magnets. The team is hoping to collaborate with major magnet manufacturers to determine this.The TakeawayThe TakeawayWhy do we write about topics that are not yet out of the laboratory stage? Because theWhy do we write about topics that are not yet out of the laboratory stage? Because theNew types of batteries that are lighter, more powerful, faster charging, less expensive, and kinder to the environment are being researched in hundreds of laboratories all around the world as you read this. We don’t know where the breakthroughs will occur but we know they will come, just as those first crude internal combustion gasoline and diesel engines became the ultra-sophisticated machines that power hundreds of millions of vehicles today.New types of batteries that are lighter, more powerful, faster charging, less expensive, and kinder to the environment are being researched in hundreds of laboratories all around the world as you read this. We don’t know where the breakthroughs will occur but we know they will come, just as those first crude internal combustion gasoline and diesel engines became the ultra-sophisticated machines that power hundreds of millions of vehicles today.There areThere areThe odds are, by 2030 electric cars will have taken a quantum leap forward as more and more new innovations become commercially available. We can’t wait!The odds are, by 2030 electric cars will have taken a quantum leap forward as more and more new innovations become commercially available. We can’t wait!Featured image:Featured image:Appreciate CleanTechnica’s originality and cleantech news coverage? Consider becoming aAppreciate CleanTechnica’s originality and cleantech news coverage? Consider becoming aDon't want to miss a cleantech story? Sign up forDon't want to miss a cleantech story? Sign up forHave a tip for CleanTechnica, want to advertise, or want to suggest a guest for our CleanTech Talk podcast?Have a tip for CleanTechnica, want to advertise, or want to suggest a guest for our CleanTech Talk podcast?AdvertisementAdvertisementCT new after-postCT new after-postzox-post-body",Researchers Discover Substitutes For Rare Earth Materials In Magnets
582,-1,-1_new_said_study_people,https://techcrunch.com/2022/06/22/zap-energy-nets-160m-series-c-to-advance-its-lightning-in-a-bottle-fusion-tech/,"Fusion startupFusion startupFusion powerFusion powerStill, clever new approaches to contain scorching hot plasma — which burns at more than 100 million degrees Celsius — have brought fusion power tantalizingly close to reality. Investors are flocking to the field, hopeful that advances wrought by continued research and increasingly sophisticated computer simulations will finally help fusion pull away from its long string of failures.Still, clever new approaches to contain scorching hot plasma — which burns at more than 100 million degrees Celsius — have brought fusion power tantalizingly close to reality. Investors are flocking to the field, hopeful that advances wrought by continued research and increasingly sophisticated computer simulations will finally help fusion pull away from its long string of failures.Zap Energy’s oversubscribed Series C was led by Lowercarbon Capital. New investors include Breakthrough Energy Ventures, Shell Ventures, DCVC and Valor Equity Partners. Existing investors Addition, Energy Impact Partners and Chevron Technology Ventures also contributed to the round. The startup’s core technology was spun out of research performed at the University of Washington and Lawrence Livermore National Laboratory.Zap Energy’s oversubscribed Series C was led by Lowercarbon Capital. New investors include Breakthrough Energy Ventures, Shell Ventures, DCVC and Valor Equity Partners. Existing investors Addition, Energy Impact Partners and Chevron Technology Ventures also contributed to the round. The startup’s core technology was spun out of research performed at the University of Washington and Lawrence Livermore National Laboratory.Generally speaking, fusion power generates electricity by fusing hydrogen isotopes (either deuterium or tritium) into helium. The process releases neutrons, which are then captured to generate heat and spin a turbine. Atomic nuclei don’t like to fuse, so to coax them close enough for fusion to happen, nuclear scientists use extreme pressure and heat, creating a fourth state of matter known as plasma.Generally speaking, fusion power generates electricity by fusing hydrogen isotopes (either deuterium or tritium) into helium. The process releases neutrons, which are then captured to generate heat and spin a turbine. Atomic nuclei don’t like to fuse, so to coax them close enough for fusion to happen, nuclear scientists use extreme pressure and heat, creating a fourth state of matter known as plasma.",Zap Energy nets $160M Series C to advance its lightning-in-a-bottle fusion tech
575,-1,-1_new_said_study_people,https://eandt.theiet.org/content/articles/2022/10/climate-change-closing-daily-temperature-gap-clouds-may-be-to-blame/,"Climate change is shrinking the difference between the daily high temperature and the daily low in many parts of the world. The gap between the two, known as the diurnal temperature range (DTR), has a significant effect on growing seasons, crop yields, residential energy consumption and human health issues related to heat stress. Why and where the DTR shrinks with climate change has proved something of a mystery.Climate change is shrinking the difference between the daily high temperature and the daily low in many parts of the world. The gap between the two, known as the diurnal temperature range (DTR), has a significant effect on growing seasons, crop yields, residential energy consumption and human health issues related to heat stress. Why and where the DTR shrinks with climate change has proved something of a mystery.Researchers who are part of a new international study that examined the DTR at the end of the 21st century believe they have found the answer: an increase in clouds, which blocks incoming shortwave radiation from the Sun during the day.Researchers who are part of a new international study that examined the DTR at the end of the 21st century believe they have found the answer: an increase in clouds, which blocks incoming shortwave radiation from the Sun during the day.This means that while both the daily maximum temperature and the daily minimum are expected to continue to increase with climate change, the daily maximum temperature will increase at a slower rate. The end result is that the DTR will continue to shrink in many parts of the world, but that the changes will vary depending on a variety of local conditions, researchers said.This means that while both the daily maximum temperature and the daily minimum are expected to continue to increase with climate change, the daily maximum temperature will increase at a slower rate. The end result is that the DTR will continue to shrink in many parts of the world, but that the changes will vary depending on a variety of local conditions, researchers said.The study, published in the journalThe study, published in the journal“Clouds are one of the big uncertainties in terms of climate projections,” said co-author Dev Niyogi, a professor at The University of Texas at Austin Jackson School of Geosciences. “When we do this with a very high spatial resolution modelling framework, it allows us to explicitly simulate clouds.”“Clouds are one of the big uncertainties in terms of climate projections,” said co-author Dev Niyogi, a professor at The University of Texas at Austin Jackson School of Geosciences. “When we do this with a very high spatial resolution modelling framework, it allows us to explicitly simulate clouds.”Lead author Doan Quang Van, an assistant professor at the University of Tsukuba Center for Computational Sciences in Japan, said this is vital for understanding the future of the DTR: “Clouds play a vital role in the diurnal temperature variation by modulating solar radiative processes, which consequently affect the heat exchange at the land surface.”Lead author Doan Quang Van, an assistant professor at the University of Tsukuba Center for Computational Sciences in Japan, said this is vital for understanding the future of the DTR: “Clouds play a vital role in the diurnal temperature variation by modulating solar radiative processes, which consequently affect the heat exchange at the land surface.”Using supercomputers at the University of Tsukuba's Centre for Computational Sciences, the team was able to model the complicated interplay of land-surface processes on climate change. These include changes in land use (such as deforestation), soil moisture, precipitation, cloud cover and other factors that can affect the temperature in a local region. By creating a model with a finer resolution grid – 2km/sq grids rather than the 100km grids used in most climate models – the researchers were able to more closely analyse the impacts of climate change.Using supercomputers at the University of Tsukuba's Centre for Computational Sciences, the team was able to model the complicated interplay of land-surface processes on climate change. These include changes in land use (such as deforestation), soil moisture, precipitation, cloud cover and other factors that can affect the temperature in a local region. By creating a model with a finer resolution grid – 2km/sq grids rather than the 100km grids used in most climate models – the researchers were able to more closely analyse the impacts of climate change.The team focused on two areas: the Kanto region of Japan and the Malaysian peninsula. Using the 10-year period from 2005-2014 as a baseline, the researchers then ran different climate scenarios to project what will happen to the DTR in the two regions at the end of the century.The team focused on two areas: the Kanto region of Japan and the Malaysian peninsula. Using the 10-year period from 2005-2014 as a baseline, the researchers then ran different climate scenarios to project what will happen to the DTR in the two regions at the end of the century.They found that the temperature gap closes by about 0.5°C in the temperate Kanto region and 0.25°C in the more tropical Malaysian peninsula. Researchers attribute these changes in large part to increased daytime cloud coverage that would be expected to develop under these climate conditions.They found that the temperature gap closes by about 0.5°C in the temperate Kanto region and 0.25°C in the more tropical Malaysian peninsula. Researchers attribute these changes in large part to increased daytime cloud coverage that would be expected to develop under these climate conditions.The researchers said the study can help scientists improve current global climate models and aid in understanding how the shrinking DTR will affect society and the environment as the climate continues to warm.The researchers said the study can help scientists improve current global climate models and aid in understanding how the shrinking DTR will affect society and the environment as the climate continues to warm.“It is very important to know how DTR will change in the future because it modulates human, animal and plant metabolisms,” said Quang Van. “It also modulates the local atmospheric circulation such as the land-sea breeze.”“It is very important to know how DTR will change in the future because it modulates human, animal and plant metabolisms,” said Quang Van. “It also modulates the local atmospheric circulation such as the land-sea breeze.”The research paper – 'The research paper – 'The research was funded by Japan Society for the Promotion of Science Grants-in-Aid for Scientific Research, Nasa Interdisciplinary Research in Earth Science, and the US Department of Agriculture National Institute for Food and Agriculture.The research was funded by Japan Society for the Promotion of Science Grants-in-Aid for Scientific Research, Nasa Interdisciplinary Research in Earth Science, and the US Department of Agriculture National Institute for Food and Agriculture.The team included scientists from the UT Jackson School’s Department of Geological Sciences, the National Centre for Atmospheric Research in Boulder, Colorado, Shanghai University of Engineering Science, National Defence Academy of Japan, and the University of Tsukuba, Japan.The team included scientists from the UT Jackson School’s Department of Geological Sciences, the National Centre for Atmospheric Research in Boulder, Colorado, Shanghai University of Engineering Science, National Defence Academy of Japan, and the University of Tsukuba, Japan.",Climate change closing daily temperature gap; clouds may be to blame
574,-1,-1_new_said_study_people,https://techcrunch.com/2022/05/17/legl-series-b/,"While valuations of public software-as-a-service businesses have beenWhile valuations of public software-as-a-service businesses have beenToday, London-basedToday, London-basedThe Series B was led by several technology investors, including existing investor Octopus Ventures (which led its Series A), although Legl isn’t specifying the round’s other backers. Previously disclosed investors in the business include Backed, Samaipata and First Round Capital, plus a number of angels.The Series B was led by several technology investors, including existing investor Octopus Ventures (which led its Series A), although Legl isn’t specifying the round’s other backers. Previously disclosed investors in the business include Backed, Samaipata and First Round Capital, plus a number of angels.The startup says it has grown its customer base from around 100 UK-based law firms back in March 2021 to 170+ now — which it specifies includes 20 of the Top 200 firms in the country.The startup says it has grown its customer base from around 100 UK-based law firms back in March 2021 to 170+ now — which it specifies includes 20 of the Top 200 firms in the country.It’ll be using the Series B to kick off planned international expansion, focusing on other markets where its UK client base has offices and ploughing cash into product dev and hiring.It’ll be using the Series B to kick off planned international expansion, focusing on other markets where its UK client base has offices and ploughing cash into product dev and hiring.“There is a global opportunity for law firms to run their businesses in a more modern, efficient, revenue-driving and client-friendly way. We are working with our client base to start expanding out to their international offices which lie across multiple different geographies,” says founder and CEO, Julia Salasky.“There is a global opportunity for law firms to run their businesses in a more modern, efficient, revenue-driving and client-friendly way. We are working with our client base to start expanding out to their international offices which lie across multiple different geographies,” says founder and CEO, Julia Salasky.“Over the past year, we’ve built out our vision of a new category in the legal space — client lifecycle management — by investing in the underlying CRM that enables law firms not only to digitize previously manual business workflows across the client lifecycle but to understand their client base better. We’ve leaned into our core competencies in risk management, compliance and payments and finance, enabling law firms to both undertake activities that touch on their regulated business processes but also improve cashflow and drive better client experience.“Over the past year, we’ve built out our vision of a new category in the legal space — client lifecycle management — by investing in the underlying CRM that enables law firms not only to digitize previously manual business workflows across the client lifecycle but to understand their client base better. We’ve leaned into our core competencies in risk management, compliance and payments and finance, enabling law firms to both undertake activities that touch on their regulated business processes but also improve cashflow and drive better client experience.“With the new funding we will expand our workflow driven approach to managing business operations and in particular focus on how law firms can drive faster revenue, better and de-risked financial management and a better client experience. We already enable law firms to manage a large proportion of their client base and payment stack and plan to drive more capabilities for more firms over the coming months,” she adds.“With the new funding we will expand our workflow driven approach to managing business operations and in particular focus on how law firms can drive faster revenue, better and de-risked financial management and a better client experience. We already enable law firms to manage a large proportion of their client base and payment stack and plan to drive more capabilities for more firms over the coming months,” she adds.Salasky, whose name may also be familiar as prior founder of theSalasky, whose name may also be familiar as prior founder of theShe declines to disclose the startup’s valuation for the Series B but confirms the raise was certainlyShe declines to disclose the startup’s valuation for the Series B but confirms the raise was certainly“This is a big up round for us! Last round, last year we raised $7M and this is an $18M round (closed in this new funding climate!), building on the revenue growth and momentum we’ve had,” she notes.“This is a big up round for us! Last round, last year we raised $7M and this is an $18M round (closed in this new funding climate!), building on the revenue growth and momentum we’ve had,” she notes.Discussing whether the SaaS startup is feeling any impact from a wider market cooling on tech and SaaS stocks, she adds: “Law firms are notoriously counter-cyclical businesses, so they don’t tend to suffer as much as traditional corporates in a downturn. But in general what we see is that as we demonstrate increased value to law firms and drive better core business operations, we become more, not less valuable, irrespective of market conditions.”Discussing whether the SaaS startup is feeling any impact from a wider market cooling on tech and SaaS stocks, she adds: “Law firms are notoriously counter-cyclical businesses, so they don’t tend to suffer as much as traditional corporates in a downturn. But in general what we see is that as we demonstrate increased value to law firms and drive better core business operations, we become more, not less valuable, irrespective of market conditions.”Legl founder and CEO, Julia Salasky (Legl founder and CEO, Julia Salasky (Legl founder and CEO, Julia Salasky (Legal and compliance tech has been an increasingly active category for startups in recent years. But Salasky suggests most of the action has focused on contract management or other targeted ‘point solutions,’ whereas Legl aims to stand apart by offering a more holistic platform for law firms to power up their ability to serve clients by providing them with a suite of digital tools that can automate and support their business operations. This frees up in-house expertise to focus on more of the core legal work.Legal and compliance tech has been an increasingly active category for startups in recent years. But Salasky suggests most of the action has focused on contract management or other targeted ‘point solutions,’ whereas Legl aims to stand apart by offering a more holistic platform for law firms to power up their ability to serve clients by providing them with a suite of digital tools that can automate and support their business operations. This frees up in-house expertise to focus on more of the core legal work.“There is an explosion of investment in contract management and other areas where the substantive legal work could be improved. But what we are doing at“There is an explosion of investment in contract management and other areas where the substantive legal work could be improved. But what we are doing atCommenting on the Series B in a statement, Malcolm Ferguson, investor at Octopus Ventures, added: “We’re delighted to continue to support Julia and the team on their mission to free up lawyers’ time so they focus on creating value for their clients.  The company has grown really strongly over the last 12 months, and is positioning itself to become the go-to solution for law firms looking to modernise and automate their non-core work. Not only does this improve a law firm’s revenues, and margins, but [it] also means they can deliver a meaningfully better experience to their clients.  We’re excited to see what Julia can achieve with this funding over the coming years.”Commenting on the Series B in a statement, Malcolm Ferguson, investor at Octopus Ventures, added: “We’re delighted to continue to support Julia and the team on their mission to free up lawyers’ time so they focus on creating value for their clients.  The company has grown really strongly over the last 12 months, and is positioning itself to become the go-to solution for law firms looking to modernise and automate their non-core work. Not only does this improve a law firm’s revenues, and margins, but [it] also means they can deliver a meaningfully better experience to their clients.  We’re excited to see what Julia can achieve with this funding over the coming years.”","Legl, a SaaS for law firm workflows, tops up with $18M"
571,-1,-1_new_said_study_people,https://www.newsweek.com/baby-alpaca-learns-walk-again-prosthetic-leg-adorable-clip-1749410?,"An alpaca has been given a second chance at life, thanks to a custom prosthetic leg.Living at Blowing Oaks Ranch in Batesville, Arkansas, the smallAccording to the British Alpaca Society, ""Alpacas originate from the Altiplano (Spanish for high plain) in west-central South America. Spanning the borders of Peru, Chile and Bolivia, this area of the Andes averages nearly 4,000 metres (more than 2 miles) above sea level."" They are closely related to the llama.At just six weeks old, Cinco suffered a broken leg. Ranch owner Caren Barnett told""We suspect that Cinco injured it playing with some of the other young alpacas, then kept using it until the fracture became so damaged that it couldn't be repaired. It was open and dirty, and the bones were shattered.""After consulting their veterinarian, it was decided that, due to the chances of Cinco getting an infection from the fracture, it would be safer to remove the leg.While the veterinary surgeon first wanted to remove the leg at the shoulder, Barnett and her husband, John, convinced the team to keep the upper leg so that Cinco had a chance of a fitted""Alpacas carry two-thirds of their weight on their front end, and we didn't want his right leg to get overused or injured,"" said Barnett.Luckily for Cinco, Barnett is a physical therapist and the director of outpatient rehab for White River Health in Batesville. She went next door to JP&O Prosthetic and Orthotic Lab and asked the certified prosthetist orthotist Paul if it was possible to make a prosthetic for Cinco.""I don't think he knew what an alpaca was at that time, but he agreed he would see what he could do,"" Barnett said: ""He had previously made aAfter taking Cinco for several fittings, on September 14, he finally got the finished product.""Paul used donated parts for the leg except for the socket, which had to be made custom from a cast of Cinco's stump,"" said Barnett: ""It has an adjustable-height pylon to adapt to his growing height.""Now firmly back on his feet, Cinco is enjoying playing in the fields at the ranch, which is home to 15 other alpacas.As he is still growing, it is likely that Cinco will need one or two more replacement prosthetic legs in the future, due to growth and general wear and tear.""He is the best baby alpaca we have ever had in our herd, so we hope to eventually use him as a herd sire,"" said Barnett.A herd sire refers to a male alpaca with particularly desirable genetics for breeding—meaning that Cinco could become father to many other baby alpacas in the future.""We hope to take him to a couple of recognized alpaca shows, but we will not be able to show him in halter classes due to his injury,"" said Barnett. ""He will be judged only on his fiber qualities and not on confirmation.""Most importantly, we will do everything to make Cinco as comfortable and happy as we can and help him live out as normal a life as is possible.""Barnett admits Cinco does have a special place in her heart: ""I have to admit to spoiling him with extra treats and kisses. He is a super-sweet alpaca and our farm favorite for sure.""",Baby Alpaca Learns to Walk Again Thanks to Prosthetic Leg in Adorable Clip
570,-1,-1_new_said_study_people,https://techcrunch.com/2022/03/21/to-raise-a-fund-this-agtech-outfit-built-a-content-company-first-now-it-has-60-million-to-put-to-work/,"Rob Leclerc has the kind of pedigree that investors tend to like: He has a master’s degree in computer science from the University of Calgary and a Ph.D. in computational biology from Yale. In fact, 10 years ago, what he really wanted to do with his degrees was to find and fund agriculture-related projects that tackle climate change.Rob Leclerc has the kind of pedigree that investors tend to like: He has a master’s degree in computer science from the University of Calgary and a Ph.D. in computational biology from Yale. In fact, 10 years ago, what he really wanted to do with his degrees was to find and fund agriculture-related projects that tackle climate change.But a decade ago, “agtech” wasn’t yet a category, and that was problematic when it came to pitching investors on the concept of an investment fund that Leclerc would run with partner Michael Dean, with whom Leclerc had previously operated an agribusiness in West Africa for several years.But a decade ago, “agtech” wasn’t yet a category, and that was problematic when it came to pitching investors on the concept of an investment fund that Leclerc would run with partner Michael Dean, with whom Leclerc had previously operated an agribusiness in West Africa for several years.At the time, “there were a handful of businesses” relating to agtech that investors were aware of, Leclerc said. Think Climate Corp and Impossible Foods and the smart machinery company Blue River. But Climate Corp hadn’t yet sold to Monsanto forAt the time, “there were a handful of businesses” relating to agtech that investors were aware of, Leclerc said. Think Climate Corp and Impossible Foods and the smart machinery company Blue River. But Climate Corp hadn’t yet sold to Monsanto for“The overarching problem was narrative,” Leclerc said.  “People didn’t care about it.”“The overarching problem was narrative,” Leclerc said.  “People didn’t care about it.”He and Dean might have just given up; instead, they started a San Francisco-based content company calledHe and Dean might have just given up; instead, they started a San Francisco-based content company called“We thought if we could get people excited about food and [agriculture], maybe we’d be in a position to [raise a fund later],” Leclerc said. It was a smart bet. Fast forward and after posting more than 4,000 articles to the site and garnering 90,000 subscribers to the site’s weekly newsletter, Leclerc said“We thought if we could get people excited about food and [agriculture], maybe we’d be in a position to [raise a fund later],” Leclerc said. It was a smart bet. Fast forward and after posting more than 4,000 articles to the site and garnering 90,000 subscribers to the site’s weekly newsletter, Leclerc saidIt’s a huge step up from previous funds that Leclerc and company began raising several years ago – beginning with their newsletter readers. “We first raised a $2.5 million friends-and-family fund,” he said, “but then five months later, we needed more money, so we raised $2 million, then six months later, we raised $5 million.” And so on. It wasn’t the most conventional way to raise money, but AgFunder had this “massive subscriber base” to which it could talk directly about its fundraising efforts, Leclerc said, “and the belief that we know what we’re doing and can spot companies started a lot of conversations that we wouldn’t have had otherwise. It became a structural advantage.”It’s a huge step up from previous funds that Leclerc and company began raising several years ago – beginning with their newsletter readers. “We first raised a $2.5 million friends-and-family fund,” he said, “but then five months later, we needed more money, so we raised $2 million, then six months later, we raised $5 million.” And so on. It wasn’t the most conventional way to raise money, but AgFunder had this “massive subscriber base” to which it could talk directly about its fundraising efforts, Leclerc said, “and the belief that we know what we’re doing and can spot companies started a lot of conversations that we wouldn’t have had otherwise. It became a structural advantage.”The strategy isn’t unprecedented. Leclerc said he was partly inspired by Michael Arrington, the founder of TechCrunch, who built a brand around entrepreneurship, then used the strength of that brand to launch an investing career. Meanwhile, Arrington was preceded in his path by investor Jason Calacanis, who earlier founded a media company, and more recent examples are beginning to emerge routinely. Among them: Londoner Harry Stebbings used his “Twenty Minute VC” podcast as a springboard into the venture world last year, and Nik Milanović, the author of a two-year-old newsletter called “This Week in Fintech,” in January launched anThe strategy isn’t unprecedented. Leclerc said he was partly inspired by Michael Arrington, the founder of TechCrunch, who built a brand around entrepreneurship, then used the strength of that brand to launch an investing career. Meanwhile, Arrington was preceded in his path by investor Jason Calacanis, who earlier founded a media company, and more recent examples are beginning to emerge routinely. Among them: Londoner Harry Stebbings used his “Twenty Minute VC” podcast as a springboard into the venture world last year, and Nik Milanović, the author of a two-year-old newsletter called “This Week in Fintech,” in January launched anStill, newsletter subscribers –  no matter how deep their pockets –  don’t invest tens of millions of dollars in a team without seeing some results first. And AgFunder (which has since broadened its LP base) already has some about which to brag. Among the 60 companies to so far receive a check from AgFunder was the autonomous tractor startup Bear Flag Robotics, which sold to John Deere last year forStill, newsletter subscribers –  no matter how deep their pockets –  don’t invest tens of millions of dollars in a team without seeing some results first. And AgFunder (which has since broadened its LP base) already has some about which to brag. Among the 60 companies to so far receive a check from AgFunder was the autonomous tractor startup Bear Flag Robotics, which sold to John Deere last year forIf you’re curious about how much the firm owned in each of these companies, keep guessing. AgFunder – which tends to write checks of $500,000 as a starting point but also just wrote a check for $3 million – doesn’t think about ownership targets or look to own a specific percentage in a company, Leclerc said. While his team has used special purpose vehicles to maintain their pro rata in several companies, including a still-private molecular coffee company calledIf you’re curious about how much the firm owned in each of these companies, keep guessing. AgFunder – which tends to write checks of $500,000 as a starting point but also just wrote a check for $3 million – doesn’t think about ownership targets or look to own a specific percentage in a company, Leclerc said. While his team has used special purpose vehicles to maintain their pro rata in several companies, including a still-private molecular coffee company calledAs for criteria, AgFunder looks broadly across the food and “ag value chain,” Leclerc said. It also invests broadly geographically, with bets in India, Brazil, Mexico and Indonesia, among other places.As for criteria, AgFunder looks broadly across the food and “ag value chain,” Leclerc said. It also invests broadly geographically, with bets in India, Brazil, Mexico and Indonesia, among other places.It’s lot of ground to cover, so the outfit relies heavily on itsIt’s lot of ground to cover, so the outfit relies heavily on itsYet AgFunder has other sources of deal flow, too, and one of these is, yes, that newsletter readership. Indeed, asked about how it found that coffee company Atomo — which says its product producesYet AgFunder has other sources of deal flow, too, and one of these is, yes, that newsletter readership. Indeed, asked about how it found that coffee company Atomo — which says its product produces“An LP who’d followed us for a long time on AgFunder News wrote us, saying, ‘You probably see a lot of things; this company you should check out.’ So we did,” Leclerc said.“An LP who’d followed us for a long time on AgFunder News wrote us, saying, ‘You probably see a lot of things; this company you should check out.’ So we did,” Leclerc said.","To raise a fund, this agtech outfit built a content company first (now it has $60 million to put to work)"
565,-1,-1_new_said_study_people,https://www.zdnet.com/article/criminals-are-using-gps-jammers-to-hijack-trucks-and-down-drones/,"By PopTika -- ShutterstockSatellite navigation and tracking via GPS has become a critical link in the world's rapidly growing logistics and freight carrying ecosystem. Companies use GPS to track trucks and keep them on time and their cargo secure.Little wonder, then, that criminals are turning to cheap GPS jamming devices to ransack the cargo on roads and at sea, a problem that's getting worse but may be ameliorated with a new generation of safety technology designed to overcome threats from jamming.In case you aren't a master criminal or a secret agent, here's some background. The core problem for any system using GPS is that the signals are extremely weak, an inevitable byproduct of the vast distances those signals need to travel. Jammers work by overpowering GPS signals by emitting a signal at the same frequency, just a bit more powerful than the original. The typical jammers used for cargo hijackings are able to jam frequencies from up to 5 miles away rendering GPS tracking and security apparatuses, such as those used by trucking syndicates, totally useless.In Mexico, jammers are used in some 85% of cargo truck thefts. Statistics are harder to come by in the United States, but there can be little doubt the devices are prevalent and widely used. Russia is currently availing itself of the technology to jam commercial planes in Ukraine.As we've covered, the proliferating commercial drone sector is also prey to attack. Drones often rely on GPS for navigation as well as security tracking, making them especially vulnerable. Drones equipped with back-up methods still often rely on GPS for positioning, navigation, and stabilization, making jammers a way to take a drone down and potentially cause harm to life and property. During a light show in Hong Kong in 2018, a jamming device caused 46 drones to fall out of the sky, raising public awareness of the issue.Technologies are emerging to counter this problem. A company called infiniDome has developed an anti-jamming solution that is compatible with almost any GPS-based telematics unit, a catch-all term for the technologies trucking companies use to track and monitor their assets on the road. InfiniDome's ""OtoSphere"" is a small add-on device created for commercial GNSS receivers, providing protection and increasing the resiliency of GPS devices against jamming attacks. By identifying and preventing instances of jamming, fleet operators are able to prevent cargo theft.Other companies, such as Sepentrio, are also taking GPS jamming in drone applications seriously with integrated sensor solutions.In the meantime, governments are hoping to fight back through regulation, although that may be a losing battle. Mexico passed an anti-jamming law in late 2020, allowing for penalties of 12-15 years in prison for persons caught using such devices while committing crimes.Nevertheless, jammers remain widely available online, where they can be purchased for as little as $50.",GPS jammers are being used to hijack trucks and down drones: How to stop them
564,-1,-1_new_said_study_people,https://www.world-nuclear-news.org/Articles/Application-submitted-for-US-molten-salt-research,"Application submitted for US molten salt research reactor19 August 2022ShareAbilene Christian University (ACU) has applied to the US Nuclear Regulatory Commission (NRC) for a construction licence for a molten salt research reactor (MSRR), to be built on its campus in Abilene, Texas, as part of the Nuclear Energy eXperimental Testing (NEXT) laboratory. ​ACU plans for the MSRR to achieve criticality by December 2025.The Science and Engineering Research Centre at Abilene, which will house the MSRR (Image: ACU)The planned reactor would be an up to 1 MWt, graphite-moderated, fluoride salt flowing fluid (fuel dissolved in the salt) research reactor. The MSRR will be used for on-campus nuclear research and training opportunities for faculty, staff and students in advanced nuclear technologies. The reactor will significantly expand the university's salt reactor research and development infrastructure, supporting US molten salt reactor design, development, deployment and market penetration.​​In March 2020, ACU submitted to the NRC a Letter of Intent to apply for a construction permit for a non-power molten salt reactor. In July 2020, it submitted a Regulatory Engagement Plan related to this project.ACU has now submitted its construction licence application - including a Preliminary Safety Analysis Report (PSAR) and an Environmental Review - to the NRC on 15 August.According to ACU, the move represents the first application for a new US research reactor of any kind in more than 30 years, as well as the first-ever university application for an advanced research reactor.The NRC will now conduct an acceptance review to determine if ACU's application is complete. It will then develop a review schedule and initiate a formal technical review of the project.ACU said it currently expects to complete construction of the MSRR at the earliest six months after issuance of the construction permit and latest by 48 months after issuance of the construction permit.The ACU-led NEXT Research Alliance (NEXTRA) - which also includes the Georgia Institute of Technology, Texas A&M University and the University of Texas-Austin - is working with a USD30.5 million research agreement sponsored by Natura Resources, with USD21.5 million going to ACU and the remaining USD9 million to the other consortium universities. The NEXTRA partnership was established to design, build and license the first-ever MSRR.ACU recently contracted Teledyne Brown Engineering (TBE) to perform Front End Engineering Design services for the MSRR. TBE is the prime contractor to ACU, providing preliminary design services (developing designs, sizes, specification and estimated costs) for the reactor.TBE said once it has completed preliminary design and engineering studies in the initial contract, follow-on work will include detailed design, reactor and systems manufacturing, site integration and installation.Researched and written by World Nuclear NewsRelated topics",Application submitted for US
562,-1,-1_new_said_study_people,https://www.wired.com/story/drought-destroying-ancient-ruins/,"The Nizari garrison at Gird Castle resisted the Mongol horde of Hulagu Khan for 17 years before surrendering in December 1270. The fortress rose 300 meters above the surrounding plains of present-day eastern Iran, with three rings of fortifications enclosing its base. But dwindling supplies and an outbreak of cholera forced the defenders to abandon their posts after one of the longest sieges in medieval history.Content This content can also be viewed on the site it originates from.Eight hundred years later, the remaining fortifications at Gird Castle face the onslaught of a new invader: sand. For the past three months, Bijan Rouhani, an archaeologist at the University of Oxford, has been monitoring about 700 sites in Iran’s Sistan region using satellite imagery. His comparison of US intelligence photos taken in 1977 and Google Earth’s most recent images of the area shows the advance of vast dunes that now almost bury the fortress at Gird.This summer, drought has revealed a number of previously hidden archaeological sites as low water levels have allowed archaeologists to access historic ruins in Spain, Iraq, and China. But just as climate change giveth, so it taketh away: Rising heat is damaging some ancient sites and spurring desertification that is burying others, Gird Castle among them. It is a growing problem with few proven solutions.“We can see many other sites from the Bronze Age to the Islamic periods in the area, as well as ancient rivers and canals,” says Rouhani. “Most of these sites are now buried under sand and impacted by the 120-day sand wind every year.”The ancient city of Zahedan Kohneh has suffered the same fate as Gird Castle. It was Sistan’s capital when Gird fell to the Mongols and was once one of the largest cities in Iran—today it is draped in a growing garment of sand. Archaeologists monitoring sites in other regions, countries, and continents report similar stories. Ahmed Mutasim Abdalla Mahmoud, a researcher specializing in sand movement at the University of Nottingham, says sand poses the biggest threat to Sudan’s Nubian pyramids, built around 4,500 years ago. He warns that the 200 pyramids at El Kurru, Jebel Barkal, and Meroe on the Nile River could soon disappear beneath sand.“The threat has been exacerbated by climate change, which has made the land more arid and sandstorms more frequent,” he writes on the Conversation. “Moving sands can engulf entire houses in rural Sudan, and cover fields, irrigation canals, and riverbanks.”",Climate Change Is Burying Archaeological Sites Under Tons of Sand
560,-1,-1_new_said_study_people,https://www.washingtonpost.com/politics/2022/10/25/lawsuit-claims-google-knew-its-incognito-mode-doesnt-protect-users-privacy/,"Comment on this story Comment Gift Article ShareHi all, Gerrit De Vynck here. I’m a tech reporter for The Post out in San Francisco. You can follow me on Twitter at @GerritD. Lawsuit challenges Google’s claim that its ‘Incognito mode’ protects users’ privacy Wp Get the full experience. Choose your plan ArrowRight It can be hard to keep track of all the lawsuits against Google. The Department of Justice filed one in 2020 and might have another one coming soon. Texas has at least two. Arizona recently settled theirs with the search giant for $85 million. And Washington state and D.C. have lawsuits, too.It’s not just governments. Video game maker Epic and dating app owner Match Group are suing Google, alleging anticompetitive behavior in how it runs its app store. The Republican National Committee is suing Google for sending politicians’ emails straight to spam folders.Here’s another one to add to the list. Right now, a California judge is deliberating on whether to allow a class-action lawsuit representing millions of Google users to go forward. A group of consumers is alleging the company misled people about what data it collected when they were using private browsing modes on both Google’s Chrome web browser and browsers built by other companies such as Apple and Mozilla.AdvertisementBecause essentially all internet users in the United States use a browser to surf the web, the potential fines should Google be found liable could be in the billions of dollars.The lawyers spearheading the lawsuit have already amassed a trove of internal Google emails they say show how the company’s executives have known for years that what the company calls “Incognito mode” is anything but incognito. Private browsing modes usually block tracking cookies — little bits of code that follow people around the internet logging their activity.But Google still logged information on people using private mode whenever they visited websites that had installed hugely popular Google software used for serving ads or measuring traffic, the lawsuit alleges.The emails released as part of the court process show how Google employees repeatedly raised concerns about private mode with their superiors. One 2019 email from Google’s Chief Marketing Officer Lorraine Twohill to CEO Sundar Pichai said Incognito was “not truly private.” An internal presentation said Google users “overestimate the protections that Incognito provides.” Another one proposed getting rid of the word “private” from the Incognito mode start screen completely.AdvertisementGoogle says the accusations are overblown and that it has always been clear with its users about the limits of Incognito mode and private browsing.“Privacy controls have long been built into our services and we encourage our teams to constantly discuss or consider ideas to improve them,” Google spokesman José Castañeda said. “Incognito mode offers users a private browsing experience, and we've been clear about how it works and what it does, whereas the plaintiffs in this case have purposely mischaracterized our statements.”If the judge gives the green light, lawyers will continue their fight to get tens of millions of Google users a payment of between $100 and $1,000 each. That’s a potential payoff in the billions of dollars.Our top tabsFTC singles out Drizly executive over data privacy abusesThe Federal Trade Commission’s proposed order will follow Drizly CEO Cory Rellas to his future businesses, forcing him to implement security programs at any companies he leads that collect data from at least 25,000 people, Cat Zakrzewski reports. The punishment came after alleged security failures under Rellas’s watch that exposed around 2.5 million customers’ personal information.AdvertisementIt also comes after Democrats pushed for more aggressive penalties for individual executives involved in major data breaches. “There are only a handful of examples of the FTC pursuing such individual liability in past cases involving online data,” Cat writes. “In 2019, the agency reached a settlement with the operator of an online rewards website, ClixSense, that will follow [the executive] to future companies. That same year, the agency also named executives in an order it brought against a dress-up games website, which allegedly violated a law that protects children under the age of 13 online.”Under the order, Rellas and Drizly, which is owned by Uber, will also have to destroy unnecessary data, put new data controls in place and train their employees about cybersecurity. The FTC will decide on finalizing the order after getting public comments for 30 days.Tech groups ask Supreme Court to take up Florida social media lawThe Computer and Communications Industry Association (CCIA) and NetChoice told the Supreme Court that a Florida law that would penalize social media companies for blocking politicians’ posts violates the First Amendment rights of tech companies, according to a Monday news release. In September, Florida’s attorney general asked the Supreme Court to consider whether such a law is constitutional.AdvertisementThe filing comes amid a flurry of activity about social media laws at the Supreme Court. An appeals court blocked much of the Florida law in May but another appeals court upheld a similar Texas law in September. The court has postponed that law from going into effect until the Supreme Court reviews the case.Chinese spies accused of trying to obstruct Huawei investigationThe Justice Department said two men working on behalf of Beijing bribed a U.S. law enforcement official to share secrets about the prosecution of a major Chinese firm that people familiar with the matter said was Huawei, Devlin Barrett, Perry Stein and Ellen Nakashima report. But the official was actually a double agent working for the U.S. government who was gathering evidence against the suspects and feeding them fake documents and information.Advertisement“The U.S. Justice Department indicted Huawei Technologies in 2019, accusing the world’s largest communications equipment manufacturer and some of its executives of violating U.S. sanctions on Iran and conspiring to obstruct justice related to the investigation — prompting furious condemnations from both the company and the country,” Devlin, Perry and Ellen write. “The new charges suggest that the Chinese government went to great lengths to try to defeat the U.S. case against the company, assigning alleged Chinese intelligence officers to obtain information about witnesses and evidence. Huawei has long insisted it operates independently of the Chinese government.”A Huawei representative didn’t respond to a request for comment.Inside the industryAdvertisementWorkforce reportTrendingMentionsMichelle Giuda is joining the Krach Institute for Tech Diplomacy as its director. Giuda previously worked as an executive vice president at Weber Shandwick and as an assistant secretary of state in the Trump administration.DaybookGeoffrey Starks and Nathan Simington , as well as Amazon and SpaceX executives, FCC Commissionersand, as well as Amazon and SpaceX executives, speak at a New America event on satellite spectrum today at 11:45 a.m.Alan F. Estevez , the undersecretary of commerce for industry and security, discusses new semiconductor export controls at an , the undersecretary of commerce for industry and security, discusses new semiconductor export controls at an event hosted by the Center for a New American Security on Thursday at 10 a.m.Dave Limp the company’s satellite internet technology at a Washington Post Live event on Thursday at 10:30 a.m. Amazon senior vice president discusses the company’s satellite internet technology at a Washington Post Live event on Thursday at 10:30 a.m.Before you log offGoing into Monday with the same enthusiasm as Tim Cook pic.twitter.com/pB3N5xV8pJ — Dan - EngineMode11 (@EngineMode11) October 24, 2022That’s all for today — thank you so much for joining us! Make sure to tell others to subscribe to The Technology 202 here. Get in touch with tips, feedback or greetings on Twitter or email.GiftOutline Gift Article",Lawsuit claims Google knew its âIncognito modeâ doesn't protect usersâ privacy
559,-1,-1_new_said_study_people,https://www.washingtonpost.com/climate-environment/2022/10/11/rain-increasing-climate-change-us/?pwapi_token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWJpZCI6IjQ5NzgxMjU3IiwicmVhc29uIjoiZ2lmdCIsIm5iZiI6MTY2NTUyMzA3MCwiaXNzIjoic3Vic2NyaXB0aW9ucyIsImV4cCI6MTY2NjczMjY3MCwiaWF0IjoxNjY1NTIzMDcwLCJqdGkiOiI5NmQ2Y2ZlYi00NzI4LTQ4NGItYjA1OC01NzUyYTZmOGJkMmIiLCJ1cmwiOiJodHRwczovL3d3dy53YXNoaW5ndG9ucG9zdC5jb20vY2xpbWF0ZS1lbnZpcm9ubWVudC8yMDIyLzEwLzExL3JhaW4taW5jcmVhc2luZy1jbGltYXRlLWNoYW5nZS11cy8ifQ.dNgq8ovACyzvsQj57auaVi2HD3v97xjEVkPMKhyiCHg,"Listen Comment on this story Comment Gift Article ShareWhen it rains, it pours. A paper published Tuesday in the journal Geophysical Research Letters finds that it’s raining harder in most of the United States. The study, written by researchers at Northwestern University, tied the results to climate change and to warmer air’s ability to hold more water. 10 steps you can take to lower your carbon footprint ArrowRight Record rain is hitting drought-stricken areas. That’s not good news. The findings echo the fundamental laws of physics and thermodynamics, as well as the evidence from decades of research, and highlight the real-time effect that humans are having on the weather and climate.The research offers confirmation of what atmospheric scientists have been warning of for years: a warmer world is, on balance, a wetter world. And as global temperatures continue to rise, an uptick in precipitation extremes is expected.Climate change is causing heavier rainsWhat the study finds is consistent with a basic tenet of atmospheric physics: For every degree Fahrenheit that air temperature rises, the atmosphere can hold 4 percent more water; this is known as the Clausius-Clapeyron relationship. Where storm clouds develop and the atmosphere is sufficiently moist, it means a warmer climate will support more intense rainfall.AdvertisementThe study reports “consistent shifts from lower to higher daily precipitation intensities, particularly in the central and eastern United States.” The authors compared rainfall over two periods — 1951 to 1980 and 1991 to 2020 — to see how patterns evolved.“When it’s raining, it’s raining more,” said Ryan Harp, the author lead author on the study, in an interview. “But what we also did was … we were able to verify some of the expectations we had based on modeling studies.”In other words, most similar studies to date had centered on projections made by forward-running computer models fed with historical data. This is among the first that, on a nationwide level, examine observed daily rainfall trends.“I think during that process we were a little surprised that this paper hasn’t been written before,” said Daniel Horton, a study co-author and a professor at Northwestern University, in an interview. “We know that precipitation should be increasing … but we just wanted to do a very straightforward paper that says, ‘Yep, we’re seeing it.’ ”AdvertisementHarp said the fact that reality matched simulations lends credence to climate models.“People should in general be trusting in climate models,” he said. “We’re constantly working to improve them to the best of our abilities.”Not necessarily more rain overall, and not everywhereIn the study, the authors examined daily data from more than 1,700 weather stations distributed in the Lower 48. Each had to have a continuous record dating back to at least 1951 to be included in the study.In the eastern United States, the researchers observed a 4.5 to 5.7 percent increase in average daily rainfall on days when it rained. That does not say there are more days with rain, or more rain overall.“That doesn’t necessarily give us the whole story,” Harp said. “There may be places where precipitation intensity is increasing but frequency is decreasing. We might not know if there’s an overall increase or decrease. That’s one thing that we’re working on.”Greg Carbin, the head of forecast operations at the National Weather Service Weather Prediction Center, who was not involved in this research, appreciated that the study touched on the “paradox of potentially fewer precipitation days with higher-intensity precipitation occurring on those days.”Carbin has noted a trend of more rain falling on fewer days in his analysis of recent precipitation.Advertisement“Overall, the area covered by the count of 1-inch rainfall days was lower than in recent years across large parts of the central and eastern U.S.,” Carbin wrote in an email. “In fact, moderate to severe drought has plagued areas of the Northeast, and extreme to exceptional drought is now expanding across part of the central U.S. Nonetheless, we have had extreme rainfall events occur in St. Louis, Dallas, eastern Kentucky, and, most recently, with Hurricane Ian in Florida.”Despite strong trends in the central and eastern United States, the authors noted that there were a few places where rainfall did not appear to be growing more intense. The paper does detail “mixed signals in the western U.S.,” but for reasons the authors are still trying to identify.“That [trend] didn’t hold true for the western U.S.,” said Harp, especially in the Pacific Northwest. He explained that changes in the overall placement of weather systems are “suppressing” any tendency for heavier precipitation in the West. A slight change in the location at which high and low pressure systems are anchored can have an enormous bearing on steering currents, and subsequently on how much precipitation falls in a given location.Changing precipitation trends poses societal challengesWith the planet continuing to warm, a continued increase in rainfall intensity can be expected. That spells concern over whether existing infrastructure can handle the downpours of the future.AdvertisementCommunities may need to turn to simulated precipitation events of the future to guide building decisions, Horton said.The past summer was a testament to the extent to which climate-supercharged downpours can wreak havoc on major metropolitan areas. During a five-week span in July and August, five 1,000-year rain events — or extremely heavy rainfall episodes that have a 0.1 percent chance of happening in any given year — occurred across the nation.“Recent years have seen some remarkable high-end precipitation events,” Carbin said.GiftOutline Gift Article",Study finds climate change is bringing more intense rains to U.S.
555,-1,-1_new_said_study_people,https://www.visaliatimesdelta.com/story/news/2022/10/31/high-poverty-l-a-neighborhoods-poor-pay-more-internet-service-delivers-less/10652544002/,"Bobbi MurrayCapital and MainA recent study by the California Community Foundation and Digital Equity Los Angeles laid out the stark differences in pricing and availability of internet services from two of L.A. County’s internet service providers — dominant ISP Charter Spectrum and Frontier.In the San Fernando Valley, Charter Spectrum offers residents of a Sylmar neighborhood with a 7% poverty rate $30 monthly service, guaranteed to stay locked in for two years. Another Sylmar community, with a 27% poverty rate, receives an offer of $70 per month with a one-year price guarantee.The two neighborhoods are a mile away from one another.“The highest speeds at the lowest costs are offered to the wealthiest and whitest communities,” lead study author Shayna Englin told a community Zoom meeting with representatives from Digital Equity Los Angeles.Page after page of the study outlines disparities similar to the example in Sylmar. The study is confined to Los Angeles County, so its focus is on Charter Communications, or Charter Spectrum, whose internet service is available to 97% of county households.Low income, minority neighborhoods may not only face pricing discrepancies but slower internet service, with fewer MBPS (megabits per second), according to a study of 38 cities in the U.S. by news organizations the Markup and the Associated Press. Download speeds were substantially faster in high income, white-dominant neighborhoods. (The Los Angeles area was not included in that study.)The California Community Foundation/Digital Equity L.A. analysis notes that in Los Angeles County, residents of high-poverty areas are routinely offered slower service at higher prices. (Disclosure: the California Community Foundation is a financial contributor to Capital & Main.)“It’s not because people don’t know what the problem is, it’s just the urgency and the depth of the problem is still kind of hidden,” says Ana Teresa Dahan, managing director at GPSN (formerly Great Public Schools Now). “Everyone feels like they have access to the internet because we have cellphone plans.”GPSN has worked on a variety of education issues but students’ struggle to learn online during the pandemic has made digital equity a priority for the organization.Michael Picker explains the problem in terms of who runs the show. That’s the large telecom companies at the table both at the federal level and in Sacramento. Picker knows; he served as president of the California Public Utilities Commission for five years. He has sat at the table with company representatives and walked the halls in Sacramento urging legislators to give the PUC more leeway on oversight.When the communications companies were deregulated in the 1990s, he says, the Public Utilities Commission became “toothless in the face of federal preemption.“States have no authority over regulation,” Picker says, “which works to the advantage of Comcast, AT&T, Verizon. The same way we don’t have the ability to set rates, we don’t have the ability to regulate.“If you can’t set rates, what leverage do you have?”The 2020 Federal Infrastructure and Jobs Act includes $65 billion for broadband investment, with $42.45 billion of it allotted to the Broadband Equity, Access and Deployment (BEAD) program.Providers are scrambling to pick up the largesse that builds internet access bridges to low income communities. It could be seen as building a customer base — when the federal money runs out, they will have a new set of consumers and all their data in order to sell them a new product.Dahan of GPSN says that grassroots groups are using the Slower and More Expensive study to figure out the way forward. “I think that as we learn what the different opportunities and solutions are, we’re going to amplify them.“I think what our report is trying to say is at minimum we should have some type of pricing transparency that can then lead to solutions on what pricing equity would look like,” Dahan says.“What we’re trying to do is figure out how to get concessions from them, on these investments on the infrastructure they need for their business. What is the government asking for in return?” A minimum demand would be pricing transparency so customers could compare offered rates.Picker has three words of advice: Joint Powers Authority (JPA). Bring together two local government agencies and create a JPA. A JPA can purchase fiber connections and become a utility. “You can fit the definition of a utility because you’re putting up fiber and serving customers.“Then you have real power — because you are competing; you have fiber, you can lease some of it to these internet service providers and make them serve all the customers. So, then you get to set the rates.”It’s a long-game strategy, he admits.“You need a plan,” Picker says. “You need power and you need the power to follow through.”","In high-poverty L.A. neighborhoods, the poor pay more for internet service that delivers less"
554,-1,-1_new_said_study_people,https://www.vice.com/en/article/y3pedm/workers-at-combined-starbucks-and-amazon-store-file-for-union-election,"Workers at a combined Starbucks and Amazon store in Times Square filed a petition for union election Friday morning, saying they’re required to do the responsibilities of two jobs for the pay of one.This is the first petition filed at Starbucks-Amazon combination store, which is only the second of its kind—the two companies opened their first joint venture in late 2021 in upper-Midtown Manhattan. Both companies are also separately experiencing unprecedented labor action. There are over 250 unionized Starbucks locations, seven of which are in New York City. Amazon warehouses, too, have been seeing efforts toward unionization, though only one in Staten Island has been successful so far.AdvertisementAccording to a spokesperson for Starbucks Workers United, the union which has successfully organized seven other cafes in downtown Manhattan, the Times Square location has high turnover, and some workers say they were transferred there from other Starbucks locations involuntarily.The stores feature Amazon Go’s famous “Just Walk Out” technology, meaning that there are no cashiers—customers just scan their phones, pick out what they want, and leave. The Starbucks counter is for mobile pick-up only, and there is a lounge area with seating for customers to eat or work. Because of its convenient location on the ground floor of the New York Times building, it has a high volume of customers.Workers say they’re required to fulfill the responsibilities of both a Starbucks employee and an Amazon Go employee—but only for the pay of one.“We’re unionizing at this Starbucks because we are doing Amazon work for Starbucks pay and we’re not given the proper resources to manage a store of this type in such a high volume area,” said one worker, who has been at the location for over a year and a half, in a statement. “We have partners that were coerced into working at this store using intimidation and miscommunication and not given any proper benefits when transferred here.”Both Starbucks and Amazon are well-known for their anti-union corporate stances. In Chelsea, just 25 blocks away, workers at the New York City Reserve Roastery—Starbucks’ flagship store—are entering a fourth day of their strike protesting alleged bed bug sightings and black mold. The Roastery unionized in April, but has still not successfully begun to negotiate a contract. Starbucks and Amazon union organizers held a joint protest on Labor Day demanding recognition from their companies. They marched from Starbucks CEO Howard Schultz’s residence in Greenwich Village all the way to Amazon founder Jeff Bezos’s luxury penthouse on Fifth Avenue.",Workers at Combined Starbucks and Amazon Store File for Union Election
553,-1,-1_new_said_study_people,https://www.vice.com/en/article/qjkwem/a-scientist-just-mathematically-proved-that-alien-life-in-the-universe-is-likely-to-exist,"ABSTRACT breaks down mind-bending scientific research, future tech, new discoveries, and major breakthroughs. See More →Humans have spent centuries wondering if we are alone in the universe, or if there are alien beings somewhere in the vast reaches of space. Given that Earth remains the only planet that we know supports life—and we are not even sure how it arose here—it remains challenging to assess the odds that extraterrestrial life exists based on this lonely sample size of one.These limitations in our knowledge prompted the theoretical physicist Brandon Carter to propose decades ago that the presence of life on Earth does not indicate that the mysterious process of abiogenesis, in which living organisms arise from inanimate matter, is more or less likely to occur on other planets. Now, a mathematician has revisited this idea and come to a very different conclusion with a more optimistic view about the existence of alien life.AdvertisementDuring the 1970s, Carter developed an influential series of arguments based on this “selection effect” of our own existence. This view suggests that humans, as a species that lives on a planet where life emerged, cannot make objective inferences about the possibility that life may be present on other worlds, in part because we have no idea if Earth is typical of planets that might host life. For this reason, we cannot exclude the possibility that Earth may be the only world in the universe that supports living beings.This argument is widely accepted in the scientific community. But now, Daniel Whitmire, an astrophysicist who teaches mathematics at the University of Arkansas, has presented a new challenge to Carter’s assumptions that suggests “the occurrence of abiogenesis on Earth-like planets is not rare,” according to a recent study published in the International Journal of Astrobiology.Whitmire told Motherboard over email that up until last year, he was one of countless researchers who thought that the Carter argument was “unassailable.” But he started to have doubts about its foundations during the peer review process for a different paper, when an anonymous reviewer offered an analogy between abiogenesis and human conception that inspired Whitmire to counter these long-held assumptions.In order to rethink Carter’s assertion that we can’t judge if abiogenesis on Earth was easy or hard, Whitmire draws a comparison to his own existence, noting that he is here regardless of whether his conception, or origin, was easy or hard. For the purposes of this thought experiment, conception would be “hard” if contraception was used, and “easy” if it was not used. The basic idea is that, rather than a person’s existence not telling us anything about whether conceiving them was easy or hard, it can be shown mathematically that it was most likely easy.Advertisement“The Conception analogy stuck in my mind and ultimately I came to believe that the Carter argument must be wrong,” Whitmire said. “But at that point I didn't know why it was wrong.”To tug on this thread, Whitmire developed a mathematics-based argument that builds on the analogy with the help of the so-called “old evidence problem” in Bayesian Confirmation Theory, which concerns the incorporation of newly-acquired data into existing hypotheses.The details are pretty complicated, but the gist is that while Carter holds that old evidence (i.e. the existence of life on Earth) has no influence on the probability of its occurrence elsewhere, Whitmire’s paper attempts to show that, actually, this “old evidence” does in fact increase the probability of it occurring in the first place. Under this novel framework, both abiogenesis on Earth and Whitmire’s conception are more likely to have been easy than hard, which suggests that life on other planets may be common.These ideas are a bit heady, and Whitmire notes that they might not actually have that much of an impact on anyone’s hopes or doubts about the probability of alien life existing somewhere in space.“My opinion is that what many scientists believe about life and intelligent life in the universe is almost political or psychological,” Whitmire said. “If they want to believe life is rare they will point to the Carter argument or some other argument, like the statistical improbability of abiogenesis, to make their case.”AdvertisementWhitmire's formula. AB means abiogenesis, and LoE means life on Earth.Likewise, Whitmire added that those who want to believe life is abundant can find evidence for that position in other studies, including his new paper.“There is no reason that I know of (outside of my paper) for any objective optimism about abiogenesis being easy, yet this is the belief of most astrobiologists in spite of the Carter argument,” he said. “Perhaps my paper will give some objective credence to this subjective belief. That said, I think that arguments like mine and Carter's have some influence but the dominant attitude is that since none of the arguments are 100 percent, only future observations will decide.”Fortunately, we live in an era packed with exciting missions focused on the search for extraterrestrial life, both inside our solar system and beyond it. NASA’s Perseverance rover is currently searching for signs of ancient life on Mars, and future missions may scout out Jupiter’s moon Europa or Saturn’s moon Enceladus, which are both considered to be potentially habitable. Next-generation observatories, including the James Webb Space Telescope, have the capacity to spot signs of life (or biosignatures) on planets in other star systems, and astronomers are scanning the skies for messages from technologically advanced aliens.Of course, all of these efforts could come up short in the search for life beyond Earth. But just one detection of life beyond our planet—even if it were microbial, or long-extinct—would validate Whitmire’s new argument that abiogenesis is not rare in the universe.",A Scientist Just Mathematically Proved That Alien Life In the Universe Is Likely to Exist
551,-1,-1_new_said_study_people,https://www.vice.com/en/article/g5vwzq/the-pacific-ocean-is-shrinking-amasia-supercontinent,"The Pacific Ocean is shrinking. Every year, it gets about an inch smaller as the tectonic plates that the Americas sit on are pushed westward. Now, thanks to calculations by a supercomputer, scientists say that a new “supercontinent” will eventually emerge due to this process: Amasia.The current world map, with its recognizable pattern of continents and oceans, is just one snapshot of our planet in time. Earth has tried on all kinds of continental configurations over its 4.5-billion-year lifespan, including periods where almost all of Earth’s land consolidates into one giant supercontinent.AdvertisementWe currently live in the broken remains of the supercontinent Pangaea, which formed 335 million years ago and disintegrated during the rise of the dinosaurs. The existence of even older supercontinents, such as Rodinia and Columbia (Nuna), suggest that Earth is locked into a “supercontinent cycle” that sees the formation and destruction of these immense landmasses on a rough timeline of 600 million years. The cycle raises the question of what kind of new supercontinent might emerge millions of years from now, prompting scientists to propose future landmasses with names like Novopangaea, Aurica, and Amasia.To shed light on this mystery, researchers led by Chuan Huang, a geophysicist at Curtin University in Australia, simulated the future of Earth with a supercomputer. The results suggest that a new supercontinent, Amasia, will form when the Pacific Ocean shrinks into nothingness some 200 million years from now, causing North America to slam into Asia, according to a recent study published in National Science Review.The future emergence of Amasia, a portmanteau of America and Asia, has been discussed by scientists for more than a decade, but there is debate over whether this supercontinent would form “inside in,” a process known as introversion, or “outside in,” which is called extroversion. Introversion involves the closure of younger post-Pangaea oceans, such as the Indian or Atlantic, whereas extroversion indicates the closure of the Pacific Ocean, which is the oldest ocean on Earth and is shrinking at a rate of about one inch per year.Advertisement“Earth's known supercontinents are believed to have formed in vastly different ways, with two endmembers being introversion and extroversion,” said Huang and his colleagues in the study. “The former involves the closure of the internal oceans formed during the break-up of the previous supercontinent, whereas the latter involves the closure of the previous external superocean.”“With our modelling results, we speculate if the next supercontinent will likely assemble through the closure of the Pacific Ocean,” which would be extroversion, “or the Indo-Atlantic oceans,” which would be introversion, the team added.As Huang and his colleagues ran their supercomputer simulations, they noticed the strength of the lithosphere, the stiff top layer of Earth that encompasses the crust and surface, is an overlooked variable in the emergence of the next supercontinent. The oceanic lithosphere has been weakening over time as a result of Earth’s slow cooling, a shift that clearly predicts the rise of Amasia from extroversion, or the closure of the Pacific Ocean.Sign up for Motherboard’s daily newsletter for a regular dose of our original reporting, plus behind-the-scenes content about our biggest stories.“Our results show that the yield strength of the oceanic lithosphere plays a critical role in determining the assembly path of a supercontinent,” the researchers said. “We found that high oceanic lithospheric strength leads to introversion assembly, whereas lower strength leads to extroversion assembly.”“This predicts that the next supercontinent Amasia could only be assembled through the closure of the Pacific Ocean,” the team concluded.In this way, the new study offers a glimpse of our planet some 200 to 300 million years from now, when an enormous landmass could unite over the ashes of the long-lived Pacific Ocean. It’s a reminder that humanity has existed for a mere split-second in geological time, and that our planet in the deep past and far future may as well be an alien world.""Earth as we know it will be drastically different when Amasia forms,” said Zheng-Xiang Li, a professor at Curtin’s Earth Dynamics Research Group who co-authored the study, in a statement. “The sea level is expected to be lower, and the vast interior of the supercontinent will be very arid with high daily temperature ranges.""","The Pacific Ocean Is Shrinking and Will Form a New Supercontinent, Scientists Say"
548,-1,-1_new_said_study_people,https://www.usatoday.com/story/news/nation/2022/10/14/alaska-snow-king-crab-disappear/10496997002/,"Alaska officials have canceled several crab harvests in a conservation effort that sent shock waves through the crabbing industry in the region.Officials canceled the fall Bristol Bay red king crab harvest and, for the first time on record, are holding off on the winter harvest of snow crab, according to multiple reports.The decision comes after stark population declines of the animals. Data from an NOAA eastern Bering Sea survey shows a 92% decline in overall snow crab abundance from 2018 to 2021, the Alaska Department of Fish and Game confirmed to USA TODAY. The population declined 83% from 2018 to 2022 as some small crab entered the population in 2022, according to the department's Division of Commercial Fisheries.'Too special':A Pennsylvania man found a purple pearl in a restaurant clam. It's worth thousands'Ran him over':677-pound grizzly bear attacks, injures bird hunter in MontanaLast year’s snow crab harvest was 5.6 million pounds, the smallest in over 40 years.Snow crab populations dropped after a 2019 Bering Sea warming, and the causes of the population crash are probably stresses from the warmer water and increased threats from predators.“Management of Bering Sea snow crab must now focus on conservation and rebuilding given the condition of the stock,” the Alaska Department of Fish and Game said in a statement Monday.Bering Sea crab harvests as recently as 2016 grossed $280 million, according to the Seattle Times. A fleet of about 60 vessels from Alaska, Washington and Oregon typically pursue the crab, and each boat employs about six people.“It’s going to be life-changing, if not career-ending, for people,” Dean Gribble Sr., a crab boat captain who has fished for snow crab since the late 1970s, told NBC News. “A lot of these guys with families and kids, there’s no option other than getting out. That’s where the hammer is going to fall – on the crew.”The fall red king crab harvest was canceled for the second year because of the low number of mature female crabs, which can indicate of the health of the broader population.Contributing: The Associated Press","After 80% population drop in 4 years, Alaska cancels snow crab season in unprecedented move"
512,-1,-1_new_said_study_people,https://www.theguardian.com/commentisfree/2022/sep/26/physics-particles-physicists#comments,"Imagine you go to a zoology conference. The first speaker talks about her 3D model of a 12-legged purple spider that lives in the Arctic. There’s no evidence it exists, she admits, but it’s a testable hypothesis, and she argues that a mission should be sent off to search the Arctic for spiders.The second speaker has a model for a flying earthworm, but it flies only in caves. There’s no evidence for that either, but he petitions to search the world’s caves. The third one has a model for octopuses on Mars. It’s testable, he stresses.Kudos to zoologists, I’ve never heard of such a conference. But almost every particle physics conference has sessions just like this, except they do it with more maths. It has become common among physicists to invent new particles for which there is no evidence, publish papers about them, write more papers about these particles’ properties, and demand the hypothesis be experimentally tested. Many of these tests have actually been done, and more are being commissioned as we speak. It is wasting time and money.Since the 1980s, physicists have invented an entire particle zoo, whose inhabitants carry names like preons, sfermions, dyons, magnetic monopoles, simps, wimps, wimpzillas, axions, flaxions, erebons, accelerons, cornucopions, giant magnons, maximons, macros, wisps, fips, branons, skyrmions, chameleons, cuscutons, planckons and sterile neutrinos, to mention just a few. We even had a (luckily short-lived) fad of “unparticles”.All experiments looking for those particles have come back empty-handed, in particular those that have looked for particles that make up dark matter, a type of matter that supposedly fills the universe and makes itself noticeable by its gravitational pull. However, we do not know that dark matter is indeed made of particles; and even if it is, to explain astrophysical observations one does not need to know details of the particles’ behaviour. The Large Hadron Collider (LHC) hasn’t seen any of those particles either, even though, before its launch, many theoretical physicists were confident it would see at least a few.Talk to particle physicists in private, and many of them will admit they do not actually believe those particles exist. They justify their work by claiming that it is good practice, or that every once in a while one of them accidentally comes up with an idea that is useful for something else. An army of typewriting monkeys may also sometimes produce a useful sentence. But is this a good strategy?Experimental particle physicists know of the problem, and try to distance themselves from what their colleagues in theory development do. At the same time, they profit from it, because all those hypothetical particles are used in grant proposals to justify experiments. And so the experimentalists keep their mouths shut, too. This leaves people like me, who have left the field – I now work in astrophysics – as the only ones able and willing to criticise the situation.There are many factors that have contributed to this sad decline of particle physics. Partly the problem is social: most people who work in the field (I used to be one of them) genuinely believe that inventing particles is good procedure because it’s what they have learned, and what all their colleagues are doing.But I believe the biggest contributor to this trend is a misunderstanding of Karl Popper’s philosophy of science, which, to make a long story short, demands that a good scientific idea has to be falsifiable. Particle physicists seem to have misconstrued this to mean that any falsifiable idea is also good science.In the past, predictions for new particles were correct only when adding them solved a problem with the existing theories. For example, the currently accepted theory of elementary particles – the Standard Model – doesn’t require new particles; it works just fine the way it is. The Higgs boson, on the other hand, was required to solve a problem. The antiparticles that Paul Dirac predicted were likewise necessary to solve a problem, and so were the neutrinos that were predicted by Wolfgang Pauli. The modern new particles don’t solve any problems.In some cases, the new particles’ task is to make a theory more aesthetically appealing, but in many cases their purpose is to fit statistical anomalies. Each time an anomaly is reported, particle physicists will quickly write hundreds of papers about how new particles allegedly explain the observation. This behaviour is so common they even have a name for it: “ambulance-chasing”, after the anecdotal strategy of lawyers to follow ambulances in the hope of finding new clients.Ambulance-chasing is a good strategy to further one’s career in particle physics. Most of those papers pass peer review and get published because they are not technically wrong. And since ambulance-chasers cite each other’s papers, they can each rack up hundreds of citations quickly. But it’s a bad strategy for scientific progress. After the anomaly has disappeared, those papers will become irrelevant.This procedure of inventing particles and then ruling them out has been going on so long that there are thousands of tenured professors with research groups who make a living from this. It has become generally accepted practice in the physics community. No one even questions whether it makes sense. At least not in public.I believe there are breakthroughs waiting to be made in the foundations of physics; the world needs technological advances more than ever before, and now is not the time to idle around inventing particles, arguing that even a blind chicken sometimes finds a grain. As a former particle physicist, it saddens me to see that the field has become a factory for useless academic papers.","No one in physics dares say so, but the race to invent new particles is pointless"
511,-1,-1_new_said_study_people,https://www.theguardian.com/business/2022/nov/02/tech-firms-becoming-too-big-to-govern-says-uber-whistleblower,"Governments are losing the battle to regulate big tech and company insiders should step forward to expose “bad apples” in the sector, according to the Uber whistleblower.Mark MacGann – the taxi firm’s former chief lobbyist in Europe, the Middle East and Africa – leaked more than 124,000 company files to the Guardian this year, revealing how the ride-hailing company flouted laws, duped police, exploited violence against drivers and secretly lobbied governments from 2013 to 2017.Speaking at the Web Summit in Lisbon on Wednesday, MacGann said governments were still struggling to rein in major tech firms.“Governments and democracy are losing this battle in trying to regulate … big tech,” said MacGann, who is Irish.“Some of these tech firms have become too big to govern, too big to regulate and are richer and more powerful than some of the states that are trying to regulate them.”Moves are afoot to regulate the tech industry in some of its biggest markets. The EU is introducing the Digital Services Act, which addresses issues such as harmful content and ad targeting, while the bloc’s Digital Markets Act aims to tackle anti-competitive behaviour within the industry.In the UK, however, the landmark online safety bill, which creates a framework for dealing with damaging social media content, is once again being paused after Rishi Sunak became prime minister last month.Paying tribute to Facebook and Instagram whistleblowers Frances Haugen and Daniel Motaung, MacGann said the list of big tech whistleblowers was nonetheless “small”.Speaking at the Web Summit last year, Haugen said the CEO of Facebook and Instagram’s parent business, Mark Zuckerberg, should step down to make way for a leader more focused on user safety.Sign up to TechScape Free weekly newsletter Alex Hern's weekly dive in to how technology is shaping our lives Privacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy . We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply.Asked if he had a message for would-be whistleblowers among the 70,000 in attendance at the summit or those watching it online, MacGann said: “Remember why you joined, remember the power of technology, the power of telecommunications, the power of brilliant software. And don’t let a few bad apples screw it all up.”MacGann added that whistleblowers did not need to go public in order to highlight problems at their companies. But people with concerns about how their workplaces are run should step forward.“You don’t have to change your life and be the public face to try to correct the wrongdoing. But if you stand back and say nothing, then you’re going to have that on your conscience for a long time,” he said.","Tech firms becoming âtoo big to governâ, says Uber whistleblower"
510,-1,-1_new_said_study_people,https://www.theglobeandmail.com/opinion/article-the-catastrophic-threat-of-thawing-permafrost-hangs-over-us-all/,"Attendees at a permafrost conference in Dawson City, Yukon, inspect a creek bank in the Klondike where ice-rich terrain has been washed away by placer mining. Climate change threatens to unlock the carbon in sediments such as this.Julien SchroderRichard Littlemore is a Vancouver-based journalist, consultant, speechwriter, and co-author of Climate Cover-up: The Crusade to Deny Global Warming.The Klondike capital of Dawson City is one of the best places in Canada to contemplate the catastrophic consequences of thawing permafrost. Or, if you’d rather, it might be a place to ignore the implications entirely, and with confidence that you are not alone in doing so. It depends on how closely you want to look – or how desperately you want to look away.Most of Dawson – like 30 per cent to 50 per cent of Canada – is underlain by permafrost, which scientists define as ground that has remained frozen, winter and summer, for at least two years. The notion may seem chilly, even forbidding, in the south, but many northerners call permafrost “our concrete.” They use it as a base for roads and bridges, as the foundation for homes, churches and businesses. In the words of Steve Kokelj, a Carleton University geographer and one of the deans of the Canadian permafrost community, it’s “the glue that holds the northern landscape together.”But if the promising prefix “perma-” suggests that this essential base is going to remain frozen forever, you might be disappointed. Thanks to climate warming, which a recent study by the Finnish Meteorological Institute showed is advancing in the North at up to seven times the global average, permafrost is thawing at an accelerating rate.Which means that stuff is starting to break. The Northwest Territories Association of Communities said in 2017 that its annual bill for permafrost-related repairs was already $51-million – a big additional tax for a small population (44,800). And it’s going to get worse. A January study in Nature Reviews Earth & Environment predicted that permafrost-related damage will affect 30 per cent to 50 per cent of all northern infrastructure by 2050.That’s obviously bad news for everyone who lives in one of the houses that is riding a permafrost thaw slump into the sea. But there is an even greater threat that hangs over us all.According to Sue Natali from the Woodwell Climate Research Center in Woods Hole, Mass., there is currently between 1.4 trillion and 1.6 trillion metric tonnes of carbon trapped in northern circumpolar permafrost, the remains of plants and animals that, over many thousands of years, died and slipped below the surface, where they have remained frozen ever since. For scale, that’s roughly twice as much carbon as is currently contained in the world’s atmosphere, three times as much as in all the forests on Earth.And when the permafrost thaws and micro-organisms start chowing down on all that suddenly available carboniferous material, those microbes wind up breathing out the greenhouse gas (GHG) carbon dioxide (CO2) or the even more powerful methane.Next comes the truly dangerous turn, the self-reinforcing feedback that could become an irreversible global tipping point. The warming atmosphere hastens the thaw of the permafrost, which belches up more greenhouse gases, which further accelerates the warming – and so on into the crisis-ridden future. The short- to medium-term impact of this “permafrost carbon feedback” loop remains speculative and controversial but by one estimate, based on calculations from the U.S. National Oceanic and Atmospheric Administration’s 2019 Arctic report card, Canadian permafrost could be emitting greenhouse gases by 2050 at a rate equal to all of the country’s current human output. If we were including those GHG emissions in our national totals (we’re not), that would mean we would have to double our emission reductions and/or carbon-capture efforts to reach the federal government’s widely heralded target of net zero by 2050. It hardly bears considering.In the slightly longer term, an overview paper published this month in the Annual Review of Environment and Resources reported that by the end of the century, even middling forecasts show that northern permafrost will be emitting carbon at a rate higher than the current output of any country other than China.A drone's-eye view of the Yukon wilderness. Much of the land in this area lies on permafrost hundreds of thousands of years old that was never scoured by glaciers.Julien SchroderWhich brings us back to Dawson, where in late August the foremost permafrost experts in the country gathered with an unprecedented contingent of First Nations delegates for the North Yukon Permafrost Conference.It is, again, a fitting venue. The fabled home of the Klondike Gold Rush, Dawson transformed in the late 1890s from a Dene community of a few hundred to a bacchanalian metropolis of up to 30,000 miners, revellers and hangers-on before crashing back down to its current permanent population of fewer than 2,000.Honouring that history, the town presents as something of a theme park – though, because it is curated by Parks Canada as a National Historic Site, it looks more quaint than Disneyfied. By municipal decree, all the downtown building fronts must remain “in conformity with the architectural and landscape … style common in Dawson during and immediately following the ‘Gold Rush.’”At a distance, the effect is pretty convincing. The landscape part is easy – 1890s-style dirt roads and rickety wooden boardwalks where the sidewalks might be. This is also robustly practical. The heaving permafrost and winter temperatures that frequently dip below -40 would quickly turn pavement or sidewalk concrete to rubble.The buildings appear similarly authentic, as long as you don’t peek behind the facades to see the ones that are, in fact, corrugated-metal Quonset huts with false fronts. If you were to lift the skirts of the actual buildings, you also would find that nearly all of them are sitting on “cribbing,” beams and posts that can be shimmed and adjusted year after year to remain level, despite the permafrost decline. You don’t have to wander far from the main streets to see drunken, slewing (and ultimately dangerous) structures that have not been kept up.“Most of us are used to relevelling our buildings quite a bit,” says Dawson Mayor Bill Kendrick. It’s more difficult, though – and even more expensive – to manage some of the other infrastructure, he says, offering the example of the sewer and water lines that snapped a few years ago, thanks to a four-foot, permafrost-driven “deflection” along Sixth Avenue.These historic buildings in Dawson, shown in the mid-2000s, are smushing together as the permafrost thaws.Chris BeacomSome of Dawson's more picturesque buildings, as seen at 2018's Canada Day parade.Darryl Dyck/The Canadian PressSo people in Dawson are hypersensitized to the local effects of permafrost – the geohazard. But many seem to actively ignore the implications of permafrost carbon feedback. Far from trying to protect the permafrost, to keep excess carbon in the ground, many dozens of placer mines are working tirelessly to destroy the permafrost layer, to get access to the gold that still lurks in Klondike gravel. The miners, usually in operations of four to six people, set up in creek beds and spray water at the banks, blowing off any surface flora and the layers of permafrost ice, soils and fossilized material to get to the good stuff. Then they sluice out the gold, thereafter covering the landscape with washed-gravel tailings that, from the plane coming in from Whitehorse, look like castings from Dune-sized sandworms.According to the Yukon government manager of surficial geology, Jeff Bond, there are roughly 150 placer mining operations in the Dawson region, generating 90 per cent of the 80,000 ounces of gold that come out of the Yukon every year. At current gold prices, about $2,200 an ounce, that’s roughly $160-million worth of income, an estimated 87 per cent of which stays in the local economy. As Mayor Kendrick says, “When COVID devastated tourism, mining helped us weather the storm.”Here, then, is an analogy for the whole climate-change dilemma: what you know versus what you don’t want to know. As in southern Canada, where governments set emission-reduction targets and then invest in oil sands pipelines or promote oil and gas developments, there is always the tension between an uncertain threat tomorrow and a reliable income today.Against that tension, the Dawson permafrost conference was set up as a multifaceted act of reconciliation. First, of course, was the challenge of engaging and reconciling with the Indigenous people on whom the climate effects are having the most immediate and, often, devastating impact. Carleton University’s Chris Burn, another giant in the Canadian permafrost panoply, describes First Nations people in the North as “the last group who are not removed from the environment.” He says, “They realize that they are facing an existential problem, not just an interesting intellectual exercise.” Dr. Burn is also beloved in these parts as an academic who came to the Yukon to research permafrost in 1982 and has been coming back almost every summer since. Unlike too many academic researchers, who fly in with thousands of dollars of equipment and head straight into the field, neither tapping the locals for their insights nor sharing any knowledge gained in the process, Dr. Burn recognized early that “the observations of the people who actually live here are always more interesting than those I can make myself.” As if to drive home the point, 40 years later, he still gets ribbed for the unlikely riverbank location of his first camp.So, in preparing for this conference, Dr. Burn and others from the Canadian Permafrost Association (CPA) invited First Nations to participate as co-organizers. Three First Nations accepted: the Trʼondëk Hwëchʼin, the Vuntut Gwichen, and the Na-cho Nyak Dun.The outcome, according to current CPA president Kumari Karunaratne, was “amazing.” Knowing that a third of the people in the room would be First Nations elders, the scientists accepted a challenge to avoid jargon-filled, insider presentations. “They’re not dumbing it down,” Dr. Karunaratne said, “but they’re breaking it down enough to be understood.” And in the multidisciplinary world of permafrost science, that turned out to be a boon for other scientists, as well.Participants at the Dawson permafrost conference listen to scientist Antoni Lewkowicz and Peggy Kormandy, a Tr'ondëk Hwëch'in elder, describe the Moosehide Slide north of town.Julien SchroderThat signals the second point of reconciliation – the drawing together of the permafrost science community and, especially, the co-ordination of permafrost research.Like other dangerous climate forcings, the permafrost threat is so dispersed that no single jurisdiction can claim or exercise responsibility. Scientists, distributed among many institutions – and many countries – tend to work in isolation and, in Canada certainly, funding agencies prioritize short-term capital-intensive research projects, resisting long-term or operating grants. As a result, the Canadian permafrost research community has been relying on what University of Montreal geographer Oliver Sonnentag calls “a coalition of the willing,” in which “co-ordination is not funded and every person, every project is disconnected.”Thanks to at least three initiatives, however, that may be about to change.The first is being led by Carleton University geographer Stephan Gruber, who is the principal investigator for the Natural Sciences and Engineering Research Council Permafrost Partnership Network (NSERC PermafrostNet), which unites researchers from 11 universities, and partners with government agencies, industry, and Indigenous communities to monitor, predict and adapt to permafrost thaw and its consequences.Dr. Natali and the Woodwell Center are driving an even larger research and co-ordination project called Permafrost Pathways, which is building a comprehensive international monitoring network to improve tracking and modelling of Arctic permafrost and carbon fluxes – i.e., how much carbon is being emitted from permafrost as opposed to that which is being absorbed as new plant material dies and is frozen in the annual cycle. Dr. Natali and company are working to partner with local leaders and to provide local and national policy makers with the data they need to fill gaps in monitoring and modelling permafrost thaw.As mentioned above, permafrost emissions are not currently included in national carbon budgets, which means that emission-reduction targets negotiated at the 2015 Paris climate conference could be way off the mark as the world seeks to keep global warming from exceeding catastrophic levels.The bad news in the Permafrost Pathways project comes in the sudden and unfortunate departure of Russia from international permafrost science collaboration, thanks to the diplomatic break driven by the Ukraine war. Two-thirds of the world’s permafrost lies within Russian borders, so permafrost researchers in the rest of the world are eager to restore the flow of scientific information.In the meantime, there may be a greater opportunity to focus on Canadian monitoring and the Woodwell team has now engaged Dr. Sonnentag, who will be taking a leave from his University of Montreal teaching duties, beginning Jan. 1, to serve as a liaison to the Canadian research community.The Cascade Institute at Royal Roads University is launching a third co-ordination project, called the Permafrost Carbon Feedback Intervention Roadmap, again seeking to expand the conversation and reduce the scientific uncertainties that some policy makers use as a rationale to delay action.Even without working through the next stage of monitoring and analysis, however, Dr. Natali bridles at the continuing policy delay. She says, “Uncertainty is just a range of possibilities. It shouldn’t frustrate action.”She also points out that we already understand the range of consequences and says that we should be making decisions based on the high end of the range, precisely because the possible consequences are so huge. (Dr. Gruber calls permafrost carbon feedback “a trillion-dollar question for coastal cities.”)Dr. Natali concludes that, in the long run, “The science will give us the numbers. For decisions, we need the wisdom to come together as human communities.”Jimmy Johnny, a Na-Cho Nyak Dun elder, speaks at the Dawson conference.Julien SchroderOne of the most popular sources of wisdom in Dawson was a Na-Cho Nyak Dun elder named Jimmy Johnny, a presenter in a panel discussion on climate-change adaptation. Mr. Johnny is a short, wiry and quietly mirthful character who has spent his life as a horse wrangler, having gotten his first job as a guide-outfitter in 1958. And he dresses the part: cowboy boots, hat, leather vest and blue jeans that stay up out of pure stubbornness, there being too little flesh on Mr. Johnny’s bones to stop them from falling down.He told a story of a recent hunting trip in which a pack horse named Big Blue fell into a huge permafrost sinkhole: “Right before my eyes, he disappeared.” Tapping the strength of all hands and a couple of other horses, Mr. Johnny worked to pull Big Blue out, but the sloppy hole was too constricted and the horse too panicked. Facing a terrible decision, Mr. Johnny walked to a high point and phoned the horse’s owner – for permission, or maybe for his blessing. But the owner just said, “Jimmy, you’re the trail boss. You do what you have to do.”Mr. Johnny walked back to his own horse and, to the consternation of the southern hunters in the party, he pulled a .30-30 rifle from his saddle. “And then I walked around behind the horse, so he didn’t have to see.”But Big Blue was neither inattentive nor new to the world of hunters. When Mr. Johnny cocked the rifle – “click, click” – the horse bounded upward, not clear of the hole, but far enough that they were able to wrestle him the rest of the way.The moderator for the adaptation panel was a Na-Cho Nyak Dun adviser named John Meikle, who noted that there is now ample evidence of the risks we all face from climate change, and the urgency of the call to action, concluding, “Maybe it’s time to take a lesson from Big Blue and imagine Jimmy Johnny standing behind you with a .30-30.”It’s a rough metaphor, but the head-nodding and mirthless laughter that greeted Mr. Meikle’s comment suggest that, in this room, at least, there is wary consensus about the looming implications of climate change generally and permafrost specifically: Even if we’re not yet capable of jumping out of the hole, it might be time to stop digging.Editor’s note: (Nov. 1, 2022): An earlier version of this article incorrectly said Woodwell Climate Research Center is linked to Harvard. This version has been corrected.What is permafrost, and what happens when it melts across Canada?",The catastrophic threat of thawing permafrost hangs over us all
441,-1,-1_new_said_study_people,https://www.pcgamer.com/nvidia-rtx-40-series-let-down/,"Update: The following opinion piece is based on the specifications Nvidia has released for its RTX 40-series GPUs and not on our own testing of those cards.And so it begins. The next-gen graphics fest we've all been waiting for is here. And yet I'm already massively disappointed. Nvidia has pulled the wraps off its new RTX 40-series graphics, otherwise known as Ada Lovelace, and the numbers don't add up.The top-end RTX 4090 (opens in new tab) board and the AD102 GPU it contains look great from a technical perspective. But what Nvidia is doing with the two RTX 4080 boards (opens in new tab) is deeply, deeply disappointing, perhaps even cynical. Let's demonstrate that with numbers. Lots of numbers.When Nvidia launched the existing Ampere generation and the RTX 30-series roughly two years ago, the RTX 3080 (opens in new tab) series board was a slightly cut-down version of the RTX 3090 using the same GA102 chip and with around 80% of the functional units of its bigger sibling. In turn, the RTX 3070 used the next-tier GA104 GPU and delivered in the region of 55% of the hardware of the RTX 3090.Now compare that with the new Ada Lovelace series. The RTX 4080 12GB uses the AD104 chip and offers just 45% of the functional units of the RTX 4090. To give one obvious example, it packs well under half the shaders of the RTX 4090—7,680 compared with 16,384. For the RTX 3080 versus the RTX 3090, it was 8,704 shaders compared with 10,496. That's 80% of the shader count.In terms of its relationship with the RTX 4090, the new RTX 4080 12GB is more akin to the RTX 3060 Ti with its 4,864 shaders. Except the RTX 3060 Ti at least had a 256-bit memory bus. The RTX 4080 12GB only has a 192-bit bus. Oh, and the RTX 4080 is $900.Seriously? A $900 card with a 192-bit bus? The RTX 4080 16GB admittedly is a bit better, what with its 256-bit bus and based on the AD103 chip rather than AD104. But it's still miles off what the RTX 3080 was to the RTX 3090.Swipe to scroll horizontally Nvidia RTX 40-series specs RTX 4080 (12GB) RTX 4080 (16GB) RTX 4090 GPU AD104-400 AD103-300 AD102-300 CUDA Cores 7,680 9,728 16,385 Base Clock 2,310MHz 2,210MHz 2,230MHz Boost Clock 2,610MHz 2,510MHz 2,520MHz Memory Bus 192-bit 256-bit 384-bit Memory Type 12GB GDDR6X 16GB GDDR6X 24GB GDDR6X Memory Speed 21Gbps 23Gbps 21Gbps Graphics Card Power (W) 285W 320W 450W Required System power (W) 700W 750W 850W Launch Price $899 $1,199 $1,599Since when did a top-tier GPU beat a lower-tier model for pure bang for buck? Since never.At first glance, it's tricky to go back one more generation to the RTX 20-series and Turing given there was no RTX 2090. But that's really just branding—the RTX 2080 Ti (opens in new tab) was equivalent to the later xx90 series boards. In that case, the gaps between the Turing tiers were bigger than those of Ampere, but still nothing like the massive drop-off with Lovelace. The RTX 2070 (opens in new tab), for instance, was well over half an RTX 2080 Ti by pretty much every measure.But perhaps the most damning indictment of what Nvidia is doing comes in the shape of value for money. At $1,600, the new RTX 4090 looks expensive enough to be irrelevant to the vast majority of gamers. But the fact that it looks like good value compared to the RTX 4080 12GB is completely crazy.Put it this way, the cost in dollars per shader, per GB of VRAM, and almost certainly per ROP, per texture unit, and per everything once the full specs are released, is lower for the RTX 4090 than the RTX 4080.Since when did a top-tier GPU beat a lower-tier model for pure bang for buck? Since never.(Image credit: Nvidia)Okay, you could argue that one way to fix all this would be to simply tweak the branding. The RTX 4080 12GB is misbranded and should be, at most, an RTX 4070 or even an RTX 4060. Let's be generous and call it an RTX 4060 Ti. But that doesn't solve the value problem. You still have a RTX 4070 or RTX 4060 that's worse value than an RTX 4090, one that gives you less hardware, not just overall but actually per dollar, too.And no, before you suggest it, the higher clocks of Ada Lovelace over Ampere don't make up for this. They would if the RTX 4080 clocked something like 50% higher than the RTX 4090. But, instead, we're expecting the gap to be a few percentage points. All of which means the RTX 4080 series looks like it will very probably suck for regular rasterized games rather than those that use lots of fancy ray-tracing effects. In other words, the vast majority of games.Hard to believe, but the RTX 4080 12GB has fewer shaders than the RTX 3080 and a much narrower memory bus. According to the best information we have, it will probably have fewer ROPs and fewer texture units, too. Yes, it has a much higher clock and Ada Lovelace's shader cores aren't directly comparable. But here's the rub. If you think the RTX 4080 12GB is going to be a big leap over the RTX 3080 in most games, just as the RTX 3080 was over the RTX 2080, you're in for a big old letdown.(Image credit: Nvidia)It had the option of a much faster RTX 4080 but chose a massively hobbled GPU at megabucks pricing.Looking forward, this is all very disappointing and worrying. If this is what the RTX 4080 looks like, what about the RTX 4070? Or the RTX 4060, how pathetic is that going to be? It's also worrying regarding what AMD has coming. At this point, Nvidia will know exactly what AMD's upcoming RDNA 3 graphics chips look like. OK, the SKUs and pricing may not be fully finalised. But Nvidia will know all the specs of the actual GPU dies themselves.And in that knowledge, Nvidia thinks the RTX 4080 is good enough. Let's be clear, the AD102 GPU in the 4090 is one heck of a chip. If the RTX 4080 was based on that chip, just as the RTX 3080 was based on the big Ampere GPU, then this would all be very different. So, it's not that Nvidia has been caught off guard or failed in technological terms. It had the option of a much faster RTX 4080, but it chose a massively hobbled GPU at megabucks pricing.Speaking of pricing, how can that be squared with what are frankly awful market conditions? After all, the world is heading into recession, ethereum mining has imploded (opens in new tab), a glut of used GPUs is sitting in the market, all the while a mountain of current-gen GPUs is lying unsold.(Image credit: Nvidia)Expect Nvidia to keep production numbers low for this generation. It knows the whole Ada Lovelace series is onto a loser in terms of market conditions. That can't be helped. It's going to be tough for the next 18 months to two years, at least, whatever Nvidia does. So, it will likely keep volumes uncharacteristically low and try to maintain higher margins courtesy of short supply, the idea being to keep very high prices for the longer term and for future generations when the market has picked up again, even if sales of Ada Lovelace suffer. That makes sense given Ada Lovelace has the potential to struggle, whatever, thanks to all those external factors.AMD may still come to the rescue with RDNA 3 (opens in new tab) and the Radeon RX 7000 series, of course. But if this disappointing Ada Lovelace launch is what Nvidia thinks is good enough compared with what AMD is planning, that can't be counted on. We'll know soon enough, RDNA 3 is set to be announced on November 3. But this is a thoroughly inauspicious start to the 2022 graphics party, that's for sure.",We've run the numbers and Nvidia's RTX 4080 cards don't add up
440,-1,-1_new_said_study_people,https://www.pcgamer.com/a-single-chip-has-managed-to-transfer-the-entire-internets-traffic-in-a-single-second/,"Audio player loading…A single chip has managed to transfer over a petabit-per-second according to research by a team of scientists from universities in Denmark, Sweden, and Japan. That's over one million gigabits of data per second over a fibre optic cable, or basically the entire internet's worth of traffic.The researchers—A. A. Jørgensen, D. Kong, L. K. Oxenløwe—and their team successfully showed a data transmission of 1.84 petabits over a 7.9km fibre cable using just a single chip. That's not quite as fast as some other alternatives with larger, bulkier systems, which have reached up to 10.66 petabits, but the key here is scale: the proposed system is very compact.By splitting a data stream into 37 sections, one for each core of a fibre optic cable, and then further splitting each of those streams into 223 channels, the researchers were able to remove a great deal of interference that slows down optical systems and therefore deliver an internet's worth of data transmission using a single chip.""You could say the average internet traffic in the world is about a petabit per second. What we transmit is two times that,"" Jørgensen says in a comment on New Scientist (opens in new tab). ""It’s an incredibly large amount of data that we’re sending through, essentially, less than a square millimetre [of cable]. It just goes to show that we can go so much further than we are today with internet connections.""The researchers also theorise that such a system could support speeds of up to 100 petabits-per-second in massively parallel systems.The research paper (opens in new tab) relies on a bank of investigations into the concept of a single chip solution across multiple researchers and papers, including one by researchers in Australia called 'Ultra-dense optical data transmission over standard fibre with a single chip source (opens in new tab)'. Catchy.Your next machine (Image credit: Future) Best gaming PC (opens in new tab): The top pre-built machines from the prosBest gaming laptop (opens in new tab): Perfect notebooks for mobile gamingEssentially, high-speed data transmission that often requires a fibre optic cable and bulky equipment is now being miniaturised into a smaller on-chip package. Instead of multiple lasers in parallel, which come with their own set of challenges, it's possible to shrink a good deal of this equipment to the silicon level. And with that even remove some of the difficulties in sending massive data packages long distances and at high speeds.A big part of these new breakthroughs are microcombs, which are a way of generating constant and measurable frequencies of light. These are not only useful for shrinking down the requirements for a system such as this, but have also recently seen breakthroughs when added to CMOS chips (opens in new tab).In fact, a whole lot more could be added to a CMOS chip to make this whole system even more integrated, says Jørgensen. So if this seems fast and compact now, it's only a matter of time before an even more integrated, speedier version is developed. Stack up more of these devices into a single parallel system and you're talking mega-bandwidth from a single server rack.Basically, the internet has a whole lot more room to grow.",A single chip has managed to transfer the entire internet's traffic in a single second
439,-1,-1_new_said_study_people,https://www.oxfam.org/en/press-releases/85-worlds-population-will-live-grip-stringent-austerity-measures-next-year,"Despite millions of people being pushed into hunger and poverty, 143 countries — including 94 developing nations — are implementing policy measures that undermine governments’ capacity to provide healthcare, education and social protection.A new report titled “End Austerity: A global report on budget cuts and harmful social reforms” shows that 85 percent of the world’s population will live in the grip of austerity measures by 2023. This trend is likely to continue until at least 2025, when 75 percent of the global population (129 countries) could still be living under these conditions.Austerity measures include scaling down social protection programs for women, children, the elderly and other vulnerable people, leaving only a small safety net for a fraction of the poorest. They also include cutting or capping the wages and number of teachers and healthcare workers, eliminating subsidies, privatizing or commercializing public services such as energy, water and public transportation, and reducing pensions and workers’ rights.Civil society organizations from across the world are launching the #EndAusterity campaign today to fight back against the wave of austerity that is sweeping across the world, supercharging inequality and compounding the effects of the cost-of-living crisis and climate breakdown.Isabel Ortiz, Director of the Global Social Justice Program at the Initiative for Policy Dialogue, said: “Decisions on budget cuts affect the lives of millions of people and should not be taken behind closed doors by a few technocrats at a Ministry of Finance, with the support of the IMF. Policies must instead be agreed transparently in a national social dialogue, negotiating with trade unions, employer federations and civil society organizations. Austerity cuts are not inevitable; in fact our report presents nine financing alternatives that are available, even to the poorest countries.”Additional analysis published today by the Financial Transparency Coalition and its partners shows that one-third less COVID-19 recovery money was spent last year compared to 2020, falling from 3.9 percent of GDP to 2.5 percent of GDP.The report “Recovery at a Crossroads: How Countries Spent COVID-19 Funds” also found that only 37 percent of COVID-19 recovery funds in 21 developing countries were invested in social protection. Meanwhile, 38 percent of these funds went to big corporations — this does not take into account tax waivers, corporate loans, and credit lines where they are not accounted for in the budgets. Smaller businesses got 20 percent of recovery funds, and informal workers 4 percent. Women have been particularly affected, since despite being hard hit by the pandemic, they only received half as much support as men.Matti Kohonen, director of the Financial Transparency Coalition, said: “Despite the cost-of-living crisis, governments in developing countries, often with their hands tied by international financial institutions, are putting big corporations ahead of the people. Nearly 40 percent of COVID-19 recovery funds went to big companies, meaning that those most impacted by the pandemic have been left behind. We should promote a people-centered recovery with progressive tax policies instead of cutting social protection and support for the most vulnerable.”Civil society organizations will kick off the #EndAusterity campaign on 28 September with a series of virtual events that will run through 30 September. These events will bring together high-profile academics and civil society activists to discuss alternatives to austerity. Alternatives include taxing corporate excess profits, eliminating illicit financial flows, canceling and restructuring sovereign debt, and increasing coverage of social security and employer’s contributions, as well as issuing new IMF Special Drawing Rights targeted to developing countries.“In the worst of times, austerity is the worst possible choice. It should not even be on the agenda. Austerity is designed to dismantle public healthcare and education and labor regulations. It enriches the wealthy and big corporations at the expense of the rest of us. Choosing austerity over many other ways to reduce deficits or even boost budget revenues, like taxing wealth and windfall profits, is not only economically disastrous — it’s deadly,” said Nabil Abdo, Oxfam International’s Senior Policy Advisor.",85% of the world's population will live in the grip of stringent austerity measures by next year
438,-1,-1_new_said_study_people,https://www.ox.ac.uk/news/2022-10-21-remote-digital-platform-working-great-if-you-live-city-oxford-study,"COVID-19 saw the rapid acceleration of workplace information and communication technologies, and the promise of remote work leading to work opportunities more evenly distributed between country and city and internationally.But the new research, published in journal PLOS One, reveals remote work conducted via online labour platforms - such as Fiverr, Freelancer and UpWork - mirrors the geographical and skills-based polarisation of labour markets, rather than spreading work more evenly.If you live in a cosmopolitan area of a developed country, you are much more likely to be employed through the digital platforms Dr Fabian BrasemannThe Oii’s Dr Fabian Brasemann, lead author of the paper says, ‘Working from anywhere is not a technical problem anymore, thanks to digitally enabled remote work. But it remains an economic-institutional one. The remote labour market is globally polarised between countries, between urban and rural areas within countries, and in particular, between job types. So, if you live in a cosmopolitan area of a developed country, you are much more likely to be employed through the digital platforms.’According to the report, ‘Countries are globally divided: North American, European, and South Asian remote platform workers attract most jobs, while many Global South countries participate only marginally….remote jobs are pulled to large cities; rural areas fall behind.’He maintains today’s findings point toward the connection between skills and place-bound institutions as enablers – even of remote work. People with access to specialised education, vocational training and local business opportunities – in other words urban dwellers – will be more likely to have in-demand, digital skills. They will find ample opportunities in the remote labour market. People who do not have the same access to enabling institutions – in other words, people in rural regions – tend not to have the most relevant digital skills. They will have a hard time finding good remote jobs.Many Global South countries participate only marginally….remote jobs are pulled to large cities; rural areas fall behindThe report states, ‘The data shows that most countries in the Global South are only marginally connected to the global web of remote work in the platform labour market. Within countries, we find that remote work flows to urban centres. These are the places where highly skilled labour is concentrated. The economic tale of the ’booming metropolis’ and the ’broken provincial city’ plays out fully in the platform economy.’Key findings reveal the global polarisation in remote labour markets:The majority of remote platform work comes from metropolitan areas in high-income countries such as North America, West Europe and AustraliaMost remote platform workers are located in urban areas in East Europe, South Asia and the PhilippinesMany countries in the Global South only marginally participate in the remote labour marketMost of the high-value remote work goes to metropolitan areas: remote platform workers in capital regions earned between 24% and 53% more per hour than their counterparts in other regions.We believe remote work can become an instrument of economic empowerment and growth...Only in regions that flourish locally, remote workers can succeed globallyThe paper recommendsPlatform apprenticeships for new remote workers: - assign first online jobs randomly to people without experience to build up their initial credibilityGovernment-led digital work programmes: - Embedding online work programmes in rural areas into larger economic and labour market development schemesFoster enablers of remote work: - investments in reliable internet access, local employment opportunities and skill-building opportunities in rural areasIncorporate remote platform work into governmental processes: - advertising short-term remote jobs on platforms while promoting living wagesConnect rural remote worker communities to global network flows: – set up co-working spaces and physical meeting points for platform workers to help with knowledge exchange and skill-buildingDr Braesemann concludes, ‘We believe remote work can become an instrument of economic empowerment and growth. But, for this to happen, remote work needs to be embedded in broader economic and labour market development schemes, supporting disadvantaged regions to invest in local skill development and infrastructure. Only in regions that flourish locally, remote workers can succeed globally.’","Remote digital platform working is great, if you live in a city - Oxford study"
436,-1,-1_new_said_study_people,https://www.overclock3d.net/news/gpu_displays/nvidia_releases_new_benchmarks_for_its_rtx_4080_graphics_cards/1,"Nvidia releases new benchmarks for its RTX 4080 graphics cardsThat's a big performance drop when compared to their RTX 4090...| Source: Nvidia Author: Mark CampbellNvidia gives gamers another look at the RTX 4080's performanceFollowing the launch of their RTX 4090 graphics card (read our review here), Nvidia has released new performance data for their RTX 4080 graphics cards.Nvidia's RTX 4080 will be releasing in two flavours, each of which offers different allotments of CUDA cores and different amounts of GDDR6X memory. Both graphics cards offer gamers completely different performance profiles, forcing us to wonder why Nvidia did not call the 16GB model the RTX 4080 Ti, or their 12GB model the RTX 4070. Nvidia's RTX 4080 16GB and RTX 4080 12GB will be launching in November with MSRP prices of $1,199 and $899 respectively.In A Plague Tale: Requiem, the vast majority of Nvidia's RTX 30/40 series lineup relies on DLSS to achieve 60+ FPS framerates. Note that this is without ray tracing enabled, which means that this game has the potential to be a lot more demanding. A Plague Tale: Requiem will support DLSS 3 at launch, giving Nvidia's RTX 40 series graphics cards a huge advantage over their RTX 30 series counterparts. Without DLSS Frame Generation, Nvidia's 12GB RTX 4080 would not be able to outperform Nvidia's last-generation RTX 3090 Ti.In F1 22, we again see that Nvidia's RTX 40 series benefits greatly from Nvidia's DLSS 3 technology, and that Nvidia's RTX 4080 12GB cannot outperform Nvidia's last-generation RTX 3090 Ti without it.Once again we see a hige performance gap between Nvidia's RTX 4090 and TX 4080 16GB, suggesting that there is space for an RTX 4080 Ti model if AMD decides to exploit this gap in Nvidia's RTX 40 series lineup.With Microsoft Flight Simulator, we once again see that DLSS 3 has a huge impact on the performance profile of their RTX 40 series GPUs. We also see that Nvidia's RTX 3090 Ti can also outperform their RTX 4080 when DLSS is disabled, even if only marginally.Nvidia's RTX 40 series relies on DLSS 3.0 to generate most of its promised performance gains. Here, the TX 4080 12GB is a lot further behind the RTX 4080 16GB than in other games. Maybe Nvidia should have called this GPU their RTX 4070, or RTX 4070 Ti.In the games above, Nvidia has given gamers a good look at what their RTX 4080 graphics cards will perform in some modern games, both with and without DLSS. DLSS 3 is a big addition for Nvidia's RTX 40 series, and you can bet that Nvidia will be working hard to get their technology integrated into as many games as possible, especially now that DLSS has competitors in the form of XeSS and FSR 2.0.You can join the discussion on Nvidia's RTX 4080 performance data on the OC3D Forums.1 - That's a big performance drop when compared to their RTX 4090... «Prev 1 Next»Most Recent Comments",Nvidia releases new benchmarks for its RTX 4080 graphics cards
433,-1,-1_new_said_study_people,https://www.nytimes.com/2022/10/07/opinion/machines-ai-employment.html,"Such developments once provoked alarm in the field. In 2016, an article in The Journal of the American College of Radiology warned that machine learning “could end radiology as a thriving speciality.” The same year, Geoffrey Hinton, one of the originators of machine learning, said that “people should stop training radiologists now” because it was “completely obvious that within five years deep learning is going to be better than radiologists.”Hinton later added that it could take 10 years, so he may still prove correct — but Handel points out that the numbers aren’t looking good for him. Rather than dying as an occupation, radiology has seen steady growth; between 2000 and 2019, the number of radiologists whose main activity was patient care grew by an average of about 15 percent per decade, Handel found. Some in the field are even worried about a looming shortage of radiologists that will result in longer turnaround times for imagining diagnoses.How did radiologists survive the A.I. invasion? In a 2019 paper in the journal Radiology Artificial Intelligence, Curtis Langlotz, a radiologist at Stanford, offered a few reasons. One is that humans still routinely outperform machines — even if computers can get very good at spotting certain kind of diseases, they may lack data to diagnose rarer conditions that human experts with experience can easily spot. Radiologists are also adaptable; technological advances (like CT scans and MRIs) have been common in the field, and one of the primary jobs of a human radiologist is to understand and protect patients against the shortcomings of technologies used in the practice. Other experts have pointed to the complications of the health care industry — questions about insurance, liability, patient comfort, ethics and business consolidation may be just as important to the rollout of a new technology as its technical performance.Langlotz concluded that “Will A.I. replace radiologists?” is “the wrong question.” Instead, he wrote, “The right answer is: Radiologists who use A.I. will replace radiologists who don’t.”Similar trends have played out in lots of other jobs thought to vulnerable to A.I. Will truck drivers be outmoded by self-driving trucks? Perhaps someday, but as The Times’s A.I. reporter Cade Metz recently pointed out, the technology is perpetually just a few years away from being ready and is “a long way from the moment trucks can drive anywhere on their own.” No wonder, then, the end of the road for truck drivers is nowhere near — the government projects that the number of truck-driving jobs will grow over the next decade.How about fast-food workers, who were said to be replaceable by robotic food-prep machines and self-ordering kiosks? They’re safe too, Chris Kempczinski, the C.E.O. of McDonald’s, said in an earnings call this summer. Even with a shortage of fast-food workers, robots “may be great for garnering headlines” but are simply “not practical for the vast majority of restaurants,” he said.It’s possible, even likely, that all of these systems will improve. But there’s no evidence it will happen overnight, or quickly enough to result in catastrophic job losses in the short term.","In the Battle With Robots, Human Workers Are Winning"
428,-1,-1_new_said_study_people,https://www.npr.org/2022/10/05/1126900340/florida-community-designed-weather-hurricane-ian-babcock-ranch-solar,"One Florida community built to weather hurricanes endured Ian with barely a scratchEnlarge this image toggle caption Carlos Osorio for NPR Carlos Osorio for NPRBABCOCK RANCH, Fla. — Like many others in Southwest Florida, Mark Wilkerson seemingly gambled his life by choosing to shelter at home rather than evacuate when Hurricane Ian crashed ashore last week as a Category 4 storm.But it wasn't just luck that saved Wilkerson and his wife, Rhonda, or prevented damage to their well-appointed one-story house. You might say that it was all by design.In 2018, Wilkerson became one of the first 100 residents of Babcock Ranch — an innovative community north of Fort Myers where homes are built to withstand the worst that Mother Nature can throw at them without being flooded out or losing electricity, water or the internet.The community is located 30 miles inland to avoid coastal storm surges. Power lines to homes are all run underground, where they are shielded from high winds. Giant retaining ponds surround the development to protect houses from flooding. As a backup, streets are designed to absorb floodwaters and spare the houses.Enlarge this image toggle caption Carlos Osorio for NPR Carlos Osorio for NPRWilkerson says he and his wife moved here from Illinois. ""We'd almost been ready to build north of Tampa, on the Gulf,"" he says. ""And then the last hurricane came through and reminded me that ... I want to be in a place where I don't have to evacuate.""Most residents chose to ride out the storm at homeSo when the storm hit, Wilkerson and his wife stayed put, as did most other residents here. Although the community didn't experience the hurricane at its most intense, Wilkerson says they felt 100-mph winds. At one point, the lights in his house flickered but ""lo and behold, we never lost power.""In fact, his house didn't even lose a shingle. That's the basic story of Babcock Ranch, post-Ian: Aside from a traffic light at the development's main entrance that's no longer there, a few street signs lying on the ground and some knocked-over palm trees, you'd hardly know that a hurricane came through.Unfortunately, not so for many of the surrounding communities, where damaged structures and power outages have not been uncommon.Enlarge this image toggle caption Carlos Osorio for NPR Carlos Osorio for NPRWilkerson has worked in the solar industry since the 1980s, and one of the things that drew him to Babcock Ranch is its innovative use of solar energy: 870 acres of land owned by the development sport 650,000 photovoltaic panels, operated by Florida Power & Light.The solar array powers the whole community — and then some. It can supply 30,000 homes. Babcock Ranch has only about 5,000 residents, though. The excess goes back into the grid and is used to power surrounding communities. At night and on cloudy days, a natural gas generator kicks in to fill the gap.Developers aim for a strong and sustainable communityBabcock Ranch is the brainchild of Syd Kitson, a 64-year-old former professional football player who made his name in the 1980s with the Green Bay Packers. He went on to found a real estate development company, Kitson & Partners, and Babcock Ranch is one of firm's showcase projects.Jennifer Languell is a sustainability engineer who helped design Babcock Ranch, and she lives here too. ""We felt you could develop and improve land, not just develop in a traditional way where people think you are destroying the land.""""We have a lot of open spaces. We have a lot of trails. We have a lot of parks,"" she says.""The things that we do, you don't see. The strength of the buildings, or the infrastructure that deals with stormwater, or the utilities. You don't see that stuff,"" she says. ""Which is good, because most people don't need or want to think about it.""Enlarge this image toggle caption Carlos Osorio for NPR Carlos Osorio for NPRAs confident as Languell is of the community's durability, even she was a little unnerved by the storm's sheer strength. ""I can definitely tell you that I pulled up my construction drawings and I verified the wind speed,"" she says.Their good fortune pays dividends for others in needAdmittedly, Babcock Ranch has a slightly insular feel to it. But partly because residents were spared the full wrath of the hurricane, they have been able to reach out and help those in need.A community center here was designed to double as a reinforced storm shelter. Everyone staying there right now has come in from other hard-hit communities. Babcock Ranch residents have been fielding requests on social media and shuttling in supplies.Enlarge this image toggle caption Carlos Osorio for NPR Carlos Osorio for NPRJudith Schrag, 70, who uses a walker, is sitting out front of the shelter smoking a cigarette. She arrived at the Babcock Ranch shelter a few days agoafter her Port Charlotte apartment was flooded out.The community has been ""absolutely phenomenal in terms of donations,"" Schrag says. ""They are what have helped to keep this place going.""Enlarge this image toggle caption Carlos Osorio for NPR Carlos Osorio for NPRHurricane Ian was a big test for this community, where houses start at around $250,000. Languell says the storm provided ""proof of concept"" for the community's design. The developers of Babcock Ranch welcome imitators, she adds. Communities elsewhere in the U.S. might benefit from what has been learned here.But there's still more to learn, Languell says.""We don't want to brag by any stretch of the imagination, because you do that, and the next thing you know, you get hit by a Category 5 and something doesn't work as well,"" she says.",One Florida community built to weather hurricanes endured Ian with barely a scratch
426,-1,-1_new_said_study_people,https://www.nih.gov/news-events/news-releases/hair-straightening-chemicals-associated-higher-uterine-cancer-risk,"Hair straightening chemicals associated with higher uterine cancer riskNIH study finds Black women may be more affected due to higher use.>Women who used chemical hair straightening products were at higher risk for uterine cancer compared to women who did not report using these products, according to a new study from the National Institutes of Health. The researchers found no associations with uterine cancer for other hair products that the women reported using, including hair dyes, bleach, highlights, or perms.The study data includes 33,497 U.S. women ages 35-74 participating in the Sister Study, a study led by the National Institute of Environmental Health Sciences (NIEHS), part of NIH, that seeks to identify risk factors for breast cancer and other health conditions. The women were followed for almost 11 years and during that time 378 uterine cancer cases were diagnosed.The researchers found that women who reported frequent use of hair straightening products, defined as more than four times in the previous year, were more than twice as likely to go on to develop uterine cancer compared to those who did not use the products.“We estimated that 1.64% of women who never used hair straighteners would go on to develop uterine cancer by the age of 70; but for frequent users, that risk goes up to 4.05%,” said Alexandra White, Ph.D., head of the NIEHS Environment and Cancer Epidemiology group and lead author on the new study. “This doubling rate is concerning. However, it is important to put this information into context - uterine cancer is a relatively rare type of cancer.”Uterine cancer accounts for about 3% of all new cancer cases but is the most common cancer of the female reproductive system, with 65,950 estimated new cases in 2022. Studies show that incidence rates of uterine cancer have been rising in the United States, particularly among Black women.Approximately 60% of the participants who reported using straighteners in the previous year were self-identified Black women, according to the study published in the Journal of the National Cancer Institute. Although, the study did not find that the relationship between straightener use and uterine cancer incidence was different by race, the adverse health effects may be greater for Black women due to higher prevalence of use.“Because Black women use hair straightening or relaxer products more frequently and tend to initiate use at earlier ages than other races and ethnicities, these findings may be even more relevant for them,” said Che-Jung Chang, Ph.D., an author on the new study and a research fellow in the NIEHS Epidemiology Branch.The findings are consistent with prior studies showing straighteners can increase the risk of hormone-related cancers in women.The researchers did not collect information on brands or ingredients in the hair products the women used. However, in the paper they note that several chemicals that have been found in straighteners (such as parabens, bisphenol A, metals, and formaldehyde) could be contributing to the increased uterine cancer risk observed. Chemical exposure from hair product use, especially straighteners, could be more concerning than other personal care products due to increased absorption through the scalp which may be exacerbated by burns and lesions caused by straighteners.“To our knowledge this is the first epidemiologic study that examined the relationship between straightener use and uterine cancer,” said White. “More research is needed to confirm these findings in different populations, to determine if hair products contribute to health disparities in uterine cancer, and to identify the specific chemicals that may be increasing the risk of cancers in women.”This team previously found that permanent hair dye and straighteners may increase breast and ovarian cancer risk.Grant Numbers: Z01-ES044005, Z1AES103332-01About the National Institute of Environmental Health Sciences (NIEHS): NIEHS supports research to understand the effects of the environment on human health and is part of the National Institutes of Health. For more information on NIEHS or environmental health topics, visit https://www.niehs.nih.gov or subscribe to a news list.About the National Institutes of Health (NIH): NIH, the nation's medical research agency, includes 27 Institutes and Centers and is a component of the U.S. Department of Health and Human Services. NIH is the primary federal agency conducting and supporting basic, clinical, and translational medical research, and is investigating the causes, treatments, and cures for both common and rare diseases. For more information about NIH and its programs, visit www.nih.gov.NIH…Turning Discovery Into Health®",Hair straightening chemicals associated with higher uterine cancer risk
422,-1,-1_new_said_study_people,https://www.newscientist.com/article/2343357-hair-follicles-grown-in-the-lab-in-a-step-towards-hair-loss-treatment/,"By modifying the embryonic skin cells of mice, researchers created hair follicles that grew up to 3 millimetres long over one monthA hair follicle generated from hair organoids – tiny, simple versions of an organ Yokohama National UniversityMature hair follicles have been grown in a laboratory for the first time, in a move that could one day treat hair loss.Artificially producing hair follicles has historically been very difficult, says Kairbaan Hodivala-Dilke at Queen Mary University of London, who wasn’t involved in the study. “Different types of cells need different sorts of nutrients and when they’re outside the body, they need different sorts of requirements compared to when they’re in the body.”Among mammals, hair follicles are typically produced in embryos as a result of interactions between skin cells and connective tissue.AdvertisementTo better understand these interactions, Junji Fukuda at Yokohama National University in Japan and his colleagues studied hair follicle organoids – tiny, simple versions of an organ.By controlling the organoids’ structure, the team was able to enhance hair follicle growth.“We examined various conditions, including growth factors, activators and inhibitors of signalling pathways and essential culture medium components,” says Fukuda.Read more: Long covid symptoms may include hair loss and ejaculation difficultiesThe team’s main breakthrough was culturing mice embryonic skin cells in a special type of gel, which allowed the cells to be reprogrammed into hair follicles.“If you think of a hair follicle, it’s got the hair down the middle of it and then it’s got layers of epithelial cells around the follicle and other specialised cells,” says Hodivala-Dilke. The gel allows these cells to grow in a laboratory in a way that means they can climb over and around each other [like they do in the body], she says.The hair follicles grew for up to one month, reaching up to 3 millimetres long. “This is probably related to the fact that the hair cycle of mice is about one month,” says Fukuda.The team is now working to recreate the experiment using human cells.According to Hodivala-Dilke, laboratory-grown human hair follicles could one day treat hair loss. “You might be able to take hair from someone whose hair is really lush and make it grow in the lab and then use those follicles to do a transplant,” she says. Existing hair transplants involve moving hair from one part of the body to an area that is thinning or bald, which can cause scarring.“This discovery is not going to cure hair loss, but it lays the foundation for somebody to potentially do so,” says Hodivala-Dilke.Journal reference: Science Advances, DOI: 10.1126/sciadv.add4603",Hair follicles grown in the lab in a step towards hair loss treatment
419,-1,-1_new_said_study_people,https://www.newscientist.com/article/2337042-two-atomic-clocks-have-been-quantum-entangled-for-the-first-time/,"Researchers have quantum entangled atomic clocks, allowing them to be synchronised more accurately. Such entangled clocks could be used to study dark matter and gravity more preciselyAtomic clocks use lasers and atoms to record time extremely accurately Andrew Brookes/National Physical Laboratory/Science Photo LibraryTwo atomic clocks have been connected using quantum entanglement – a property that intrinsically links them so that changes in one instantaneously affect the other. The connection makes it easier to synchronise the clocks, which could be used to make more accurate measurements of dark matter and gravity.Atomic clocks consist of atoms that are very precisely controlled by lasers. Each “tick” corresponds to a frequent and measurable change in energy that occurs in the atoms’ electrons. The result is …",Two atomic clocks have been quantum entangled for the first time
417,-1,-1_new_said_study_people,https://www.nbcnewyork.com/news/national-international/the-world-is-running-out-of-helium-worrying-doctors/3918574/,"A global helium shortage has doctors worried about one of the natural gas’s most essential, and perhaps unexpected, uses: MRIs.Strange as it sounds, the lighter-than-air element that gives balloons their buoyancy also powers the vital medical diagnostic machines. An MRI can’t function without some 2,000 liters of ultra-cold liquid helium keeping its magnets cool enough to work. But helium — a nonrenewable element found deep within the Earth’s crust — is running low, leaving hospitals wondering how to plan for a future with a much scarcer supply.Get Tri-state area news and weather forecasts to your inbox. Sign up for NBC New York newsletters.“Helium has become a big concern,” said Mahadevappa Mahesh, professor of radiology at the Johns Hopkins School of Medicine Baltimore. “Especially now with the geopolitical situation.”Helium has been a volatile commodity for years. This is especially true in the U.S., where a Texas-based federal helium reserve is dwindling as the government tries transferring ownership to private markets.Until this year, the U.S. was counting on Russia to ease the tight supply. An enormous new facility in eastern Russia was supposed to supply nearly one-third of the world’s helium, but a fire last January derailed the timeline. Although the facility could resume operations any day, the war in Ukraine has, for the most part, stopped trade between the two countries.Read the full story here at NBCNews.com.","The World is Running Out of Helium, Worrying Doctors"
416,-1,-1_new_said_study_people,https://www.nbcnews.com/health/sexual-health/sexual-assault-emergency-visits-increased-rcna53171,"Emergency department visits related to sexual assault increased more than tenfold over a span of 13 years, according to a new study that experts and advocates say reflects a growing cultural shift around confronting sexual assault.The research, published Thursday in JAMA Network Open, showed that those visits increased 1,533% from 2006 to 2019 — a jump from 3,600 annual visits to 55,200.The largest increase occurred between 2015 and 2016, when visits went from about 17,700 to 47,700, according to the findings.The study authors said their research represents the largest longitudinal study of sexual assault-related visits to emergency rooms in the U.S. The analysis relied on federal data from the Nationwide Emergency Department Sample, which tracks emergency department visits, as well as the FBI’s Uniform Crime Reporting Program, which compiles data from 18,000 law enforcement agencies.Keme Carter, an emergency physician and associate professor of medicine at the University of Chicago who has researched the medical treatment of sexual assault victims, said she, too, thinks the new research ""is and will continue to be a landmark study.""The authors speculated that a combination of factors drove the trend: an increased number of sexual assaults, population growth and awareness-raising social movements like #MeToo.""We’ve moved a long way, thankfully, in the acknowledgement that any time there’s non-consensual sexual activity, that is sexual assault,"" said study co-author Erica Marsh, a professor of obstetrics and gynecology at the University of Michigan Medical School.Scott Berkowitz, president and founder of the Rape, Abuse & Incest National Network, who was not involved with the study, called the results ""really encouraging.""""There’s been a huge incremental change in awareness over the last couple decades, which I think #MeToo really accelerated,"" he said.According to the data, most visits to emergency departments following sexual assault were by young adult women. Over 90% of the victims who visited emergency rooms were women, and young people between 18 and 25 accounted for over 40% of annual visits.Women and young people experience sexual assault at higher rates than other demographics, according to RAINN.Lower-income people were also an overrepresented group, the study found.The rise applied to men as well: Their share of assault-related ER visits rose from 3.9% in 2007 to slightly above 8% from 2016 to 2019, the study noted.About 1 in 33 American men have experienced an attempted or completed rape in their lifetime, compared to 1 in 6 women, according to RAINN.Among the study's other findings were that the share of assault victims who were admitted to the hospital after going to the ER decreased by 8.3%. That could be because cultural understandings of what constitutes sexual assault have expanded beyond encounters involving extreme physical violence.Additionally, the results showed that the rise in sexual assault-related visits to emergency departments outpaced the increase in reports to law enforcement. The latter grew 23% during the study period. The number of reports to law enforcement is much larger than emergency visits, though. In 2019, for example, there were more than 139,800 reports of sexual assault to law enforcement compared to 55,296 ER visits.Sexual assault overall makes up just 0.06% of emergency room visits, according to the study. The authors cited previous research suggesting that just 21% of sexual assault survivors seek medical care afterward.Marsh and Berkowitz both emphasized that medical care following an assault is crucial in order to test for and prevent sexually transmitted infections and viruses, as well as to collect evidence that a victim may want to provide to police.The new study comes with limitations, however. For one, the NEDS database that logs ER visits could represent patients who visited emergency departments more than once.The FBI data, meanwhile, relies on voluntary reports from law enforcement and narrow definitions of sexual assault, according to the study. And, of course, it does not capture sexual assaults that went unreported — which more than two in three do, according to RAINN.Also impacting the data, potentially, is a 2015 change in how the International Classification of Diseases, a diagnostic tool maintained by the World Health Organization, characterized sexual assault during the study period. Prior to 2015, the ICD offered only one code to describe adult sexual abuse; starting in 2015, it offered multiple, more specific codes.Furthermore, the study does not account for transgender and nonbinary people, who experience sexual violence at especially high rates, according to RAINN.","Sexual assault-related ER visits increased more than tenfold since 2006, study finds"
415,-1,-1_new_said_study_people,https://www.nbc4i.com/news/local-news/groveport/move-over-diesel-ohio-gets-first-of-its-kind-renewable-gas-station/,"GROVEPORT, Ohio (WCMH) — A new kind of gas station has just opened for business in Groveport, and its first client is a massive national corporation.Calling it the “first station of its kind,” Clean Energy Fuels Corporation cut the ribbon Wednesday on a renewable natural gas station at 5900 Green Pointe Dr. The RNG fuel it provides is intended as a replacement for diesel, and is sourced from methane in livestock manure. CEFC’s intention is an environmentally-friendly fuel, which it said cuts carbon emissions by around 300% when used instead of diesel in freight trucks.The station’s first “anchor customer” will also ensure plenty of freight trucks get that opportunity. CEFC said the Groveport station was built exclusively for Amazon’s distribution truck fleet to use RNG. However, a CEFC spokeswoman added it would be open to taking on additional local fleets interested in trying the cleaner fuel. They noted that 19 similar gas stations are under construction across the country.The new renewable natural gas station will also open up a new income opportunity for Ohio farmers. Specifically for the Groveport station, CEFC said it made a partnership with South Fork Dairy Farm to source their manure for RNG. Construction crews will also build a large tank to collect manure at the farm. This will trap methane gas as the manure decomposes, preventing it from contributing to greenhouse gas emissions and allowing it to be processed and purified to make RNG locally.(Courtesy Photo/Raleigh Gerber)From here, RNG processing tanks like the one at South Fork Dairy Farm will have a direct pipeline to RNG stations for fueling. Learn more about RNG and CEFC’s new station here.","Move over, diesel: Ohio gets âfirst of its kindâ renewable gas station"
414,-1,-1_new_said_study_people,https://www.nature.com/articles/s42005-022-00997-x,"We start by describing the dataset we have analyzed and briefly explaining the methodology we have used to build the citation network and the pairs of similar papers. Then, we proceed to study gender disparities, first at the aggregate level and then by comparing pairs of similar papers.Data descriptionWe study an American Physical Society (APS) dataset from 1893 to 2009, which contains articles’ metadata, the authors’ basic information, and the citations within the papers. The metadata consists of authors’ full names and a unique digital object identifier (DOI) of the publication in a string format. For those names that are repeated in the dataset, we used name disambiguation methods proposed by Sinatra et al.19 to detect unique authors and correctly match authors to publications (see Supplementary Fig. 1). To infer gender from names, we implemented a gender-detection procedure that combines author names with an image-based gender inference technique applied to search results from Google Images20. This combined method results in high accuracy in the gender identification of scholars from different nationalities (see Supplementary Methods). The final dataset consists of 541,448 scholarly articles published over the course of 116 years, categorized into 11 journals. Among those 541,448 papers, we were able to identify at least one participating author’s gender of 375,736 papers. We have identified 120,776 gendered names, 17,763 women and 103,013 men. The evolution of the number of authors per year is shown in Fig. 1a.Fig. 1: Rate of growth of women participation, average publications by career age, dropout rate and annual ratio of men/women self-citations. a Number of men (blue) and women (orange) authors per year. b Average number of publications by authors' career age. The shaded area shows the standard deviation. c Proportion of men and women authors who drop out compared to the remaining active authors per career age. d Normalized ratio of men/women self-citations computed from (1) during the time period of interest. The horizontal dashed line is the line of equilibrium; data points above the equilibrium line indicate a higher ratio of men's self-citation, and points below the line imply a higher ratio of women's self-citation. Full size imageHere, the notion of “gender” refers neither to the sex of the authors nor to the gender that the author self-identifies as. By the word “woman”, we mean an author whose name has a high probability of being assigned to female at birth or being identified as a woman due to facial characteristics. Given this limitation, we can safely argue that these methodologies are in accordance with social constructs and what people perceive as gender in society.Constructing citation networks and assessing similar pairsWe build the citation networks by considering each paper as a node and making a link from paper i to paper j if i includes a citation to j. We measure the similarity between two papers using the bibliographic coupling strength21,22; that is, the number of publications that both papers cite. Two papers that cover similar topics in a comparable way are assumed to include a similar set of outgoing citations. However, within subfields there is usually a handful of classic publications that are cited in most works, so their inclusion in two different papers may not indicate actual similarity, but a citation convention. To avoid such shortcomings of naive bibliographic coupling, and guarantee the significance of the overlapping set of citations, we apply a statistical test based on the hypergeometric distribution. This test controls for the incoming citations of the commonly cited papers and checks whether the size of the common set of citations is so large that it cannot be explained by randomness. The problem of identifying similar papers to assess gender disparities has also been approached recently using machine learning techniques23.To explore gender disparities, we select pairs of similar papers respectively written by men and women primary authors. Then, we compare the future incoming citations to each of the pair. This comparison allows us to detect potential inequalities in the citation patterns. We have summarized this methodology in the diagram of Fig. 2 and provided all the technical details in Methods.Fig. 2: Assessing similar pairs. We use bibliographic coupling and hypergeometric statistical tests to select couples of similar papers based on their outgoing citation activities. Then we compare their respective popularity (incoming citations). Each node and each arrow represent a paper and a citation respectively, whereas each dashed arrow represents a potential citation that is missing. The pair of papers being assessed (i and j) are shown in blue and orange, the papers cited by them in yellow, and the papers that cite them in green. The black arrow at the bottom represents a timeline showing the publication times of the papers. Full size imageAggregate gender disparity trendsTo characterize the gender disparities at the aggregate level, we first analyze the aspects of scientific production that depend primarily on individual choices and ability: in particular, productivity, dropout rate, and self-citations. Then, we discuss authorship order, which depends on the internal organization of research groups. Finally, we study the behavior of the scientific community as a whole by comparing the citations received by men and women.ProductivityWe define productivity as the number of publications that scholars produce during their career. In physics, we observe that women have a lower average number of publications compared to men across all their career ages (Fig. 1b). While in the first two years of author’s career the publication gap is closing, we observe a sudden increase in the gap from the second to the eleventh year. After this point, the publication gap starts decreasing again. These fluctuations in publication productivity can be associated with, among other things, the disproportionate family responsibilities that women have to take on compared with men24. For the aggregate results, see the productivity distributions by gender in Supplementary Fig. 4.Although a researcher’s productivity can be considered to be determined mainly by individual skills, the collaborative nature of scientific work makes it dependent on external factors such as other team members or departmental organization. Likewise, these factors, together with other aspects like social perception or family responsibilities, affect women’s motivation to keep working in academia, potentially leading to the leaky pipeline phenomenon. To quantify this phenomenon, in the next section we explore the differences in dropout rates between men and women.Dropout rateWe compute dropout as a lack of publication activity for at least five years to distinguish the authors who are active in publishing from those who have dropped out. We investigate the ratio of dropout scholars at each career age compared to the number of active scholars by gender. Figure 1c shows that women authors have a higher dropout ratio throughout their whole career. The largest gaps appear in the early career years, with a 2.28% difference between men and women in the first year and a 2.26% difference in the sixth year. The dropout rates of authors who leave academia after their first year (career age 0) are not shown in Fig. 1c. This career age presents the highest dropout rates, with 39.94% for men authors and 47.55% for women authors.Self-citationSelf-citation refers to cases where authors cite their own previous works. Self-citations increase the total citation count and the visibility of scholars25,26,27, potentially enhancing academic promotion and attention. We have measured the relative number of self-citations by all men and women authors with the following metric (r) to study the difference in self-citation ratios between the two genders over time25:$$r=\frac{\frac{ \% {{{{{{{\rm{men}}}}}}}}{{\hbox{'}}} {{{{{\rm{s}}}}}}\,{{{{{\rm{self}}}}}}-{{{{{\rm{citations}}}}}}}{ \% {{{{{\rm{men}}}}}}{{\hbox{'}}} {{{{{\rm{s}}}}}}\,{{{{{\rm{citations}}}}}}}}{\frac{ \% {{{{{\rm{women}}}}}}{{\hbox{'}}} {{{{{\rm{s}}}}}}\,{{{{{\rm{self}}}}}}-{{{{{\rm{citations}}}}}}}{ \% {{{{{\rm{women}}}}}}{{\hbox{'}}} {{{{{\rm{s}}}}}}\,{{{{{\rm{citations}}}}}}}}$$ (1)Figure 1d shows the temporal evolution of the ratio r. This result shows that women tend to cite themselves less than men and that this trend is consistent over the years (See Supplementary Table 2 for more details). Consequently, women’s visibility in the citation network is partly penalized by the higher ratio of men citing their own previous works.Another fundamental factor that affects an author’s visibility is the position in which her name appears in the list of authors. This position depends on how the whole research group is organized and, crucially, in most cases it depends on the perceived level of contribution of each collaborator.Authorship order analysisIn the majority of the scientific fields, including physics, the authorship order indicates relative contribution and seniority by putting emphasis on the first, the last, and the second positions28,29. In order to compare the positions of authors, we first discard those papers for which authorship order is alphabetical. For this purpose, we perform a string comparison of the last names of the contributing authors and consider them to be in alphabetical order if the paper has at least four authors and all of them follow this order. Around 3.54% of the papers can be considered as alphabetically ordered; in Supplementary Table 3 we detail their fraction by PACS subfield (Physics and Astronomy Classification Scheme). After discarding those papers from the analysis, we study the authorship order in each publication and compare the proportion of women and men in each position of the author list (first, second, middle and last). We perform this comparison using a two-proportion z-test (see Methods). If there is only one author in a paper, we consider her the first author. Middle authors are those between second and last in papers with more than three authors.The results show that there are more women than expected by chance in the first, second and middle author positions, and they are heavily under-represented as last authors (see Supplementary Table 4). The last author in physics papers is usually the most senior member of the team, so this trend can be explained by the later and slower rate of arrival of women, combined with their higher dropout rate throughout their career. This is in line with previous findings that women feature only rarely as the last authors in leading journals30.While the authorship order reflects how a researcher’s coworkers perceive her contribution, the collective perception of the scientific community regarding the importance of a paper is manifested in the citations of papers. In the following sections we will thoroughly compare the relative popularity of publications led by women and men.Citation centrality analysisThe flow of citations determines the visibility and recognition of papers both locally and globally. To measure the local influence of papers we use the in-degree metric, and to measure the global influence, we use the PageRank centrality. Our aim is to verify if the visibility of papers written by women is proportionate to what we expect from their overall population size. To do that, we focus on the ranking of the nodes according to their respective centrality.Understanding ranking centrality is important for three reasons. First, the authors of papers in top ranks gain more visibility for themselves and those central papers influence future citation patterns31,32,33. Second, the visibility of papers in top ranks is being exacerbated by algorithmic tools such as Google Scholar. Third, since citation networks follow a heavy-tailed distribution, those in top ranks stabilize their ranking position and give few opportunities for other papers to catch up34. Because of these network effects, it is important to study how minorities are represented in top network centrality ranks.We assigned to each paper a gender by labeling it based on its first author. Then, we analyzed the top h% in-degree/PageRank centrality of the papers. Figure 3a suggests that papers written by women have significantly lower in-degree and pagerank centrality than expected from their overall proportion. Women-led publications are substantially under-represented in the highest 20th, 30th, and 40th percentages, and the deviation between the observed and the expected proportions likewise increases in the highest rank positions. While in-degree and PageRank follow a similar trend as expected, the proportion of women with high PageRank centrality is even lower when compared to the in-degree centrality. This suggests not only that papers written by women receive less attention but also that they are disadvantaged in terms of their position within the entire citation network. Statistical tests confirm these findings (see Supplementary Table 5).Fig. 3: Women author proportions in degree and PageRank centrality, evolution of centrality difference by year and relationship between time of publication and citation. a Proportion of publications with a woman primary author per top h% of degree (black) and PageRank centrality (red). The dotted horizontal line signifies the proportion of women primary authors in the observed samples. b Citation and temporal differences between man–woman pairs of papers with validated similarity. The colors indicate the quadrant each pair belongs to (black—quadrant 1, red—quadrant 2, green—quadrant 3, and purple—quadrant 4). c Heat map showing the probability anomaly of the joint probability distribution of citation and temporal differences computed with equation (2). d Centrality differences of similar man-man pairs and similar man–woman pairs over the years. The two papers within each pair are published no more than 3 years apart, and the publication year of the pair is defined by the year of the latter paper. The lines are the mean values and the shaded areas the standard errors. The evolution of the distribution as a whole is shown in Supplementary Fig. 7 as a percentile plot. Full size imageSo far, the global gender analysis points towards a notable disparity in productivity and citation of men and women. This could be partly due to historical reasons, to the cumulative advantage that early arrival confers to men, as well as to the high dropout rate of women7. The slower rate of arrival of women (see Fig. 1a) may also play a relevant role. Together, these factors affect women’s global visibility. The question that arises from these global results is, are scholars intentionally ignoring (and therefore, under-citing) research works led by women? To explore this possibility, in the following section we study pairs of papers written by men and women that are statistically validated twins, and measure the citations that each paper receives.Pair-wise citation analysisWe identified statistically validated pairs of similar papers (one with a man as first author and the other with a woman) using the methodology described in Methods and summarized in Supplementary Fig. 2. Then, we computed the difference in the number of citations each member of the pair receives. The overall expectation is that similar pairs of papers should have a similar number of incoming citations on average. The first sign of gender bias that we have found is that, within similar pairs of man–woman papers, men get more citations in 45% of the pairs, women in 39%, and in 16% they receive the same number of citations. We performed binomial tests against the null hypothesis that men and women should be equally likely to get citations within each similar pair and obtained a strong rejection (p-value ≈ 0).To quantify men’s advantage, we computed the average citation difference between the man-led and the woman-led paper of each pair. Then we normalized it using the standard deviation of men’s and women’s citations to obtain Cohen’s d, a measure of effect size for the difference of means. We evaluated the significance of these differences using z-tests (see Methods). As shown in Table 1, men’s average citation count is significantly higher than women’s both in aggregate and when we consider each PACS subfield separately to control for potential differences in the citation biases per subfield. We obtained similar results by controlling for journal instead of subfield (see Supplementary Note 1 and Supplementary Table 10). We performed analogous analyses for last authors, finding consistent results for most subfields and journals (see Table 2 and Supplementary Table 12). The only noteworthy difference appears in PACS 80 (Interdisciplinary Physics & Related Studies), where women get more citations on average as first authors.Table 1 Differences in received citations among similar pairs of publications labeled by their first-author gender. Full size tableTable 2 Differences in received citations among similar pairs of publications labeled by their last-author gender. Full size tableIt is known that the publication time of a paper influences its citation count, and previous studies1,35 have used different strategies to control for it. To check whether the temporal difference between two papers is responsible for the citation disparity for women (an older paper has had more time to accumulate citations), we add a maximum 3-year difference restriction between two similar papers and redo the citation difference analyses. Tables 1 and 2 show that when the time constraint is applied, the citation difference between two similar publications decreases significantly (see Supplementary Tables 11 and 13 for the journal-wise analyses). The effect is stronger for first than for last authors. The subfield Interdiscplinary Physics & Related Studies (PACS 80) presents an anomalous behavior, as women have the citation advantage as first authors while men have it as last authors. In contrast to the rest of subfields, this advantage increases after applying the time constraint.However, citations have a very heterogeneous distribution, with a tiny fraction of papers gathering a huge number of citations, so these discrepancies may be caused by a few papers written by women with many citations. To mitigate the influence of such outliers, we have performed analogous tests for the difference of medians. In particular, we have used the Wilcoxon test to quantify the significance of the difference and the rank biserial correlation (rc)36 to estimate its effect size. The rc metric takes values between −1 when women have more citations in every pair and +1 when men do. The results, presented in Supplementary Tables 14 and 15, show that the apparent advantage of women in PACS 80 (and in PACS 00—General Physics) after applying the time constraint, were mostly driven by outliers, as rc is positive in all cases; although, consistent with the previous analyses, it is smaller when the time constraint is applied.Throughout these analyses, we have seen that the gender disparity within similar man–woman pairs is small (small effect sizes), but significant (p-values close to 0). However, we should be cautious when interpreting those p-values. The statistical tests rely on the assumption of independent samples, but in our methodology one paper can be part of several statistical twins, so those pairs would not be independent. The independence violation results in narrower standard errors and, in turn, lower p-values. Nevertheless, the consistency of the gender asymmetries should not be underestimated.The temporal dimension is fundamental when comparing citation counts, as the first-mover advantage plays a crucial role in scientific success37. Within similar man–woman pairs, the man’s paper is published first in 47.7% of the pairs, the woman’s paper in 41.3%, and approximately at the same time (the same year) in 11.0% of the pairs. These results point to a clear first-mover advantage by men.First-mover advantage within similar pairs of papersGiven the above results, we now seek to confirm whether the time of publication is a main driver for the citation disparity and whether the first-mover advantage in publication affects men-led papers and women-led papers similarly. We define Δ t = Y m − Y f as the year difference between the publication dates of man–woman pairs of similar papers and Δ C = c m − c f as their citation difference. We plotted the year difference Δ t against the citation difference Δ C in Fig. 3b. We likewise elaborated ten analogous plots after categorizing the data into subfields by PACS number (shown in Supplementary Fig. 5) to control for variations between subfields. Note that for this analysis we impose no time restriction between the publication times of the two papers of each pair.To verify that the disparity in citations is caused by the first-mover advantage, we first need to test whether a first-mover advantage in fact exists. If that is the case, when a man publishes first (Δ t < 0) he should get more citations (Δ C > 0) on average, but when a woman publishes first (Δ t > 0) she is the one who should get more citations (Δ C < 0) on average; that is, in Fig. 3b, quadrants Q2 and Q4 should be more populated than expected if we treated Δ t and Δ C as independent random variables. Equivalently, we should observe a negative correlation between Δ t and Δ C .To test this hypothesis, we compared the empirical joint probability distribution of Δ t and Δ C (P emp (Δ t , Δ C )) with the one that we would obtain if they were independent variables (P null (Δ t , Δ C ) = p(Δ t )p(Δ C )) by computing the probability anomaly as:$${P}_{{{{{{{{\rm{diff}}}}}}}}}({\Delta }_{t},{\Delta }_{C})=\frac{{P}_{{{{{{{{\rm{emp}}}}}}}}}({\Delta }_{t},{\Delta }_{C})-{P}_{{{{{{{{\rm{null}}}}}}}}}({\Delta }_{t},{\Delta }_{C})}{{P}_{{{{{{{{\rm{null}}}}}}}}}({\Delta }_{t},{\Delta }_{C})}$$ (2)The resulting values of P diff (Δ t , Δ C ) are shown in Fig. 3c and, as can be observed, they support the hypothesis of the first-mover advantage, since Q2 and Q4 present positive anomalies while Q1 and Q3 present negative ones. It is worth emphasizing that a positive (resp. negative) anomaly indicates higher (resp. lower) density of points with respect to a situation of no correlation between Δ t and Δ C . To quantify this trend we computed the Pearson and Spearman correlations between Δ t and Δ C , obtaining − 0.13 and − 0.34, respectively.Once the existence of the first-mover advantage has been confirmed, we need to test whether there exists an asymmetry in the relative advantage that men and women obtain when they publish first. If there is no asymmetry, the average number of citations that a woman obtains by publishing a certain number of years ahead of a man should be comparable to the number of citations that a man obtains in the equivalent situation.To verify this, we compared the citation differences of Q2 with Q4 (pairs where the earlier paper received more citations) and Q1 with Q3 (pairs where the earlier paper received fewer citations) for each temporal difference; in other words, we compared the average absolute value \(|{\Delta }_{C}|\) of points from Q2 with the average \(|{\Delta }_{C}|\) of points from Q4 for each \(|{\Delta }_{t}|=1,2,\ldots\) separately (analogously for Q1 and Q3). To perform this comparisons, we used z-tests for difference of means for each year difference (see Methods). The results of the tests for the whole dataset, shown in Table 3, indicate that men have an asymmetric advantage, gaining comparatively more citations when they publish first. We obtain similar results for each subfield (see Supplementary Table 16). The exceptions are General Physics (PACS 00) and Interdisciplinary Physics & Related Studies (PACS 80), where women get an asymmetric advantage.Table 3 Statistical tests of gender asymmetry in the first-mover advantage. Full size tableResearcher seniority as a temporal advantageWhile we have verified that the first-mover advantage plays a relevant role in the citation disparities between genders in a microscopic level, the differences between similar pairs, even if significant, are fairly small. Therefore, the temporal advantage gained by individual papers published earlier than their statistical twins may not be enough to explain the visibility differences manifested in the centrality rankings shown in Fig. 3a. As mentioned above, there are group-level temporal disparities that should also be taken into account: women’s delayed arrival, their slower rate of arrival, and their higher dropout rate, captured in Fig. 1.These factors can have dramatic effects on the distribution of seniority of researchers (see Fig. 4a), which is another potential source of inequality. As a researcher progresses through her career, she not only gathers citations, but also recognition, which in turn attracts more citations. As we observe in Fig. 4b, the proportion of male to female authors increases with career age, indicating a strong gender bias in the seniority distribution. This bias in the proportion of senior researchers is transferred to the ranking of centrality of papers (see Fig. 4c), which shows, on the one hand, that the higher ranks are occupied on average by older researchers, and on the other hand, that the average age of women authors is consistently lower throughout all ranks.Fig. 4: Seniority distribution of researchers by gender. a Number of men and women authors by their career age. b Proportion of men to women by career age. c Average age of men and women authors of papers in each top h% of degree centrality (number of citations). The inset shows the same result zooming in on the higher ranks. Full size imageThis thorough analysis indicates that temporal advantages are critical factors in the emergence of gender inequalities. From the individual’s perspective, researchers that publish a result earlier gain the first-mover advantage. Men publish earlier more frequently and obtain an asymmetrical advantage when they do so. At the population level, historical disadvantages driven by the late arrival and higher dropout rate of women cause a deficit of female senior researchers, which may explain women’s low visibility in the citation network.Historical trend in citationFinally, we hypothesize that the physics community might have been less receptive to the contribution of women in the past compared to the present. To test this hypothesis, we measure the temporal evolution of the centrality differences (Δ C ) between man–woman pairs by year and limit the publication time difference between the two papers to a trailing window of 3 years. Then, we compute the mean and standard error of Δ C for all the pairs within each window. For comparison, we perform an analogous computation for random samples of similar man-man pairs. In each time window, we matched the number of sampled man-man pairs with the number of similar man–woman pairs. We repeated the man-man computation 100 times independently and computed the average Δ C and the standard error, which we use as a baseline.",Influence of the first-mover advantage on the gender disparities in physics citations
413,-1,-1_new_said_study_people,https://www.nature.com/articles/s41598-022-22179-z,"The AD portrait provides an overview of consistent gene expression dysregulation with AD across multiple brain regions and fits within the framework that similar pathologies are found across the CNS with AD1. An advantage of the AD portrait is that it allows for insights into global patterns, although it is less likely to reveal AD alterations that are specific to a region. However, given that many treatments have similar effects across brain regions, the portrait provides a platform for evaluating treatments, such as exercise, that could reverse the global patterns. As discussed below, the results of the AD portrait with treatments matches well those from individual AD datasets from specific regions with treatments, suggesting the portrait provides a robust platform for evaluating AD and potential treatments.The top dysregulated gene was inositol-trisphosphate kinase, ITPKB (upregulated), that phosphorylates and converts the second messenger IP3 to IP4 and has been found to localize with actin32 and with amyloid plaques in human postmortem AD tissue33. While ITPKB is not commonly listed as being in the pathway for AD, its ability to affect IP3 levels that in turn influence intracellular calcium could connect it to the disease. Recent studies that support overexpression of ITPKB as causative of AD found in animal models that elevated ITBKB is linked to apoptosis and increased amyloid peptide production34 as well as TAU pathologies33,35. However, other studies suggest elevated ITPKB could be helpful in counteracting high Ca++ in mitochondria in neurologic pathologies36. Other inositol related genes dysregulated with AD were also upregulated, including ITPRID2, ITPRID, and ITPRIPL2A. The phospholipase C gene, PLCE1, that could affect IP3 levels was also upregulated. An understanding of whether or how elevated ITPKB contributes to AD remains to be determined. Interestingly, exercise reverses this overexpression pattern as discussed below.The second most dysregulated gene was the astrocyte-specific gene, GFAP (upregulated), that is upregulated during inflammation and reactive gliosis, including within AD37,38,39. Other injury related genes were BDNF, AQP4, GAP43, GJA1, SOX9, CDK2, and EGFR. Overexpression of GFAP as occurs in Alexander's disease due to GFAP mutations, can lead to AD-like pathologies40. While early elevations of GFAP are expected as neuroprotective, prolonged overexpression could contribute to AD pathologies.The Rho GTPase gene, RHOQ, was the third most dysregulated gene (upregulated) and it plays a role in actin cytoskeleton assembly41. Actin related processes were highly enriched among the top dysregulated AD genes. The transcription factor, NACC2 (upregulated), was the fourth most dysregulated gene and expression changes have been connected to Lewy bodies42. The dystrophin-related gene, DNTA (upregulated), was fifth ranked and DNTA localizes with the perivascular astrocytic endfoot and elevated levels are associated with increased AD pathologies43. Neuritin 1 (NRN1; downregulated) is involved in neuronal plasticity and associated with neurofibrillary tangles44 and was the sixth most altered gene in AD. The G protein regulating gene, RGS7 (downregulated) ranked as the seventh most disrupted gene and has been linked to multiple neuronal disorders45. While the majority of dysregulated genes are likely to have a contributory effect on AD, in future studies it will be important to evaluate which gene patterns are the most causative and which (if any) may seem like a dysregulation but are neuroprotective.41 genes from a recent human GWAS AD study25 were among the top 1000 AD dysregulated genes (Fig. 1, Supplementary File 1), including the opioid neuropeptide, PNOC, downregulated and connected to AD46, the immune related gene, C4B, upregulated and linked with AD47, and the transcription factor, BCL11A, downregulated and associated with cortical neuron differentiation48. Additional GWAS genes previously linked with AD included: ANK3, MS4A6A, AGFG2, CYC1, HLA-DRA, MEG3, MT2A, NCALD, NEU1, PSMC3, SERPINB6, and SPARC (see full list in Supplementary File 1). How GWAS AD genes interact with or affect transcription of dysregulated AD genes remains to be determined.In terms of common neurotransmitter pathways, the neuropeptides CRH, PNOC, VIP and SST (and its receptor SSTR1) were all downregulated as were the enzymes involves in GABA synthesis, GAD1 and GAD2. Potential roles for these signaling molecules in AD have been suggested46,49,50,51. Further, the GABA receptors GABBR2, GABRA1, GABRB3, GABRG2 and the glutamate receptors, GRIK1 and GRIK2 were also downregulated. While these neural signaling pathways were downregulated in AD, others were not suggesting potential specific roles for these signaling molecules in AD. As an example, elevation of GABA signaling has been proposed as a route for treatment of AD52.Genes with the highest connections to one another (per STRING) within the top 1000 AD dysregulated genes included: ACTB, GAPDH, EGRF, SNAP25, STAT3, CYCS, SNCA, ACTG1, and BDNF (Fig. 1). It is possible that highly connected genes have an oversized effect on the AD phenotype given synergistic actions, but this is speculative. SNCA, GAPDH, and CYCS are part of the KEGG AD pathway53 and SNCA plays a role in the development of amyloid plaques54. Enrichment for the highly connected genes included mitochondrion, cell junction, immune system, actin cytoskeleton, and regulation of phosphorylation which is consistent with AD pathologies. BDNF is down regulated in the AD portrait and is part of both the BDNF and MAPK signaling pathways.Forty-two transcription factors were identified with the top 1000 dysregulated AD genes also modified expression of other AD dysregulated genes (Supplementary File 2). STAT3, SOX9, ELK1, and SOX2 affected the most genes, but other transcription factors of interest included PAWR, GLIS3, AFF1, TCF3, FOXO1, BCL6, CEBPD, YAP1, RXRA, NFKB1, and NEUROD6 as these have previously been linked to AD within curated databases of diseases53,55,56,57. Transcription factors altered in AD that in turn affect expression of other AD genes could have a large effect on AD pathologies. Upregulated STAT3 contributes to astrogliosis and reversal of this pattern can mitigate AD phenotypes58. Similarly, SOX9 is elevated with AD and decreases in expression can reverse some AD markers59,60.The female and male AD portraits were highly congruent with one another with 571 and 570 common genes the top 1000 up and downregulated genes, respectively. Further, no top 1000 gene was in the opposite direction. The RRHO heat map in Fig. 2 highlights the high congruence across levels of dysregulation. When focusing on the most dysregulated genes within each sex that were not found in the other sex, there were differences in highly interacting genes whereby in females GAPDH, CYCS, SOX2, and PHGDH had strong interactions, but in males TLR2, ITGB2, NFKB1, and CD53 interacted the most strongly. Our findings are similar to a recent study combining multiple datasets that also found significant overlap between males and females with AD when examining differential expression22. However, that study also found some sex differences in enrichment pathways and more extensive sex differences with AD when using Weighted Gene Co-expression Network Analysis (WGCNA)61 to identify gene networks. Our findings may differ for a few reasons, including that we did not systematically explore enrichment differences, we did not use WGCNA, and the results are based on different datasets. Given that the male AD portrait matched at high level the individual female AD datasets used to create the female AD portrait (and vice versa) (Supplementary File 1), we do not think our approach masked sex differences, but we cannot exclude this possibility.Out of over 250 treatment datasets, the top three treatments were for exercise. A human CNS study comparing hippocampal gene expression in individuals with high versus low or high versus medium lifetime activity16 were the first and third top matches, respectively. The activity study also included a comparison of exercise modulated and AD modulated genes16 and identified some of the same genes of interest as in this study. The present approach differs in a few ways including that here multiple exercise/activity datasets were used, an exercise composite was created and used, and exercise was one of many possible treatments analyzed. The exercise composite that combined results from 11 exercise datasets, including those from human and rodents, was the second ranked treatment. In the top 20 were also two datasets examining exercise effects on the CNS in mice. Using UMAP as an alternative approach to identify the best treatments, exercise again stood out as a promising treatment (Fig. 7). These findings are consistent with a multitude of studies suggesting exercise in humans provides neuroprotective effects against development and progression of AD or related pathologies17,18,19,20,62,63,64.The potential ability of exercise to reverse AD patterns was striking. For the first and second ranked treatments 409 and 344 AD genes were reversed while only 20 and 45 genes were in the same direction, respectively. Enrichment for AD genes reversed by the top exercise treatment included cell adhesion, cytoskeletal binding, neuron projection as well as multiple entries related to blood vessels, including blood vessel morphogenesis, circulatory system development, blood vessel development (Fig. 3B). These latter categories are of interest as decreased blood flow is associated with AD65 and exercise is posited to elevate brain blood flow as part of its effect on cognition66.For the exercise composite (2nd ranked) enrichment of reversed AD genes included: transcription factor binding, actin binding, synapse organization, cell junction organization, brain development, BDNF signaling pathway, and a gene set from a recent study identifying interferon-stimulated network that is relevant to the innate immune response67. Previous studies have shown a link between interferon and exercise68 as well as possible roles for the innate immune system in the development of AD69. Whether or how exercise may invoke aspects of the innate immune response in the reversal of AD genes remains to be elucidated.CDC42 (down in AD) is of interest as it had the greatest number of interactions with other genes reversed by exercise for the top two exercise datasets (Figs. 3, 5). CDC42 is a small GTPase of the Rho-subfamily and is connected to multiple pathways relevant to AD, including MAPK signaling, actin organization, cell junction, and CNS development. The possible role of CDC42 in AD may be complex as one line of research suggests inhibition as a pathway for treatment70, while another suggests activation as an approach to offset AD-like pathologies71. BDNF (down in AD) is reversed by the exercise composite and BDNF is well studied in terms of how it is upregulated by exercise and positively affects CNS function72,73 while its role in AD is still being explored74,75. Other genes reversed by exercise that are in the BDNF pathway include CDC42, NFKB1, MAPK8, and MAPK9. Twenty-two of the AD genes reversed by the exercise composite are part of the KEGG AD pathway, including SNCA, PSEN2, CALM3, GRIN2A, NFKB1, INSR, and TUBB. As indicated above, the top AD portrait dysregulated gene, ITPKB, is reversed by exercise. Together, exercise reverses a wide range of genes involved in a number of important AD-related processes. RRHO heat map analysis suggested exercise would also advantageously affect genes outside of the top 1000 dysregulated portrait genes (Figs. 4, 5).The male and female portrait both had the same human exercise dataset as the top ranked treatment and the exercise composite ranked third in both. Further, the MetaVocano AD portrait as well as three recent meta-analysis datasets22,23,24 also had exercise as the top two treatments. While other AD meta-analysis studies exist76,77,78, full lists of genes were not provided so comparisons to treatments were not made. Importantly, exercise was also top ranked for both males and females within the two GEO datasets (GSE33000 and GSE44771)6,7 that were produced using the highest number of AD and control samples and came from prefrontal cortex and visual cortex, respectively. Results for 25 AD datasets that are brain region specific with each of the treatments are provided in Supplementary File 4. Exercise was highly ranked across the individual AD datasets with exercise ranked as the first and second best treatment for 13 of the 25 datasets. For another 8 datasets, one or more exercise treatments were in the top 10. There was not a clear pattern where tissue source affected matching to treatments and the best match to exercise (from hippocampus) was from an AD study in medial temporal cortex. The ROSMAP, Mayo, and MSBB studies each had exercise as a first or second top treatment, but interestingly there were high rankings for acetyl-L-carnitine which has been investigated for possible AD-related treatments79,80. In summary, the high ranking of exercise was robust across meta-analysis studies, across the individual datasets with the highest number of samples, and across most of the other AD datasets.The antidepressant, fluoxetine, ranked fourth and a composite of fluoxetine that included results from 13 fluoxetine treatment datasets ranked 13th. In the top 25, there were four fluoxetine datasets plus the fluoxetine composite. Some of the top connected AD genes reversed by fluoxetine included BDNF, SYN1, VAMP2, GAD2, STX1A, STXBP1, and HDAC1 (Fig. 6). In both female and male AD portraits, the fluoxetine composite ranked 11th and 12th, respectively. This finding is consistent with recent work in animals and humans that fluoxetine can be a useful treatment for AD-related conditions81,82,83. When examining AD genes reversed by both fluoxetine and exercise composites, 44 genes were common, including BDNF, and a theoretical combining of the two treatments would reverse 549 AD genes. This finding supports recent work exploring the combination of both exercise and fluoxetine for AD and other disorders84.Curcumin, the plant chemical from Turmeric, acting in cortex was ranked fifth and this is consistent with studies examining the therapeutic effects of curcumin in the treatment of AD85,86,87. However, curcumin acting in hippocampus had a slightly negative effect and datapoints from different ages had to be combined to achieve sufficient numbers. To our knowledge, a study examining curcumin effects in the CNS with a high number in each group has yet to be performed. Desipramine was in the top 10 and while animal studies indicate it can improve AD-related deficiencies88, the effects on cognition in humans with AD is less clear89. Safflower oil in a high fat diet was the 6th highest treatment, but the control in this study was flaxseed oil90, so whether the match is more due to one oil over the other is less clear. While D-serine matched as a possible treatment, the datasets for D-serine included an unusually high ratio of upregulated relative to downregulated genes overall91 and caution is needed when interpreting that result. The stimulant, cocaine, had three matches in the top 25 and while prolonged use of drugs of abuse induces clear cognitive deficits, the finding is consistent with studies exploring the ability of stimulants to mitigate some aspects of AD, such as apathy92. Overall, the ranking of treatments was similar for male and female portrait, although in males, curcumin was the second highest ranked treatment (File 1, Supplementary File 2). 25 individual AD datasets that are brain region specific were compared with each of the treatments and these can be explored in Supplementary File 4. Given that most treatments include only a few regions, comprehensive analysis that involves the same region is still limited. Future studies could focus more on data from specific brain regions (or cell types) from both AD datasets and treatments.The two treatments that had the lowest treatment scores and were most similar to AD related to alcohol abuse and were from human datasets. Thus, these treatments could be viewed as risk factors for AD. The association of alcohol and AD is complex and still being evaluated93,94, but the findings are consistent with work suggesting alcohol abuse as a risk factor for AD95.As a final step, the AD portrait was compared with a recent portrait of depression14 as comorbidity of depression and AD occurs in some individuals96,97. As shown in Fig. 8, there was a high matching of downregulated genes in both and these included BDNF, CRH, SST, GAD2, and PSEN2. The role of BDNF in depression is actively studied98,99 and a connection between exercise and increased BDNF as part of the antidepressant aspects of exercise have been evaluated100. In the depression portrait study, exercise also ranked as the top treatment14, but the extent of gene reversal was not nearly as large as for AD.The treatments examined should be viewed as theoretical as the treatment expression studies varied widely across multiple factors, including sex, species (the majority of which were from rodents), numbers, brain region, treatment length, and platform. Most of the datasets were not created with the goal of understanding how the treatment might reverse AD dysregulation patterns and an understanding of experimental design is relevant for interpretation. Also, some recent proposed treatments for AD, such as aducanumab101, do not have corresponding large scale gene expression datasets, so they could not be included in this analysis.We recently used a depression portrait to identify animal models with congruence to depression102 and ongoing useful steps could involve use of the AD portrait (or similar portraits) to evaluate and identify what animal model has the highest concordance with the AD brain signature. Advances in this area have already begun as a recent study identified mouse models that were congruent with coexpression modules found in AD24.One goal for producing the AD portrait is to gain new insights into AD but also to produce a platform for identifying and evaluating new treatments at the large-scale gene expression level potential. As shown in Supplementary File 4, a final step involved modifying the AD portrait to include six datasets that came from three studies that were not in the original portrait. The two portraits are extremely similar and ITPKB and GFAP are the top two genes in both. With new datasets and integration approaches any AD portrait will always be ‘in progress’. With evolving portraits and individual datasets, a potentially promising approach is to identify multiple complementary treatments for AD, such as with exercise and fluoxetine.",Alzheimerâs disease large-scale gene expression portrait identifies exercise as the top theoretical treatment
445,-1,-1_new_said_study_people,https://www.psychologytoday.com/us/blog/how-do-you-know/202210/facebook-caused-poor-mental-health-the-beginning,"Source: Photo by Pixabay on Pexels.There is an epidemic of mental health problems in the U.S., and social science research is increasingly finding a connection between social media and poor mental health. This has been documented in correlational studies–which examine when two things are related, without being able to show one definitely causes the other. It has also been seen in experiments, which do allow us to establish cause and effect. For example, people who were randomly assigned to give up social media reported feeling less depression and anxiety after a week, compared to those randomly assigned to keep using social media as normal.A prominent argument on this issue is that a change in the feed—what users see when they log in to Facebook (and similar social media sites)—led social media to be particularly harmful to mental health. Yet new research using an innovative design to test whether Facebook caused mental health problems contradicts this story. Facebook has been causing poorer mental health in college students since it was first rolled out among a small group of elite colleges and universities. The study also provides some evidence for why Facebook might be causing mental health problems.First, the design: This study looked at 2004 to 2006, the period when Facebook expanded from just being available to Harvard students to then being rolled out at an increasing number of colleges and universities. National surveys were being collected on the mental health of college students throughout this time, which could be connected to the dates at which Facebook was introduced on each campus. This allowed the researchers to use a new tool for establishing cause-and-effect: a difference-in-differences analysis.The analysis looks at levels of mental health problems before Facebook was introduced, and then compares it to the levels afterwards. This allows for statistical adjustments to be made that account for baseline mental health at that college and any trends that might have been going on before Facebook was introduced. The fact that there are 58 different schools, each gaining access to Facebook at different times, means that this comparison can be made over and over. The results therefore reflect the pattern seen as Facebook was introduced at each new school. While this study didn’t include an experiment (the gold standard for establishing cause-and-effect), it was able to use advanced statistical techniques to check their assumptions about a cause-effect relationship.Source: Photo by mikoto.rawPhotographer on PexelsThe key findings, which were confirmed after checking multiple assumptions, are that after Facebook was introduced to a campus, more students on that campus reported having depression and anxiety disorders. In particular, they were more likely to say they felt hopeless, exhausted, and “severely depressed.” When Facebook was introduced to a campus, more students were also reporting that mental health problems affected their academics. Further, the effect was strongest among students who were already most susceptible to mental illness. In other words, Facebook made college students already predisposed to depression or anxiety more likely to actually experience poor mental health.Prior discussions of Facebook emphasized the way the platform evolved over time. ‘It started out simple, with friends sharing photos and details of their lives,’ the popular narrative goes. ‘Only later did the ability to gain a large audience from a post drive people to change their behavior to be better picked up by the algorithm.’ The modern experience of Facebook—with direct marketing, personal brand building, and political flamethrowing—is the problem, and it’s something that emerged after people learned how to ‘game the algorithm’ of the social media site. For example, psychologist Jonathan Haidt suggests that Facebook began to cause more problems after three changes to the site: the introduction of the feed–a constantly updating timeline of posts—in 2006, the addition of the “Like” button in 2009, and later the use of internal algorithms to show content that was predicted to drive engagement. The new research shows that this isn’t true: Facebook was causing poorer mental health from the start, before these innovations were rolled out.If Facebook had a negative effect on college students’ mental health from the beginning, this suggests that it’s not just trolls, propagandists, and a new breed of bad or cynical actors on the site that caused it. It’s something that’s been baked in from the beginning. The authors of the new research paper suggest that this deeper culprit is something much older and more common in our culture: social comparison.Source: Photo by Helena Lopes on Pexel.To support this idea, the new study does analyses of several groups of students who would be expected to compare themselves negatively to others at their college or university. For example, they found that Facebook led to even large declines in mental health for students who lived off campus. These students might believe that they weren’t experiencing as much of the “full college experience” as students living on campus. Several other related categories were examined, including not being in a fraternity or sorority; having to work on top of going to school; being overweight; and having credit card debt. In each case, being in the category that does not match the “idealized” version of a college experience was related to having worse mental health—but not always at a statistically significant level.Social media allows you to see large amounts of your peers’ lives in intimate detail, which can magnify opportunities for comparing your own life to theirs. With so many more people to compare yourself to, and so much more of your time spent looking at these comparisons, college students may have naturally started to find themselves lacking. It would be nice to believe that just by clearing out bots and advertisements, we could make Facebook a healthy source of connection. But it may be the core feature of social media itself—the ability to share private moments of daily life with friends and peers—that actually causes harm.",Facebook Caused Poor Mental Health From the Beginning
453,-1,-1_new_said_study_people,https://www.psypost.org/2022/10/new-study-identifies-an-increasing-disinterest-in-fatherhood-among-childless-men-in-the-united-states-64072,"For most people throughout time, the idea of an ideal future included starting a family. Currently, over one third of American men have no children, prompting the question of why? A study published in Journal of Marriage and Family suggests that this is partially due to an increasing disinterest in fatherhood.Fertility rates ebb and flow due to many factors, including socioeconomic stability and cultural norms. In recent years, economic uncertainty and a decreasing focus on a traditional family unit seem to have led to the decrease in birth rates, which is of concern to many people due to the fact that America’s birth rate is now below replacement level. Most research on family planning and fertility focus on women, but the new research sought to understand the perspective of childless men.For his study, Robert Bozick utilized data from 3 sources: the National Survey of Family Growth, the Monitoring the Future study, and the Panel Study of Income Dynamics’ Transition to Adulthood supplement. All data was focused on the years 2000-2020.Bozick utilized data from 18,183 American men from the National Survey of Family Growth, which included questions about if participants see themselves having children in the future and how much it would bother them if they never had children. The Monitoring the Future study focused on high school seniors and asked them what number children they would have and how likely they were to want children. The Panel Study of Income Dynamics’ transition to adulthood supplement examined men aged 18 to 28. Bozick used data from 6 waves asking about the importance of family leave as an aspect of their jobs for participants.Results showed that over the past two decades, the interest in having children among childless men has decreased. In fact, the number of men reporting that they do not want children at all doubled during this time frame.Similarly, men reporting they wouldn’t be bothered if they never had kids doubled. Among high school seniors, the percentage of people who were confident they did not want children remained steady while the percentage of people reporting they were very likely to want kids decreased. Additionally, the number of men reporting that it is very important to them that their job has good parental leave decreased between 2005 and 2015.This study took important steps into better understanding the trends occurring in regard to fatherhood. Despite this, there are some limitations to note. One such limitation is that this study was only able to track descriptive trends and cannot truly answer why we are seeing these patterns.“The descriptive trends documented in this brief report clearly show that childless men are increasingly shying away from fatherhood, but the question remains: Why?” Bozick wrote in his study. “Without directly addressing this question, the contemporary research landscape of family formation and family planning is incomplete.”“On the heels of the COVID-19 pandemic and a longer-term decline in fertility rates, new questions have emerged regarding what considerations are most relevant to couples making decisions about having children—with an eye toward ensuring that couples have a broad array of options to plan for the families they so desire. Men in general, but childless men in particular, have received little attention in these scholarly conversations about family planning.”“Should the trends observed here continue, attempts at boosting fertility rates will need to consider what factors are driving this increasing disinterest among childless men,” Bozick wrote. “A logical next step is for family researchers to identify these factors – be they structural, evolutionary, cultural, or biological.”The study, “An increasing disinterest in fatherhood among childless men in the United States: a brief report“, was published July 30, 2022.",New study identifies an increasing disinterest in fatherhood among childless men in the United States
454,-1,-1_new_said_study_people,https://www.psypost.org/2022/10/new-study-suggests-trumps-2020-election-conspiracy-theories-undermined-gop-turnout-in-the-2021-georgia-runoffs-64076,"New research published in the Proceedings of the National Academy of Sciences provides evidence that voters in Georgia who embraced Donald Trump’s claims of widespread election fraud were less likely to cast their ballot in a pivotal runoff election.In the aftermath of the 2020 election, Trump made a series of baseless allegations that the election had been stolen from him. These claims were quickly debunked by election experts, but Trump continued to push these conspiracy theories in an attempt to undermine the legitimacy of Joe Biden’s victory. The study authors were interested in exploring how this rhetoric might have impacted the 2021 runoff elections in Georgia, where Democrats were able to flip two U.S. Senate seats.“Trends in U.S. politics are such that Democratic and Republican politicians are both vocally suspicious of their opponents’ respect for the ‘rules of the game’ in our democracy,” said study author Jon Green, a senior research scientist in the Network Science Institute at Northeastern University, and a fellow at the Shorenstein Center on Media, Politics and Public Policy at the Harvard Kennedy School.“To be clear, these suspicions come with very different levels of empirical support. These conspiracy theories about widespread voter fraud on the part of Democrats really are baseless. But that doesn’t change the fact that both parties’ core voters are hearing from their leaders that the other side is trying to take away their ability to vote their preferred candidates into office – either by making it harder to vote, loosening the relationship between votes and outcomes (through gerrymandering, e.g.), or through fraud that ignores legally cast votes altogether.”“And there are debates within the parties about whether this kind of rhetoric is counterproductive,” Green continued.” People who work in Democratic politics, for instance, are sometimes worried that Democrats’ allegations that Republicans are engaging in voter suppression could discourage potential Democratic voters from voting, on the margins, because it could create the impression that voting will be more difficult or inconvenient.”“During the Georgia runoffs, we saw local Republican activists raising similar concerns about these fraud allegations – that they were discouraging Republican voters from turning out because they were introducing uncertainty into whether voting was going to have any relationship with the runoffs’ outcome. So this question of whether there are any behavioral implications of this kind of elite rhetoric was what sparked my initial interest in the specific research question.”The researchers analyzed Twitter data and voter records to explore the link between the endorsement of Trump’s election fraud claims and voter turnout in Georgia’s 2021 runoff elections. The sample included 45,431 Twitter users who had a voter record in the state of Georgia.The study authors evaluated whether the Twitter users had liked or retweeted content supporting or detracting from election-fraud conspiracy theories. They also evaluated whether the users had liked or retweeted posts by the official Twitter account of then-President Donald Trump, and whether they had engaged with other prominent accounts that posted content supporting or opposing election-fraud conspiracies.Green and his colleagues found that online engagement with content detracting from election-fraud conspiracy theories was associated with higher turnout than expected in the runoff election. On the other hand, those who liked or shared tweets supporting fraud-related conspiracy theories were slightly less likely to vote.“We can’t say for sure whether the ‘Big Lie’ cost Republicans control of the Senate, but our study provides some evidence that it didn’t help their candidates in the Georgia runoffs,” Green told PsyPost. “All of our findings surprised us in the sense that there were good reasons to expect different results and we really weren’t sure what we’d find until we analyzed the data.”The researchers proposed several mechanisms by which widespread claims of election fraud could affect turnout. They had theorized, for instance, that engagement with 2020 election conspiracy theories on Twitter could possibly help to boost Republican voter turnout due to their potential to elicit anger and strengthen partisan ties.“It was reasonable to expect that endorsing election conspiracy theories would have been associated with higher turnout in the runoffs because the conspiracy theory made people angry, and prior work has linked anger to things like party loyalty and intentions to vote. So if we had observed that positive relationship, it would have made sense,” Green explained.“However, translating anger into action typically requires it to be paired with efficacy – a belief that you can do something to address the threat you perceive. Here, a central premise of the conspiracy theory – that election results in Georgia are not directly based on the votes its citizens cast – likely reduces efficacy, so the results we found also make sense.”Another proposed mechanism was negative evaluations of co-partisan candidates. In other words, voters who bought into election-fraud conspiracies might have been more likely to see Republican Senate candidates as being insufficiently supportive of Trump.“It’s also possible that the way the Republican Senate candidates campaigned turned these voters off,” Green explained. This election determined control of the Senate when Democrats had already won control of the House and presidency. If you’re a Republican running for Senate in that situation, you typically want to appeal to the (small but non-trivial) slice of the electorate that sincerely prefers divided government. You want to pitch yourself as the last check against a Democratic trifecta.”“But if you say this as a Republican candidate in the Georgia runoffs, you are implicitly acknowledging that Joe Biden is going to be the president. That is, you’re behaving as if the election was not stolen. If you’re really invested in the idea that the election was stolen and we need to do everything in our power to correct this obvious injustice, then why bother turning out to vote for a candidate like that? Again, we don’t know if that’s what drove our finding, we just can’t rule it out.”The findings are in line with press reports, which claimed that some Trump supporters in Georgia were intent on “boycotting” the runoff election because of alleged fraud. But the observational nature of the study prevents the researchers from making strong causal claims.“The big caveat is that because this is all observational, we can’t nail down why we see the relationships we see, and we can’t completely rule out that the turnout behavior is being driven by something other than the conspiracy theory,” Green said. “We don’t have causal leverage, and we spend some time in the paper talking about why causal inference is especially hard in a case like this. We also talk about a few different causal mechanisms that we see as plausible, and I’ve alluded to some above, but we can’t say for sure which mechanism(s) are producing the relationships we see.”Trump continues to claim that the 2020 election was stolen from him, but whether this rhetoric will help Democrats in other elections remains to be seen. “In terms of questions that still need to be addressed, the big one is that we don’t know how generalizable our findings are,” Green said. “Georgia’s 2021 runoff elections were unique for a couple of reasons, so it’s unclear whether we’d expect the same results given overlapping-but-not-identical conditions in another context in the future.”“I think it’s really important to keep in mind that conspiracy theories undermining faith in elections’ outcomes are bad regardless of any relationships they might have with voting behavior in the short term,” the researcher added.The study, “Online engagement with 2020 election misinformation and turnout in the 2021 Georgia runoff election“, was authored by Jon Green, William Hobbs, Stefan McCabe, and David Lazera.",New study suggests Trumpâs 2020 election conspiracy theories undermined GOP turnout in the 2021 Georgia runoffs
458,-1,-1_new_said_study_people,https://www.psypost.org/2022/10/pornography-is-not-to-blame-for-erectile-dysfunction-according-to-new-research-64109,"Can watching porn give men erectile dysfunction? A study published in the International Journal of Impotence Research suggests that pornography use does not predict problems in erectile functioning or sexual satisfaction.Pornography use is a hotly contested issue in many relationships. Since the rise of the internet, porn is easily accessible, affordable, commonly used, and able to be consumed without anyone else knowing. Pornography usage has been linked to negative outcomes, such as impersonal sexual attitudes, negative body image, more acceptance of sexual aggression, and delayed ejaculation. It also has been shown to have positive effects, such as providing sexual education and aiding in sexual dysfunction. Previous research showing pornography causes erectile dysfunction have had many methodological flaws, and this study seeks to explore that question once again.For their study, David L. Rowland and colleagues examined a sample of 3,586 men recruited online from English-speaking countries and Hungary. Respondents who were not having sex with their partner or who had never had a partner were eliminated. Participants completed a survey consisting of demographic questions, anxiety/depression, medical conditions, sexual orientation, number of current sexual partners, interest and importance of sex, relationship and sexual satisfaction, masturbation, partnered sex, and frequency of pornography usage. Participants completed measures on premature ejaculation, erectile functioning, and answered questions regarding delayed ejaculation.Results showed that factors that increased likelihood of erectile dysfunction were advanced age, anxiety, depression, medical issues, less frequent sex, lower importance of sex, and decreased sexual and relationship satisfaction. Problems with erectile functioning was a predictor of decreased satisfaction in this sample.Men with erectile dysfunction did not significantly differ in their pornography usage than men without erectile dysfunction. Despite this, there was a small effect of frequent masturbation being related to problems with erectile functioning. Pornography consumption was not linked to decreased relationship or sexual satisfaction when masturbation frequency was controlled for.This study took important steps into addressing methodological issues in previous research on this topic. Despite this, this study also has limitations to note. One such limitation is that online and self-report studies are vulnerable to bias or to participants not paying attention. Additionally, this study utilized only Western participants; future research could include cultures who have more restrictive views of sex to see if the effects differ.“Findings of this study reiterate the relevance of long-known risk factors such as age, anxiety, and relationship satisfaction for understanding impaired erectile functioning during partnered sex, but they do not support the notion that pornography use is widely associated with poorer erectile functioning or increased ED severity during partnered sex,” the researchers concluded.“Masturbation frequency appears to have discernable though weak effects on erectile functioning during partnered sex. Although further study is needed for verification, heavy reliance on pornography use coupled with a high frequency of masturbation may nevertheless represent a risk factor for diminished sexual performance and/or poor relationship satisfaction in some men (e.g., in younger, less experienced men or where mitigating cultural factors likely play a role). From a clinical perspective, these factors deserve assessment and, if relevant, may be addressed as part of a remediation component of psychosexual therapy.”The study, “Do pornography use and masturbation play a role in erectile dysfunction and relationship satisfaction in men?“, was authored by David L. Rowland, Joseph M. Castleman, Katelyn R. Bacys, Balazs Csonka, and Krisztina Hevesi.","Pornography is not to blame for erectile dysfunction, according to new research"
506,-1,-1_new_said_study_people,https://www.techspot.com/news/96463-5g-users-think-technology-has-overhyped-many-fail.html,"In brief: Remember when 5G was just about to roll out and companies promised it would revolutionize the world? Now that it's seeing more widespread adoption, do you agree with those claims? According to a new study, many people feel 5G connectivity is overhyped and have failed to notice any speed or reliability improvements since upgrading.Research from UK-based price comparison service and switching website Uswitch found that one in six 5G users feel the technology doesn't live up to its promise. Less than half said they notice faster speeds or more stable connections since jumping to the fifth-generation cellular network.One of the problems with 5G has long been its coverage in rural areas. That's an issue everywhere mobile networks are found, including the UK, where 17% of those who live outside of urban areas say they've never been able to connect to a 5G signal—three times more than those in cities.Some mobile users in the countryside can't even get reliable connections to older networks. Only 48% of people in the county of Yorkshire said they could access 4G reliably, and 14% said they often had to revert to 2G.5G advocates have talked about its lower latency and higher speeds enabling more applications for consumers, including the use of mobile virtual and augmented reality. Many point to the metaverse as one of the biggest areas that will benefit from 5G connectivity. But most people remain apathetic towards the concept, and a recent report predicting most business projects in this virtual realm will close by 2025 hasn't helped increase enthusiasm.While it might not be living up to the hype for some users, 5G still has the distinction of being the fastest-growing mobile communications technology ever. Ericsson said coverage reached roughly 25% at the end of 2021, hitting the milestone about 18 months faster than 4G. 5G is also expected to have around one billion users by the end of the year.Part of the reason why people feel somewhat disappointed in 5G is that we're yet to see its full potential; the technology is still in its relative infancy. As 5G expands—it's expected to be the dominant network with 4.4 billion users by 2027—so will the number of applications that take advantage of it. At least that's the plan.","Some 5G users think the technology has been overhyped, fail to notice speedÂ improvements"
498,-1,-1_new_said_study_people,https://www.space.com/spacex-starlink-internet-service-antarctica,"SpaceX's Starlink internet constellation is getting a serious remote-service test.SpaceX has long touted Starlink's potential as a world connector, allowing people in rural areas and other underserved communities to access high-speed internet. And Starlink broadband is now beaming into one of the most remote communities on Earth — McMurdo Station, a research outpost run by the United States Antarctic Program (USAP).""NSF-supported USAP scientists in #Antarctica are over the moon! Starlink is testing polar service with a newly deployed user terminal at McMurdo Station, increasing bandwidth and connectivity for science support,"" the U.S. National Science Foundation said via Twitter on Wednesday (opens in new tab) (Sept. 14).Related: SpaceX's Starlink megaconstellation launches in photosNSF-supported USAP scientists in #Antarctica are over the moon! Starlink is testing polar service with a newly deployed user terminal at McMurdo Station, increasing bandwidth and connectivity for science support. pic.twitter.com/c3kLGk8XBVSeptember 14, 2022 See moreSpaceX celebrated the milestone as well. ""Starlink is now on all seven continents! In such a remote location like Antarctica, this capability is enabled by Starlink's space laser network ,"" the company tweeted on Wednesday (opens in new tab).SpaceX has already launched more than 3,200 Starlink satellites to low Earth orbit. And the constellation will grow to even more enormous proportions, if all goes according to plan: SpaceX has permission to loft 12,000 Starlink spacecraft, and the company has applied for permission from an international regulator to orbit 30,000 more satellites on top of that.Starting next year, SpaceX plans to begin launching Starlink Version 2 satellites, which are much larger and more capable than the current iteration. Those next-gen spacecraft will be able to beam service directly to smartphones, SpaceX founder and CEO Elon Musk has said.Indeed, late last month, Musk announced that SpaceX and T-Mobile had signed a deal to provide such service, via a project called "" Coverage Above and Beyond .""Starlink is crucial to SpaceX's long-term exploration goals. Musk has said that revenues from the broadband constellation are helping to fund Starship , the giant rocket-spaceship combo that SpaceX is developing to take people and cargo to the moon and Mars. SpaceX also plans to launch Starlink Version 2 satellites to orbit using Starship.",SpaceX's Starlink internet service reaches Antarctica
497,-1,-1_new_said_study_people,https://www.space.com/james-webb-space-telescope-miri-glitch-september-2022,"James Webb Space Telescope's ultracold camera has experienced a technical glitch that is forcing the ground team to postpone some observations.The problem affected the James Webb Space Telescope 's Mid-Infrared Instrument's (MIRI) grating wheel, which allows scientists to choose the wavelength of light they want to focus on. The wheel is used in only one of MIRI 's four observation modes, the medium-resolution spectroscopy (MRS) mode, in which the camera takes not images but light spectra (fingerprints of light absorption of the various chemical elements in the observed objects).The ground control teams first detected friction in the wheel in late August, NASA officials wrote in a statement (opens in new tab) released on Tuesday (Sept. 20), and after further investigation decided to pause observations in the affected mode. On Sept. 6, the agency convened an anomaly review board ""to assess the best path forward,"" the statement added.Related: Marvel at the James Webb Space Telescope's largest image of the cosmos yet""The Webb team has paused in scheduling observations using this particular observing mode while they continue to analyze its behavior and are currently developing strategies to resume MRS observations as soon as possible,"" NASA officials wrote in the statement. ""The observatory is in good health, and MIRI's other three observing modes — imaging, low-resolution spectroscopy and coronagraphy — are operating normally and remain available for science observations.""MIRI, one of four high-tech instruments on the James Webb Space Telescope, is a combined camera and spectrograph, which means it takes both images and light spectra of the distant universe . A specialist in detecting mid-infrared wavelengths, MIRI can see light from far-away galaxies , as well as stars forming inside of shrouds of dust. While all of Webb's instruments require extremely low temperatures to observe properly, MIRI is the coldest of them all.The other three instruments — NIRCam, NIRSpec and FGS/NIRISS — rely on the telescope's tennis-court-sized sunshield to reach temperatures of minus 369.4 degrees Fahrenheit (minus 223 degrees Celsius). MIRI, in addition to the sunshield, requires special cryocoolers to achieve an even colder temperature of minus 447 degrees F (minus 266 degrees C), only 12 degrees F (7 degrees C) above absolute zero, the temperature at which the motion of atoms stops.",NASA investigating glitch on James Webb Space Telescope's ultracold camera
496,-1,-1_new_said_study_people,https://www.space.com/emit-instrument-international-space-station-methane-super-emitters,"A powerful eye in the sky is helping scientists spy ""super-emitters"" of methane, a greenhouse gas about 80 times more potent than carbon dioxide.That observer is NASA's Earth Surface Mineral Dust Source Investigation instrument, or EMIT for short. EMIT has been mapping the chemical composition of dust throughout Earth's desert regions since being installed on the exterior of the International Space Station (ISS) in July, helping researchers understand how airborne dust affects climate.That's the main goal of EMIT's mission. But it's making another, less expected contribution to climate studies as well, NASA officials announced on Tuesday (Oct. 25). The instrument is identifying huge plumes of heat-trapping methane gas around the world — more than 50 of them already, in fact.Related: Climate change: Causes and effectsTwelve plumes of methane stream westward east of Hazar, Turkmenistan, a port city on the Caspian Sea. The plumes were detected by NASA’s Earth Surface Mineral Dust Source Investigation mission, and some of them stretch for more than 20 miles (32 kilometers). (Image credit: NASA/JPL-Caltech)""Reining in methane emissions is key to limiting global warming . This exciting new development will not only help researchers better pinpoint where methane leaks are coming from, but also provide insight on how they can be addressed — quickly,"" NASA Administrator Bill Nelson said in a statement (opens in new tab).""The International Space Station and NASA's more than two dozen satellites and instruments in space have long been invaluable in determining changes to the Earth's climate,"" Nelson added. ""EMIT is proving to be a critical tool in our toolbox to measure this potent greenhouse gas — and stop it at the source.""EMIT is an imaging spectrometer designed to identify the chemical fingerprints of a variety of minerals on Earth's surface. The ability to spot methane as well is a sort of happy accident.""It turns out that methane also has a spectral signature in the same wavelength range, and that's what has allowed us to be sensitive to methane,"" EMIT principal investigator Robert Green, of NASA's Jet Propulsion Laboratory (JPL) in Southern California, said during a press conference on Tuesday afternoon.Green and other EMIT team members gave some examples of the instrument's sensitivity during the Tuesday media call. For example, the instrument detected a plume of methane — also known as natural gas — at least 3 miles (4.8 kilometers) long in the sky above an Iranian landfill. This newfound super-emitter is pumping about 18,700 pounds (8,500 kilograms) of methane into the air every hour, the researchers said.That's a lot, but it pales in comparison to a cluster of 12 super-emitters EMIT spotted in Turkmenistan, all of them associated with oil and gas infrastructure. Some of those plumes are up to 20 miles (32 km) long, and, together, they're adding about 111,000 pounds (50,400 kg) of methane to Earth's atmosphere per hour.That's comparable to the peak rates of the Aliso Canyon leak, one of the largest methane releases in U.S. history. (The Aliso Canyon event, which occurred at a Southern California methane storage facility, was first noticed in October 2015 and wasn't fully plugged until February 2016.)EMIT spotted all of these super-emitters very early, during the instrument's checkout phase. So it should make even greater contributions as it gets fully up and running, and as scientists gain more familiarity with the instrument's capabilities, team members said.""We are really only scratching the surface of EMIT's potential for mapping greenhouse gases,"" Andrew Thorpe, a research technologist at JPL, said during Tuesday's press conference. ""We're really excited about EMIT's potential for reducing emissions from human activity by pinpointing these emission sources.""",Methane 'super-emitters' on Earth spotted by space station experiment
493,-1,-1_new_said_study_people,https://www.smithsonianmag.com/science-nature/new-technologies-are-helping-paleontologists-track-the-history-of-squishy-cephalopods-180980565/,"Finding and studying fossils of Earth’s squishiest prehistoric creatures is a difficult task. The fossil record often tells the history of life through hard tissues. Bones, teeth, shells and other mineralized, durable parts of living things have a far better chance of being preserved as fossils than the softer tissues like muscle and internal organs. That’s a huge challenge for all paleontologists, but especially experts on ancient cephalopods—the fossil relatives of today’s nautilus, squid, cuttlefish and octopus that live from the shore to the dark depths. Mollusks have soft bodies that often decayed away before getting a chance to become fossils, leaving experts only with shells or beaks from what was once a complete animal. Yet the cephalopod fossil record is full of surprises, and experts have become ever more inventive in finding ways to visualize creatures that have been extinct for millions of years.One of the latest surprises comes from an ancient relative of today’s vampire squid, a fossil relative called Vampyronassa. Vampyronassa was originally described twenty years ago. At the time, experts had to rely on what they could see with the naked eye. Paleontologists saw one of the cephalopod’s eyes and its sucker-lined arms, but much of its anatomy was obscured by the encasing rock. The outer details allowed researchers to categorize this strange cephalopod as a distant relative of the “vampire squid” that floats through the ocean depths today, but little more could be said of the animal’s biology. It seemed reasonable to assume that the fossil species lived much like it’s modern-day counterpart.But advances in visualization technology and greater availability of micro CT scans allowed paleontologists to take a new look at the fossil. Especially when soft-bodied animals are preserved as fossils, there are often hidden aspects of their anatomy that can only be seen by looking beneath the surface of the fossil. “We chose to reanalyze these specimens as we now have access to non-destructive, powerful X-ray based imaging techniques that allow us to observe previously unseen internal structures,” says Sorbonne University paleontologist Alison Rowe, the lead author of a recent Scientific Reportsstudy redescribing the fossil.Being able to look inside the fossil yielded unexpected results that couldn’t been seen just from the outside. Micro CT scans revealed parts of the gills, stomach, esophagus and other internal organs of this creature, the closest experts could hope to get to seeing this animal alive. “We were able to determine that the sucker attachment of Vampyronassa is the same type seen only in modern Vampyroteuthis,” Rowe says, though the shape of those suckers look like those of octopus. The shape of the suckers and the way they are anchored to the arms of Vampyronassa is a combination never seen before, what Rowe says “provides a small window on the diversity of character combinations that occurred in the Jurassic that are now lost.”Looking closely did more than answer some anatomical questions, however. Today’s Vampyroteuthis has sometimes been called a living fossil, the assumption being that these cephalopods found a cozy home in deep, oxygen-poor waters and stayed there in a cozy niche, eating detritus that falls from above, since the Jurassic. But the new study of Vampyronassahas revealed something different. The arms and internal anatomy of the fossil cephalopod indicate that it was an active predator that pursued prey closer to the surface. Vampyronassa zipped around to hunt and nab prey with its sucker-lined arms, with its later relatives retiring to a deep sea existence sometime after 33 million years ago.The fossil of Vampyronassa was a rare case. Fossils of cephalopods like ancient octopus and squid, which had very few hard parts, are difficult to find. Cephalopods such as the coil-shelled ammonoids are much more common, sometimes found in vast beds of empty shells. Such fossils have often been used to tell time in the fossil record as the evolution and extinction of ammonoid species was so rapid that particular species are often associated with particular rock layers–find an ammonoid and you can get a pretty good idea of where you are in the fossil record. Until recently, it seemed that the shells couldn’t tell us very much about how these animals lived. But paleontologists are an inventive bunch, and technological advances have allowed them to get closer to understanding how the beautiful and prolific ammonoid made a living during the deep past.Case in point, paleontologists didn’t really know what ammonoids ate. The cephalopods were clearly an important part of ancient food webs from 66 to 450 million years ago, and were even fodder from marine reptiles like mosasaurs given some Cretaceous ammonoid shells are found with bite marks on them, but paleontologists were missing what ammonoids themselves ate. Only in 2011 did paleontologist Isabelle Kruta and colleagues announce that they were able to use high-powered X-rays to detect plankton inside the mouth of one particular ammonoid that was a little better preserved than others. Ammonoids fed on microscopic organisms floating in the water column. This became a critical realization. The last ammonoids went extinct about 100,000 years after the impact that wiped out the non-avian dinosaurs, during a time when oceans were struggling to rebuild their food webs from the bottom up. If ammonoids ate plankton, but also produced offspring that were so small they were part of the ocean’s plankton, the poor cephalopods may have practically cannibalized themselves into oblivion.Prior to those final years, though, ammonoids came in a variety of shapes and sizes, up to species with shells the size of a Mini Cooper. How did these creatures swim, and why did evolution seem to favor some shapes over others? Scientists have turned to ammonoid robots to help answer those questions.True ammonoids haven’t swum in the seas for about 66 million years, but their shells, at least, have been put through their paces in a college swimming pool. Starting with high-definition scans of ammonoid shells, University of Utah paleontologist David Peterman created three dimensional models of ammonoid shells that he then turned into swimming robots. These models mimic the swimming behavior of the extinct species, allowing experts to get a better idea of how these animals actually moved in the water. “Thanks to computation advances and 3-D prints,” Peterman says, “we were able to explore paleoecological and biomechanical questions with unprecedented levels of detail.” Scientists combined engineering and even video game software with scans of fossils tens of millions of years old, ancient and modern coming together to let ammonoids swim once again.The tests in the pool have helped resolve some longstanding questions about these animals. Some prehistoric, shelled cephalopods have cone-shaped shells rather than whorls. Did these cephalopods swim in a horizontal position, vertical or crawl along the sea floor as in old museum dioramas? No one really knew. But the biomechanical tests revealed that these shells did best in a vertical position, meaning the cone-shelled cephalopods didn’t so much jet around in search of food but bobbed with the currents as they snagged what they could with their sucker-lined arms.Frustrating as it might be that we lack as much detail on the soft tissues of prehistoric squid relatives as we might like, Peterman says, being able to scan, visualize and even replicate parts of these ancient creatures is telling us more than ever before. “These animals tell the remarkable story of how seafloor-dwelling critters evolved into living, jet-propelled submarines,” Peterman says, “leaving behind an unparalleled treasure trove of information.”","What New Tech Is Revealing About Squishy, Prehistoric Cephalopods"
491,-1,-1_new_said_study_people,https://www.scimex.org/newsfeed/reddit-comments-show-equal-interest-in-female-versus-male-politicians-but-less-respect,"Reddit users' comments suggest interest in male versus female politicians is relatively equal, but gender biases exist across the platform, according to an international analysis of 10 million comments on the social media platform. The equal levels of interest in female versus male politicians is in contrast to previous research, the researchers say. However, they add that while interest in female politicians did not appear to be significantly driven by overt misogyny or benevolent sexism, interest in female politicians was less professional and respectful than interest in male politicians. Female politicians were far more likely to be mentioned by their first names than male politicians, who were mostly mentioned by their last names. In addition, women were far more likely to be described with words referencing their body or family than male politicians, who were more likely to be described according to their profession.Media releasePLOSReddit comments show equal interest in female versus male politicians, but less respectFemale politicians were more likely to have their first names, bodies and families referenced; male politicians were more discussed in terms of their professionAn analysis of 10 million comments on the social media platform Reddit suggests that public interest in male versus female politicians is relatively equal, but that gender biases exist across the platform. Sara Marjanovic and colleagues at the University of Copenhagen, Denmark, present these findings in the open-access journal PLOS ONE on October 26, 2022.Around the world, women remain underrepresented in politics, likely because of gender biases. Analysis of words and language used in public discussions can be useful for studying political gender biases, but most prior studies have focused on communication addressed directly to politicians, as opposed to conversations about them.To better understand political gender bias, Marjanovic and colleagues conducted a study of 10 million comments from public Reddit conversations about politicians. They used a multifaceted approach to investigate different types of gender bias, including subtler forms of bias such as benevolent sexism, which involves seemingly positive stereotypes. The researchers not only addressed specific language used, but also extra-linguistic factors, such as how often different politicians were mentioned.This analysis revealed relatively equal levels of public interest in female versus male politicians; the researchers note that this finding contrasts with prior research showing greater media coverage of male versus female politicians.However, while interest in female politicians did not appear to be significantly driven by overt misogyny or benevolent sexism, interest in female politicians was less professional and respectful than interest in male politicians. For instance, female politicians were far more likely to be mentioned by their first names than male politicians, who were mostly mentioned by their last names. In addition, women were far more likely to be described with words referencing their body or family than male politicians, who were more likely to be described according to their profession.These gender biases appeared in comments across different communities on Reddit, known as subreddits. However, left-leaning subreddits displayed them to a lower degree than right-leaning subreddits, and alt-right subreddits showed the most bias.The researchers have publicly released the curated dataset they developed for this analysis in order to encourage additional future studies.The authors add: “Unlike men, female politicians are often named by their first names and are described in relation to their body, clothing, or family; these findings suggest how societal expectations for women affect their public perception as political players.”","Redditors show equal interest in female and male politicians, but not the same respect"
490,-1,-1_new_said_study_people,https://www.scimex.org/newsfeed/avoiding-extinction-some-asian-animals-found-thriving-near-humans,"Media releaseThe University of QueenslandSome of Asia’s largest animals, including tigers and elephants, are defying 12,000 years of extinction trends by thriving alongside humans, a University of Queensland-led study has revealed.Researchers scoured paleontological records to compare the historic distribution of Asia’s 14 largest species with their populations in present-day tropical forests.PhD candidate Zachary Amir, from UQ’s School of Biological Sciences and the Ecological Cascades Lab, said four species – tigers, Asian elephants, wild boars and clouded leopards – showed increased populations in areas with human infrastructure.“These results show that, under the right conditions, some large animals can live nearby humans and avoid extinction,” Mr Amir said.“These results challenge the narrative within some conservation circles that humans and megafauna are incompatible.“Globally there is a trend towards ‘trophic downgrading’, a term referring to the disproportionate loss of the world’s largest animals.“Trophic downgrading is usually worst near humans because hunters target larger species.“But in the case of tigers, elephants, wild boars and clouded leopards, their Asian populations are higher nearby humans.“This may be the outcome of tougher anti-poaching efforts in the national parks that are closer to human settlements and are more frequently visited by tourists.”The study also found deforestation was still impacting species, and clouded leopard numbers in particular experienced a strong decline in those areas.But, Mr Amir said the research showed that if the large animal species were not hunted, they could live in relatively small habitats and near humans.“Previously, there have only been a few examples of large Asian species thriving in small habitats near humans, notably in Mumbai, India where leopards in an urban park prey on stray dogs,” Mr Amir said referring to a prior UQ study.“Thankfully, we found that a wider range of animals can coexist with humans.”At one of their study sites in Singapore, where poaching has been eliminated and there are considerable forest restoration efforts, two large animal species are thriving again.“Singapore has actually experienced the natural re-wilding of sambar deer and wild boars, which are now frequently observed in an urban forest, the Bukit Timah Nature Reserve,” Mr Amir said.“If we replicate those protection efforts in larger forests and other counties, we may see positive impacts right around the world.“But before this can happen, humans need to get our act together and limit poaching.”While there are some positive results, UQ’s Dr Matthew Luskin said the study also noted strong declines in tapirs, Sumatran rhinoceros, sun bears, guar and other large animals.“The key innovation of this work was to systematically investigate the population trends of many different wildlife species across the region,” Dr Luskin said.“Then we tested if all species showed consistent trends and if similar parks retained similar species.“Remarkably, we found no two forests currently possess the same group of wildlife compared to thousands of years ago.”Dr Luskin said the research offered an opportunity to shape the future of nature.“These results provide hope for wildlife in forests previously considered too far degraded or too close to cities,” he said.“Now we’re exploring new conservation strategies for these surprising places.”",Avoiding extinction: Some Asian animals found thriving near humans
158,-1,-1_new_said_study_people,https://techcrunch.com/2018/04/30/pentagon-radio-frequency-vehicle-stopper/,"Vehicular terrorism is on the rise, but technology under development by the U.S. Department of Defense could save lives by disabling a weaponized car before it ever reaches its target. The Pentagon’s Joint Non-Lethal Weapons Program (JNLWD) is working on a device called a Radio Frequency Vehicle Stopper to address the prevalence of vehicle-based attacks targeting civilians, Defense One reports.To prevent this kind of violence and other kinds of vehicular attacks (an unauthorized car rushing behind a military security gate, for instance), the Pentagon’s Radio Frequency Vehicle Stopper points high-powered microwaves at a vehicle, disabling its electrical components via the engine control unit and making the engine stall out. You can watch the technology in action in the Department of Defense video below.https://www.youtube.com/watch?time_continue=2&v=H-tNhWSLufAAs Defense One reports, the group is developing two versions of its technology, one with a 50-meter range small enough to fit in a truck bed and another larger version with a range of more than 100 meters designed to remain in place. The latter would particularly be useful in the kind of open public spaces that lend themselves to violent vehicular attacks in popular urban areas like markets and shopping hubs. This kind of technology is only becoming possible now due to breakthroughs in powering the concentrated beams emitted in these kind of notoriously energy-hungry weapons.While vehicle-based attacks were once rarely observed outside of war-zones, they’ve occurred with increasing frequency in high-density urban areas and tourist destinations in recent years. As the attack in Toronto last week proved, the results are effortlessly deadly to unsuspecting pedestrians. It’s unfortunate that such a device is necessary at all, but if they were to become readily available, these Radio Frequency Vehicle Stoppers could discourage the rising trend of vehicular attacks, protect victims when they do occur and help law enforcement obtain additional intelligence by apprehending suspects without resorting to lethal violence.",The Pentagon is working on a radio wave weapon that stops a speeding car in its tracks
489,-1,-1_new_said_study_people,https://www.scientificamerican.com/article/a-supersmeller-can-detect-the-scent-of-parkinsons-leading-to-an-experimental-test-for-the-illness/,"A Scottish woman named Joy Milne made headlines in 2015 for an unusual talent: her ability to sniff out people afflicted with Parkinson’s disease, a progressive neurodegenerative illness that is estimated to affect nearly a million people in the U.S. alone. Since then a group of scientists in the U.K. has been working with Milne to pinpoint the molecules that give Parkinson’s its distinct olfactory signature. The team has now zeroed in on a set of molecules specific to the disease—and has created a simple skin-swab-based test to detect them.Milne, a 72-year-old retired nurse from Perth, Scotland, has hereditary hyperosmia, a condition that endows people with a hypersensitivity to smell. She discovered that she could sense Parkinson’s with her nose after noticing her late husband, Les, was emitting a musky odor that she had not detected before. Eventually, she linked this change in scent to Parkinson’s when he was diagnosed with the disease many years later. Les passed away in 2015.In 2012 Milne met Tilo Kunath, a neuroscientist at the University of Edinburgh in Scotland, at an event organized by the research and support charity Parkinson’s UK. Although skeptical at first, Kunath and his colleagues decided to put Milne’s claims to the test. They gave her 12 T-shirts, six from people with Parkinson’s and six from healthy individuals. She correctly identified the disease in all six cases—and the one T-shirt from a healthy person she categorized as having Parkinson’s belonged to someone who went on to be diagnosed with the disease less than a year later.Subsequently, Kunath, along with chemist Perdita Barran of the University of Manchester in England and her colleagues, has been searching for the molecules responsible for the change in smell that Milne can detect. The researchers used mass spectrometry to identify types and quantities of molecules in a sample of sebum, an oily substance found on the skin’s surface. They discovered changes to fatty molecules known as lipids in people with Parkinson’s.In their latest study, published on September 7 in the American Chemical Society journal JACS Au, the researchers revealed the results of using a simple skin-swab-based test to detect the lipid signature that is indicative of Parkinson’s. By comparing sebum samples from 79 people with Parkinson’s and 71 people without the illness, the team zeroed in on a set of large lipids that could be detected in a matter of minutes using a special type of mass spectrometry in which substances are rapidly transferred from a swab to an analyzer using just a piece of paper.“I think it’s a very promising set of biomarkers,” says Blaine Roberts, a biochemist at Emory University, who wasn’t involved in the work. He adds that one of the big open questions that remains is how exacting this test can be. While the authors of the September 7 study reported the detailed chemical profile of the unique Parkinson’s signature, they did not include an assessment of its accuracy. According to Barran, based on not-yet-published data, their test appears to be able to determine whether an individual has Parkinson’s with more than 90 percent accuracy.Tiago Outeiro, a neuroscientist at the University of Göttingen in Germany, who was not involved with the research, says the sebum-based swab test is novel and has clear advantages, such as the ease of sample collection. Outeiro wonders whether people with diseases that share symptoms and pathologies with Parkinson’s disease, such as multiple system atrophy, also have similar chemical markers.The team is now working with local hospitals to determine whether this sebum-based test can also be conducted in clinical labs—a key step toward determining whether it can be used as a diagnostic tool. Ultimately, Barran says, the hope is to use the test to help identify individuals who have been referred to their neurologists by their general practitioner for suspected Parkinson’s so they can receive a faster diagnosis. Currently, there are thousands of people waiting to see a neurologist in the U.K.’s National Health Service, and it will take an estimated two years to clear that list, Barran says. A skin-swab test could enable those patients to mail in skin swabs to be analyzed in the hospital laboratory and pinpoint those who need help most urgently. Barran’s research team is approaching people on the waiting list to see if they are willing to take part in a trial to see whether such skin-swab tests could prove effective in helping to speed up the triage process.Barran and her colleagues are also collaborating with a group at Harvard University to determine whether sebum-based biomarkers are detectable in people who have constipation, a reduced sense of smell or other early signs of Parkinson’s but have not yet received a diagnosis.Milne has inspired groups elsewhere to search for biomarkers based on the disease’s olfactory signature. This year researchers in China published a paper describing an electronic nose—an artificial-intelligence-based sensor modeled after the olfactory system—that sniffs out molecules present in the sebum of patients with Parkinson’s disease. Other groups in China, the U.K. and elsewhere have also been training dogs to sniff out the disease.Parkinson’s may not be the only disease Milne has a nose for. She’s also reported noticing a unique smell in people with Alzheimer’s, cancer, and tuberculosis and is working with scientists to see whether a specific olfactory signature of those diseases can be deduced.For Milne, the hope is that this work will ultimately benefit patients with these conditions. “My husband suffered from [Parkinson’s] for 21 years after his diagnosis, but he had it many years before that,” Milne told Scientific American in 2015. “I would like to see that people don’t suffer the way he suffered.”","A Supersmeller Can Detect the Scent of Parkinsonâs, Leading to an Experimental Test for the Illness"
487,-1,-1_new_said_study_people,https://www.sciencenews.org/article/james-webb-space-telescope-stars-earliest-born-sparkler-galaxy,"Some of the earliest stars yet seen are now coming to light in one of the first images from the James Webb Space Telescope.Formed roughly 800 million years after the Big Bang, the stars live in dense groups called globular clusters and surround a distant galaxy dubbed the Sparkler, astronomers report in the Oct. 1 Astrophysical Journal Letters. Globular clusters often host some of the oldest stars in contemporary galaxies such as our own, but it’s hard to tell their exact age. The new finding could help researchers pinpoint when such clusters began to form.Compared to a galaxy, globular clusters are tiny, which makes them hard to see from across the universe. But this time, a gargantuan natural lens in space helped. The Sparkler is one of thousands of galaxies that lie far behind a massive, much closer galaxy cluster called SMACS 0723, which was the subject of the first publicly released science image from the James Webb Space Telescope, or JWST (SN: 7/11/22). The cluster distorts spacetime such that the light from the more distant galaxies behind it is magnified.For all those remote galaxies, that extra magnification brings out details that have never been seen before. One elongated galaxy surrounded by yellowish blobs got the attention of astronomer Lamiya Mowla and her colleagues.“When we first saw it, we noticed all those little dots around it that we called ‘the sparkles,’” says Mowla, of the University of Toronto. The team wondered if the sparkles could be globular clusters, close-knit families of stars that are thought to have been born together and stay close to each other throughout their lives (SN: 10/15/20).“The outstanding question that there still is, is how were the globular clusters themselves born?” Mowla says. Were they born at “cosmic noon,” 10 billion years ago, when star formation throughout the universe peaked? Or did they form 13 billion years ago at “cosmic dawn,” when stars were first able to form at all (SN: 3/4/22)?Light from the Sparkler takes about 9 billion years to reach Earth, so if the sparkles are globular clusters that shone that long ago, they might help astronomers answer that question.Zooming into one part of JWST’s image of the galaxy cluster SMACS 0723, astronomers zeroed in on the yellow dots around this one elongated background galaxy, which they called the Sparkler. Some of the dots may be globular clusters of same-age stars formed just a few hundred million years after the Big Bang. L. Mowla et al/The Astrophysical Journal Letters 2022Mowla and her colleagues used data from JWST to analyze the wavelengths of light coming from the sparkles. Some of them appear to be forming stars at the time when their light left the clusters. But some had formed all their stars long before.“When we see them, the stars are already about 4 billion years old,” says astrophysicist Kartheik Iyer, also of the University of Toronto.That means the oldest stars in the sparkles could have formed roughly 13 billion years ago. Since the universe is 13.8 billion years old, “there’s only a short amount of time after the Big Bang when these could have formed,” he says.In other words, these clusters were born at dawn, not at noon.Studying more globular clusters around ancient galaxies could help determine if such clusters are common or rare early on in the universe’s history. They could also help unravel galaxies’ formation histories, say Mowla and Iyer. Their team has proposed observations to be made in JWST’s first year that could do just that.Being able to pick out tiny structures like globular clusters from so far away was almost impossible before JWST, says astronomer Adélaïde Claeyssens of Stockholm University. She was not involved in the new work but led a similar study earlier this year of multiple galaxies magnified by the SMACS 0723 cluster.“It’s the first time we showed that, with James Webb, we will observe a lot of these type of galaxies with really tiny structures,” Claeyssens says. “James Webb will be a game changer for this field.”",The James Webb Space Telescope spied the earliest born stars yet seen
486,-1,-1_new_said_study_people,https://www.sciencealert.com/tiny-robots-have-successfully-cleared-pneumonia-from-the-lungs-of-mice,"Scientists have been able to direct a swarm of microscopic swimming robots to clear out pneumonia microbes in the lungs of mice, raising hopes that a similar treatment could be developed to treat deadly bacterial pneumonia in humans.The microbots are made from algae cells and covered with a layer of antibiotic nanoparticles. The algae provide movement through the lungs, which is key to the treatment being targeted and effective.In experiments, the infections in the mice treated with the algae bots all cleared up, whereas the mice that weren't treated all died within three days.The technology is still at a proof-of-concept stage, but the early signs are very promising.""Based on this mouse data, we see that the microrobots could potentially improve antibiotic penetration to kill bacterial pathogens and save more patients' lives,"" says Victor Nizet, a physician and professor of pediatrics at the University of California, San Diego.The nanoparticles on the algae cells are made of tiny polymer spheres coated with the membranes of neutrophils, a type of white blood cell. These membranes neutralize inflammatory molecules produced by bacteria and the body's own immune system, and both the nanoparticles and the algae degrade naturally.Harmful inflammation is reduced, improving the fight against infection, and the swimming microbots are able to deliver their treatment right where it's needed – it's the precision that makes this approach work so well.The researchers also established that the microbot treatment was more effective than an intravenous injection of antibiotics – in fact, the injection dose had to be 3,000 times higher than the one loaded on to the algae cells to achieve the same effect in the mice.""These results show how targeted drug delivery combined with active movement from the microalgae improves therapeutic efficacy,"" says Joseph Wang, nanoengineer from UC San Diego.In humans, the pneumonia caused byPseudomonas aeruginosa bacteria used in this study occurs after patients are put on a mechanical ventilator in intensive care. The infection often prolongs stays in hospital and significantly increases the risk of death.The researchers are confident that their new method can be scaled up as required, and would be straightforward to administer to the lungs of ventilated patients (the microbots were delivered to the mice through a tube in the windpipe).Next up for the team is more research into how the microbots interact with the immune system, then scaling up the work and getting it ready to be tested in larger animals – and then eventually, humans.""Our goal is to do targeted drug delivery into more challenging parts of the body, like the lungs,"" says chemical engineer Liangfang Zhang from UC San Diego. ""And we want to do it in a way that is safe, easy, biocompatible, and long-lasting.""""That is what we've demonstrated in this work.""The research has been published in Nature Materials.",Tiny Robots Have Successfully Cleared Pneumonia From The Lungs of Mice
483,-1,-1_new_said_study_people,https://www.salon.com/2022/10/16/ukraine-isnt-the-worlds-only-nuclear-flashpoint-taiwan-is-getting-ugly_partner/,"Thanks to Vladimir Putin's recent implicit threat to employ nuclear weapons if the U.S. and its NATO allies continue to arm Ukraine — ""This is not a bluff,"" he insisted on Sept. 21 — the perils in the Russo-Ukrainian conflict once again hit the headlines. And it's entirely possible, as ever more powerful U.S. weapons pour into Ukraine and Russian forces suffer yet more defeats, that the Russian president might indeed believe that the season for threats is ending and only the detonation of a nuclear weapon will convince the Western powers to back off. If so, the war in Ukraine could prove historic in the worst sense imaginable — the first conflict since World War II to lead to nuclear devastation.But hold on! As it happens, Ukraine isn't the only place on the planet where a nuclear conflagration could erupt in the near future. Sad to say, around the island of Taiwan — where U.S. and Chinese forces are engaging in ever more provocative military maneuvers — there is also an increasing risk that such moves by both sides could lead to nuclear escalation.While neither American nor Chinese officials have explicitly threatened to use such weaponry, both sides have highlighted possible extreme outcomes there. When Joe Biden last spoke with Xi Jinping by telephone on July 29, the Chinese president warned him against allowing House Speaker Nancy Pelosi to visit the island (which she nonetheless did, four days later) or offering any further encouragement to ""Taiwan independence forces"" there. ""Those who play with fire will perish by it,"" he assured the American president, an ambiguous warning to be sure, but one that nevertheless left open the possible use of nuclear weapons.As if to underscore that point, on Sept. 4, the day after Pelosi met with senior Taiwanese officials in Taipei, China fired 11 Dongfeng-15 (DF-15) ballistic missiles into the waters around that island. Many Western observers believe that the barrage was meant as a demonstration of Beijing's ability to attack any U.S. naval vessels that might come to Taiwan's aid in the event of a Chinese blockade or invasion of the island. And the DF-15, with a range of 600 miles, is believed capable of delivering not only a conventional payload, but also a nuclear one.In the days that followed, China also sent nuclear-capable H-6 heavy bombers across the median line in the Taiwan Strait, a previously respected informal boundary between China and that island. Worse yet, state-owned media displayed images of Dongfeng-17 (DF-17) hypersonic ballistic missiles, also believed capable of carrying nuclear weapons, being moved into positions off Taiwan.One day after Nancy Pelosi met with senior officials in Taipei, China fired 11 Dongfeng-15 (DF-15) ballistic missiles — all capable of carrying a nuclear payload — into Taiwanese waters.Washington has not overtly deployed nuclear-capable weaponry in such a brazen fashion near Chinese territory, but it certainly has sent aircraft carriers and guided-missile warships into the area, signaling its ability to launch attacks on the mainland should a war break out. While Pelosi was in Taiwan, for example, the Navy deployed the carrier USS Ronald Reagan with its flotilla of escort vessels in nearby waters. Military officials in both countries are all too aware that should such ships ever attack Chinese territory, those DF-15s and DF-17s would be let loose against them — and, if armed with nuclear warheads, would likely provoke a U.S. nuclear response.The implicit message on both sides: A nuclear war might be possible. And although — unlike with Putin's comments — the American media hasn't highlighted the way Taiwan might trigger such a conflagration, the potential is all too ominously there.""One China"" and ""strategic ambiguity""In reality, there's nothing new about the risk of nuclear war over Taiwan. In both the Taiwan Strait crises of 1954-1955 and 1958, the United States threatened to attack a then-non-nuclear China with such weaponry if it didn't stop shelling the Taiwanese-controlled islands of Kinmen (Quemoy) and Mazu (Matsu), located off that country's coast. At the time, Washington had no formal relations with the communist regime on the mainland and recognized the Republic of China (ROC) — as Taiwan calls itself — as the government of all China. In the end, however, U.S. leaders found it advantageous to recognize the People's Republic of China (PRC) in place of the ROC and the risk of a nuclear conflict declined precipitously — until recently.Credit the new, increasingly perilous situation to Washington's changing views of Taiwan's strategic value to America's dominant position in the Pacific as it faces the challenge of China's emergence as a great power. When the U.S. officially recognized the PRC in 1978, it severed its formal diplomatic and military relationship with the ROC, while ""acknowledg[ing] the Chinese position that there is but one China and [that] Taiwan is part of China."" That stance — what came to be known as the ""One China"" policy — has, in fact, underwritten peaceful relations between the two countries (and Taiwan's autonomy) ever since, by allowing Chinese leaders to believe that the island would, in time, join the mainland.Want a daily wrap-up of all the news and commentary Salon has to offer? Subscribe to our morning newsletter, Crash Course.Taiwan's safety and autonomy has also been preserved over the years by another key feature of U.S. policy, known as ""strategic ambiguity."" It originated with the Taiwan Relations Act of 1979, a measure passed in the wake of the U.S. decision to recognize the PRC as the legal government of all China. Under the act, still in effect, the U.S. is empowered to supply Taiwan with ""defensive"" arms, while maintaining only semi-official ties with its leadership. It also says that Washington would view any Chinese attempt to alter Taiwan's status through violent means as a matter ""of grave concern,"" but without explicitly stating that the U.S. will come to Taiwan's aid if that were to occur. Such official ambiguity helped keep the peace, in part by offering Taiwan's leadership no guarantee that Washington would back them if they declared independence and China invaded, while giving the leaders of the People's Republic no assurance that Washington would remain on the sidelines if they did.Since 1980, both Democratic and Republican administrations have relied on such strategic ambiguity and the One China policy to guide their peaceful relations with the PRC. Over the years, there have been periods of spiking tensions between Washington and Beijing, with Taiwan's status a persistent irritant, but never a fundamental breach in relations. And that — consider the irony, if you will — has allowed Taiwan to develop into a modern, prosperous quasi-state, while escaping involvement in a major-power confrontation (in part because it just didn't figure prominently enough in U.S. strategic thinking).From 1980 to 2001, America's top foreign-policy officials were largely focused on defeating the Soviet Union, dealing with the end of the Cold War, and expanding global trade opportunities. Then, from Sept. 11, 2001, to 2018, their attention was diverted to the Global War on Terror. In the early years of the Trump administration, however, senior military officials began switching their focus from the War on Terror to what they termed ""great-power competition,"" arguing that facing off against ""near-peer"" adversaries, namely China and Russia, should be the dominant theme in military planning. And only then did Taiwan acquire a different significance.The Pentagon's new strategic outlook was first spelled out in the National Defense Strategy of February 2018 in this way: ""The central challenge to U.S. prosperity and security is the reemergence of long-term, strategic competition"" with China and Russia. (And yes, the emphasis was in the original.) China, in particular, was identified as a vital threat to Washington's continued global dominance. ""As China continues its economic and military ascendance,"" the document asserted, ""it will continue to pursue a military modernization program that seeks Indo-Pacific regional hegemony in the near-term and displacement of the United States to achieve global preeminence in the future.""An ominous ""new Cold War"" era had begun.Taiwan's strategic significance risesTo prevent China from achieving that most feared of all results, ""Indo-Pacific regional hegemony,"" Pentagon leaders devised a multi-pronged strategy, combining an enhanced U.S. military presence in the region with beefed-up, ever more militarized ties with America's allies there. As that 2018 National Defense Strategy put it, ""We will strengthen our alliances and partnerships in the Indo-Pacific to a networked security architecture capable of deterring aggression, maintaining stability, and ensuring free access to common domains."" Initially, that ""networked security architecture"" was only to involve long-term allies like Australia, Japan, South Korea and the Philippines. Soon enough, however, Taiwan came to be viewed as a crucial part of such an architecture.To grasp what this meant, imagine a map of the Western Pacific. In seeking to ""contain"" China, Washington was relying on a chain of island and peninsular allies stretching from South Korea and Japan to the Philippines and Australia. Japan's southernmost islands, including Okinawa — the site of major American military bases (and a vigorous local anti-base movement) — do reach all the way into the Philippine Sea. Still, there remains a wide gap between them and Luzon, the northernmost Philippine island. Smack in the middle of that gap lies… yep, you guessed it, Taiwan.In seeking to ""contain"" China, Washington relies on a chain of island and peninsular allies stretching from South Korea and Japan to the Philippines and Australia. Smack in the middle of that chain lies Taiwan.In the view of the top American military and foreign policy officials, for the U.S. to successfully prevent China from becoming a major regional power, it would have to bottle up that country's naval forces within what they began calling ""the first island chain"" — the string of nations stretching from Japan to the Philippines and Indonesia. For China to thrive, as they saw it, that nation's navy would have to be able to send its ships past that line of islands and reach deep into the Pacific. You won't be surprised to learn, then, that solidifying U.S. defenses along that very chain became a top Pentagon priority — and, in that context, Taiwan has, ominously enough, come to be viewed as a crucial piece in the strategic puzzle.Last December, Assistant Secretary of Defense for Indo-Pacific Security Affairs Ely Ratner summed up the Pentagon's new way of thinking about the island's geopolitical role when he appeared before the Senate Foreign Relations Committee. ""Taiwan,"" he said, ""is located at a critical node within the first island chain, anchoring a network of U.S. allies and partners that is critical to the region's security and critical to the defense of vital U.S. interests in the Indo-Pacific.""This new perception of Taiwan's ""critical"" significance has led senior policymakers in Washington to reconsider the basics, including their commitment to a One China policy and to strategic ambiguity. While still claiming that One China remains White House policy, President Biden has repeatedly insisted all too unambiguously that the U.S. has an obligation to defend Taiwan if attacked. When asked recently on ""60 Minutes"" whether ""U.S. forces…would defend Taiwan in the event of a Chinese invasion,"" Biden said, without hesitation, ""Yes."" The administration has also upgraded its diplomatic ties with the island and promised it billions of dollars' worth of arms transfers and other forms of military assistance. In essence, such moves constitute a de facto abandonment of ""One China"" and its replacement with a ""one China, one Taiwan"" policy.Not surprisingly, the Chinese authorities have reacted to such comments and the moves accompanying them with increasing apprehension and anger. As seen from Beijing, they represent the full-scale repudiation of multiple statements acknowledging Taiwan's indivisible ties to the mainland, as well as a potential military threat of the first order should that island become a formal U.S. ally. For President Xi and his associates, this is simply intolerable.""The repeated attempts by the Taiwan authorities to look for U.S. support for their independence agenda as well as the intention of some Americans to use Taiwan to contain China"" are deeply troubling, Xi told Biden during their telephone call in November 2021. ""Such moves are extremely dangerous, just like playing with fire. Whoever plays with fire will get burned.""Since then, Chinese officials have steadily escalated their rhetoric, threatening war in ever more explicit terms. ""If the Taiwanese authorities, emboldened by the United States, keep going down the road for independence,"" Qin Gang, China's ambassador to the U.S., typically told NPR in January 2022, ""it most likely will involve China and the United States, the two big countries, in military conflict.""To demonstrate its seriousness, China has begun conducting regular air and naval exercises in the air- and sea-space surrounding Taiwan. Such maneuvers usually involve the deployment of five or six warships and a dozen or more warplanes, as well as ever greater displays of firepower, clearly with the intention of intimidating the Taiwanese leadership. On Aug. 5, for example, the Chinese deployed 13 warships and 68 warplanes in areas around Taiwan and, two days later, 14 ships and 66 planes.Each time, the Taiwanese scramble their own aircraft and deploy coastal defense vessels in response. Accordingly, as China's maneuvers grow in size and frequency, the risk of an accidental or unintended clash becomes ever more likely. The increasingly frequent deployment of U.S. warships to nearby waters only adds to this explosive mix. Every time an American naval vessel is sent through the Taiwan Strait — something that occurs almost once a month now — China scrambles its own air and sea defenses, producing a comparable risk of unintended violence.This was true, for example, when the guided-missile cruisers USS Antietam and USS Chancellorsville sailed through that strait on Aug. 28. According to Zhao Lijian, a spokesperson for the foreign ministry, China's military ""conducted security tracking and monitoring of the U.S. warships' passage during their whole course and had all movements of the U.S. warships under control.""No barriers to escalation?If it weren't for the seemingly never-ending war in Ukraine, the dangers of all of this might be far more apparent and deemed far more newsworthy. Unfortunately, at this point, there are no indications that either Beijing or Washington is prepared to scale back its provocative military maneuvers around Taiwan. That means an accidental or unintended clash could occur at any time, possibly triggering a full-scale conflict.Imagine, then, what a decision by Taiwan to declare full independence or by the Biden administration to abandon the One China policy could mean. China would undoubtedly respond aggressively, perhaps with a naval blockade of the island or even a full-scale invasion. Given the increasingly evident lack of interest among the key parties in compromise, a violent outcome appears ever more likely.If a U.S.-China conflict erupts, it may be difficult to contain the fighting to a ""conventional"" level. Both sides have shaped their military forces for rapid, intensive combat and decisive victory.However such a conflict erupts, it may prove difficult to contain the fighting at a ""conventional"" level. After all, both sides are wary of another war of attrition like the one unfolding in Ukraine and have instead shaped their military forces for rapid, firepower-intensive combat aimed at securing a decisive victory quickly. For Beijing, this could mean firing hundreds of ballistic missiles at U.S. ships and air bases in the region with the aim of eliminating any American capacity to attack its territory. For Washington, it might mean launching missiles at China's key ports, air bases, radar stations, and command centers. In either case, the results could prove catastrophic. For the U.S., the loss of its carriers and other warships; for China, the loss of its very capacity to make war. Would leaders of the losing side accept such a situation without resorting to nuclear weapons? No one can say for sure, but the temptation to escalate would undoubtedly be great.Unfortunately, at the moment, there are no U.S.-China negotiations under way to resolve the Taiwan question, to prevent unintended clashes in the Taiwan Strait or to reduce the risk of nuclear escalation. In fact, China quite publicly cut off all discussion of bilateral issues, ranging from military affairs to climate change, in the wake of Pelosi's visit to Taiwan. So it's essential, despite the present focus on escalation risks in Ukraine, to recognize that avoiding a war over Taiwan is no less important — especially given the danger that such a conflict could prove of even greater destructiveness. That's why it's so critical that Washington and Beijing put aside their differences long enough to initiate talks focused on preventing such a catastrophe.",Ukraine isn't the world's only nuclear flashpoint: Taiwan crisis is getting ugly
480,-1,-1_new_said_study_people,https://www.reuters.com/technology/uber-whistleblower-says-current-business-model-absolutely-unsustainable-2022-11-02/,"[1/2] Mark MacGann, founder of Moonshot Ventures and the senior executive behind the Uber Files, holds a news conference during the Web Summit, Europe's largest technology conference, in Lisbon, Portugal, November 2, 2022. REUTERS/Pedro NunesLISBON, Nov 2 (Reuters) - Mark MacGann, the whistleblower behind the so-called Uber Files, said on Wednesday that the ride-hailing company seemed to be taking steps toward improving its work culture, but that its business model was still ""absolutely"" unsustainable.The Guardian and Le Monde newspapers reported in July that Uber Technologies Inc (UBER.N) broke laws and secretly lobbied politicians as part of an aggressive drive to expand into new markets from 2013 to 2017. read moreMacGann, who led Uber's lobbying efforts to win over governments, identified himself as the source who leaked the more than 124,000 company files.MacGann said he decided to speak out because he believed Uber knowingly flouted laws and misled people about the benefits to drivers of the company's gig-economy model.Uber said in July, in response to the Guardian and Le Monde reports: ""We have not and will not make excuses for past behaviour that is clearly not in line with our present values.""MacGann said Uber's current CEO, Dara Khosrowshahi, and his executive team ""have done a lot of good things, but they have so, so far to go.""When asked for a comment, an Uber spokesman on Wednesday referred Reuters to a 2020 New York Times opinion piece by Khosrowshahi in which he said ""our current employment system is outdated and unfair.""Khosrowshahi had said gig workers would lose the flexibility they have today if they became employees and that rides would be more expensive. The CEO wrote that workers want both flexibility and benefits and added that new laws are required to help them.""I'm proposing that gig economy companies be required to establish benefits funds which give workers cash that they can use for the benefits they want, like health insurance or paid time off,"" Khosrowshahi wrote in the op-ed.""My message to Uber is: 'you've done well, (but) you can do it so much better (because) the current model is absolutely not sustainable,'"" MacGann told a news conference during Europe's largest tech conference, the Web Summit, in Lisbon.He said Uber recently reiterated that the ""core of its business model is independent contractors, since everybody wants to be self-employed, everybody wants flexibility.""He said the facts, however, contradict this view as there are Uber drivers suing the company in various countries to ""have a basic minimum of social protection such as sick pay.""""Uber is pumping tens of millions of dollars in Europe, United States, other parts of the world fighting legislation,"" he said.Reporting by Sergio Goncalves in Lisbon Additional reporting by Nivedita Balu and Ann Maria Shibu in Bengaluru; Editing by Matthew Lewis and Sherry Jacob-PhillipsOur Standards: The Thomson Reuters Trust Principles.",Uber whistleblower says current business model 'absolutely' unsustainable
467,-1,-1_new_said_study_people,https://www.psypost.org/2022/10/women-are-more-critical-of-female-toplessness-than-men-which-may-be-explained-by-objectification-theory-64093,"A study published in the journal Sexuality & Culture examined U.S. residents’ attitudes toward women going topless in public places. The findings suggest that for some, female toplessness is intertwined with sexuality and represents a moral issue. In line with objectification theory, women were more critical of female toplessness compared to men.Western societies tend to be far more disapproving of women going topless in public compared to men. The rationale seems to be that female breasts are inherently sexual and thus inappropriate for display in family areas like public beaches. This view fits within objectification theory, which suggests that a woman’s value is based on her appearance, as perceived through the heterosexual male gaze.Researchers Colin R. Harbke and Dana F. Lindemann conducted a study to investigate Americans’ attitudes toward female toplessness in public. While past studies have focused on the public’s perceptions of the legality of female toplessness, the current study aimed to target people’s reactions to female toplessness itself. The researchers also wanted to explore whether attitudes toward female toplessness vary by geographic region, a phenomenon that has been previously reported in Canadian samples.“We were interested in attitudes toward breastfeeding and disparate reactions to being topless in public, without nursing, came to the forefront as an potential contributing factor,” explained Harbke, a professor of psychology at Western Illinois University – Quad Cities.“We began thinking of both of these behaviors as relatively simple and innocuous on the surface, but that each are quite complicated when women’s objectification, morality, and sexism are added to the mix. Also, around that time there were some legal decisions that highlighted differences in the legality of public toplessness between not only men and women, but also between one region or state and the next.”“Most all the prior research on attitudes toward public toplessness focused on this legality issue (e.g., do people think that it should be legal for women to be topless while in public?) and we wanted to expand on this by getting a sense of how people are likely to react if they were to see someone who was topless while out in public.”The study participants were 326 U.S. residents, most of whom (78%) were women. The participants were shown a series of 60 images as part of a larger study. Interspersed within these images were six photos of topless women in one of three public settings — a beach, a park, or a city street. The photos were selected from Internet image searches and consisted of unedited photos of women who were not celebrities or models. To control for implicit biases related to body shape, skin tone, and other factors, the researchers selected only young adult White women with similar appearances.For each photo, participants rated their “impression or feelings when seeing the images” on an 11-point scale from very positive to very negative. The participants then completed demographic questionnaires and measures of disgust sensitivity, child protectiveness beliefs, and sexual attitudes and awareness.According to the findings, 80% of the variance in participants’ ratings was driven by individual differences, rather than differences in the photos. Geographic region was not related to participants’ attitudes.“It was really clear that the driving force in how someone is likely to respond to seeing a topless woman in public is not the setting, region, or the legality of where the toplessness occurs, but rather the characteristics, traits, and opinions of the person who is doing the reacting in the first place,” Harbke told PsyPost. “Even though prior surveys have shown that many people feel that being able to go topless in public should be within women’s legal rights, these findings suggest that that doesn’t necessarily mean that they will react favorably to seeing it around them.”Living in a state where female toplessness is prohibited was associated with less favorable attitudes toward the topless photos, while living in a state with ambiguous policies was related to more favorable attitudes toward the photos. These findings highlight how laws and social norms can influence people’s attitudes toward particular issues.“Much of the prior legality-based research had identified that attitudes toward public female toplessness differed based on the setting (or context) where the toplessness occurred (e.g., on a beach or at a pool, in a public park, or if one were to be walking around the city),” Harbke told PsyPost. “We expected to see differences in reactions to the pictures across settings also, and we did, but they were much smaller in magnitude than in prior legality-based studies.”“The differences in reactions for participants from states where public female toplessness was explicitly legal, explicitly illegal, or where the legality of topless was ambiguous, albeit still present, were also smaller than we anticipated.”There were also significant gender differences in ratings, with women rating the topless photos more negatively compared to men. This finding remained significant after controlling for various demographic and attitudinal variables.“What really surprised us was the magnitude of the difference that we saw between the males and females in our study who rated the images; the differences based on participant sex were about 3-times larger than that for context and nearly twice as large as both context and legality combined,” Harbke explained. “This pattern was consistent the idea that women will sometimes criticize and police other women’s behavior as sexual objects, along with other predictions and extensions from objectification theory.”In their study, Harbke and Lindemann discussed two potential interpretations for this sex difference, both of which can be explained by objectification theory. For one, men might express more favorable attitudes toward the topless photos since they are appealing to them and since they reinforce the sexual objectification of women. For another, women’s less favorable attitudes toward the photos could reflect their “policing” of other women’s behavior.Sexually-objectifying contexts have been found to encourage competition among women, and such dynamics may have led the female participants to object to the sexualized photos. The authors said it is likely that both these explanations play a role.Researchers further unearthed a pattern of findings suggesting that female toplessness is viewed as a moral issue. Higher SES, greater religiosity, and stronger child protectiveness beliefs were related to less positive ratings of the topless photos. By contrast, more positive attitudes toward sexual permissiveness and more egalitarian views of birth control were tied to more positive ratings of the topless photos.A strength of the study was high ecological validity since participants viewed real photos of topless women who they could have hypothetically encountered in real life. However, future research should replicate the study with a larger data set of diverse women of different body shapes, skin tones, and other characteristics.“This is an area of research where efforts to recruit a more diverse sample could lead to some really informative comparisons, especially when it comes to how gender identity and sexual orientations relate to attitudes toward public female toplessness,” Harbke said. “Comparisons that involve pictures of women who are not topless (e.g., wearing a swimsuit) or bare-chested men could also lead to valuable insights.”“Although a relatively understudied area when it comes to attitude research, the issue of public female toplessness brings with it a complex host of factors (e.g., objectification, morality, discrimination, legality) that are shared with a variety or other equal rights and social issues,” the researcher added. “As such, gaining understanding for how demographic and attitudinal factors contribute to reactions to public female toplessness may extend to a multitude of other areas.”The study, “Objectification and Reactions toward Public Female Toplessness in the United States: Looking Beyond Legal Approval“, was authored by Colin R. Harbke and Dana F. Lindemann.","Women are more critical of female toplessness than men, which may be explained by objectification theory"
463,-1,-1_new_said_study_people,https://www.psypost.org/2022/10/trump-voters-conspiracy-beliefs-about-the-democratic-party-increased-after-the-2020-election-according-to-a-five-wave-study-64154,"The outcome of an election can influence voters’ conspiracy beliefs, according to study findings published in the journal Applied Cognitive Psychology. Following the 2020 U.S. presidential election, Trump voters endorsed more conspiracy beliefs about the Democratic party, while Biden voters endorsed fewer conspiracy beliefs about the Republican party.Following Donald Trump’s 2020 election loss, conspiracy theories were widely circulated in conservative media claiming that the election outcome was rigged. Such conspiracy beliefs can have troubling consequences for society, and scientists are working to uncover how these beliefs develop.“In general, I am interested in conspiracies, because sometimes conspiracies can cause social problems like distrust and polarization,” said study author Haiyan Wang, a PhD candidate at Vrije Universiteit Amsterdam and the Netherlands Institute for the Study of Crime and Law. “There are many conspiracies related to the U.S. presidential elections, we were wondering what’s the difference between election losers’ and winners’ conspiracy beliefs.”Wang and his co-author Jan-Willem van Prooijen launched a five-wave longitudinal study to examine how Americans’ conspiracy beliefs changed over time in the wake of the 2020 election. Specifically, they assessed how the election results influenced voters’ belief that the election was rigged by the other party. The researchers further explored how the election results impacted Americans’ general conspiracy mentality.Wang and van Prooijen recruited a total of 376 Americans to participate in the study. Data collection took place between October 13 and December 20, 2020 — two waves took place before the election, and three waves took place after the election. At each wave, participants rated the plausibility of specific conspiracy theories about the election. They also completed a measure of conspiracy mentality that assessed a general tendency to believe in conspiracy theories.At every wave, participants also indicated which candidate they intended to vote for (Waves 1 and 2) or which candidate they had voted for (Waves 3 to 5). The researchers focused their analysis on Biden and Trump voters only, resulting in a sample of 229 Biden voters and 71 Trump voters.First, the findings revealed that conspiracy mentality remained relatively constant over the two months, for both Biden and Trump voters. This is consistent with the view that conspiracy mentality is a stable trait. However, participants’ belief in specific conspiracy theories about the election did change over time, and these changes looked different for Biden and Trump voters.Outgroup conspiracy beliefs — conspiracy beliefs about the opposing political party — decreased over time among Biden voters. Specifically, Biden voters’ belief that the Republican party had conspired during the election decreased over time. However, the opposite was true for Trump voters. For them, belief that the Democratic party had conspired during the election (e.g., ‘The elections will be (were) rigged to favor Joe Biden’) increased over time.“After the election, conspiracy beliefs of voting fraud increased among people who voted for Trump,” Wang told PsyPost. “However, they decreased among people who voted for Biden.”Ingroup conspiracy beliefs — conspiracy beliefs about one’s preferred political party — decreased over time among both Trump and Biden voters. Specifically, Biden voters’ belief that the Democratic party had conspired during the election and Trump voters’ belief that the Republican party had conspired during the election decreased following the results. Notably, ingroup conspiracy beliefs were more common among Trump voters, especially at the first wave. This finding may support previous evidence that conspiracy beliefs are more prevalent among Republicans compared to Democrats.Wang and van Prooijen say their study results demonstrate that election events can influence voters’ conspiracy beliefs, but not conspiracy mentality. This suggests the possibility that the two types of conspiracy thinking involve different cognitive processes. The new findings also support previous research that found supporters of a losing candidate are especially likely to endorse conspiracy theories, since Trump voters’ outgroup conspiracy beliefs increased after the election results while Biden voters’ decreased.Among other limitations, the study sample was small and non-representative of the American electorate. The authors also say that future research should include a longer study period since it is possible that changes in conspiracy mentality would have been captured with additional waves.The study, “Stolen elections: How conspiracy beliefs during the 2020 American presidential elections changed over time”, was authored by Haiyan Wang and Jan-Willem van Prooijen.","Trump votersâ conspiracy beliefs about the Democratic party increased after the 2020 election, according to a five-wave study"
459,-1,-1_new_said_study_people,https://www.psypost.org/2022/10/severe-covid-19-may-increase-the-risk-for-schizophrenia-64088,"A new study published in Psychiatry Research suggests that experiencing a serious case of COVID-19 is associated with increased risk of developing schizophrenia.COVID-19 has had massive effects on society and many individuals’ health that will continue for years to come. In addition to serious physical health outcomes, it has been shown to be linked to declines in mental health that come from neuropsychological effects of the disease. COVID-19 can affect the nervous system, metabolism, and brain function. Despite our increasing knowledge of the negative effects of COVID-19, people have begun a return to normalcy and society has lifted mask mandates.For their new study, Ancha Baranova and colleagues utilized data from two Genome-Wide Association Studies (GWAS). This included one for the SARS-CoV-2 infection and one for hospitalized COVID-19 cases. Datasets included 122,616 cases of SARS, 32,519 cases of COVID-19, and 53,386 cases of schizophrenia, in addition to controls for each group.A genome-wide association study (GWAS) is a type of genetic research that looks for associations between genes and particular traits or conditions. Often, GWAS research is used to identify genes that may be involved in certain diseases.The data allowed the researchers to examine the topic using the Mendelian randomization framework, a research method that uses genetic variation to study the relationship between exposures and outcomes. By comparing the effects of different variants of a gene, researchers can identify which exposures are associated with which outcomes. This approach has several advantages over traditional observational studies. First, it can help to control for confounding variables. Second, it can provide insight into causal relationships.The researchers found that COVID-19 cases that resulted in hospitalization were associated with an 11% increase in risk for developing schizophrenia. This points to a poignant need for assessment for schizophrenia as a post-COVID hospitalization protocol. While genetic liability to severe COVID-19 was associated with increased schizophrenia risk, this was not true for genetic liability to SARS-CoV-2. The risk of schizophrenia from COVID-19 was found to be dependent on severity of the illness.This study took steps into better understanding a very serious outcome that can occur after a severe coronavirus infection. Despite this, there are limitations to note. One such limitation is that this study only took genetic factors into account. This is significant because it is well-known that environment is an important factor in both schizophrenia and COVID-19. Additionally, all participants in the GWAS datasets were of European descent; future research could have a more inclusive sample.The study, “Severe COVID-19 increased the risk of schizophrenia“, was authored by Ancha Baranova, Hongbao Cao, and Fuquan Zhang.",Severe COVID-19 may increase the risk for schizophrenia
488,-1,-1_new_said_study_people,https://www.sciencenews.org/article/seabirds-typhoons-shearwaters-survival,"Some seabirds don’t just survive storms. They ride them.Streaked shearwaters nesting on islands off Japan sometimes head straight toward passing typhoons, where they fly near the eye of the storm for hours at a time, researchers report in the Oct. 11 Proceedings of the National Academy of Sciences. This strange behavior — not reported in any other bird species — might help streaked shearwaters (Calonectris leucomelas) survive strong storms.Birds and other animals living in areas with hurricanes and typhoons have adopted strategies to weather these deadly storms (SN: 10/2/15). In recent years, a few studies using GPS trackers have revealed that some ocean-dwelling birds — such as the frigatebird (Fregata minor) — will take massive detours to avoid cyclones.This is an understandable strategy for birds that spend most of their time at sea where “there is literally nowhere to hide,” says Emily Shepard, a behavior ecologist at Swansea University in Wales. To find out whether shearwaters also avoid storms, she and her colleagues used 11 years of tracking data from GPS locators attached to the wings of 75 birds nesting on Awashima Island in Japan.By combining this information with data on wind speeds during typhoons, the researchers discovered that shearwaters that were caught out in the open ocean when a storm blew in would ride tailwinds around the edges of the storm. However, others that found themselves sandwiched between land and the eye of a strong cyclone would sometimes veer off their usual flight patterns and head toward the center of the storm.Flight path As Typhoon Cimaron moved across the Sea of Japan (black track) in August 2018, GPS trackers monitored the movements of 32 streaked shearwaters (Calonectris leucomelas) just off the coast of Japan. The tracking data show three birds (seen here in red and teal) flew toward the eye of the storm through some of the highest winds. Two other birds (light green) began heading toward the eye as the storm swept past. Tracking streaked shearwaters during Typhoon Cimaron E. Lempidakis et al/PNAS 2022 ( CC BY NC-ND 4.0Of the 75 monitored shearwaters, 13 flew to within 60 kilometers of the eye — an area Shepherd calls the “eye socket,” where the winds were strongest — for up to eight hours, tracking the cyclone as it headed northward. “It was one of those moments where we couldn’t believe what we were seeing,” Shepard says. “We had a few predictions for how they might behave, but this was not one of them.”The shearwaters were more likely to head for the eye during stronger storms, soaring on winds as swift as 75 kilometers per hour. This suggest that the birds might be following the eye to avoid being blown inland, where they risk crashing onto land or being hit by flying debris, Shepard says.While this is the first time this behavior has been spotted in any bird species, flying with the winds could be a common tactic for preserving energy during cyclones, says Andrew Farnsworth, an ornithologist at Cornell University who was not involved in the study. “It might seem counterintuitive,” he says. “But from the perspective of bird behavior, it makes a lot of sense.”",Some seabirds survive typhoons by flying into them
155,-1,-1_new_said_study_people,https://techcrunch.com/2017/11/06/i-want-to-hear-your-pitch-in-oman-this-thursday/,"I want to hear your pitch in Oman this ThursdayJust a remind that I’m holding one of our patented MicroMeetups in Muscat, Oman on Thursday November 9th from 3pm to 6:30pm.We will meet at the Sheraton Oman.There is no RSVP list so just show up! You can also apply to pitch on stage. If you do not receive an email that you have been chosen to pitch you are not pitching.You can sign up to pitch here. I will pick eight companies to pitch and they will have two minutes to tell their story and two minutes of questions. There are no slides.The winner of the pitch-off will receive a free table at Disrupt Berlin and the second-place winner will receive one ticket to the event.Thanks to the folks from CryptoExplorers.com for the help on the event.",I want to hear your pitch in Oman this Thursday
590,-1,-1_new_said_study_people,https://arstechnica.com/tech-policy/2022/10/company-that-makes-rent-setting-software-for-landlords-sued-for-collusion/,"Front page layoutSite themeProPublica is a Pulitzer Prize-winning investigative newsroom. Sign up forRenters filed a lawsuitThe lawsuit was filed days afterThe proposed class-action lawsuit was filed in US District Court in San Diego.In an email, a RealPage representative said that the company “strongly denies the allegations and will vigorously defend against the lawsuit.” She declined to comment further, saying the company does not comment on pending litigation.The nine property managers named in the lawsuit did not respond immediately to a request for comment.They included some of the nation’s largest landlords, such as Greystar, Lincoln Property Company, Equity Residential, Mid-America Apartment Communities, and FPI Management—which together manage hundreds of thousands of apartments.Four of the five renters named in the suit were Greystar tenants. A fifth rented from Security Properties. Their apartments were located in San Diego, San Francisco, and two Washington state cities, Redmond and Everett.The lawsuit accused the property managers and RealPage of forming “a cartel to artificially inflate the price of and artificially decrease the supply and output of multifamily residential real estate leases from competitive levels.”RealPage’s software uses an algorithm to churn through a trove of data each night to suggest daily prices for available rental units. The software uses not only information about the apartment being priced and the property where it is located, but also private data on what nearby competitors are charging in rents. The software considers actual rents paid to those rivals—not just what they are advertising, the company told ProPublica.Join the Ars Orbital Transmission mailing list to get weekly updates delivered to your inbox.CNMN Collection",Company that makes rent-setting software for landlords sued for collusion
129,-1,-1_new_said_study_people,https://tech.hindustantimes.com/tech/news/signatures-of-alien-technology-could-be-how-humanity-first-finds-extraterrestrial-life-71666434581103.html,"Signatures of alien technology could be how humanity first finds extraterrestrial lifeIf an alien were to look at Earth, many human technologies – from cell towers to fluorescent light bulbs – could be a beacon signifying the presence of life.We are two astronomers who work on the search for extraterrestrial intelligence – or SETI. In our research, we try to characterise and detect signs of technology originating from beyond Earth.These are called technosignatures. While scanning the sky for a TV broadcast of some extraterrestrial Olympics may sound straightforward, searching for signs of distant, advanced civilisations is a much more nuanced and difficult task than it might seem.Saying ‘hello' with radios and lasersThe modern scientific search for extraterrestrial intelligence began in 1959 when astronomers Giuseppe Cocconi and Philip Morrison showed that radio transmissions from Earth could be detected by radio telescopes at interstellar distances.The same year, Frank Drake, launched the first SETI search, Project Ozma, by pointing a large radio telescope a two nearby Sun-like stars to see if he could detect any radio signals coming from them.Following the invention of the laser in 1960, astronomers showed that visible light could also be detected from distant planets.These first, foundational attempts to detect radio or laser signals from another civilisation were all looking for focused, powerful signals that would have been intentionally sent to the solar system and meant to be found.Given the technological limitations of the 1960s, astronomers did not give serious thought to searching for broadcast signals – like television and radio broadcasts on Earth – that would leak into space.But a beam of a radio signal, with all of its power focused towards Earth, could be detectable from much farther away – just picture the difference between a laser and a weak light bulb.The search for intentional radio and laser signals is still one of the most popular SETI strategies today. However, this approach assumes that extraterrestrial civilizations want to communicate with other technologically advanced life.Humans very rarely send targeted signals into space, and some scholars argue that intelligent species may purposefully avoid broadcasting out their locations. This search for signals that no one may be sending is called the SETI Paradox.Leaking radio wavesThough humans don't transmit many intentional signals out to the cosmos, many technologies people use today produce a lot of radio transmissions that leak into space. Some of these signals would be detectable if they came from a nearby star.The worldwide network of television towers constantly emits signals in many directions that leak into space and can accumulate into a detectable, though relatively faint, radio signal.Research is ongoing as to whether current emissions from cell towers in the radio frequency on Earth would be detectable using today's telescopes, but the upcoming Square Kilometre Array radio telescope will be able to detect even fainter radio signals with 50 times the sensitivity of current radio telescope arrays.Not all human-made signals are so unfocused, though. Astronomers and space agencies use beams of radio waves to communicate with satellites and space craft in the solar system.Some researchers also use radio waves for radar to study asteroids. In both of these cases, the radio signals are more focused and pointed out into space.Any extraterrestrial civilisation that happened to be in the line of sight of these beams could likely detect these unambiguously artificial signals.Finding megastructuresAside from finding an actual alien spacecraft, radio waves are the most common technosignatures featured in sci-fi movies and books. But they are not the only signals that could be out there.In 1960, astronomer Freeman Dyson theorised that, since stars are by far the most powerful energy source in any planetary system, a technologically advanced civilisation might collect a significant portion of the star's light as energy with what would essentially be a massive solar panel.Many astronomers call these megastructures, and there are a few ways to detect them.After using the energy in the captured light, the technology of an advanced society would re-emit some of the energy as heat.Astronomers have shown that this heat could be detectable as extra infrared radiation coming from a star system.Another possible way to find a megastructure would be to measure its dimming effect on a star. Specifically, large artificial satellites orbiting a star would periodically block some of its light.This would appear as dips in the star's apparent brightness over time. Astronomers could detect this effect similarly to how distant planets are discovered today.A whole lot of pollutionAnother technosignature that astronomers have thought about is pollution.Chemical pollutants – like nitrogen dioxide and chlorofluorocarbons on Earth are almost exclusively produced by human industry. It is possible to detect these molecules in the atmospheres of exoplanets with the same method the James Webb Space Telescope is using to search distant planets for signs of biology.If astronomers find a planet with an atmosphere filled with chemicals that can only be produced by technology, it may be a sign of life.Finally, artificial light or heat from cities and industry could also be detectable with large optical and infrared telescopes, as would a large number of satellites orbiting a planet.But a civilisation would need to produce far more heat, light and satellites than Earth does to be detectable across the vastness of space using technology humans currently possess.Which signal is best?No astronomer has ever found a confirmed technosignature, so it's hard to say what will be the first sign of alien civilisations.While many astronomers have thought a lot about what might make for a good signal, ultimately, nobody knows what extraterrestrial technology might look like and what signals are out there in the Universe.Some astronomers support a generalised SETI approach which searches for anything in space that current scientific knowledge cannot naturally explain. Some, like us, continue to search for both intentional and unintentional technosignatures.The bottom line is that there are many avenues for detecting distant life. Since no one knows what approach is likely to succeed first, there is still a lot of exciting work left to do.By Macy Huston and Jason Wright, Penn State (The Conversation)",Signatures of alien technology could be how humanity first finds extraterrestrial life
71,-1,-1_new_said_study_people,https://interestingengineering.com/innovation/worlds-largest-carbon-removal-facility,"""With the passage of the Inflation Reduction Act, the proliferation of companies seeking high-quality carbon removal credits, and a disruptive low-cost technology, we now have the ingredients needed to scale DAC to megaton levels by the end of this decade,"" said Adrian Corless, CEO, and CTO, CarbonCapture Inc.""We plan to have our first DAC modules fielded by the end of next year and to continue installing capacity as quickly as modules come off our production line. Our goal is to leverage economies of scale to offer the lowest priced DAC-based carbon removal credits in the market.""Artist’s rendering of Project Bison, a 5-megaton carbon removal project in Wyoming. Business WireMassive amounts of CO2 can be removed from the environment by connecting direct air capture (DAC) devices that CarbonCapture has developed and deployed in enormous arrays, as per the company description.The companies chose Wyoming due to the state's extensive supply of renewable and carbon-free energy sources, as well as its advantageous operating and regulatory conditions for carbon storage.Project Bison generates carbon removal credits by filtering CO2 out of the atmosphere and permanently storing it underground via Class VI injection wells. Business WireWhen will the project start?The project will be the first atmospheric carbon removal facility in the United States to use Class VI wells for long-term storage. It is anticipated to be operational by late 2023.The goal of this collaboration project, called ""Project Bison,"" is to absorb five million tonnes of CO2 annually by 2030, which is roughly the same number of roundtrip flights between London and New York.",Worldâs largest carbon removal facility could suck up 5 million metric tonnes of CO2 yearly
68,-1,-1_new_said_study_people,https://interestingengineering.com/innovation/safe-micro-nuclear-reactor-truck,"New solution provides safer nuclear powerNuclear reactors in the U.S. usually fall under the category of Light-Water Reactor. These split uranium atoms to create energy, with leftover materials and heat stored in solid fuel rods that require running water to keep them cool. If the rods don't receive enough water and overheat, the entire facility is at risk of a meltdown.The new solution proposed by the BYU scientists will store the leftover radioactive elements in molten salts instead of fuel rods. ""Nuclear energy can be extremely safe and extremely affordable, if done the right way,"" Memmott explained. ""It’s a very good solution to the energy situation we're in because there are no emissions or pollution from it.""The new micro-reactor proposal would dissolve all of the radioactive byproducts in molten salt. Salt has an incredibly high melting temperature of about 550°C, meaning it doesn't take long for these byproducts to drop below the melting point. The radiated heat is absorbed into the salt, which doesn't remelt, removing the risk of a meltdown.The products of the reaction are also safely stored within the molten salts, meaning nuclear waste is eliminated. Some of the products can also be extracted and resold. Valuable elements such as cobalt-60, gold, platinum, and neodymium can all be removed from the salt. Molten salt reactors date back to the 1960s, and recent developments have lead to a new surge in experiments, such as the ones at BYU.",Engineers created a safe micro-nuclear reactor that fits in the back of a truck
126,-1,-1_new_said_study_people,https://spectrum.ieee.org/smart-clothes-artificial-muscles,"IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our Privacy Policy.",Artificial Muscles Woven Into Smart Textiles Could Make Clothing Hyperfunctional
67,-1,-1_new_said_study_people,https://interestingengineering.com/innovation/new-chip-transmits-record-breaking-184-petabits-data-per-second,"It saw the use of a photonic chip, a microchip containing two or more photonic components which form a functioning circuit. This technology detects, generates, transports, and processes light to divide a stream of data into thousands of separate channels and transmit them all at once over 7.9 kilometers.“First, the team split the data stream into 37 sections, each of which was sent down a separate core of the fibre-optic cable. Next, each of these channels was split into 223 data chunks that existed in individual slices of the electromagnetic spectrum. This 'frequency comb' of equidistant spikes of light across the spectrum allowed data to be transmitted in different colours at the same time without interfering with each other, massively increasing the capacity of each core,” explained New Scientist.In the past, we have witnessed data transfer rates of up to 10.66 petabits per second but they were created through the use of bulky inefficient and impractical equipment. This new and improved research sets a record for transmission using a single computer chip as a light source. The technology could see energy costs significantly slashed and bandwidths severely increased.Using dummy dataThe experiment used so much data that no computer today exists that could supply or receive this much information at this rate. The team had to therefore pass “dummy data” through all channels, says Jørgensen, and experiment on the output one channel at a time to ensure that it was all being sent and recovered adequately.",New chip transmits a record breaking 1.84 petabits of data per second
127,-1,-1_new_said_study_people,https://step.ukaea.uk,"Spherical Tokamak for Energy ProductionSTEP is a UKAEA programme that will demonstrate the ability to generate net electricity from fusion. It will also determine how the plant will be maintained through its operational life and prove the potential for the plant to produce its own fuel.The first phase of the programme is to produce a concept design by 2024. It will be a spherical tokamak, connected to the National Grid and producing net energy, although it is not expected to be a commercially operating plant at this stage.",Spherical Tokamak for Energy Production
130,-1,-1_new_said_study_people,https://techcrunch.com/2008/02/21/uks-new-badass-binocs-combo-infrared-gps-to-target-baddies-three-miles-away/,"[photopress:jtasbino.jpg,full,center]Like a Certs being a candy mint and a breath mint, the SSRF is a rangefinder and a GPS targeting computer. Using a combination of technologies, the Surveillance System and Range Finder is used to identify enemies up to three miles away, then using the GPS, they call in the big guns to wipe them off the Earth. Kinda creepy.Look for a Terminator-embedded version shortly after Skynet goes active later this year.J-Tas Surveillance System Has GPS, Thermal Imaging, Hunts Predators [The Giz]","UK's new badass binocs combo infrared, GPS to target baddies three miles away"
131,-1,-1_new_said_study_people,https://techcrunch.com/2008/12/16/rubber-tracks-for-military-vechicles/,"One of the weak points of tracked military vechicles are the metal plates. They need to be repaired after every 250 miles while constantly shaking soldiers inside the vechicle. This is about to change for rubber tracks are more durable and convenient while allowing the tanks to use less fuel. According to Treehugger, fuel consumption could be improved by 1/3 by using rubber tracks on MPG tanks and APC’s. There are limitations though since the rubber tracks are only strong enough to be used on vechicles up to 30 tons. Yet.If you happen to have a tank with steel plated tracks, you still have a few days to upgrade it so it meets the new standards.",Rubber tracks for military vechicles
59,-1,-1_new_said_study_people,https://inews.co.uk/news/environment/roads-uk-so-congested-less-healthy-more-lonely-1940265?ITO=newsnow,"ExclusiveBritain’s congested roads are blighting the lives and health of millions of people because they are acting as physical barriers that prevent local journeys by foot, new research has found.Unable to cross roads, that are either clogged or made dangerous by speeding traffic, residents are just opting out of what should be quick trips to local shops, friends or amenities, according to a University College London study.Or else, they are adding to the problem by getting in their cars.Researchers have found that one billion walking and cycling trips don’t take place every year because people can’t face dealing with their local traffic – that means 20 “lost” journeys per person per year.Some 135 million of those trips are replaced by car journeys, 90 million by public transport, and 775 million trips are “suppressed – trips that people want to make but end up not making because of the fear and inconvenience of road traffic”, the study has found.“Britain has a major problem with busy roads that is taking a significant mental and physical health toll in people all over the country,” Paulo Anciaes, of University College London, the lead researcher behind the findings, told i .“It is likely that millions of people in Britain have seen their quality of life reduced, to a greater or lesser extent, by living close to a busy road – with speeding cars or high volumes of traffic – and the problem appears to be getting worse.”The poorest in the country, who are more likely to live near a busy road, are most affected, along with children and people with mobility issues, such as the elderly and disabled, the report, published in the journal Transportation Research, finds.It means that millions of Britons could be missing out on valuable exercise as well as the benefit of being outdoors and socialising, at a cost to their health and wellbeing.Furthermore, shops and other local businesses are suffering from reduced customers as roads put up barriers to access.In total, the report – which includes a survey of a representative sample of 3,038 British adults – estimates that busy roads are costing local communities across Britain £3.2bn a year, or £64 per person, in the form of lower revenues for nearby businesses.Charles Musselwhite, professor at Aberystwyth University and editor-in-chief of Journal of Transport and Health, argues that “we have let cars, vans and lorries take over”, leading to a “spiral of decline in communities where people don’t know each other”.“The more traffic in an area, the less likely we are able to walk and to cycle in the local area, and this reduces our ability to know our neighbours; and the less we know our neighbours, the less there is to have a sense of community,” he said.Some 35 per cent of those surveyed for the UCL report – equating to 17.7 million British adults – said they lived near a road with heavy traffic. And a quarter – equating to 12.6 million British adults – said they lived close to a road where traffic was fast.‘It’s absolutely terrifying’ Allison Pepper lives right by the busy A540 Chester High Road just outside the town of Neston in Cheshire.This makes her family’s life much more difficult, with her children’s journey to the local secondary school, and just walking the dog, harder than it should be. “We live right by the main road by one junction, and to get to school, the children need to walk up by a very busy road which has a big distribution centre along it, so massive lorries go past,” says Ms Pepper, who, by coincidence, is a road traffic collisions solicitor. Allison Pepper lives by the A540 Chester High Road just outside Neston in Cheshire (Photo: Tom Bawden) “To get to the local secondary school, you have to walk along a very narrow pavement and then try and cross a junction. I can tell you it’s scary trying to cross that junction in a car, let alone as a pedestrian in the morning and after school. “We’re probably less than half a mile away from the school – but it takes much longer than it should. They either get a bus, which means going backwards to go forwards – with me driving them backwards to a local village for them to go forwards on a bus.” “Or they can walk twice the distance to get round using safer routes to avoid crossing a really busy junction and the road itself. It’s all a bit crackers really. And I’ve stopped walking the dog up there because it’s absolutely terrifying.”“Traffic volumes tend to be inversely related to speeds, because of congestion,” Dr Anciaes said. “This means that the problem ends up affecting all types of areas, but in different ways.“In cities, the problem is traffic volume – reported as high by 41 per cent of residents, rising to 45 per cent in London. And in rural areas, the problem is traffic speed – reported as fast by 33 per cent in villages and 39 per cent in hamlets.”Dr Anciaes says Finchley Road in north London, between Swiss Cottage and Finchley Road, is one of the biggest offenders when it comes to blocking off residents.“This has been a major barrier to walking trips for several decades. We talked with the local residents – many said they felt afraid of crossing Finchley Road and avoided going to the area on the other side of the road, as a result of that fear,” he said.“One local, a woman of 60, told us ‘Finchley Road is just a big pain, traffic is so heavy – buses, coaches and lorries. It’s not the speed as such, sometimes there is too much congestion for anyone to speed – it’s a river of traffic, constant, non-stop and you don’t want to breathe in the air it’s so full of exhaust fumes’.“The probability of living near a road with a speed perceived as high decreases almost linearly with income. And the problem seems to be worsening in the UK because road traffic volumes are still increasing. This is mostly due to the increase of traffic of light goods vehicles.”Experts have welcomed the report. Dr Stephen Watkins, a former director of public health for Stockport, and chair of the Transport and Health Science Group, said “vibrant local communities are vitally important to the health of urban residents and a busy main road through the middle of them is seriously disruptive”.Tanya Braun, director of policy and communications, Living Streets, the UK charity for everyday walking, said: “A lack of suitable crossings is a real barrier to people getting out and about. We need to see many more measures that protect pedestrians and encourage walking.”Adrian Davis, professor of transport and health at Edinburgh Napier University, said the study revealed “the barrier effect” of busy roads.“Reducing private motorised travel is the only way to solve this,” he said. “Reallocate road space to high quality public transport and walking and cycling”.Not that everyone gives in to the traffic. Dr Anciaes points to one extreme case that hit the headlines some years ago, of an 89-year-old woman living in Dorset, who was partially blind and walked using a frame.For her, a visit to the post office or shop she could see from her home on the other side of a very busy road involved a 90 minute round trip – involving a bus journey to the nearest pedestrian crossing three miles away and back again.",Roads in the UK are so congested that they are making us less healthy and more lonely
58,-1,-1_new_said_study_people,"https://healthcare.utah.edu/publicaffairs/news/2022/10/stillbirth-genetics.php#:~:text=Newly%20published%20research%20is%20the,%2C%20uncles%2C%20or%20male%20cousins.",For All U of U Health Patients & Visitors,"Increased Risk for Stillbirth Passed Down Through Fathers, Male Relatives"
53,-1,-1_new_said_study_people,https://global.chinadaily.com.cn/a/202210/18/WS634e386ea310fd2b29e7d280.html,"chinadaily.com.cn | Updated: 2022-10-18 13:23A model of the transportation system involving a high-speed maglev train running in a low vacuum pipeline. [File photo/The Third Research Institute of China Aerospace Science and Industry Corp]Chinese researchers have successfully carried out a systematic test on a cutting-edge transportation system involving a high-speed maglev train running in a low vacuum pipeline in North China.The test has achieved a maglev train running up to 130 kilometers per hour along the 2 kilometers test line in Yanggao county of Datong, North China's Shanxi province, according to a news release from the North University of China.This is the first time that such a transportation system anywhere in the world underwent a full-scale and full-process integrated test. A series of tests will be carried out in the future.A groundbreaking ceremony for the planned Shanxi provincial laboratory for high speed maglev vehicles operating in a low vacuum pipeline and the proposed Datong (Yanggao) test line project took place on May 24 last year.The provincial lab is jointly built by the North University of China and the Third Research Institute of China Aerospace Science and Industry Corp, to provide a key test platform for low-vacuum pipeline magnetic levitation technology.The lab will build a full-scale 60-kilometer test track in Yanggao county, with the construction to be implemented in three phases, which will finally achieve a maximum speed of 1,000 km/h.The system uses the latest superconducting magnetic levitation technology to disengage from the ground to eliminate frictional resistance, while employing internal pipelines similar to a vacuum to greatly reduce air resistance.",Systematic test of maglev running in tube successful
49,-1,-1_new_said_study_people,https://gizmodo.com/facebook-meta-photos-ads-race-gender-age-study-1849706492,"Facebook’s promise to advertisers is that its system is smart, effective, and easy to use. You upload your ads, fill out a few details, and Facebook’s algorithm does its magic, wading through millions of people to find the perfect audience.The inner workings of that algorithm are opaque, even to people who work at Meta, Facebook’s parent company. But outside research sometimes offers a glimpse. A new study, published Tuesday in the Proceedings of the 22nd ACM Internet Measurement Conference, finds that Facebook uses image recognition software to classify the race, gender, and age of the people pictured in advertisements, and that determination plays a huge role in who sees the ads. Researchers found that more ads with young women get shown to men over 55; that women see more ads with children; and that Black people see more ads with Black people in them.In the study, the researchers created ads for job listings with pictures of people. In some ads they used stock photos, but in others they used AI to generate synthetic pictures that were identical aside from the demographics of the people in the images. Then, the researchers spent tens of thousands of dollars running the ads on Facebook, keeping track of which ads got shown to which users.AdvertisementThe results were dramatic. On average, the audience that saw the synthetic photos of Black people was 81% Black. But when it was a photo of a white person, the average audience was only 50% Black. The audience that saw photos of teenage girls was 57% male. Photos of older women went to an audience that was 58% women.The study also found that the stock images performed identically to the pictures of artificial faces, which demonstrates that it’s just demographics, not other factors, which determines the outcome.Assuming Facebook targeting is effective, this may not be problematic when you’re considering ads for products. But “when we’re talking about advertising for opportunities like jobs, housing, credit, even education, we can see that the things that might have worked quite well for selling products can lead to societally problematic outcomes,” said Piotr Sapiezynski, a researcher at Northeastern University, who co-authored the study alongside PhD candidate Levi Kaplan, undergraduate Nicole Gerzon, and professor Alan Mislove.In response to a request for comment, Meta said the research highlights an industry-wide concern.“We are building technology designed to help address these issues,” said Ashley Settle, a Meta spokesperson. “We’ve made significant efforts to prevent discrimination on our ads platform, and will continue to engage key civil rights groups, academics, and regulators on this work.”AdvertisementFacebook’s ad targeting by race and age may not be in advertisers’ best interests either. Companies often choose the people in their ads to demonstrate that they value diversity. They don’t want fewer white people to see their ads just because they chose a picture of a Black person. Even if Facebook knows older men are more likely to look at ads depicting young women, that doesn’t mean they’re more interested in the products. But there are far bigger consequences at play.“Machine learning, deep learning, all of these technologies are conservative in principle,” Sapiezynski says. He added that systems like Facebook’s optimize systems by looking at what’s worked in the past, and assume that’s how things should look in the future. If algorithms are using crude demographic assumptions to decide who sees ads for housing, jobs, or other opportunities, that can reinforce stereotypes and enshrine discrimination.AdvertisementThat’s already happened on Facebook’s platform. A 2016 ProPublica investigation found Facebook let marketers hide ads for housing from Black people and other protected groups in violation of the Fair Housing Act. After the Department of Justice stepped in, Facebook stopped letting advertisers target ads based on race, religion, and certain other factors.But even if advertisers can’t explicitly tell Facebook to discriminate, the study found that the Facebook algorithm might be doing it based on the pictures they put in their ads anyway. That’s a problem if regulators want to force a change.AdvertisementSettle, the Meta spokesperson, said that Meta has invested in new technology to address its housing discrimination problem and that the company will extend those solutions to ads related to credit and jobs. The company will have more to share in the coming months, she added.AdvertisementYou could look at these results and think, “so what?” Facebook doesn’t publish the data, but maybe ads with pictures of Black people perform worse with white audiences. Sapiezynski said even if that’s true, it’s not a reasonable justification.In the past, newspapers separated job listings by race and gender. Theoretically, that’s efficient if the people doing the hiring were prejudiced. “Maybe this was effective, at the time, but we decided that this is not the right way to approach this,” Sapiezynski said.AdvertisementBut we don’t have even enough data to prove Facebook’s methods are effective. The research may demonstrate that the platforms ad system isn’t as sophisticated as they want you to think. “There isn’t really a deeper understanding of what the ad is actually for. They look at the image, and they create a stereotype of how people behaved previously,” Sapiezynski said. “There is no meaning to it, just crude associations. So these are the examples, I think, that show that the system is not actually doing what the advertiser wants.”Correction: 10/27/2022 3:45 p.m. ET: The original version of this story misstated the name of the journal where the study was published.","Facebook Segments Ads by Race and Age Based on Photos Whether Advertisers Want It or Not, Study Says"
47,-1,-1_new_said_study_people,https://futurism.com/scientists-discovered-secret-ingredient-life,"A team of researchers from Purdue University claim to have discovered the ""chemistry behind the origin of life"" on Earth in simple droplets of water, and they're using strikingly strong language to celebrate the findings.Graham Cooks, chemistry professor at Purdue and lead author of a new paper published in the journal Proceedings of the National Academy of Sciences, called it a ""dramatic discovery"" and the ""secret ingredient for building life"" in a statement.""This is essentially the chemistry behind the origin of life,"" he added. ""This is the first demonstration that primordial molecules, simple amino acids, spontaneously form peptides, the building blocks of life, in droplets of pure water.""And it's not just the principle researchers who are heralding the discovery, saying it could have some vast implications on our understanding of how life formed on Earth billions of years ago.""I find this discovery truly fascinating,"" Alan Doucette, associate professor at Dalhousie University in Nova Scotia, Canada, and expert in the field of mass spectrometry, who was not involved in the research, told Futurism.""To me, the evidence seems to be growing that there is something really quite unique and extraordinary about the chemistry at or within small water droplet surfaces,"" he added.In simple terms, the research supports the decades-old theory that life on Earth started in oceans. Amino acids, which scientists believe came to Earth billions of years ago via meteorite showers, can bond together to form peptides, which have long been considered the building blocks of proteins, and eventually life itself.But what has confounded scientists is the fact that this process requires both aqueous and non-aqueous environments. Now, using state-0f-the-art spectrometers to get a close look at chemical reactions inside water droplets, Cooks and his colleagues found that ""extremely quick reactions can take place"" where water droplets meet the atmosphere.That conclusion could explain how life thrived where the sea meets the land, or where fresh water crosses fertile landscapes.Cooks and his team go as far as to claim that these building blocks of life can spontaneously form in water itself, without the need of other catalysts.""The rates of reactions in droplets are anywhere from a hundred to a million times faster than the same chemicals reacting in bulk solution,"" Cooks said in the statement.Cooks and his team hope their finding could help us understand the basic processes involved in the formation of life on Earth — which, in turn, could also help us investigate if we're alone in the universe, or even create our own versions of living matter.In the short term, however, these reactions could prove useful in other areas such as drug discovery.""We all know water was and is essential to life on Earth,"" Doucette told Futurism. ""But we still have a lot to learn.""More on life: Harvard Study: We Shouldn't Rule Out Underground Life on the Moon and Mars","Scientists Claim to Have Discovered the ""Secret Ingredient for Building Life"""
46,-1,-1_new_said_study_people,https://futurism.com/garth-illingworth-james-webb,"UC Santa Cruz astronomer Garth Illingworth, former Deputy Director of the Space Telescope Science Institute, has had a hell of a career.He's dedicated decades to the pursuit of finding and understanding the most distant galaxies, and was a leader on the team that built the Hubble Space Telescope. And before the Hubble was even in the sky, he'd already started to develop the James Webb Space Telescope (JWST) — yes, that James Webb Space Telescope, the one that's currently blowing Earthlings' minds on the daily with wildly beautiful images of our universe.While most of us look at those JWST pictures and just see pictures, Illingworth and his peers see all that and more: data. Over its few operational months, Webb has already offered an illuminating breadth of information — findings that have confirmed, confounded, and even contradicted existing theories about the cosmos. Curious about what that data means ourselves, we caught up with Illingworth to talk about space telescopes, far away worlds, and the ever evolving scientific process.This interview has been edited and condensed for length and clarity.Futurism: Your work has been extensive. Can you tell us a bit about your research and where it's taken you?Garth Illingworth: Sure, I'll give you the scientific framework. I'm an astronomer, and my key interests have been the early galaxies in the universe. Basically, we live 13.8 billion years after the Big Bang in a great, wonderful spiral galaxy, the Milky Way. But we had to get to this point.The very beginnings have intrigued me for a long time, ever since I saw the Hubble Deep Field back in 1995 — the first deep Hubble image of a blank part of the sky, which turned out not to be blank, but just absolutely packed with galaxies. That's what I've been working on for 25 or so years. Actually, back in the 80s, when I first started thinking about Webb, we hadn't even launched Hubble. Riccardo Giacconi, the director of the Space Telescope Science Institute at that time, said to me: ""You guys really need to work on the next big telescope. Trust me, it's gonna take a long time.""We had to do a rather interesting thing at that point. We had to project forward, even when we didn't know what Hubble was going to discover. We realized that we should go to longer wavelengths, we should really go into the infrared — we felt there were so many ways in which that could reveal aspects of the universe that Hubble would never reveal. It had to be a big telescope to work in the infrared. It had to be really cold, which meant it needs to be a long way from here. When we look back at the drawings now, these very simple-minded drawings, it's completely different from Webb, but in fact Webb operates and has the characteristics we thought of then. It's a big telescope, it's infrared, it's really cold, it's a hell of a long way away from us [laughs].Correct me if I'm wrong, but you and your team discovered what's believed to be the most distant and earliest galaxy that humans have yet seen, dating back to about 400 million years after the Big Bang.Yes. So, about seven or eight years ago, using Hubble, we amazingly found an object that was about 400 million, 450 million years after the Big Bang. I think if you'd asked me 10 years ago whether Hubble would have done that, I would have said no way. But it turned out that right at the limits of Hubble, we were able to find this early galaxy, and we could actually see it with the Spitzer Space Telescope — we could show there was a fuzzy blob there. That sat around as a real enigma for, like, seven years. We couldn't learn much about it, but it pointed to a very interesting change in the way galaxies were building up at early times. So the moment Webb got operational, the big question was: is this object unique? Or are there lots of others like that?Within four days of the Webb data being released in early-to-mid July, we already had a paper submitted to the preprint server. Actually, there were two groups to do it the same day, saying that we've discovered a couple of other objects like that one, and one of them was even further away. This was the sort of step that we had hoped that Webb would do — that it would expand our horizons into earlier times, and it did that incredibly quickly and very well.I think that goes back to the point about working on getting Hubble into space, but already thinking about the next thing. Now, it seems like the James Webb is happening very quickly — but it's because there's already such a large scientific foundation.Yeah, exactly. In the late 1990s, after the Hubble Deep Field came out, the goal of finding the first galaxies became the central goal for Webb. But right around that time, we discovered the first exoplanets. Dark energy was being discussed, and dark matter. There were so many things that Hubble was finding that we knew Webb would make a difference on — we just ended up having to wait 23 years.In July, when the first images were released, we had an hour where we were all seeing them for the first time. I was sitting in the same auditorium at Space Telescope where we had held the first meeting 33 years ago. It was a bit bizarre sitting there, looking around going God, this room looks pretty much the same as when we first talked about Webb, and here we're now seeing the first images coming in. And they're absolutely amazing.One particularly juicy takeaway from the James Webb is that some new data appears to contradict previous findings. Can you tell us more about that early galaxy that was a lot more massive than previously expected?Yes, sure. So this one, which we gave the name GNZ11 — not a very imaginative name, but astronomers are pretty boring when it comes to naming objects [laughs] — pointed to something unusual at these very early times.So in the in the first four days after the Webb images were released, we wrote these papers, and we realized that GNZ11 wasn't unique — there were others of these very bright, very luminous galaxies, which we interpreted as being unusually massive. Then, within weeks, there was another one even further back in time, closer to the Big Bang, that was still very massive. That has really been a surprise. We have to ask ourselves: is it really massive? Or does it have really unusual stars in it that are very bright, but not so much mass? We just don't know at this point, but Webb can answer these questions.What we need to do now is go in and look at those objects in more detail, see if we can learn more about what's actually in that galaxy. What the stars are like, whether there's lots of smaller stars that contribute a lot of mass. Theorists are now wondering: how do you build a galaxy like this so quickly, and does it have a black hole that's been building extremely rapidly in there as well? Are we been deceived? Galaxies can be pretty tricky. The universe can play games with you, even when you have Webb-quality data, but not enough of it.What do you think that a situation like this says about the scientific process itself?This is interesting, because I would say that in times past there was a very slow process of doing things. Data didn't come in very fast. We spent a lot of time working with it, sometimes you'd have to go back and get some more. Then, you know, the papers would come out, and we'd be pretty definitive. Papers come out, everybody thinks ""oh, this is great."" Then a year later, some new data comes along that goes ""well, that was wrong."" You have to recognize you can be wrong at any point, but when you're wrong, you learn new things.I think I've never felt particularly bad if people take the care to do as well as they can at the time, and then go back and revise things. Being wrong isn't bad, it's part of the process. And it's probably inevitable at this stage.Webb has been busy. Is there an upcoming target on its list that you're particularly excited to see and learn more about?Yes, the big image that was shown originally, of the cluster of galaxies, that was pointed to what I think will be extremely valuable in the future for learning more about galaxies. But I don't want to just emphasize the distant galaxies — exoplanets are going to be amazing, and then of course those star-forming regions like Carina and the Tarantula Nebula. Those look magnificent, but there's an incredible amount of science in those as well.And I would just say, you know, when I was sitting there watching the first images, I was just blown away by their beauty and the character there, the information. But one of the things I was thinking afterwards was: in that hour, I saw, like, six sets of data. I have to say, that's more data than I've ever seen from anything in any sort of reasonable time period in my whole life. Scientists are going to be working on those alone for ages, because there's so much information in those. And that was just a pathfinder — I mean, that was tens of hours of time, so we're gonna multiply that by 100, 1000 times every year.One of the things that I often get asked is: why does it matter? It's a lot of money. I've often thought about this, and I think the human race has a deep interest in our origins. We're interested in how we came about, how life came about. And then you really go, well, we're sitting on this little planet, how do the planets form? You can take this origins question, and that's what astronomy is really about. Webb, Hubble, these things are just origins machines. And what I really like about this, in so many ways, is that we're living in a very divisive environment, and this interest cuts across the lot of these political and otherwise areas beautifully.It's one of those places where we still have some common interests — which I hope we can expand in the future! Webb at least should contribute to that.More on the James Webb Space Telescope: Scientists Puzzled Because James Webb Is Seeing Stuff That Shouldn't Be There",The JWST's Data Is So Incredible That Even Those Who Built It Are Questioning Previous Science
138,-1,-1_new_said_study_people,https://techcrunch.com/2016/08/30/vr-on-the-battlefield-to-the-couch-and-back-again-sort-of/,"Fifteen years ago, a team of scientists working at Johns Hopkins University was asked to do what seemed like the impossible: Design a headset that would create an image so immersive and realistic that it would allow anyone to experience what it feels like to be in a car — before that car actually exists. One of the most popular sedans in the U.S. was born this way.It wasn’t until a couple of years ago that the popularity of virtual reality technology really took off. With video games, VR found a home in the world’s living room. Gamers were able to experience imaginary worlds, all from the comfort of their couches. There’s no sign of this slowing down: Business Insider estimates that shipments of VR headsets will grow at 99 percent annually between 2015 and 2020.Indeed, VR is finding widespread adoption in design, education and new forms of entertainment almost every day. You’ve probably been hearing about it online, in the news and on social media. But before VR hit the mainstream, it was first and foremost an incredibly important resource for our armed forces — and still is today.After decades of perfecting VR in military applications, we are finally making this concept of “inhabiting” a computer-generated environment work for home users. And, ironically, VR is going back out to the battlefield — but instead of training fighters, it’s being used to save the world from zombies.VR’s start on the real-world battlefieldFor years, high-end VR technology has helped soldiers. If it was too dangerous or too expensive, you trained in VR. You could learn the layout of buildings before setting foot into a dangerous situation. You could train to operate machinery and weapons, acclimate to new social situations and treat PTSD. Your team could train in battlefield simulations to help reduce casualties. Many of the immersive features that make VR so compelling today were honed in these early days in military applications.Georgia-based Motion Reality has been training first responders, law enforcement and military personnel for many years. A wide-area, high-end tracking system determines the position of multiple trainees and their weapons in a large space. Motion Reality then uses this information to provide a training experience in hostile situations.The key ingredients that make VR so compelling are the same regardless if you are saving the world in an arcade or preparing for a real-life battlefield.The technology used by Motion Reality and others for military training has been critical to bringing VR to public spaces. While the popularity of VR has skyrocketed over the past year, it’s still inaccessible to many. Some don’t want to spend the money to buy high-end gaming PCs. Others don’t have the space in their homes to experience VR beyond a seated encounter. By bringing VR out of the home, the industry has infinite possibilities and new momentum.VR is coming to a public space near youFor the first time, VR is hitting our public theme parks, arcades and movie theaters. An estimated two-dozen VR rollercoasters have debuted this summer in theme parks across North America, Europe and Asia. And companies like Zero Latency and The Void are debuting arcade-like experiences that mix real life and VR.For decades, the industry has been building up the necessary knowledge and tools to take VR to the mainstream. The key ingredients that make VR so compelling are the same regardless if you are saving the world in an arcade or preparing for a real-life battlefield. Even more importantly, the technological requirements of public venues are almost identical to those used in military training. They require sophisticated multi-player tracking, easy cleaning between users, untethered experience functions and more.Zero Latency, an emerging VR company out of Australia, uses wireless weapons and VR headsets powered by powerful backpack computers to allow for untethered walking in a simulated VR theme park. This has been deployed in the SEGA Joypolis park in Tokyo. These systems track the movements of visitors in the game space and allow them to interact with their environment. With the addition of powerful fans and directional audio, users receive a full sensory experience while completing missions. Sound similar?VR will continue to advance in all application areas: gaming, enterprise, military and public entertainment — but the full, simulated sensory experience that was born out of military VR is what is truly going to take VR out of the living room and into our lives.From sci-fi concept to a true part of our reality, the VR market has undergone an incredible transformation. It might be hard to imagine being able to step into a completely virtual, foreign world during your next visit to Disney World or at your daughter’s next birthday party — but it’s coming sooner than you think. Some, including myself, would argue that it is already here.","VR on the battlefield, to the couch and back again â sort of"
139,-1,-1_new_said_study_people,https://techcrunch.com/2016/09/09/military-veterans-provide-a-new-competitive-advantage-for-tech-companies/,"Ask any Silicon Valley CEO what some of their biggest challenges are and you likely will hear “finding and retaining great people.” Tech is booming, yet even now that valuations and financing rounds are coming back to earth, it remains incredibly hard to attract and keep talent amid a competitive ecosystem where there are so many companies going after massive ideas.A few companies, however, have figured out a competitive advantage through a relatively untapped source of talent: military veterans — and the idea is starting to catch on.While the tech industry has been rapidly expanding, the U.S. has seen large numbers of military veterans returning to or looking to enter the private sector. Tech companies can’t find enough skilled people, and veterans are looking for exciting careers to utilize their skills. It should be the perfect match.But as I talked to a number of veterans looking for jobs in the tech industry, many were frustrated that employers often didn’t know how to interpret the relevance of their skills or appreciate their capacity for such all-purpose skills as rapid learning, leadership and pure smarts.That realization nearly three years ago led me to cold-call a group called VetsinTech. I’m not a military veteran myself, but I wanted to help; as a venture capitalist, I get to work with a lot of people in tech companies. At the time, VetsinTech was just a small operation with a mission to help train, connect and find jobs for military veterans wanting careers in tech.What happened next inspired all of us. Led by the indefatigable Katherine Webster and a host of champions from the tech community, such as Craig Newmark and Craig Mullaney (to be clear, you don’t have to be named “Craig” to help veterans, but we don’t mind the name either), VetsinTech started hosting career networking events with top tech employers. This included training workshops, mentor sessions and even hackathons to introduce to some of the most exciting tech companies across the country talented men and women who had served in our armed forces. Katherine pointed out recently that over the next five years, approximately one million servicemen and women will return to the workforce.Military veterans are growing and, in some cases, transforming their careers at technology companies.We also found that veterans make some of the best entrepreneurs. A 2011 Small Business Administration (SBA) study concluded that “veterans are at least 45 percent more likely than those with no active-duty military experience to be self-employed.” Mark Rockefeller, CEO and co-founder of StreetShares (and a veteran) noted that one organization has produced more business owners than any other: the U.S. military.So we ran entrepreneurship programs to mentor, educate and network veterans. We even launched a new national initiative out of a White House and Joining Forces working group that we named “VetCap” (capital for veterans), with a workshop program to teach veterans where and how to raise capital for their businesses.A number of titans in the tech world have stepped up, such as Marc Benioff, the founder and CEO of Salesforce. They have a great heart for helping veterans, but they’re also doing it to bring exceptional people into their companies. For a while, these efforts went unnoticed, but that is changing, and a growing number of companies are seeing this competitive advantage in talent acquisition.LinkedIn recently worked with VetsinTech to sponsor an employer meet-up at Ten-X. In May, Joining Forces, under the leadership of First Lady Michelle Obama and Dr. Jill Biden, convened tech companies and announced “110,000 new hiring commitments and nearly 60,000 new training commitments for veterans and military spouses over the next five years, primarily in the fields of aerospace, telecommunications and tech.”VetsinTech and its partners, like Intuit, Salesforce, Microsoft, Palo Alto Networks, Cisco, HPE, Ten-X and Accenture, were all in attendance to support the Joining Forces initiative. Palo Alto Networks and VetsinTech piloted a cybersecurity training program for veterans. Salesforce is running a training program for veterans called VetForce. Facebook has hosted a couple of hackathons for veterans, including the first ever hackathon for female veterans.Overall, VetsinTech has grown to 12 veteran-led chapters across the U.S. in just three years, and gained support from more than 20 top tech companies to hire veterans and develop training programs.Military veterans are growing and, in some cases, transforming their careers at technology companies in Silicon Valley and the rest of the country. There is more work to do, and still far too many qualified veterans looking for an opportunity. But if there’s one thing the tech industry is good at, it’s recognizing the power of extraordinary people, and tech companies have started to tap into a new talent source that will bolster them for the next big innovations ahead.",Military veterans provide a new competitive advantage for tech companies
35,-1,-1_new_said_study_people,https://electrek.co/2022/09/23/second-largest-us-electric-school-bus-fleet-just-crossed-500k-miles/,"The school year is beginning, so the buses will be out in full force. However, you may notice a significant difference this year as emission-free electric school buses roll out across the United States. One of the nation’s leading school bus manufacturers, Thomas Built Buses, just achieved a major milestone with help from its Virginia-based dealer Sonny Merryman — Saf-T-Liner C2 Jouley electric school buses have now driven more than 500,000 miles.Electric school buses are designed for a cleaner, sustainable future. Not only do they produce zero emissions, but they are also more efficient, can cost less to maintain, and have abilities their gas-powered counterparts lack.450,000 yellow school buses across the United States travel over 4.3 billion miles each year, according to information from the US Department of Transportations National Highway Traffic Safety Administration (NHTSA).More importantly, toxic emissions from traditional school buses can harm students, bus drivers, and the communities they drive in.Although the EPA has introduced stricter standards, it’s not enough as many school buses still emit harmful diesel exhaust. With federal funding more accessible than ever for electric school buses, making the transition makes sense.As of June 2022, 38 states had adopted electric school buses thanks to several initiatives such as the $5 billion Clean School Bus program.Meanwhile, states like Virginia are taking the initiative to provide funding and accelerate the transition. For example, in 2019, Virginia’s Governor Ralph Northam and Dominion Energy announced an initiative to provide 13,000 electric school buses by the end of 2030.Through programs like these, Virginia has grown to become the country’s second-largest electric bus fleet, currently operating 64 Thomas Built electric school buses. In a significant milestone, the electric school buses have now traveled over 500,000 miles, with more buses expected to be delivered as the school year progresses.Growth of electric school buses in Virginia Source: Sonny MerrymanThomas Built Buses achieves 500,000 electric miles in VirginiaThe first electric school bus to roll out in the state of Virginia was Thomas Built Buses’ Saf-T-Liner C2 Jouley in November 2020.One C2 Jouley electric bus can transport 81 students with up to 138 miles of range and a 226 kWh standard battery capacity.Virginia now has 64 electric buses in total. The first 50 were purchased and deployed through the Dominion Energy program, and the remaining 14 were bought using funds from the American Power electric school bus program.The 500,000 miles driven include several different terrains (city, rural, and hills) and distances from less than 20 miles to more than 90 miles. Although the electric buses have primarily been used for regular school routes, a few have made their way to field trips, band competitions, and more.By using electric buses for these trips, 447.7 short tons of greenhouse gases were avoided, according to the AFLEET tool.Through the experience so far, Thomas Built Buses dealer Sonny Merryman and its customers have learned a few critical takeaways that can help others deploy electric buses safely and efficiently:Properly train drivers and technicians for a smooth transition.Consider assigning a partner or team to help with the deployment.Perhaps, most importantly, the electric buses have withstood various operational tests, and drivers who have switched to the electric Jouley school buses have loved them so far, according to the school bus dealer.Electrek’s TakeFirst things first, congratulations to Thomas Built Buses and Sonny Merryman on the huge milestone. Electric school buses protect students and communities from harmful emissions while saving school districts money on fuel and maintenance in the long run.At the same time, I think there is a major takeaway from this case study. State funding works, and electric buses are the future. Virginia is proving it. California has proved it. State leaders need to get on board to speed up the transition. There are no excuses now.","The second largest electric school bus fleet in the US just crossed 500,000 service miles"
140,-1,-1_new_said_study_people,https://techcrunch.com/2016/10/07/poland-builds-a-solar-powered-bike-path-that-glows-a-ghostly-blue/,"Poland can do some cool stuff. To wit: this cool bike path in a town called Pruszków. The path is made of a light-emitting material that charges in the sun and can glow for up to 10 hours in the dark, bathing cyclists in a calming blue glow.The company that made it, TPA sp. z o.o is an engineering firm focused on future tech. They expect this sort of road to be useful in larger projects – highways, say – but for now they’re limiting it to bike paths until they can test the material in the wild. They said that this type of path may be installed in Warsaw soon and that it can glow in multiple colors.The lane uses luminophores – chemicals that “ingest” light – to keep the bike path nicely lit at night. They chose blue to “match the Mazurian landscape” where lakes abound. You can read a bit more at Gazeta Wyborcza if your Polish isn’t too rusty or you can just bask in the cold beauty of a glowing bike lane in deepest Poland.",Poland builds a solar-powered bike path that glows a ghostly blue
72,-1,-1_new_said_study_people,https://interestingengineering.com/science/korea-nuclear-fusion-reactor-100-million-degrees,"Can this be scaled up?The researchers do not completely understand the mechanisms at play that made the plasma stable at such high temperatures but believe that the fast-ion-regulated enhancement (FIRE) or more energetic ions at the core of the plasma were integral to the stability.The KSTAR device has now been shut down, and the carbon components of its inner walls are being replaced with tungsten to improve the reproducibility of the experiments, New Scientist said in its report. The researchers are hopeful that future experiments will be longer and help them move towards a nuclear fusion reactor.Experts told New Scientist that such discoveries were definitely advancing the field of nuclear fusion. However, the problems of the technology were now moving away from physics. The biggest question to address is whether we can harness energy from a fusion reactor in an economical way where the heat can be utilized to get some work. Without this, the technology will not see scale up.Luckily, we can expect more answers to our questions when an international collaboration for nuclear fusion, ITER, attempts to produce net energy at the world's largest nuclear fusion reactor by 2025.The findings of the work conducted at KSTAR were published in the journal Nature.Nuclear fusion is one of the most attractive alternatives to carbon-dependent energy sources1. Harnessing energy from nuclear fusion in a large reactor scale, however, still presents many scientific challenges despite the many years of research and steady advances in magnetic confinement approaches. State-of-the-art magnetic fusion devices cannot yet achieve a sustainable fusion performance, which requires a high temperature above 100 million kelvin and sufficient control of instabilities to ensure steady-state operation on the order of tens of seconds2,3. Here we report experiments at the Korea Superconducting Tokamak Advanced Research4 device producing a plasma fusion regime that satisfies most of the above requirements: thanks to abundant fast ions stabilizing the core plasma turbulence, we generate plasmas at a temperature of 100 million kelvin lasting up to 20 seconds without plasma edge instabilities or impurity accumulation. A low plasma density combined with a moderate input power for operation is key to establishing this regime by preserving a high fraction of fast ions. This regime is rarely subject to disruption and can be sustained reliably even without a sophisticated control, and thus represents a promising path towards commercial fusion reactors.",Nuclear fusion reactor in Korea reaches 100 million degrees Celsius
28,-1,-1_new_said_study_people,https://arstechnica.com/tech-policy/2022/11/us-senator-seeks-antitrust-review-of-apartment-price-setting-software/,"ProPublica is a Pulitzer Prize-winning investigative newsroom. Sign up for The Big Story newsletter to receive stories like this one in your inbox.The chair of a U.S. Senate committee asked the Federal Trade Commission on Tuesday to review whether a Texas-based property tech company’s rent-setting software violates antitrust laws.The move comes after ProPublica published an investigation on October 15 into RealPage’s pricing software, which suggests new rents daily to landlords for all available units in a building. Critics say the software may be helping big landlords operate as a cartel to push rents above competitive levels in some markets.“Alarmingly, recent reporting by ProPublica highlighted that RealPage’s algorithm-based price optimization software, YieldStar, is being used by a growing number of property managers and landlords, potentially impacting pricing and the supply of homes in the rental market,” said the letter signed by US Sen. Sherrod Brown, the Ohio Democrat who chairs the Senate Committee on Banking, Housing, and Urban Affairs. “Renters should have the power to negotiate fairly priced housing, free from illicit collusion and deceptive pricing techniques.”RealPage’s software applies a complex set of mathematical rules to a vast trove of data collected by the company from landlords who are its clients. That data includes the otherwise private data of nearby competitors.“Troublingly, ProPublica reported that a former RealPage executive stated that the data could give insight into how competitors within a half-mile or mile radius are pricing their units,” said the letter, which was addressed to FTC chair Lina Khan.RealPage has said the data fed into its pricing tool is anonymized and aggregated. It said the company “uses aggregated market data from a variety of sources in a legally compliant manner.”In a statement Tuesday, the company said it had not seen the letter, “but we are always willing to engage with policy stakeholders to ensure they have the facts about the competitive dynamics of the housing market and the value and benefits that RealPage creates for renters and housing providers.”Critics say the use of private data is one of the reasons the software invites scrutiny from antitrust enforcers such as the FTC. RealPage also claims its analytics “balance supply and demand to maximize revenue growth.” And the company organizes forums for competitors to meet and discuss aspects of its software, including its pricing algorithms. One legal expert told ProPublica that such collaborations “could raise an antitrust red flag.”AdvertisementIn one neighborhood in Seattle, ProPublica found, 70 percent of apartments were overseen by just 10 property managers, all of which used pricing software sold by RealPage in at least some of their buildings.The Senate letter said the recent reporting on RealPage “raises serious concerns about collusion in the rental market.” It said “the FTC should review whether rent setting algorithms that analyze rent prices through the use of competitors’ private data, such as YieldStar, violate antitrust laws.”RealPage said previously that its revenue management software prioritizes a property’s own internal supply and demand dynamics over external factors such as competitors’ rents. The software helps eliminate the risk of collusion that could occur with manual pricing, which often relies on phone surveys of competitor prices, the company said.An FTC spokesperson said the agency does not comment on letters or requests from Congress.The letter also raised concerns that the pricing software is potentially restricting the supply of apartments. It said that the national rental vacancy rate was just 5.6% at the end of 2021, the lowest since 1984. Even in the tight market, however, it said, there are reports that RealPage’s algorithm sometimes encourages property owners to keep units vacant or push tenants out to increase profits.The letter cited ProPublica’s story, which quoted from a 2017 earnings call with RealPage’s then-CEO, Steve Winn. He explained how one large property company found it could increase profits by raising rents and leaving more apartments vacant.Winn has not responded to requests for comment.“Intentionally holding units vacant, when there are so few homes available, decreases a consumer’s negotiating power and exacerbates the housing shortage,” the letter said.RealPage’s influence over apartment pricing has grown substantially in recent years, following its 2017 acquisition of its biggest pricing competitor, software called Lease RentOptions, or LRO, from The Rainmaker Group. RealPage was pricing 1.5 million units at the time, and the purchase allowed it to double that number. The Department of Justice’s antitrust division took a close look at the merger, but allowed it to proceed.By 2020, RealPage had expanded its number of clients to 31,700 across all its products, which also include accounting, lease management and other software. Private equity firm Thoma Bravo bought RealPage last year for $10.2 billion. It now calls its pricing software AI Revenue Management.After ProPublica published its investigation, a group of tenants filed a lawsuit against RealPage and nine of the country’s biggest landlords, alleging they were colluding to artificially inflate rents.A RealPage spokesperson has denied the allegations and said the company “will vigorously defend against the lawsuit.” She declined to comment further, saying the company does not comment on pending litigation.",US senator seeks antitrust review of apartment price-setting software
73,-1,-1_new_said_study_people,https://interestingengineering.com/science/problems-finding-alien-life-politicians,"How would politicians react to alien contact?The new paper delves into the ""realpolitik"" of a scenario in which global governments react to the discovery of alien life — meaning it outlines how it believes that scenario would play out on the global stage.They highlight a number of troubling scenarios if aliens were to be detected.One of the scenarios outlined sees nations aim to gain a communication and information monopoly with any alien intelligence. This would almost certainly lead to international conflict with other nations fearing those in contact with the extraterrestrials could then gain and harness alien technology to subjugate other nations.However, the scientists also highlight the fact that any nation that was in contact with aliens wouldn't necessarily benefit from their technologies, which would likely be too advanced to comprehend. They suggest that hypothetical technologies allowing ""propulsionless drives, perfect cloaking, or teleportation"" would simply be too complex for a nation to suddenly develop based on initial alien contact.The authors write that such a scenario would be comparable to medieval scholars being handed a textbook on nuclear weapon design — which would be useless in the absence of an understanding of nuclear physics.The paper does also suggest any unrest caused by the perceived technological superiority of a nation in contact with extraterrestrial intelligence could lead to nuclear war and the end of our civilization.Suggestions for a successful SETI scenarioThe researchers, from Penn State Extraterrestrial Intelligence Center and NASA, highlight three key recommendations based on their findings: prioritize transparency between organizations and nations, develop ""post-detection"" protocols, and educate the world's policymakers.","Scientists outlined one of the main problems if we ever find alien life, it's our politicians"
121,-1,-1_new_said_study_people,https://scitechdaily.com/quantum-breakthrough-researchers-demonstrate-full-control-of-a-three-qubit-system/,"Error correction in a silicon qubit system was demonstrated by the researchers.By demonstrating error correction in a three-qubit silicon-based quantum computing device, researchers from RIKEN in Japan have made a significant advancement toward large-scale quantum computing. This research, which was published in Nature, could help make practical quantum computers a reality.Quantum computers are a prominent field of study right now because they promise to solve important problems that are unsolvable with conventional computers. Instead of employing the straightforward 1 or 0 binary bits inherent in traditional computers, they use superimposition states of quantum physics. They are, however, very sensitive to ambient noise and other difficulties, such as decoherence, due to their fundamentally different design and need error correction to do precise calculations.The selection of systems that can serve as the best “qubits,” or basic units needed to do quantum calculations, is a significant challenge today. Each prospective system has advantages and disadvantages of its own. Today’s popular systems include superconducting circuits and ions, which have the benefit of having some type of error correction demonstrated, enabling them to be used in real-world applications, although on a limited scale.Silicon-based quantum technology, which has just recently started to be developed, is known to offer an advantage in that it uses a semiconductor nanostructure comparable to what is frequently used to integrate billions of transistors on a compact chip, and hence potentially benefits from existing manufacturing technology.However, one major problem with silicon-based technology is that there is a lack of technology for error connection. Researchers have previously demonstrated control of two qubits, but that is not enough for error correction, which requires a three-qubit system.In the current research, conducted by researchers at the RIKEN Center for Emergent Matter Science and the RIKEN Center for Quantum Computing, the group achieved this feat, demonstrating full control of a three-qubit system (one of the largest qubit systems in silicon), thus providing a prototype for the first time of quantum error correction in silicon. They achieved this by implementing a three-qubit Toffoli-type quantum gate.According to Kenta Takeda, the first author of the paper, “The idea of implementing a quantum error-correcting code in quantum dots was proposed about a decade ago, so it is not an entirely new concept, but a series of improvements in materials, device fabrication, and measurement techniques allowed us to succeed in this endeavor. We are very happy to have achieved this.”According to Seigo Tarucha, the leader of the research group, “Our next step will be to scale up the system. We think scaling up is the next step. For that, it would be nice to work with semiconductor industry groups capable of manufacturing silicon-based quantum devices on a large scale.Reference: “Quantum error correction with silicon spin qubits” by Kenta Takeda, Akito Noiri, Takashi Nakajima, Takashi Kobayashi, and Seigo Tarucha, 24 August 2022, Nature.DOI: 10.1038/s41586-022-04986-6",Quantum Breakthrough: Researchers Demonstrate Full Control of a Three-Qubit System
94,-1,-1_new_said_study_people,https://news.osu.edu/scientists-hit-their-creative-peak-early-in-their-careers/,"A new study provides the best evidence to date that scientists overall are most innovative and creative early in their careers.Findings showed that, on one important measure, the impact of biomedical scientists’ published work drops by between one-half to two-thirds over the course of their careers.“That’s a huge decline in impact,” said Bruce Weinberg, co-author of the study and professor of economics at The Ohio State University.“We found that as they get older, the work of biomedical scientists was just not as innovative and impactful.”But the reasons behind this trend of declining innovativeness make the findings more nuanced and show why it is still important to support scientists later in their careers, Weinberg said.The study was published online Oct. 7, 2022 in the Journal of Human Resources.Researchers have been studying the relationship between age or experience with innovativeness for nearly 150 years, but no consensus has emerged. Findings, in fact, have been “all over the map,” Weinberg said.“For a topic that so many people with so many approaches have studied for so long, it is pretty remarkable that we still don’t have a conclusive answer.”One advantage of this study is that the authors had a huge dataset to work with – 5.6 million biomedical science articles published over a 30-year period, from 1980 to 2009, and compiled by MEDLINE. These data include detailed information on the authors.This new study measured the innovativeness of the articles by biomedical scientists using a standard method – the number of times other scientists mention (or “cite”) a study in their own work. The more times a study is cited, the more important it is thought to be.With detailed information on the authors of each paper, the researchers in this study were able to compare how often scientists’ work was cited early in their careers compared to later in their careers.As they analyzed the data, Weinberg and his colleagues made a discovery that was a key to understanding how innovation changes over a career.They found that scientists who were the least innovative early in their careers tended to drop out of the field and quit publishing new research. It was the most productive, the most important young scholars who were continuing to produce research 20 or 30 years later.“Early in their careers, scientists show a wide range of innovativeness. But over time, we see selective attrition of the people who are less innovative,” Weinberg said.“So when you look at all biomedical scientists as a group, it doesn’t look like innovation is declining over time. But the fact that the least innovative researchers are dropping out when they are relatively young disguises the fact that, for any one person, innovativeness tends to decline over their career.”Results showed that for the average researcher, a scientific article they published late in their career was cited one-half to two-thirds less often than an article published early in their careers.But it wasn’t just citation counts that suggest researchers were less innovative later in their career.“We constructed additional metrics that captured the breadth of an article’s impact based on the range of fields that cite it, whether the article is employing the best and latest ideas, citing the best and latest research, and whether the article is drawing from multiple disciplines,” said Huifeng Yu, a co-author, who worked on the study as a PhD student at the University at Albany, SUNY.“These other metrics also lead to the same conclusion about declining innovativeness.”The findings showing selective attrition among less-innovative scientists can help explain why previous studies have had such conflicting results, Weinberg said.Studies using Nobel Laureates and other eminent researchers, for whom attrition is relatively small, tend to find earlier peak ages for innovation. In contrast, studies using broader cross-sections of scientists don’t normally find an early peak in creativity, because they don’t account for the attrition.Weinberg noted that attrition in the scientific community may not relate only to innovativeness. Scientists who are women or from underrepresented minorities may not have had the opportunities they needed to succeed, although this study can’t quantify that effect.“Those scientists who succeeded probably did so through a combination of talent, luck, personal background and prior training,” he said.The findings suggest that organizations that fund scientists have to maintain a delicate balance between supporting youth and experience.“Young scientists tend to be at their peak of creativity, but there is also a big mix with some being much more innovative than others. You may not be supporting the very best researchers,” said Gerald Marschke, a co-author of the study and associate professor of economics at the University at Albany,“With older, more experienced scientists, you are getting the ones who have stood the test of time, but who on average are not at their best anymore.”Other co-authors on the study were Matthew Ross of New York University and Joseph Staudt of the U.S. Census Bureau.The research was supported by the National Institute on Aging, the Office of Behavioral and Social Science Research, the National Science Foundation, the Ewing Marion Kauffman and Alfred P. Sloan foundations, and the National Bureau of Economic Research.",Scientists hit their creative peak early in their careers
98,-1,-1_new_said_study_people,https://noctua.at/en/noctua-confirms-am5-heatsink-compatibility-and-announces-free-of-charge-upgrades-for-low-profile-coolers-and-older-heatsink-models,"says Roland Mossig (Noctua CEO).AM5 (LGA1718) is AMD’s upcoming socket for its next-generation Ryzen 7000 (Zen 4) series processors. In short, all Noctua coolers and mounting-kits that support AM4 are upwards compatible with socket AM5, except the NH-L9a-AM4 and the NM-AM4-L9aL9i.All Noctua AM4 mountings except the ones of the NH-L9a-AM4 and the NM-AM4-L9aL9i attach to the threads of the standard AM4 stock backplate. Since these backplate threads and their pattern are identical on AM4 and AM5, our AM4 mountings that attach to the standard AMD backplate also support AM5.This means that all SE-AM4 models as well as all Noctua multi-socket coolers purchased since 01/2019 already support socket AM5. Multi-socket coolers purchased before this date that have already been upgraded to AM4 using the NM-AM4 or NM-AM4-UxS kits also require no further upgrades. Older multi-socket coolers that have been purchased before 2019 and have not yet been upgraded to AM4 can be made compatible with AM5 using these upgrade kits. For easier identification, the NM-AM4 or NM-AM4-UxS kits will be renamed to NM-AM5/4-MP83 (for coolers with 83mm mounting pitch) and NM-AM5/4-MP78 (for coolers with 78mm mounting pitch).The socket compatibility overview in the Noctua Compatibility Centre (NCC) allows customers to see at a glance which Noctua CPU cooler models support socket AM5 out of the box or via mounting upgrade kits and which upgrade kit is required for which cooler model. The AM5 motherboard compatibility list is currently being built up and will be expanded over the coming weeks so that customers can verify that their cooler is fully compatible with their new AM5 motherboard.For ordering the mounting-kits free of charge via Noctua’s website, a proof of purchase of both an eligible Noctua CPU cooler and either an AM5/4 CPU or an AM5/4 motherboard will be required. Depending on the country, express shipping options may be available but will be subject to service charges. Alternatively, customers who need the kits urgently will be able to purchase them on Amazon for a suggested service charge of EUR/USD 7.90.explains Roland Mossig (Noctua CEO).The NH-L9a-AM4 and the NM-AM4-L9aL9i are not compatible with AM5 because they require replacing the standard AMD backplate with a custom one, which isn’t possible on AM5. Therefore, Noctua has announced the new NM-AM5-L9aL9i mounting-kit that will allow the NH-L9a, NH-L9a-AM4 and NH-L9i heatsinks to be upgraded to AM5. The NH-L9i-17xx cannot be used because it has been tailored to the different height specifications of the LGA17xx platform. Like the other mounting-kits, the NM-AM5-L9aL9i will be available free of charge via Noctua.at (from end of October). A new version of the NH-L9a cooler that already includes AM5 mounting hardware is planned for Q1 2023.Designed in Austria, Noctua’s premium cooling components are internationally renowned for their superb quietness, exceptional performance and thoroughgoing quality. Having received more than 6000 awards and recommendations from leading hardware websites and magazines, Noctua’s fans and heatsinks are serving hundreds of thousands of satisfied customers around the globe.",Noctua confirms AM5 heatsink compatibility and announces free-of-charge upgrades for low-profile coolers and older heatsink models
93,-1,-1_new_said_study_people,https://news.osu.edu/humble-leaders-can-help-make-groups-more-effective/,"Leaders of teacher groups who were thought of as humble helped improve professionalism and collaboration among team members, new research has shown.The study, done in China, found that teachers in the Chinese equivalent of professional learning communities (PLCs) were more willing to share their knowledge and expertise when they rated their PLC leaders as being higher in humility.The reason was that humble leaders made teachers feel more empowered to share their knowledge because they felt psychologically safe to take risks, said study co-author Roger Goddard, Novice G. Fawcett Endowed Chair and professor of educational studies at The Ohio State University.“A little humility on the part of leaders goes a long way in helping groups be more productive and collaborative,” Goddard said.“When people feel their leader admits mistakes and is open to learning from others, everyone contributes more and makes these groups more effective.”Goddard conducted the study with Yun Qu of Beijing Normal University in China and Jinjie Zhu, a doctoral student in education at Ohio State. The study was published online recently in the journal Educational Studies.In the United States and elsewhere, PLCs are designed to facilitate professional development through discussions in which teachers share their best practices and what they have learned through their experiences in the classroom.“Teachers can feel fairly isolated in the classroom,” Goddard said. “PLCs help teachers build a sense of community and learn from each other about how to improve classroom instruction.”In China, the equivalent of PLCs are called Teaching Research Groups (TRGs). The leaders of TRGs are experienced teachers who are not traditional administrators, but do serve as supervisors and coordinators and are involved in teacher evaluations, lesson planning and teacher selection.This study involved 537 teachers from 238 TRGs in a variety of both urban and rural schools in China.Teachers rated their TRG leaders on three dimensions of humility: their willingness to view themselves accurately, such as admitting when they didn’t know how to do something; their appreciation of others’ strengths; and their teachability, such as being open to other teachers’ advice.Results showed that teachers who rated their TRG leaders as being higher in humility were more likely to report that they shared their knowledge and expertise in TRG meetings.“The whole point of these groups is for teachers to share their knowledge, so the fact that humble leaders inspired individuals in their groups to be more willing to do this is very significant,” Goddard said.The study also found why humble leaders were so effective in helping their teachers share their knowledge.Results showed that in TRGs with more humble leaders, teachers reported higher levels of psychological safety – they felt they could take risks and knew that others would not act in a way to undermine their efforts.That feeling of safety led them to feel more psychologically empowered: They felt their jobs had meaning, they had autonomy to do their work, and they felt they were competent and that their work had impact in the school.So humble leadership led to teachers feeling psychologically safe, which made them feel empowered and ultimately led them to share their experience and knowledge more fully with their colleagues, Goddard said.“This feeling of teachers that they could safely share their knowledge comes from having a leader who has humility – an openness to learning from others, a willingness to revise opinions, and an appreciation for the strengths of others,” he said.While this research was done in China, Goddard said he believes the results would be similar in the United States and elsewhere.“There’s a lot of evidence that suggests trust is a key part of successful organizations. And feeling psychologically safe and empowered to share your knowledge in the workplace is part of building trust, and that’s what humble leaders help create,” he said.“That is as true in the United States as it is in China.”In the same way, the results should be applicable outside of education.“Many of the same principles that make successful organizations cut across cultures and fields. It makes sense that humble leaders will build trust and better relationships that will increase the effectiveness of any groups that have to work together,” Goddard said.",Humble leaders can help make groups more effective
92,-1,-1_new_said_study_people,https://news.osu.edu/considering-covid-a-hoax-is-gateway-to-belief-in-conspiracy-theories/,"Belief that the COVID-19 pandemic was a hoax – that its severity was exaggerated or that the virus was deliberately released for sinister reasons – functions as a “gateway” to believing in conspiracy theories generally, new research has found.In the two-survey study, people who reported greater belief in conspiracy theories about the pandemic – for which there is no evidence – were more likely to later report they believed that the 2020 presidential election had been stolen from Donald Trump through widespread voter fraud, which is also not true. Participants’ overall inclination to believe in conspiracy theories also increased more among those who reported believing COVID-19 was a hoax.Based on the results, the Ohio State University researchers have proposed the “gateway conspiracy” hypothesis, which argues that conspiracy theory beliefs prompted by a single event lead to increases in conspiratorial thinking over time.Preliminary evidence suggests a sense of distrust may function as one trigger.“It’s speculative, but it appears that once people adopt one conspiracy belief, it promotes distrust in institutions more generally – it could be government, science, the media, whatever,” said senior author Russell Fazio, professor of psychology at Ohio State. “Once you start viewing events through that distrustful lens, it’s very easy to adopt additional conspiracy theories.”The study is published today (Oct. 26, 2022) in the journal PLOS ONE.The field of conspiracy theory research is relatively young, and to date has tended to look for traits that predict the tendency to believe in conspiracy theories at a given point in time.“But if you read interviews or forums frequented by conspiracy theorists, you see a phenomenon where people tend to go down the rabbit hole after something happens in their life that triggers general interest in conspiracy theories,” said first author Javier Granados Samayoa, who completed the work while a graduate student in psychology at Ohio State. “With COVID-19, there was this large event that people could not control, so how could they make sense of it? One way is by adhering to conspiracy theories.”The researchers asked 501 participants in a June 2020 survey to answer questions assessing their beliefs in COVID-19 conspiracy theories, political ideology and what is called conspiracist ideation, or one’s overall affinity for conspiracy theories. In this section, participants used a 5-point scale ranging from “definitely not true” to “definitely true” to rate statements such as “Some UFO sightings and rumors are planned or staged in order to distract the public from real alien contact” and “New and advanced technology which would harm current industry is being suppressed.”Six months later, in December 2020, 107 of those same participants again responded to statements gauging their level of conspiratorial thinking. Researchers further assessed conspiracist ideation by asking participants to report the extent to which they believed that there had been extensive voter fraud in the 2020 presidential election.Statistical analysis showed that participants who reported greater belief that the SARS-CoV-2 virus was released for dark purposes and that the severity of COVID-19 disease was blown out of proportion also reported greater belief that the 2020 election had been stolen from Trump. And compared to their baseline conspiracist ideation measured in the June survey, COVID skeptics had higher levels of general endorsement of conspiracy theories six months later.The association held true even after the analysis took into account the association between belief in conspiracy theories about COVID-19 and voter fraud and conservative political views, said Granados Samayoa, now a postdoctoral fellow at the University of Pennsylvania.The team also cited data from a large United Kingdom multi-part survey conducted during the early spring and late fall of 2020 that supported the gateway conspiracy hypothesis: The Ohio State team’s analysis showed that belief among a nationally representative sample of UK adults that the pandemic was a hoax predicted increases in conspiracist ideation over time.The Ohio State data showed one strong trend suggesting that financial distress during the lockdown could have been a factor in adopting conspiracy theory beliefs about the pandemic – even among those who started off with low levels of conspiracist ideation.“And then there is the question: Once that happens, what changes over time? That’s where we got into this longitudinal work, which has been absent in previous research,” Fazio said.While some past conspiracy theories have turned out to be true, this study focused on beliefs that are not supported by evidence and are undermined by the evidence that does exist. The researchers noted that a better understanding of the dynamics of conspiratorial thinking could help stop the spread of conspiracist ideation, which is associated with a higher risk for violence and discrimination and poor health choices, among other negative individual and societal outcomes.“These findings show that we need to be prepared for any additional large-scale events similar to COVID-19 to stem off conspiracist ideation because once people go down the rabbit hole, they may get stuck,” Granados Samayoa said.This work was supported by the National Science Foundation. Additional co-authors, all from Ohio State, included Fazio lab members Courtney Moore, Shelby Boggs, Jesse Ladanyi and Benjamin Ruisch, now at the University of Kent in England.",Considering COVID a hoax is âgatewayâ to belief in conspiracy theories
105,-1,-1_new_said_study_people,https://phys.org/news/2022-08-infrared-wirelessly-transmit-power-meters.html,"Researchers created a new system that uses infrared light to safely transfer high levels of power over distances of up to 30 meters. This type of long-range optical wireless power transfer system could enable real-time power transmission to fixed and mobile receivers. Credit: Jinyong Ha, Sejong UniversityImagine walking into an airport or grocery store and your smartphone automatically starts charging. This could be a reality one day, thanks to a new wireless laser charging system that overcomes some of the challenges that have hindered previous attempts to develop safe and convenient on-the-go charging systems.""The ability to power devices wirelessly could eliminate the need to carry around power cables for our phones or tablets,"" said research team leader Jinyong Ha from Sejong University in South Korea. ""It could also power various sensors such as those in Internet of Things (IoT) devices and sensors used for monitoring processes in manufacturing plants.""In Optics Express, the researchers describe their new system, which uses infrared light to safely transfer high levels of power. Laboratory tests showed that it could transfer 400 mW light power over distances of up to 30 meters. This power is sufficient for charging sensors, and with further development, it could be increased to levels necessary to charge mobile devices.Several techniques have been studied for long-range wireless power transfer. However, it has been difficult to safely send enough power over meter-level distances. To overcome this challenge, the researchers optimized a method called distributed laser charging, which has recently gained more attention for this application because it provides safe high-power illumination with less light loss.""While most other approaches require the receiving device to be in a special charging cradle or to be stationary, distributed laser charging enables self-alignment without tracking processes as long as the transmitter and receiver are in the line of sight of each other,"" said Ha. ""It also automatically shifts to a safe low power delivery mode if an object or a person blocks the line of sight.""Going the distanceDistributed laser charging works somewhat like a traditional laser but instead of the optical components of the laser cavity being integrated into one device, they are separated into a transmitter and receiver. When the transmitter and receiver are within a line of sight, a laser cavity is formed between them over the air—or free space—which allows the system to deliver light-based power. If an obstacle cuts the transmitter-receiver line of sight, the system automatically switches to a power-safe mode, achieving hazard-free power delivery in the air.In the new system, the researchers used an erbium-doped fiber amplifier optical power source with a central wavelength of 1550 nm. This wavelength range is in the safest region of the spectrum and poses no danger to human eyes or skin at the power used. Another key component was a wavelength division multiplexing filter that created a narrowband beam with optical power within the safety limits for free space propagation.""In the receiver unit, we incorporated a spherical ball lens retroreflector to facilitate 360-degree transmitter-receiver alignment, which maximized the power transfer efficiency,"" said Ha. ""We experimentally observed that the system's overall performance depended on the refractive index of the ball lens, with a 2.003 refractive index being the most effective.""Laboratory testingTo demonstrate the system, the researchers set up a 30-meter separation between a transmitter and a receiver. The transmitter was made of the erbium-doped fiber amplifier optical source, and the receiver unit included a retroreflector, a photovoltaic cell that converts the optical signal to electrical power and an LED that illuminates when power is being delivered. This receiver, which is about 10 by 10 millimeters, could easily be integrated into devices and sensors.The experimental results showed that a single-channel wireless optical power transfer system could provide an optical power of 400 mW with a channel linewidth of 1 nm over a distance of 30 meters. The photovoltaic converted this to an electrical power of 85 mW. The researchers also showed that the system automatically shifted to a safe power transfer mode when the line of sight was interrupted by a human hand. In this mode, the transmitter produced an incredibly low intensity light that did not pose any risk to people.""Using the laser charging system to replace power cords in factories could save on maintenance and replacement costs,"" said Ha. ""This could be particularly useful in harsh environments where electrical connections can cause interference or pose a fire hazard.""Now that they have demonstrated the system, the researchers are working to make it more practical. For example, the efficiency of the photovoltaic cell could be increased to better convert light into electrical power. They also plan to develop a way to use the system to charge multiple receivers simultaneously.More information: Nadeem Javed et al, Long-range wireless optical power transfer system using an EDFA, Optics Express (2022). DOI: 10.1364/OE.468766 Journal information: Optics ExpressProvided by Optica",Researchers use infrared light to wirelessly transmit power over 30 meters
107,-1,-1_new_said_study_people,https://phys.org/news/2022-10-scientists-exotic-quantum-state-room.html,"Researchers at Princeton found that a material known as a topological insulator, made from the elements bismuth and bromine, exhibit specialized quantum behaviors normally seen only under extreme experimental conditions of high pressures and temperatures near absolute zero. Credit: Shafayat Hossain and M. Zahid Hasan of Princeton UniversityFor the first time, physicists have observed novel quantum effects in a topological insulator at room temperature. This breakthrough, published as the cover article of the October issue of Nature Materials, came when Princeton scientists explored a topological material based on the element bismuth.The scientists have used topological insulators to demonstrate quantum effects for more than a decade, but this experiment is the first time these effects have been observed at room temperature. Typically, inducing and observing quantum states in topological insulators requires temperatures around absolute zero, which is equal to -459 degrees Fahrenheit (or -273 degrees Celsius).This finding opens up a new range of possibilities for the development of efficient quantum technologies, such as spin-based electronics, which may potentially replace many current electronic systems for higher energy efficiency.In recent years, the study of topological states of matter has attracted considerable attention among physicists and engineers and is presently the focus of much international interest and research. This area of study combines quantum physics with topology—a branch of theoretical mathematics that explores geometric properties that can be deformed but not intrinsically changed.""The novel topological properties of matter have emerged as one of the most sought-after treasures in modern physics, both from a fundamental physics point of view and for finding potential applications in next-generation quantum engineering and nanotechnologies,"" said M. Zahid Hasan, the Eugene Higgins Professor of Physics at Princeton University, who led the research.""This work was enabled by multiple innovative experimental advances in our lab at Princeton,"" added Hasan.The main device component used to investigate the mysteries of quantum topology is called a topological insulator. This is a unique device that act as an insulator in its interior, which means that the electrons inside are not free to move around and therefore do not conduct electricity.However, the electrons on the device's edges are free to move around, meaning they are conductive. Moreover, because of the special properties of topology, the electrons flowing along the edges are not hampered by any defects or deformations. This device has the potential not only of improving technology but also of generating a greater understanding of matter itself by probing quantum electronic properties.Until now, however, there has been a major stumbling block in the quest to use the materials and devices for applications in functional devices. ""There is a lot of interest in topological materials and people often talk about their great potential for practical applications,"" said Hasan, ""but until some macroscopic quantum topological effect can be manifested at room temperature, these applications will likely remain unrealized.""This is because ambient or high temperatures create what physicists call ""thermal noise,"" which is defined as a rise in temperature such that the atoms begin to vibrate violently. This action can disrupt delicate quantum systems, thereby collapsing the quantum state. In topological insulators, in particular, these higher temperatures create a situation in which the electrons on the surface of the insulator invade the interior, or ""bulk,"" of the insulator, and cause the electrons there to also begin conducting, which dilutes or breaks the special quantum effect.The way around this is to subject such experiments to exceptionally cold temperatures, typically at or near absolute zero. At these incredibly low temperatures, atomic and subatomic particles cease vibrating and are consequently easier to manipulate. However, creating and maintaining an ultra-cold environment is impractical for many applications; it is costly, bulky and consumes a considerable amount of energy.But Hasan and his team have developed an innovative way to bypass this problem. Building on their experience with topological materials and working with many collaborators, they fabricated a new kind of topological insulator made from bismuth bromide (chemical formula α-Bi 4 Br 4 ), which is an inorganic crystalline compound sometimes used for water treatment and chemical analyses.""This is just terrific that we found them without giant pressure or an ultra-high magnetic field, thus making the materials more accessible for developing next-generation quantum technology,"" said Nana Shumiya, who earned her Ph.D. at Princeton, is a postdoctoral research associate in electrical and computer engineering, and is one of the three co-first authors of the paper.She added, ""I believe our discovery will significantly advance the quantum frontier.""The discovery's roots lie in the workings of the quantum Hall effect—a form of topological effect that was the subject of the Nobel Prize in Physics in 1985. Since that time, topological phases have been intensely studied. Many new classes of quantum materials with topological electronic structures have been found, including topological insulators, topological superconductors, topological magnets and Weyl semimetals.While experimental discoveries were rapidly being made, theoretical discoveries were also progressing. Important theoretical concepts on two-dimensional (2D) topological insulators were put forward in 1988 by F. Duncan Haldane, the Sherman Fairchild University Professor of Physics at Princeton.He was awarded the Nobel Prize in Physics in 2016 for theoretical discoveries of topological phase transitions and a type of 2D topological insulators. Subsequent theoretical developments showed that topological insulators can take the form of two copies of Haldane's model based on electron's spin-orbit interaction.Hasan and his team have been on a decade-long search for a topological quantum state that may also operate at room temperature, following their discovery of the first examples of three-dimensional topological insulators in 2007. Recently, they found a materials solution to Haldane's conjecture in a kagome lattice magnet that is capable of operating at room temperature, which also exhibits the desired quantization.""The kagome lattice topological insulators can be designed to possess relativistic band crossings and strong electron-electron interactions. Both are essential for novel magnetism,"" said Hasan. ""Therefore, we realized that kagome magnets are a promising system in which to search for topological magnet phases, as they are like the topological insulators that we discovered and studied more than ten years ago.""""A suitable atomic chemistry and structure design coupled to first-principles theory is the crucial step to make topological insulator's speculative prediction realistic in a high-temperature setting,"" said Hasan. ""There are hundreds of topological materials, and we need both intuition, experience, materials-specific calculations, and intense experimental efforts to eventually find the right material for in-depth exploration. And that took us on a decade-long journey of investigating many bismuth-based materials.""Insulators, like semiconductors, have what are called insulating, or band, gaps. These are in essence ""barriers"" between orbiting electrons, a sort of ""no-man's-land"" where electrons cannot go. These band gaps are extremely important because, among other things, they provide the lynchpin in overcoming the limitation of achieving a quantum state imposed by thermal noise.They do this if the width of the band gap exceeds the width of the thermal noise. But too large a band gap can potentially disrupt the spin-orbit coupling of the electrons—this is the interaction between the electron's spin and its orbital motion around the nucleus. When this disruption occurs, the topological quantum state collapses. Therefore, the trick in inducing and maintaining a quantum effect is to find a balance between a large band gap and the spin-orbit coupling effects.Following a proposal by collaborators and co-authors Fan Zhang and Yugui Yao to explore a type of Weyl metals, Hasan and team studied the bismuth bromide family of materials. But the team was not able to observe the Weyl phenomena in these materials. Hasan and his team instead discovered that the bismuth bromide insulator has properties that make it more ideal compared to a bismuth-antimony based topological insulator (Bi-Sb alloys) that they had studied before.It has a large insulating gap of over 200 meV (""milli electron volts""). This is large enough to overcome thermal noise, but small enough so that it does not disrupt the spin-orbit coupling effect and band inversion topology.""In this case, in our experiments, we found a balance between spin-orbit coupling effects and large band gap width,"" said Hasan. ""We found there is a 'sweet spot' where you can have relatively large spin-orbit coupling to create a topological twist as well as raise the band gap without destroying it. It's kind of like a balance point for the bismuth-based materials that we have been studying for a long time.""The researchers knew they had achieved their goal when they viewed what was going on in the experiment through a sub-atomic resolution scanning tunneling microscope, a unique device that uses a property known as ""quantum tunneling,"" where electrons are funneled between the sharp metallic, single-atom tip of the microscope and the sample.The microscope uses this tunneling current rather than light to view the world of electrons on the atomic scale. The researchers observed a clear quantum spin Hall edge state, which is one of the important properties that uniquely exist in topological systems. This required additional novel instrumentation to uniquely isolate the topological effect.""For the first time, we demonstrated that there's a class of bismuth-based topological materials that the topology survives up to room temperature,"" said Hasan. ""We are very confidant of our result.""This finding is the culmination of many years of hard-won experimental work and required additional novel instrumentation ideas to be introduced in the experiments. Hasan has been a leading researcher in the field of experimental quantum topological materials with novel experimentation methodologies for over 15 years; and, indeed, was one of the field's early pioneer researchers.Between 2005 and 2007, for example, he and his team of researchers discovered topological order in a three-dimensional bismuth-antimony bulk solid, a semiconducting alloy and related topological Dirac materials using novel experimental methods. This led to the discovery of topological magnetic materials. Between 2014 and 2015, they discovered a new class of topological materials called magnetic Weyl semimetals.The researchers believe this breakthrough will open the door to a whole host of future research possibilities and applications in quantum technologies.""We believe this finding may be the starting point of future development in nanotechnology,"" said Shafayat Hossain, a postdoctoral research associate in Hasan's lab and another co-first author of the study. ""There have been so many proposed possibilities in topological technology that await, and finding appropriate materials coupled with novel instrumentation is one of the keys for this.""One area of research where Hasan and his team believe this breakthrough will have particular impact is on next-generation quantum technologies. The researchers believe this new breakthrough will hasten the development of more efficient, and ""greener"" quantum materials.Currently, the theoretical and experimental focus of the group is concentrated in two directions, said Hasan.First, the researchers want to determine what other topological materials might operate at room temperature, and, importantly, provide other scientists the tools and novel instrumentation methods to identify materials that will operate at room and high temperatures.Second, the researchers want to continue to probe deeper into the quantum world now that this finding has made it possible to conduct experiments at higher temperatures.These studies will require the development of another set of new instrumentations and techniques to fully harness the enormous potential of these materials. ""I see a tremendous opportunity for further in-depth exploration of exotic and complex quantum phenomena with our new instrumentation, tracking more finer details in macroscopic quantum states,"" Hasan said. ""Who knows what we will discover?""""Our research is a real step forward in demonstrating the potential of topological materials for energy-saving applications,"" added Hasan. ""What we've done here with this experiment is plant a seed to encourage other scientists and engineers to dream big.""More information: Nana Shumiya et al, Evidence of a room-temperature quantum spin Hall edge state in a higher-order topological insulator, Nature Materials (2022). DOI: 10.1038/s41563-022-01304-3 Journal information: Nature Materials",Scientists discover exotic quantum state at room temperature
87,-1,-1_new_said_study_people,https://news.ku.dk/all_news/2022/10/republican-party-lost-core-supporters-after-the-attack-on-capitol/,"Five people were killed and many more serious injured when Trump supporters attacked the congressional building on Capitol Hill on January 6, 2021. In the immediate aftermath, the Republican Party lost core supporters in great numbers. This is documented by three researchers from the Department of Political Science in a new study.A violent attack on democratic institutions limits party loyalty, even among core supporters. There is therefore a measurable cost to encouraging or even exercising political violence. Frederik Hjorth, associate professor""The attack on Congress caused a large drop in people who outwardly identified with the Republican Party and Donald Trump – without re-identification in the following weeks,"" says associate professor Frederik Hjorth, who is one of the authors behind the study. The others are Gregory Eady, assistant professor, and Peter Thisted Dinesen, professor.The researchers' findings indicate that there are limits to party loyalty:""A violent attack on democratic institutions limits party loyalty, even among core supporters. There is therefore a measurable cost to encouraging or even exercising political violence,"" Frederik Hjorth assesses.Few supporters returnThe study is based on panel data from a large group of American Twitter users. These data show that a significant number of Republicans removed their statements of sympathy for the party and Donald Trump after the attack on Capitol Hill.The results contribute on several levels to the understanding of how violent political protests influence political identities and behaviour.""We see, among other things, that the reactions to the riots in 2021 were not just local but national, and that they continued after the attack on the US Congress. In the two months that followed, only a small number of disaffected Republicans re-identified publicly with the party,” explains Gregory Eady.Good news for democracyThe study of the Republicans' reaction to the attack on Congress is encouraging news for democracy, believes Peter Thisted Dinesen.""Political violence is used in many places in the world – but it can have costs in the form of demobilization of party loyalists,"" observes Peter Thisted Dinesen.However, he emphasizes that there may be other considerations which will continue to cause people to use political violence.""Radicalized party members, like those who attacked the Congress, may choose to resort to violence based on their own interests – regardless of how it may harm the party. Our study does not uncover all aspects of political violence - but it contributes to increasing the overall understanding of what attracts or repels people from parties,"" concludes Peter Thisted Dinesen.Read more about the study in this research article published in the American Political Science Review.",Republican Party lost core supporters after the attack on Capitol
109,-1,-1_new_said_study_people,https://propermanchester.com/trending/primary-school-kids-to-be-fed-insects-as-an-eco-friendly-alternative-protein/,"Pupils at four primary schools will be offered edible insects as scientists urge young people to embrace ‘alternative protein’ and eco-friendly meat substitutes.While most children expect to eat the likes of lasagne and fish and chips while at school, pupils at four Welsh primary schools will soon be given the chance to sample bugs and insects as part of a new environmental study.Researchers hope to feed the pupils a product called VeXo, a combination of insect and plant-based protein said to resemble ‘conventional’ mince.The children will also take part in workshops organised by scientists and teachers to inform them about the benefits of eating ‘alternative protein’ like bugs.The study will also use surveys, interviews and focus groups to explore pupil’s understandings of alternative proteins – and as part of the research they will be offered a sample if they wish to try it.According to i newspaper, researchers are hoping to use data from the study to learn how best to educate children about the nutritional and environmental benefits of eating bugs and insects – such as crickets, silkworms, locusts and mealworms – as an alternative protein source.Read More: School dinners could become smaller or use ‘cheaper ingredients’ amid cost of living crisisThe study lead, Christopher Bear from Cardiff University, said: “We want the children to think about alternative proteins as real things for now, rather than just as foods for the future, so trying some of these foods is central to the research.“Although edible insects are – for now – not sold widely in the UK, they form part of the diet of around 2 billion people worldwide.“Much of this is in parts of the world where they are part of long-standing culinary traditions. And they are increasingly popular elsewhere.”The headteacher of Roch Community Primary in Pembrokeshire, one of the schools taking part in the study, said the issue was ‘important’ but acknowledged it was ‘difficult’ for youngsters to make sense of the issue.He said: “There is an important connection between our local community, food production and wider global issues surrounding sustainable development.“These issues are important to children, but also difficult to make sense of and can often be confusing for them.”",Primary school kids to be fed insects as an eco-friendly âalternative proteinâ
110,-1,-1_new_said_study_people,https://pubmed.ncbi.nlm.nih.gov/35952344/,"Access DeniedYour access to the NCBI website at www.ncbi.nlm.nih.gov has been temporarily blocked due to a possible misuse/abuse situation involving your site. This is not an indication of a security issue such as a virus or attack. It could be something as simple as a run away script or learning how to better use E-utilities, http://www.ncbi.nlm.nih.gov/books/NBK25497/, for more efficient work such that your work does not impact the ability of other researchers to also use our site. To restore access and understand how to better interact with our site to avoid this in the future, please have your system administrator contact info@ncbi.nlm.nih.gov.","Walking or body weight squat ""activity snacks"" increase dietary amino acid utilization for myofibrillar protein synthesis during prolonged sitting"
86,-1,-1_new_said_study_people,https://news.ku.dk/all_news/2022/10/dictatorships-use-sporting-events-as-a-smokescreen-for-political-repression/,"In November, the FIFA World Cup 2022 will take place in Qatar. Allowing the Gulf monarchy, notorious for not taking human rights seriously, to host one of the world's biggest sporting events has been met with widespread criticism. Others argue that it can promote dialogue and understanding of human rights when dictatorships are allowed to hold international sporting events.However, a new study by Adam Scharpf, assistant professor at the University of Copenhagen, and his colleagues from the Hertie School and Carnegie Mellon University, sheds a depressing light on the matter. The study shows that international sporting events often trigger a wave of repression when they take place in autocracies.Struck during football matchesIn particular, regimes use the prelude to major sporting events to crack down on potential ""troublemakers"" – typically dissidents and political opponents. The researchers find a core example in Argentina, where a military dictatorship hosted the soccer World Cup in 1978.Here, the researchers have examined the circumstances of the thousands of disappearances and murders presented by the Argentine Truth Commission after the fall of the dictatorship in 1983. The results reveal three phases of state repression: before, during and after the World Cup.""Several weeks before the opening match, the Argentine regime carried out a huge operation, in which the authorities systematically kidnapped or murdered potential troublemakers - especially at night and in the early hours of the morning,"" says Adam Scharpf. He elaborates:""During the World Cup itself, the regime struck discreetly while the matches were being played and the journalists were busy covering the matches. After the final and the departure of the foreign journalists, the regime ramped up another wave of violence,"" explains Adam Scharpf.Nazi Olympics and jungle boxingAccording to the authors, autocratic regimes do a cold-blooded cost-benefit analysis as hosts of international sporting events. Once the competitions are underway, the autocrats receive almost undivided attention from around the world. They use that attention to paint a picture of openness, hospitality and togetherness.""But the limelight also contains dangers for those in power. Their political opponents can use the sporting events to demonstrate their discontent – under the indirect protection of foreign journalists. This is why the autocrats come down hard on their critics before the sporting events take place,"" emphasizes Adam Scharpf.He and his research colleagues have found signs of a similar pattern of violence at the 1936 Olympics held in Berlin (hosted by the Nazi regime), at the legendary boxing match ""The Rumble in the Jungle"" between Muhammed Ali and George Foreman in Zaire (under dictator Mobutu Sese Seko) and at the 2008 Beijing Olympics.""We have discovered a clear and very worrying trend - not least because the proportion of autocratic hosts of major sporting events has more than quadrupled since the end of the Cold War,"" Adam Scharpf points out.He concludes that the awarding of international sporting events to dictatorships only exacerbates human rights abuses.""But it will require a broad, social alliance to pressure politicians and international sports federations to prevent dictatorships from hosting major sporting events in the future,"" assesses Adam Scharpf.Read more about the study in this research article published by American Political Science Review.",Dictatorships use sporting events as a smokescreen for political repression
84,-1,-1_new_said_study_people,https://newatlas.com/telecommunications/optical-chip-fastest-data-transmission-record-entire-internet-traffic/,"The speed record for data transmission using a single light source and optical chip has been shattered once again. Engineers have transmitted data at a blistering rate of 1.84 petabits per second (Pbit/s), almost twice the global internet traffic per second.It’s hard to overstate just how fast 1.84 Pbit/s really is. Your home internet is probably getting a few hundred megabits per second, or if you’re really lucky, you might be on a 1-gigabit or even 10-gigabit connection – but 1 petabit is a million gigabits. It’s more than 20 times faster than ESnet6, the upcoming upgrade to the scientific network used by the likes of NASA.Even more impressive is the fact this new speed record was set using a single light source and a single optical chip. An infrared laser is beamed into a chip called a frequency comb that splits the light into hundreds of different frequencies, or colors. Data can then be encoded into the light by modulating the amplitude, phase and polarization of each of these frequencies, before recombining them into one beam and transmitting it through optical fiber.In experiments, researchers from the Technical University of Denmark (DTU) and Chalmers University of Technology used the setup to transmit data at 1.84 Pbit/s, encoded in 223 wavelength channels, down a 7.9-km-long (4.9-mile) optical fiber that contained 37 separate cores. For reference, the global internet bandwidth has been estimated at just shy of 1 Pbit/s, meaning this system could potentially handle all of that at once with plenty of room to grow.This data transmission speed greatly exceeds the previous record of 1.02 Pbit/s, which was only set in May this year. A previous optical chip design, similar to that used in the new study, managed 44 terabits per second in mid-2020.But the new chip is far from finished breaking records, according to the team behind it. Using a computational model to scale the data transmission potential of the system, the researchers claim that it could eventually reach eye-watering speeds of up to 100 Pbit/s.“The reason for this is that our solution is scalable – both in terms of creating many frequencies and in terms of splitting the frequency comb into many spatial copies and then optically amplifying them, and using them as parallel sources with which we can transmit data,” said Professor Leif Katsuo Oxenløwe, lead author of the study. “Although the comb copies must be amplified, we do not lose the qualities of the comb, which we utilize for spectrally efficient data transmission.”The research was published in the journal Nature Photonics.Source: DTU",Record-breaking chip can transmit entire internet's traffic per second
114,-1,-1_new_said_study_people,https://religionmediacentre.org.uk/news/the-future-of-religion-in-britain/,"A leading academic predicts Britain will see the continuing decline of Christianity, the resurgence of fundamentalism, the rise of non-religion, the emergence of British Islam and a flourishing interest in “magic”.Professor Linda Woodhead, speaking at the Religion Media Centre’s annual lecture in London, gave the context to the 2021 census results on religion, which are due to be published next month or in November.She said the census was expected to show a fall in those identifying as Christian in England and Wales — from 72 per cent in 2001, to 59 per cent in 2011, — to perhaps below 50 per cent this time.Second, she said a steady rise in the number identifying as “none”, having no religion, increased from 15 per cent in 2001, to 25 per cent in 2011, with predictions that this could rise to 33 per cent this time.And third, a steady rise in the number of those identifying as Muslim went from 3 per cent in 2001 to 5 per cent in 2011, and would perhaps increase to 8 per cent this time.Woodhead, the F. D. Maurice professor and head of the department of theology and religious studies at King’s College London, has written widely on religion in society.During her lecture at St Bride’s Church, Fleet Street, she offered four predictions for the future of religion in the coming decades, building on work from the 1970s and 1980s that had proved remarkably prescient.Her method for analysing which religious movements and phenomena would survive, was to judge them against five signs of vitality: intellectual understanding of the metaphysical, continuing institutional practices, community and belonging, ethics and values, and spirituality connecting with the sacred.She used this method to understand four trends in the religious life of people in Britain, to suggest which would thrive or die. The trends identified are: fundamentalism, winning the battle, but losing the war; Muslims finding their place, or Islam finding its place; magic and the decline of religion; and the fire sale of the churches and the growth of non-religion.“Fundamentalism has had a fantastic 30 years, maybe 60 years in all the world religions and in virtually all the churches in Britain,” she suggested. Espousing “plain facts” such as the bodily resurrection of Jesus, combined with moral puritanism was inherently exclusive: “We’ve got the truth. You haven’t.”Fundamentalism had captured the churches — and the media. In many media interviews when she started talking about versions of Christianity, the response of the reporter was that this was “too wishy-washy” and was not real religion.Fundamentalism had won the battle but lost the war, she said. Revivals had not stemmed the relentless decline of Christianity, in fact the post-war strict moral stands against divorce, remarriage, women’s equality and LGBTQ+ rights had turned Christianity into a toxic brand and the next generation did not want to be associated with it.There had been a failure of liberal Christianity to provide an intellectual theology, credible answers on matters of belief, so it had lost its cultural capital. She said Christians in this country were just as liberal in their ethics as the general population: Anglicans were indistinguishable from the general population in their ethical views on such issues as euthanasia, divorce, and abortion. But their leaders were reacting against ideas that were absolutely dominant.Another trend, she explained, was the “fire sale” of the “cultural property” of the churches and the growth of non-religion.This meant that different secular organisations had siphoned off work in education and knowledge. Other providers had offered similar services, not connected to a religious name. The churches’ rituals, values, beliefs, practices, community had come apart.Challenged to state the importance of faith action on the front line of welfare support, for example after the Grenfell tower fire, she said this function was a charitable role and not a lot to do with religion.Charities with religious foundations had tried to wipe this from their branding. Association with faith caused charities problems of legitimacy with the statutory authorities, funders and their beneficiaries, whose fear of proselytization or conversion had to be overcome.But it was the very serious failure over values that indicated the decline of Christianity would continue.“For a majority religion to offend deeply against people’s moral sensibility — that’s a very, very serious failure. The churches have failed so dramatically as moral authorities, with the abuse scandals being just the latest nail in that coffin.” – Professor Linda WoodheadProfessor Woodhead observed that the result was a free-for-all in the area of values leadership, with schools, businesses and universities publishing values statements and then trying to embody and enforce them.Queen Elizabeth II was a valued leader of much greater stature than any Christian leader in the country, she said. It was a big question as to how self-sacrifice and love of others would be underpinned after her death.Into this void created by failures over morals and new providers taking away work, non-religion had grown. It was just a label used in surveys and the census, the professor said. Its meaning could be gleaned by looking at the context in which it developed from 1992 to date.Professor Woodhead did, however, identify two areas of strong growth over the next decades. One is British Islam, or British Muslims, finding their place.She said Islam in Britain was not really visible until the 1970s. The first generation from south Asia had to create their infrastructure from scratch — mosques, halal butchers and schools. The second generation set up civil society organisations and charities.Now British Muslims were finding their voice with Muslim MPs and confident young people entering higher education, mixing with the many different ethnic Muslim communities and creating a new cultural kind of British Islam with food, clothing, music, and creative places.“British Muslim cultural output is cool around the world and is getting a global profile,” she said. On her list of how to judge whether a movement had vital signs, she said British Islam ticked all the boxes for community, institutions and spirituality.The key challenge was keeping it intellectually healthy. There were now centres of Islamic studies in many universities, with moves to study metaphysics and theology, and not just Islamic law.Asked about fears that a rise in the British Muslim population would stoke a backlash with alarmist headlines, Professor Woodhead said: “It’s about becoming the most vital part of religion in this country. That’s a very frightening prospect to a lot of people who don’t know about Islam.”And finally her prediction is for the continuing fascination and flourishing of magic — fairy beliefs, ghosts, omens, popular prophecies, village healers, wise women, souls and the afterlife — alongside a decline in religion.She said: “More of my students and Gen Z as a whole are actively interested in astrology and tarot. They access it on apps on their phone. And those who want to go deeper into intellectually read widely, they listen to podcasts, they go to festivals, they might join a pagan group or a Wiccan group.”Such fascination had probably been there in every society in every age. But the difference now was that it had lost its stigma. People spoke to one another about it. “Paganism has grown and diversified and created lots of different sub-cultures, both right wing, and left wing, and it’s vital and many young people are involved,” she said.“Paganism has grown and diversified and created lots of different sub-cultures, both right wing, and left wing, and it’s vital and many young people are involved” – Professor Linda WoodheadProfessor Woodhead has noticed that magic has crept into the wider culture, with words like chi, spirit, energy in common use. The word for death has changed: now it is “passing”, a spiritualist word. Angel statues in graveyards have been joined by butterflies signifying new life, windmills so the spirit can catch the soul, with “dream-catchers” hanging in trees.She predicted that magic would continue to be vital and, on her tick box assessing the strength of religion, it was strong in most areas.Magic was ridiculed for being superficial and shallow but, she said, the reverse was true. It was the subject of deep reflection and was much better dealing with evolution or science and cosmology, using ideas around cycles of birth and death, life and growth. Everything was energy. The planet was Gaia.“My thesis is that about one in five, about 20 per cent of people, in any generation, in any era in any society other than where there is religious repression, are devoutly interested engaged in religion, magic or spirituality. And it doesn’t ever really change. I think they were always in minority. And I think they will always continue to be in minority.”",The future of religion in Britain: a rise in Islam as Christianity declines. And then thereâs magic â¦
80,-1,-1_new_said_study_people,https://metro.co.uk/2022/10/13/our-patients-arent-dead-look-inside-the-us-cryogenic-freezing-lab-17556468,"Alcor Life Extension Foundation is freezing people in the hope of reviving them in the future (Picture: Reuters)For some people in Arizona, time and death is ‘on pause’.Inside tanks filled with liquid nitrogen are the bodies and heads of 199 humans who opted to be cryopreserved with the hopes of being revived in the future.Many of the patients – as Alcor Life Extension Foundation calls them – are people who were terminally ill with cancer, ALS or other diseases with no cure in the present day.One of the patients is Matheryn Naovaratpong, the youngest person to be cryogenically frozen.Alcor’s former CEO, Max More, pointed to a picture of the girl as reporters were given a tour of the facility.‘A little girl from Thailand who had brain cancer. Both her parents were doctors and she had multiple brain surgeries and nothing worked, unfortunately. So they contacted us,’ More explained.To view this video please enable JavaScript, and consider upgrading to a web browser that supports HTML5 videoNaovaratpong’s case is one of only a few at Alcor that is public. Another with a terminal disease is Hal Finney, who passed away from ALS. Finney is well-known in the cryptocurrency community for being the recipient of the first Bitcoin transaction.When it comes to famous figures, Paris Hilton has reportedly signed up for cryopreservation. America’s Got Talent, Simon Coldwell, had publicly announced his membership in 2011 but later opted out.Rumors about Walt Disney being frozen have circulated for decades, but that was debunked by his own family.Legendary baseball player, Ted Williams, who died in 2002 is currently one of Alcor’s frozen patients.The bodies are kept in these giant tanks (Credit: Reuters)More says he thinks of cryonics as an extension of emergency medicine.‘We come at the stage where doctors today have given up. Today’s medicine and technology is not sufficient to keep you going. But we’re saying instead of just disposing of the patient, give them to us.‘We’re going to stabilize them, stop them getting worse, and hold them for as long as it takes for technology to catch up and allow them to come back to life and continue living,’ he said.To view this video please enable JavaScript, and consider upgrading to a web browser that supports HTML5 videoNatasha Vita-More, futurist and author, is married to Max Moore, and also one of the company’s 1,392 living members. The couple met in 1992 during a futurist event in Los Angeles and discovered both were members of the company.Vita-More is signed up to be ‘neurosuspended’, meaning only her brain will be cryopreserved.Best case scenario, Vita-More says, is that patients who were frozen, will meet in the future with other family members or pets who were also preserved. ‘And the destination would be the future where a person who had a cancer or ALS or some other type of injury or disease is revived,’ she said.‘The disease or injury cured or fixed, and the person has a new body cloned or a whole body prosthetic or their body reanimated and meet up with their friends again.’Would you like to be cryopreserved? (Credit: Reuters)But there are skeptics in the medical world.Doctor Arthur Caplan, director of the Division of Medical Ethics and professor of Bioethics at the New York University Grossman School of Medicine in New York City, reckons the idea is far-fetched.‘The only group that you really see getting excited about the possibility are people who are sort of, people who specialize in studying the distant future or people who have a stake in wanting you to pay the money to do it,’ he said in a Zoom interview with news agency Reuters.Alcor Life Extension Foundation is based in Scottsdale, Arizona in the US (Credit: Getty Images)Cryonics is based on anticipation and in the hope that one day science will be advanced enough for the restoration of cryopreserved humans. Alcor does not make any promises or guarantees, but that hasn’t stopped people from giving the company their money.It costs a minimum of $200,000 (£180,000) to freeze a body and $80,000 (£72,000) for the brain alone.Alcor says that the vast majority of its members don’t have that money, so they use life insurance equal to that amount, making Alcor the beneficiary of the policy.Of course, there are various ongoing running costs associated with the facility, which Alcor says can depend on what’s being frozen and for how long.‘Alcor knows quite accurately the total costs involved in keeping all its patients cryopreserved, the allocation of those costs among specific patients is not so clear cut,’ the company says.It won’t go into specifics surrounding individual costs per patient because of the sums involved. But, suffice to say, keeping your body and brain cryopreserved for generations doesn’t come cheap.MORE : Here’s how much coffee you should drink each day, according to scienceMORE : Petting dogs makes people ‘more sociable’ and science just proved it",âOur patients arenât deadâ: Inside the cryogenic freezing facility with 199 humans on ice
77,-1,-1_new_said_study_people,https://link.springer.com/article/10.1007/s11127-022-00998-y,"Many powerful political movements arise from seemingly insignificant events that set in motion a cascade of consequences. In some cases, the process ultimately results in a change of government or the entire dissolution of a nation. Early theoretical studies struggled to explain the emergence of rebellions, since the reward they provide is a public good, whereas the potentially large costs of participation are borne by the individual (Tullock, 1971). Subsequent literature proposed a variety of explanations consistent with rational choice theory that can reconcile this seeming paradox of revolution. These explanations include bloc mobilization (Oberschall, 1994), uncertainty about the repressive capabilities of the regime (Boix & Svolik, 2013), and social preferences (Shadmehr & Bernhardt, 2011).Footnote 1 In a similar spirit, Kuran (1989) described a framework in which privately held and publicly voiced political preferences can diverge. This results in a bandwagon effect, where individuals hold their political views private until a sufficiently large number of individuals voice similar views.A recent example of such a movement is Black Lives Matter (BLM), which, although officially founded in 2013, mushroomed into a global movement of an almost unparalleled scale following the death of George Floyd in police custody on May 25, 2020. Driven by concerns about perceived racial injustices, protests occurred across the United States, as well as in many cities worldwide.Despite the large scale of the movement and its associated protests, little is known about its political consequences. Although the protests primarily targeted perceived racial injustices, they commonly involved calls to get out the vote and emphasized the importance of registering to vote to achieve political change (New York Times, 2020). Moreover, the protests received a large amount of media coverage across the political spectrum. However, this coverage was marked by a deep ideological divide, as some conservative commentators emphasized the occurrence of violent outbursts at some of these protests, seeking to reinforce their narrative that a Democratic government would threaten public safety (FiveThirtyEight, 2020).These factors suggest that the BLM protests might have contributed in important ways to the record-breaking voter registration levels and turnout observed in the 2020 presidential election by encouraging voters in support of the movement, as well as those opposing it, to cast their vote. In this study, we focus specifically on the impact of local protests on the political mobilization of previously unregistered voters by comparing temporal patterns in voter registration across observationally similar communities with and without large-scale BLM protests.The vast majority of US states require voters to register to vote, a procedure that has long been acknowledged as potentially detrimental to voter turnout, since it compels prospective voters to expend energy at a time when political interest is relatively low (Highton, 1997; Rosenstone & Wolfinger, 1978). Recent years have seen a variety of efforts to increase political participation, including the abolition of voter registration deadlines (Brians & Grofman, 2001), widespread registration drives (Nickerson, 2015), and automatic voter registration when a citizen engages with government entities (McGhee et al., 2021). Despite these advances, there remains a substantial population of eligible yet unregistered voters (Pew Charitable Trusts, 2012), particularly among low-income Americans (Brians & Grofman, 1999). This gap is highly relevant, since interventions aimed at increasing voter registrations have been shown to translate directly into higher voter turnout (Nickerson, 2015).The above-cited research thus suggests that drivers of voter registrations are an important factor to study as we seek to understand political participation in the United States.A distinct advantage of voter registration data over traditional measures of electoral participation is their availability with high frequency. Compared to biennial turnout data, this data availability considerably mitigates potential confounding. One might be concerned about the possibility that protests are endogenous to places where they maximize political mobilization due to unobserved factors, such as the potential for new registrations. As outlined by Azam (2019), such behavior would lead to a biased estimate of the effect of protests in purely cross-sectional regressions. By observing voter registrations in a panel, we can account for such unobserved factors if they are constant during the observed time period (Wooldridge, 2015). We argue that focusing on a short time horizon before and after the protests lends credibility to the assumption that confounding variables did indeed remain constant during our sampling period.However, the use of voter registrations as an outcome also has some limitations that qualify our conclusions in important ways. First, registrations capture only the political engagement of previously unregistered voters. Although studying this population is interesting in its own right, its non-representative nature limits the extent to which findings can be extrapolated to the electorate as a whole (Jackman & Spahn, 2021). Second, the analysis of timing variation in voter registrations requires assumptions about why individuals prefer to register to vote at one point in time rather than another. While time-varying costs of registration are likely important (Cantoni, 2020; Kaplan & Yuan, 2020), we argue that the salience of political events can be a strong motivating factor, especially considering the availability of online voter registration in most states by 2020.Our research contributes to several strands of the literature in economics and political science. Most notably, we analyze the impact of political protests on voter mobilization. This question has previously been studied by Madestam et al. (2013), who found that protests by the Tea Party movement led to a local increase in the vote share for the Republican party. We add to this body of knowledge by providing estimates on the local political mobilization effects of another large-scale political movement, using an alternative outcome and identification strategy. To the best of our knowledge, we are the first to estimate the effect of the Black Lives Matter protests on political mobilization.Footnote 2We further contribute to the vast literature on how voters react to dramatic external events. For example, terrorism has been found to affect voting, even though the violence was committed by independent actors without the support of political parties (Geys & Hernæs, 2020; Montalvo, 2011). The lootings and riots that accompanied some of the BLM protests provided conservative commentators with a powerful narrative contending that under a Democratic government, lootings and riots would be the norm.Footnote 3 Alternatively, the BLM movement can be viewed as an expression of dissatisfaction with prior policy. Voters might be motivated by seeing many citizens openly demand more progress on racial equality and policing. The previous literature on retrospective voting has generally confirmed that voters hold policymakers accountable for failure to control crime (Arnold & Carnes, 2012; Bateson, 2012) or failures of the education system (Holbein, 2016). Experimental evidence has shown that the context and framing in the media matter in determining how voters attribute blame (Healy & Malhotra, 2013; Malhotra & Kuo, 2008). Since conservative and liberal leaning voters consume different news sources (Allcott & Gentzkow, 2017; Bakshy et al., 2015; Gentzkow & Shapiro, 2011), we can expect that they would receive different interpretations of the legitimacy of the Black Lives Matter protests, which might change the extent to which voters are mobilized.Our results do not support the notion that local political protests affected voter mobilization, either in the aggregate or on either side of the political spectrum. Furthermore, we find similar null effects also for the subset of counties in which protests turned violent, although the considerably reduced sample size does not allow us to confidently rule out meaningful effect sizes.Although our results stand in contrast to earlier findings in the literature on the mobilization effects of political protests, these differences might be attributed to the scale of the BLM movement and its extensive media coverage. Similar to the prior literature, our analysis cannot identify the overall impact of the BLM movement but, rather, focuses on the differential in the mobilization effects induced by local protests. The vast media coverage of the BLM movement might have reduced the importance of local exposure, thus contributing to the null effect we estimate in this study.Footnote 4 The major national cable TV networks spent almost 2.5 hours per day reporting about the protests on the weekend after George Floyd’s death (FiveThirtyEight, 2020). In addition, we show that despite considerable variation in interest across states, even areas with little exposure to local protests exhibited substantial interest in BLM, as measured by Google Trends data.Section 2 of this paper describes our data sources and presents descriptive statistics of the sample; Sect. 3 describes our empirical strategy; Sect. 4 indicates our main results, heterogeneity analyses and robustness checks; and Sect. 5 provides a brief conclusion.",Do political protests mobilize voters? Evidence from the Black Lives Matter protests
75,-1,-1_new_said_study_people,https://jamanetwork.com/journals/jamapsychiatry/fullarticle/2797528,"Key PointsQuestion To what extent do young autistic female and male individuals differ in their psychiatric diagnoses and hospitalizations compared with nonautistic individuals?Findings In this population-based cohort study, autistic individuals had higher cumulative incidences for psychiatric disorders and hospitalizations. Compared with autistic male individuals, autistic female individuals were more likely to receive diagnoses and be hospitalized for psychiatric disorders, particularly anxiety, sleep, and depressive disorders.Meaning These findings show the high mental health needs of autistic young adults, particularly autistic female individuals who are at higher risk of receiving a diagnosis of, as well as being hospitalized for, psychiatric problems compared with autistic male individuals.Importance Psychiatric disorders are common among autistic children and adults. Little is known about sex differences in psychiatric disorders and hospitalization in early adulthood.Objective To examine sex differences in psychiatric diagnoses and hospitalizations in autistic compared with nonautistic young adults.Design, Setting, and Participants This population-based cohort study assessed all individuals born in Sweden between 1985 and 1997. A total of 1 335 753 individuals, including 20 841 autistic individuals (7129 [34.2%] female individuals), were followed up from age 16 through 24 years between 2001 and 2013. Analysis took place between June 2021 and August 2022.Exposures Autism was defined as having received at least 1 clinical diagnosis of autism based on the International Classification of Diseases.Main Outcomes and Measures The cumulative incidence of 11 psychiatric diagnoses up until age 25 years was estimated, and birth year–standardized risk difference was used to compare autistic female and male individuals directly. Sex-specific birth year–adjusted hazard ratios (HRs) with 95% CIs were calculated using Cox regression. Analyses were repeated for inpatient diagnoses to assess psychiatric hospitalization.Results Of 1 335 753 individuals included in this study, 650 314 (48.7%) were assigned female at birth. Autism was clinically diagnosed in 20 841 individuals (1.6%; 7129 [34.2%] female) with a mean (SD) age of 16.1 (5.1) years (17.0 [4.8] years in female individuals and 15.7 [5.2] years in male individuals) for the first recorded autism diagnosis. For most disorders, autistic female individuals were at higher risk for psychiatric diagnoses and hospitalizations. By age 25 years, 77 of 100 autistic female individuals and 62 of 100 autistic male individuals received at least 1 psychiatric diagnosis. Statistically significant standardized risk differences were observed between autistic female and male individuals for any psychiatric disorder (−0.18; 95% CI, −0.26 to −0.10) and specifically for anxiety, depressive, and sleep disorders. Risk differences were larger among autistic than nonautistic individuals. Compared with nonautistic same-sex individuals, autistic female individuals (HR range [95% CI], 3.17 [2.50-4.04.]-20.78 [18.48-23.37]) and male individuals (HR range [95% CI], 2.98 [2.75-3.23]-18.52 [17.07-20.08]) were both at increased risk for all psychiatric diagnoses. Any psychiatric hospitalization was statistically significantly more common in autistic female individuals (32 of 100) compared with autistic male individuals (19 of 100). However, both autistic female and male individuals had a higher relative risk for psychiatric hospitalization compared with nonautistic female and male individuals for all disorders (female individuals: HR range [95% CI], 5.55 [4.63-6.66]-26.30 [21.50-32.16]; male individuals: HR range [95% CI], 3.79 [3.22-4.45]-29.36 [24.04-35.87]).Conclusions and Relevance These findings highlight the need for profound mental health services among autistic young adults. Autistic female individuals, who experience more psychiatric difficulties at different levels of care, require increased clinical surveillance and support.IntroductionMental health problems are a major concern in autistic individuals1,2 (note that we use identity-first language [autistic person] rather than person-first language [person with autism] throughout this article according to preferences reported by autistic individuals and their families3). Around 70% of autistic children meet diagnostic criteria for at least 1 psychiatric disorder,4 and 54%5 to 79%6 of autistic adults receive at least 1 psychiatric diagnosis. Mental health problems are reported even among autistic individuals showing good outcomes in other areas of functioning.7Sex differences in mental health have been observed in the general population.8 (Note: The term sex refers to biological attributes of being female or male and is assigned at birth, whereas gender refers to socially constructed attributes9 as discussed in detail elsewhere.10,11) Yet, very few studies have directly investigated sex differences in psychiatric disorders among autistic individuals. Existing evidence suggests that autistic women are particularly vulnerable to psychiatric disorders5 and access psychiatric care more often than autistic men.12 One study13 on health care claims data of children and youth younger than 21 years suggested higher odds of psychiatric difficulties such as anxiety, mood, and sleep disorders in autistic girls compared with boys. A study14 using self-reported gender (instead of sex assigned at birth) in a smaller adult sample found that autistic women experience anxiety, depression, and eating disorders at higher rates than autistic men. To our knowledge, there is only 1 large population-based investigation15 of sex differences across mental health problems in a Danish cohort of autistic and nonautistic children and adolescents. This study showed that autistic female children and adolescents were at increased risk for several psychiatric disorders, compared with autistic male counterparts.A limitation of the aforementioned study is that individuals were only followed up until age 16 years. Therefore, disorders with a later onset, specifically in young adulthood, were not covered. The median age at onset for psychiatric disorders was reported to be 18 years, and more than 60% of the psychiatric problems observed in adulthood emerge for the first time in the transitional period across adolescence and young adulthood before age 25 years.16 This highlights the importance of young adulthood as a particularly sensitive period when psychiatric disorders commonly develop, which is supported by a higher prevalence of psychiatric conditions reported in autistic compared with nonautistic transition-aged youth.17Moreover, no study to date and to our knowledge has examined sex differences in psychiatric diagnoses at different levels of psychiatric care among autistic individuals. This is especially important given higher unmet health care needs in autistic individuals18 due to difficulties in accessing treatment, which might exacerbate mental health problems.19Using representative data from Swedish population registers, we aimed to explore sex differences in psychiatric diagnoses and psychiatric hospitalization among autistic young adults compared with nonautistic individuals.MethodsStudy PopulationThis register-based cohort study was approved by the Regional Ethics Review Board in Stockholm and follows the Strengthening the Reporting of Observational Studies in Epidemiology (STROBE) reporting guideline. Register studies do not require informed consent in Sweden. The study was preregistered via the Open Science Framework on June 16, 2021 (https://doi.org/10.17605/OSF.IO/QZHJS).We linked several nationwide Swedish registers (eAppendix in the Supplement). From the Medical Birth Register20 we identified all individuals born in Sweden between 1985 and 1997 (N = 1 407 253). The follow-up period was 2001 to 2013. We excluded individuals with chromosomal abnormalities (eTable 1 in the Supplement), stillbirths, individuals who died or migrated before their 16th birthday, and whose biological mother was unidentifiable. As data on gender were unavailable, we used sex assigned at birth (male/female) in the Medical Birth Register. We additionally identified sex using the Total Population Register21 and excluded individuals whose sex was missing or reported differently in the 2 registers, since we could not determine the cause of the mismatch or missing values. After exclusions, the eligible cohort included 1 335 753 individuals who were followed up from age 16 years until a diagnosis of the respective psychiatric disorder, their 25th birthday, death, emigration, or end of follow-up on December 31, 2013, whichever came first. The cohort selection process is depicted in eFigure 1 in the Supplement.MeasuresWe defined exposure as receiving at least 1 autism diagnosis from age 1 year onward based on International Classification of Diseases (ICD), Ninth Revision code 299A and ICD-10 codes F84 in the National Patient Register,22 excluding Rett syndrome, other childhood disintegrative disorders, and overactive disorder associated with intellectual disability (ID) and stereotyped movements. The validity of register-based autism diagnosis was reported to be high.23 Autism is a lifelong condition, and age at diagnosis is unlikely to accurately indicate age at onset. We therefore adopted a lifetime approach to autism (with sensitivity analyses to examine the impact of this approach). We defined psychiatric disorders as receiving a clinical outpatient or inpatient diagnosis in the National Patient Register between ages 16 and 24 years; diagnoses received before and after this age span were not considered. We looked at 11 individual disorders, recording the first occurrence of each diagnosis separately as well as first among all diagnoses (any diagnosis). For psychotic, bipolar, and sleep disorders we identified additional individuals through a dispensed prescription of medication in the Prescribed Drug Register24,25 (eTable 2 in the Supplement for ICD and Anatomical Therapeutic Chemical codes). Psychiatric hospitalization was assessed by considering only inpatient admission where psychiatric disorders were the primary reason for hospitalization.Birth year, attention-deficit/hyperactivity disorder (ADHD), and ID were selected as covariates due to their strong associations with autism.4,26,27 ADHD and ID were identified based on a clinical diagnosis in the National Patient Register. Additional individuals with ADHD were identified through a prescription of ADHD medication in the Prescribed Drug Register (eTable 3 in the Supplement).Statistical AnalysesAnalysis took place between June 2021 and August 2022. Data management was performed in SAS statistical software version 9.4.6 (SAS Institute). Data were analyzed using R version 4.0.5 (R Foundation) with the survival,28 drgee,29 and stdReg packages.30Based on matching 10 nonautistic individuals to each autistic individual on sex and birth year, we calculated the sex-specific cumulative incidence at age 25 years as the proportion of individuals who received the respective diagnosis prior to that age, using Kaplan-Meier estimation and thus accounting for censoring during follow-up. To compare autistic female and male individuals, we calculated the birth year–standardized survival probability and risk difference. We further calculated standardized risk differences in nonautistic individuals for comparison. In the full sample, we then used sex-stratified Cox regression models to compare autistic and nonautistic individuals of the same sex while accounting for differences in follow-up time. We calculated hazard ratios (HRs) and 95% CIs for any as well as for each individual psychiatric diagnosis. Attained age was the underlying time scale. In all analyses, we fitted a crude model, a second model adjusted for birth year and a third model further adjusted for ADHD and ID. To explore sex differences in psychiatric hospitalization, we repeated all analyses using only inpatient diagnoses.To account for multiple testing, we used Bonferroni-corrected significance levels in all analyses, adjusted for the number of psychiatric disorders investigated (any disorder and 11 individual psychiatric disorders). Two-sided P values were statistically significant at α = .004. For all models, CIs were estimated using a cluster robust sandwich estimator to account for related individuals in the sample using the maternal identity number.Sensitivity AnalysesTo examine the impact of the lifetime approach to defining autism, we reran the analyses restricting autistic individuals to those diagnosed with autism before age 16 years (a total of 9747 individuals, of which 2731 [28.1%] were female). We performed another sensitivity analysis including only individuals who received an autism diagnosis on more than 1 occasion (a total of 15 735 individuals, of which 5460 [34.7%] were female) to account for diagnostic uncertainty.ResultsCohort DescriptionDescriptive statistics for the study population are presented in Table 1. The cohort included 1 335 753 individuals (650 314 [48.7%] female). Detailed information on race and ethnicity was not available. Autism was clinically diagnosed in 20 841 individuals (1.6%; 7129 [34.2%] female) with a mean (SD) age of 16.1 (5.1) years (17.0 [4.8] years in female individuals and 15.7 [5.2] years in male individuals) for the first recorded autism diagnosis. Percentages of individuals receiving each psychiatric diagnosis are presented in eTable 4 in the Supplement and Figure 1.Sex Differences in Psychiatric DiagnosesWe observed sex differences in the cumulative incidence of psychiatric diagnoses between age 16 and 25 years among autistic individuals: 77 of 100 autistic female individuals, compared with 62 of 100 autistic male individuals, received at least 1 psychiatric diagnosis. Cumulative incidence was higher for autistic female individuals (0.016 [95% CI, 0.012-0.020]-0.52 [95% CI, 0.50-0.53]) than autistic male individuals (0.001 [95% CI, 0.000-0.002]-0.39 [95% CI, 0.38-0.40]) and nonautistic individuals across all individual disorders (eTable 5 in the Supplement). Comparing the standardized survival probability of autistic female and male individuals at age 25 years, we observed a statistically significant standardized risk difference for any diagnosis (−0.15; 95% CI, −0.17 to −0.13), indicating higher risk in female individuals (Figure 2). The same pattern was seen for sleep, depressive, and anxiety disorders (−0.28 [95% CI, −0.34 to −0.23] to −0.12 [95% CI, −0.14 to −0.11]; eFigure 2 in the Supplement). For most disorders risk differences between autistic female and male individuals were in the same direction yet larger than in nonautistic individuals (range of standardized risk differences at age 25 years: autistic individuals, −0.28 [95% CI, −0.34 to −0.23] to −0.007 [95% CI, −0.03 to 0.01]; nonautistic individuals, −0.10 [95% CI, −0.16 to −0.05] to 0.002 [95% CI, −0.03 to 0.03]; eTable 6 in the Supplement).Sex-specific birth year-adjusted HRs showed an elevated relative risk for all disorders for autistic female individuals (HR range [95% CI], 3.17 [2.50-4.04]-20.78 [18.48-23.37]) and male individuals (HR range [95% CI], 2.98 [2.75-3.23]-18.52 [17.07-20.08]) compared with same-sex individuals without an autism diagnosis (Figure 3, model birth year). When adjusting for ADHD and ID, most of the HRs for female and male individuals remained statistically significant, except for alcohol use disorders (Figure 3, model birth year, ADHD, and ID; eTable 7 in the Supplement). The results of the sensitivity analyses including only individuals diagnosed before age 16 years are shown in Table 2 and eFigure 3 in the Supplement. Despite attenuated HRs, autistic female individuals showed higher cumulative incidences for all disorders except alcohol use disorders compared with autistic male individuals. Results from analyses including only individuals with multiple autism diagnoses resembled the main analyses (eTable 8 in the Supplement).Sex Differences in Psychiatric HospitalizationsPercentages of psychiatric hospitalizations are presented in eTable 9 in the Supplement and Figure 1. By age 25 years, 32 of 100 autistic female individuals compared with 19 autistic male individuals, 5 nonautistic female individuals, and 3 nonautistic male individuals were hospitalized due to any psychiatric disorder. Cumulative incidence for inpatient diagnoses was higher in autistic female individuals for all individual disorders (cumulative incidence at age 25 years [95% CI], 0.01 [0.004-0.009]-0.16 [0.14-0.17]), compared with autistic male individuals and nonautistic individuals (eTable 10 in the Supplement). The standardized risk difference comparing autistic female and male individuals was statistically significant for any psychiatric disorder (−0.18; 95% CI, −0.26 to −0.10; Figure 2) but not for individual disorders (eFigure 4 in the Supplement). However, all risk differences showed a higher absolute risk for female individuals. Standardized risk differences were larger for autistic compared with nonautistic individuals (range of standardized risk differences at age 25 years: autistic individuals, −0.18 [95% CI, −0.26 to −0.10] to −0.006 [95% CI, −0.04 to 0.03]; nonautistic individuals, −0.03 [95% CI, −0.17 to 0.11] to 0.002 [95% CI, −0.04 to 0.04]; eTable 11 in the Supplement).Sex-specific birth year–adjusted HRs indicated a higher risk of hospitalizations for autistic female and male individuals compared with same-sex individuals without autism for all inpatient diagnoses (female individuals: HR range [95% CI], 5.55 [4.63-6.66]-26.30 [21.50-32.16]; male individuals: HR range [95% CI], 3.79 [3.22-4.45]-29.36 [24.04-35.87]; Figure 3, model birth year). After adjusting for ADHD and ID, sex-specific HRs remained statistically significant for all disorders, except alcohol use disorder (Figure 3, model birth year, ADHD, and ID; eTable 12 in the Supplement).DiscussionTo our knowledge, this is the largest study on sex differences in psychiatric disorders in autism to date, and the first study to comprehensively investigate psychiatric problems at different levels of psychiatric care in autistic young adults. Autistic young female individuals showed more mental health problems at multiple psychiatric care levels. Compared with autistic male individuals, autistic female individuals were at higher risk for any psychiatric disorder and specifically anxiety, depressive, and sleep disorders. They were also more likely to have been hospitalized for any psychiatric disorder compared with autistic male individuals and nonautistic individuals. Overall, sex differences observed between autistic female and male individuals resembled those in nonautistic individuals (namely higher incidence in female individuals for most disorders), but the differences in cumulative incidence were larger among autistic individuals. The findings of this large, population-based sample, including, to our knowledge, the highest number of autistic female individuals (n = 7129) studied so far, demonstrate a high level of psychiatric difficulties among young autistic female individuals, and thus clearly emphasize this group’s pressing mental health needs. Nevertheless, we need to consider psychiatric disorders in both sexes as psychiatric diagnoses and hospitalizations were more likely in autistic female and male individuals compared with nonautistic individuals of the same sex.This investigation provides an important and novel contribution by exploring sex differences in psychiatric inpatient diagnoses. Our study crucially showed that psychiatric hospitalizations are relatively common in autistic young adults. By as young as age 25 years, 22.1% of autistic female individuals and 10.9% of autistic male individuals (compared with less than 4% among nonautistic individuals) had been hospitalized for psychiatric difficulties, which have the potential to worsen over the course of their lives, if not treated appropriately. These high hospitalization rates partly reflect the severity of the disorder, indicating severe mental health problems in autistic individuals, particularly autistic female individuals. However, person-level factors often interact with system-level factors, such as availability of and barriers to services.31 Facing these barriers might make autistic individuals more likely to delay and avoid health care, thereby exacerbating their mental health problems, potentially leading to acute psychiatric crises that require hospitalization.31,32 Which factors lead to hospitalization and whether autistic individuals benefit from inpatient treatment or whether the hospital environment negatively impacts their mental health should be addressed in future studies.Possible Underlying Mechanisms for the Observed Sex DifferencesDifferent factors may exacerbate psychiatric disorders in autistic female individuals compared with male individuals. One theoretical approach is the multiple minority theory.33 Being autistic and nonmale can be viewed as a form of minority identity.14 Individuals with a minority identity tend to experience increased distress, which adversely impacts their mental health33 and could explain the results observed in this study. More proximal explanations, specifically related to the experience of being an autistic female individual, include female autism presentation34 (a qualitatively and/or quantitatively different expression of autistic symptoms and behaviors which might not be covered by current diagnostic criteria), compensatory behaviors and camouflaging, which may be more common in autistic female individuals,35 delays in diagnosis36,37 and access to support.19 These tend to be interrelated and impact mental health.38,39Besides contributing to psychiatric disorders through delayed diagnosis and access to support,19 the diagnostic bias often observed in autism36 (ie, the earlier identification of autism in young male individuals), could have directly impacted our findings. Likelihood for an autism diagnosis is increased in autistic female individuals presenting with additional problems.39,40 It has therefore been suggested that diagnosed female individuals represent the extreme end of the autistic female population.41 Consequently, autistic female individuals without such comorbidities may be missed and not diagnosed. If additional difficulties in the form of co-occurring disorders are inherent in diagnosed autistic female individuals, this might have introduced bias toward an overestimation of psychiatric disorders in this study. However, the percentages of co-occurring ADHD and ID in our study appeared similar for both autistic female and male individuals, and our estimates are in line with community-based samples recruited from outside clinics.4Clinical ImplicationsResults from this study can inform clinical practice in 2 important ways. First, services for autistic adults are scarce42 and barriers to care are pervasive, subsequently causing gaps and delays in treatment.18,43 Expanding mental health services in the transitional period from childhood to adulthood, particularly for female individuals, to reduce disruption and discontinuation of essential services is a necessary first step to accommodate the needs of autistic young adults.Second, it is essential to tailor services to autistic individuals’ needs. Autistic people, particularly female individuals, often report a lack of autism knowledge and understanding of co-occurring psychiatric disorders19,38,43-45 among medical professionals, sometimes resulting in misdiagnosis.46,47 Improving communication between autistic individuals and medical staff is key, as miscommunication tends to complicate identification and management of co-occurring disorders.48Strengths and LimitationsThe main strength of this study is the large nationwide sample, including a high number of autistic female individuals, which enabled us to comprehensively investigate psychiatric disorders, including rarer disorders. Using both outpatient and inpatient diagnoses based on reliable register data allowed us to investigate psychiatric problems at different psychiatric care levels and to draw more generalizable and robust conclusions. Nevertheless, the study is not without limitations.We cannot exclude the possibility that autistic individuals in our cohort were misdiagnosed with psychiatric disorders or that autism was undiagnosed in individuals without an autism diagnosis. Misdiagnosis of psychiatric disorders,47 and co-occurring disorders overshadowing autistic traits,39 is relatively common in autistic individuals who often report disagreeing with assigned diagnoses.46 Psychiatric diagnoses have yet to be validated in autistic individuals. The extent to which this, together with differences in validity between outpatient and inpatient diagnoses, might have influenced our findings remains uncertain.Although adjusting our analyses for ADHD/ID attenuated our estimates, we did not further stratify our analyses. How complex phenotypes with additional neurodevelopmental difficulties influence mental health in autism should be explored in future research. Initially, we aimed to account for the age of first recorded autism diagnosis, which was shown to influence co-occurring disorders in childhood.15 Underdiagnosis, misdiagnosis, or late diagnosis of autistic women36,49 alongside delays in support access50 may further exacerbate their psychiatric difficulties.19,37 However, outpatient care was only covered from 2001, restricting follow-up time. Earlier autism diagnoses, especially among older individuals, might have been missed, as indicated by the relatively high observed mean age of diagnosis. Studying the association of late diagnosis with mental health could help inferring mechanisms contributing to increased psychiatric difficulties in autistic female individuals.Because no gender variable is available in the registers, we relied on sex assigned at birth to differentiate between female and male individuals. Importantly, as effects of sex and gender are entangled, this does not imply that the observed differences are solely due to biological mechanisms.10 This is relevant because a comparably higher proportion of autistic than nonautistic individuals do not identify with their assigned sex at birth or a binary gender.51 Nonbinary and nonconforming gender identity are particularly prevalent among autistic individuals assigned female at birth,51 potentially contributing to the observed sex differences. Findings from a study14 on mental health in autistic men, women, and nonbinary/transgender individuals indicated higher rates of psychiatric disorders for the latter 2, highlighting the need to identify to which degree autistic nonbinary/transgender individuals face additional barriers, stigma, and exacerbated psychiatric difficulties. Studies on intersectionality, including gender identity as well as other factors such as race, ethnicity, and socioeconomic status, their interaction and subsequent effect on autistic individuals’ mental health constitute important avenues for future research.ConclusionsIn this cohort study, between ages 16 and 25 years, autistic female individuals experienced increased psychiatric difficulties at different levels of psychiatric care, from outpatient diagnoses to hospitalization, compared with autistic male individuals and nonautistic individuals. Higher rates compared with autistic male individuals were found for most psychiatric disorders with sex differences larger than among nonautistic individuals. This study expands the growing body of literature on autistic female individuals’ experiences and consequently the recognition of differing needs in this understudied and underserved group.Back to top Article InformationAccepted for Publication: September 7, 2022.Published Online: October 26, 2022. doi:10.1001/jamapsychiatry.2022.3475Corresponding Author: Miriam I. Martini, MSc, Department of Medical Epidemiology and Biostatistics, Karolinska Institutet, Nobels väg 12a, 17177 Stockholm, Sweden (miriam.martini@ki.se).Open Access: This is an open access article distributed under the terms of the CC-BY License. © 2022 Martini MI et al. JAMA Psychiatry.Author Contributions: Ms Martini had full access to all of the data in the study and takes responsibility for the integrity of the data and the accuracy of the data analysis.Concept and design: Martini, Butwicka, Du Rietz, D’Onofrio, Martin, Rosenqvist, Taylor.Acquisition, analysis, or interpretation of data: Martini, Kuja-Halkola, Butwicka, Du Rietz, Happe, Kanina, Larsson, Lundstrom, Rosenqvist, Lichtenstein, Taylor.Drafting of the manuscript: Martini, Kanina, Taylor.Critical revision of the manuscript for important intellectual content: Martini, Kuja-Halkola, Butwicka, Du Rietz, D’Onofrio, Happe, Larsson, Lundstrom, Martin, Rosenqvist, Lichtenstein, Taylor.Statistical analysis: Martini, Kuja-Halkola, Kanina, Martin, Rosenqvist.Obtained funding: D’Onofrio, Larsson, Lundstrom, Taylor.Administrative, technical, or material support: Kuja-Halkola, Lichtenstein.Supervision: Kuja-Halkola, Butwicka, Du Rietz, Larsson, Lichtenstein, Taylor.Conflict of Interest Disclosures: Dr Du Rietz reported grants from the Swedish Society of Medical Research, the Strategic Research Areas in Epidemiology and Biostatistics (SFOepi), Fredrik & Ingrid Thurings Stiftelse, and Fonden for Psykisk Halsa during the conduct of the study and personal fees from Shire Sweden AB (a Takeda company) outside the submitted work. Dr Happe reported personal fees from Outcomes First Group outside the submitted work and royalties from Taylor & Francis and Routledge publishers for 2 recent books on autism, one of which is an edited book about autism and women and girls. Dr Larsson reported grants and personal fees from Shire/Takeda, personal fees from Evolan, and personal fees from Medici outside the submitted work. Dr Rosenqvist reported grants from the Swedish Research Council during the conduct of the study. No other disclosures were reported.Funding/Support: This study was funded by MQ Mental Health Research (MQF20/19).Role of the Funder/Sponsor: The funder had no role in the design and conduct of the study; collection, management, analysis, and interpretation of the data; preparation, review, or approval of the manuscript; and decision to submit the manuscript for publication.",Sex Differences in Mental Health Problems and Psychiatric Hospitalization in Autistic Young Adults
119,-1,-1_new_said_study_people,https://scitechdaily.com/new-compound-discovered-that-destroys-the-mrsa-superbug/,"A compound that both inhibits the MRSA superbug and renders it more vulnerable to antibiotics in lab experiments has been discovered by researchers at the University of Bath in the UK.Antibiotic resistance poses a major threat to human health around the world, and Staphylococcus aureus has become one of the most notorious multidrug-resistant pathogens. Led by Dr. Maisem Laabei and Dr. Ian Blagbrough at the University of Bath, scientists have discovered a compound that both inhibits the Methicillin-resistant Staphylococcus aureus (MRSA) superbug and renders it more vulnerable to antibiotics.Staphylococcus aureus (staph) is a type of bacteria found on people’s skin. Staph bacteria are usually harmless, but they can cause serious infections that can lead to sepsis or death. Methicillin-resistant Staphylococcus aureus (MRSA) is a cause of staph infection that is difficult to treat because of resistance to some antibiotics.The novel compound – a polyamine – seems to destroy S. aureus, the bacterium that causes (among other things) deadly MRSA infections, by disrupting the pathogen’s cell membrane.The compound was tested in-vitro against 10 different antibiotic-resistant strains of S. aureus. Some of the strains tested are known to be resistant to vancomycin – the final drug of choice given to patients fighting an MRSA infection. The new compound was completely successful against all strains, resulting in no further bacterial growth.As well as destroying S. aureus directly, the study demonstrates that the compound is able to restore the sensitivity of multidrug resistant strains of the bacteria to three important antibiotics (daptomycin, oxacillin, and vancomycin). This could mean that antibiotics that have lost their effectiveness through decades of overuse may, in time, reclaim their capacity to bring serious infections under control.“We’re not entirely sure why these synergies occur between the compound and antibiotics, but we’re keen to explore this further,” said Dr. Laabei, researcher from the Department of Live Sciences at Bath.The pathogen’s vulnerabilityPolyamines are naturally occurring compounds found in most living organisms that interact with negatively charged molecules such as DNA, RNA, and proteins. Until a decade ago, they were thought to be essential to all life, but scientists now know they are both absent in, and toxic to, S. aureus. Since making this discovery, scientists have been attempting to exploit the pathogen’s unusual vulnerability to polyamines to inhibit bacterial growth.Now Dr. Laabei and his colleagues have found that a modified polyamine (named AHA-1394) is far more effective at destroying antibiotic-resistant strains of S. aureus than even the most active natural polyamine.Explaining, Dr. Laabei said: “Using our novel compound, the pathogen is destroyed – meaning growth is inhibited – when it’s used at a concentration that’s over 128 times lower than that required to destroy the pathogen when we use a natural polyamine.“This is important, as drugs that have the lowest minimum inhibitory concentration are likely to be more effective antimicrobial agents, and to be safer to the patient.”Though further research is needed, Dr. Laabei believes the new compound “could have important implications in a clinical setting as a new treatment option.”He said: “Preliminary research suggests the compound is non-toxic to humans, which of course is essential. In our next study, for which we’re seeking funding, we hope to focus on the precise mechanisms used by the compound to inhibit S. aureus. We believe the compound attacks the membrane of S. aureus, resulting in the membrane becoming permeable, resulting in bacterial death.”The compound was also tested against biofilm – the thin, hard-to-treat layer of microorganisms that grows on hard surfaces (seen, for instance, as plaque on teeth or a stubborn film on urinary catheters) and can result in serious infection. The results were promising here too, with the compound preventing the formation of new biofilm, though not disrupting established biofilm.Antibiotic resistanceAntibiotic resistance (or antimicrobial resistance – AMR) poses a major threat to human health around the world, and S. aureus has become one of the most notorious multidrug-resistant pathogens.A recent study looking back at the health effects of AMR in 2019 finds the pathogen was associated with one-million deaths worldwide, as a result of infections not responding to antibiotics.S. aureus is found in 30% of the population, living in people’s nasal passages and on the skin, and mostly it does not cause infection. Until quite recently, an MRSA infection was regarded as a hospital problem, and those affected were mostly people with an already compromised immune system. Over the past 20 years, however, for complex and only partially understood reasons, there has been an upswing in community-wide infections even among otherwise healthy individuals, bringing a sense of urgency to the quest to find fresh ways to tackle the problem.“New treatments are urgently needed to treat infections,” said Dr. Laabei.Reference: “Antibacterial activity of novel linear polyamines against Staphylococcus aureus” by Edward J. A. Douglas, Abdulaziz H. Alkhzem, Toska Wonfor, Shuxian Li, Timothy J. Woodman, Ian S. Blagbrough and Maisem Laabei, 22 August 2022, Frontiers in Microbiology.DOI: 10.3389/fmicb.2022.948343Funding for this research came from the GW4 Generator Award (GW4-GF2-015).",New Compound Discovered That Destroys the MRSA Superbug
120,-1,-1_new_said_study_people,https://scitechdaily.com/new-invention-triggers-one-of-quantum-mechanics-strangest-and-most-useful-phenomena/,"Through the Quantum Looking GlassBy helping scientists control a strange but useful phenomenon of quantum mechanics, an ultrathin invention could make future computing, sensing, and encryption technologies remarkably smaller and more powerful. The device is described in new research that was recently published in the journal Science.This device could replace a roomful of equipment to link photons in a bizarre quantum effect called entanglement, according to scientists at Sandia National Laboratories and the Max Planck Institute for the Science of Light. It is a kind of nano-engineered material called a metasurface and paves the way for entangling photons in complex ways that have not been possible with compact technologies.When photons are said to be entangled, it means they are linked in such a way that actions on one affect the other, no matter where or how far apart the photons are in the universe. It is a spooky effect of quantum mechanics, the laws of physics that govern particles and other very tiny things.Although the phenomenon might seem bizarre, researchers have harnessed it to process information in new ways. For example, entanglement helps protect delicate quantum information and correct errors in quantum computing, a field that may someday have sweeping impacts on science, finance, and national security. Entanglement is also enabling advanced new encryption methods for secure communication.Research for the groundbreaking device, which is a hundred times thinner than a sheet of paper, was conducted, in part, at the Center for Integrated Nanotechnologies, a Department of Energy Office of Science user facility operated by Sandia and Los Alamos national laboratories. Sandia’s team received funding from the Office of Science, Basic Energy Sciences program.Light goes in, entangled photons come outThe new metasurface acts as a portal to this unusual quantum phenomenon. In some ways, it’s like the mirror in Lewis Carroll’s “Through the Looking-Glass,” through which the young protagonist Alice experiences a strange, new world.Instead of walking through their new device, scientists shine a laser through it. The beam of light passes through an ultrathin sample of glass covered in nanoscale structures made of a common semiconductor material called gallium arsenide.“It scrambles all the optical fields,” said Sandia senior scientist Igal Brener. He is an expert in a field called nonlinear optics and led the Sandia team. Occasionally, he said, a pair of entangled photons at different wavelengths emerge from the sample in the same direction as the incoming laser beam.Brener said he is enthusiastic about this device because it is designed to produce complex webs of entangled photons. Instead of just one pair at a time, it can produce several pairs all entangled together, and some that can be indistinguishable from each other. Some technologies require these complex varieties of so-called multi-entanglement for sophisticated information processing schemes.Although other miniature technologies based on silicon photonics can also entangle photons, they lack the much-needed level of complex, multi-entanglement. Until now, the only way to produce such results was with multiple tables full of lasers, specialized crystals, and other optical equipment.“It is quite complicated and kind of intractable when this multi-entanglement needs more than two or three pairs,” Brener said. “These nonlinear metasurfaces essentially achieve this task in one sample when before it would have required incredibly complex optical setups.”The Science paper outlines how the team successfully tuned their metasurface to produce entangled photons with varying wavelengths. This was a critical precursor to generating several pairs of intricately entangled photons simultaneously.However, the scientists note in their paper that the efficiency of their device — the rate at which they can generate groups of entangled photons — is lower than that of other techniques and will need to be improved.What is a metasurface?A metasurface is a synthetic material that interacts with light and other electromagnetic waves in ways conventional materials can’t. Brener said that commercial industries are busy developing metasurfaces because they take up less space and can do more with light than, for instance, a traditional lens.“You now can replace lenses and thick optical elements with metasurfaces,” Brener said. “Those types of metasurfaces will revolutionize consumer products.”Sandia is one of the leading institutions in the world performing research in metasurfaces and metamaterials. Between its Microsystems Engineering, Science and Applications complex, which manufactures compound semiconductors, and the nearby Center for Integrated Nanotechnologies, scientists have access to all the specialized tools they need to design, fabricate, and analyze these ambitious new materials.“The work was challenging as it required precise nanofabrication technology to obtain the sharp, narrowband optical resonances that seed the quantum process of the work,” said Sylvain Gennaro, a former postdoctoral researcher at Sandia who worked on several aspects of the project.The device was designed, fabricated, and tested through a partnership between Sandia and a research group led by physicist Maria Chekhova. She is an expert in the quantum entanglement of photons at the Max Planck Institute for the Science of Light.“Metasurfaces are leading to a paradigm shift in quantum optics, combining ultrasmall sources of quantum light with far-reaching possibilities for quantum state engineering,” said Tomás Santiago-Cruz. He is a member of the Max Plank team and first author on the paper.Brener, who has studied metamaterials for more than a decade, said this newest research could possibly spark a second revolution — one that sees these materials developed not just as a new kind of lens, but as a technology for quantum information processing and other new applications.“There was one wave with metasurfaces that is already well established and on its way. Maybe there is a second wave of innovative applications coming,” he said.Reference: “Resonant metasurfaces for generating complex quantum states” by Tomás Santiago-Cruz, Sylvain D. Gennaro, Oleg Mitrofanov, Sadhvikas Addamane, John Reno, Igal Brener and Maria V. Chekhova, 25 August 2022, Science.DOI: 10.1126/science.abq8684",Scientists discover exotic quantum state at room temperature
74,-1,-1_new_said_study_people,https://interestingengineering.com/science/webb-telescope-captures-first-images-of-exoplanet,"The images of the exoplanet are seen through four different light filters. They show a gas giant, a planet with no rocky surface that could not be habitable. The light filters show how the infrared telescope's gaze is easily capturing images of planets out beyond our solar system. These images lead the way toward future observations that can reveal a broad range of information never before seen on exoplanets.Named HIP 65426 b is a gas giant about six to eight times the size of Jupiter, which is an enormous planet indeed. It is only about 15 to 20 million years old, which in planet years is very young. Our own Earth is about four to five billion years old.In 2017 astronomers from Chile, using the SPHERE instrument on the European Southern Observatory's Very Large Telescope (VLT), took images of the HIP 65426 b using short infrared wavelengths of light. The image today is taken in mid-infrared light. It reveals much more detail than the ground-based telescopes at the VLT campus. This is due to the intrinsic infrared glow of the Earth's atmosphere.",James Webb Telescope captures its first images of an exoplanet
26,-1,-1_new_said_study_people,https://arstechnica.com/tech-policy/2022/10/comcast-wants-internet-users-to-pay-more-because-customer-growth-has-stalled/,"Comcast has a problem—it isn't signing up many new broadband customers. But Comcast also has a solution—get more money from existing subscribers.Comcast failed to add any broadband customers in Q2 2022, holding steady at 32,163,000 residential and business Internet customers combined. In its Q3 earnings report released yesterday, Comcast said it gained only 14,000 broadband users in the latest quarter. Comcast also lost 561,000 video customers and 316,000 VoIP phone customers.That's why Comcast executives focused on ARPU (average revenue per user) in an earnings call yesterday. With new customers few and far between, Comcast is aiming for growth in the average amount each existing customer pays.""We expect ARPU growth will continue to be the primary driver of our residential broadband revenue growth in the near term,"" Comcast President and CFO Michael Cavanagh said.Comcast could get more customers by expanding into new territory or by connecting homes in neighborhoods where some people are stuck without broadband even though their neighbors have Comcast Internet service. But Comcast seems content to stick with its current territory and often refuses to provide new hookups unless homeowners pay tens of thousands of dollars upfront—or even $210,000, as described in one of our recent stories.CEO doesn’t expect much subscriber growthComcast CEO Brian Roberts said the nation's largest cable company is ""still in a challenging environment in terms of depressed move activity and increased competition from new entrants."" Robert said there are four primary drivers of growth in Comcast's cable division: ""residential broadband units, residential broadband ARPU, wireless, and business services.""Advertisement""While we don't anticipate residential broadband units to be a significant driver for now, we expect to maintain healthy growth in the other three, leading to continued strong financial performance at cable for the foreseeable future,"" Roberts said. Cavanagh said that ""broadband revenue increased 5.7 percent driven by growth in ARPU and in our customer base compared to last year. Broadband ARPU increased 3.7 percent year over year, consistent with the growth rate in the second quarter.""Comcast also discussed ARPU growth in its earnings call three months ago, suggesting that price increases helped boost per-user revenue in Q2. ""In broadband alone, we had really healthy ARPU growth, 3.6 percent, half being rate-driven, the other half just how we manage tier mix,"" Comcast cable division CEO David Watson said at the time.Meanwhile, Roberts emphasized yesterday that Comcast is ""returning a substantial amount of capital to our shareholders. We pay nearly $5 billion in dividends per year, and we bought back $9.5 billion of our shares year-to-date through the third quarter.""Broadband revenueBroadband revenue was $6.135 billion during the three-month period. That amounts to about $63.55 monthly per subscriber, but includes both business and residential accounts. The Q3 2022 broadband revenue is up from $6.107 billion in Q2 2022, and up from $5.8 billion since Q3 2021.Comcast has multiple ways of getting more money from existing subscribers. That includes selling mobile plans—Comcast added 333,000 wireless lines in the quarter, hitting 4.95 million wireless lines in total. Wireless revenue increased 30.8 percent, reaching $789 million. Comcast also sells home security services.But the Roberts and Cavanagh statements referred specifically to ""broadband ARPU,"" suggesting they want to keep raising broadband bills. That could include increases to basic monthly rates, boosts to fees that raise the cost above advertised prices, or requiring subscribers to purchase the $25-per-month xFi Complete add-on in order to get unlimited data and higher upload speeds.",Comcast wants Internet users to pay more because customer growth has stalled
95,-1,-1_new_said_study_people,https://news.osu.edu/why-it-is-more-difficult-to-be-poor-in-some-states-than-others/,"Poverty rates vary between U.S. states as much as they do between European countries, a new study suggests.Findings showed that in 2016, poverty rates in the European Union ranged from 6-16% -- compared to 7-29% between U.S. states that same year.There were also sizable differences between states in the risks of poverty – such as unemployment and single motherhood – as well as how much those risks translated into the probability that households would become impoverished.One implication from the findings is that state policies play a pivotal role in how many of their residents live in poverty, said D. Adam Nicholson, author of the study and President’s Postdoctoral Scholar in sociology at The Ohio State University.“If poverty on the national level is to decrease, it likely must start with states,” Nicholson said.“Some states are much more successful than others in minimizing the risks associated with poverty and the penalties that come for those who carry those risks.”The study was published online recently in the journal The Sociological Quarterly.Nicholson used data from the 1993-2016 Annual Social and Economic Supplement of the U.S. Census Bureau’s Current Population Survey, supplemented with other data to improve income estimates. Overall, 3.5 million people were sampled over the 24 consecutive years.For this study, poverty was defined as living in a household with less than 50% of the national median equalized disposable income.Results showed that, over the 24 years, the poverty rate in the poorest state (Mississippi) averaged 24.3% - more than 3.5 times higher than the 6.6% average rate in New Hampshire, which was the lowest.One key to understanding how many people live in poverty is seeing how many residents in a state have one or more of four common risks that often push people below the poverty line: low education, single motherhood, unemployment and having a lead earner under 25 years old.As expected, states with more residents who have one or more of those risks tended to have more people in poverty. On average, one in three people in the U.S. has at least one of these risks.But in high-poverty Mississippi, 42% of residents had at least one of the four risks, compared to 25% in low-poverty New Hampshire.States not only vary in the prevalence of risk, but also in the penalties associated with those risks, Nicholson said. Penalties are the probability that any of the risks will actually push a person into poverty.And the penalties varied widely, results showed. An individual with all four risks would almost certainly experience poverty in Alabama (over 92% probability), while fewer than 25% of individuals with all four risks in Hawaii would expect to be poor.“Poverty in some states may be driven by a high prevalence of risk, while in others it may be driven by high penalties,” Nicholson said. “That’s why it is important to study poverty at the state level and not just the national level.”Changes in some trends identified in the study may be associated with the notable federal welfare reforms in 1996 which allowed states to impose work requirements and other policies that could restrict aid to those at risk of poverty.For example, after the welfare reforms, the variation in the single motherhood penalty across states more than doubled, results showed. “The penalties for single mothers differ a lot depending on the state where they live,” Nicholson said.The penalty for unemployment varied the least among the states, but it is the highest by far, making it the most consequential risk for poverty. And the penalty has increased over time, the study showed.“This supports arguments that work requirements attached to welfare benefits may have important implications for poverty levels,” he said.Race and ethnicity of residents undoubtedly play a key role in the variation in poverty levels between states, and in how states respond to poverty risks, Nicholson said. He will focus more on this issue in a future study.Overall, the results suggest that one way to help reduce poverty in the United States would be for states to mimic policies in their counterparts that have done the best at lowering risks and penalties.If the prevalence of poverty risks in the U.S. matched the lowest prevalence found across states, and the average penalties matched the lowest penalties at the state level, then the U.S. poverty rate would be nearly 5 percentage points lower than it was in 2020, Nicholson said.“That percentage may seem small, but it translates into between 11 and 16 million people being moved out of poverty,” he said. “That would be a huge success.”",Why it is more difficult to be poor in some states than others
15,-1,-1_new_said_study_people,https://apnews.com/article/technology-north-carolina-us-department-of-agriculture-mitch-landrieu-tom-vilsack-e521854ebdaf6262202713abb2aa8415,"FILE - Jason Morisseau, a installation and maintenance technician with Waitsfield and Champlain Valley Telecom, uses a fusion splicer to install fiber optic cable that is being run to a home, in Concord, Vt., Feb. 10, 2022. The Agriculture Department is announcing on Thursday $759 million worth of grants and loans to enable rural communities to access high-speed internet. It's part of the broader $65 billion push for high-speed connectivity from last year's infrastructure law. (AP Photo/Wilson Ring, File)FILE - Jason Morisseau, a installation and maintenance technician with Waitsfield and Champlain Valley Telecom, uses a fusion splicer to install fiber optic cable that is being run to a home, in Concord, Vt., Feb. 10, 2022. The Agriculture Department is announcing on Thursday $759 million worth of grants and loans to enable rural communities to access high-speed internet. It's part of the broader $65 billion push for high-speed connectivity from last year's infrastructure law. (AP Photo/Wilson Ring, File)RALEIGH, N.C. (AP) — The Agriculture Department announced Thursday it is making available $759 million in grants and loans to enable rural communities to access high-speed internet , part of the broader $65 billion push for high-speed connectivity from last year’s infrastructure law .Agriculture Secretary Tom Vilsack and White House senior adviser Mitch Landrieu unveiled the grants during a visit to North Carolina.There are 49 recipients in 24 states. One is North Carolina’s AccessOn Networks, which will receive $17.5 million to provide broadband service to 100 businesses, 76 farms and 22 educational facilities in the state’s Halifax and Warren counties. Both counties are rural and have predominantly Black populations.“Rural America needs this,” Vilsack said. “Rural America deserves this.” He made the announcement in front of John Deere equipment, noting that rural areas tend to be where the electricity for cities is generated and where city dwellers and suburbanites go for vacations.ADVERTISEMENTThe announcement and visit to North Carolina, a state with an open U.S. Senate seat, come as President Joe Biden and other top Democratic officials are trying to sell their achievements to voters before the Nov. 8 midterm elections . Landrieu, the infrastructure coordinator and former New Orleans mayor, told reporters on a Wednesday call that the Biden administration has already released $180 billion for various infrastructure projects.The administration is specifically targeting support for small towns and farm communities, places that generally favor Republicans over Democrats.“Rural communities are the backbone of our nation, but for too long they’ve been left behind and they have been underrecognized,” Landrieu said. “We all know how essential the internet is in order to access lifesaving telemedicine, to tap into economic opportunity, to connect with loved ones, to work on precision agriculture and so much more. That’s just beyond unacceptable that that’s not available to rural America.”Vilsack said he and Landrieu would “learn firsthand” from people in North Carolina about the opportunities internet access can create. They met with state and local officials including North Carolina Gov. Roy Cooper at Wake Technical Community College. They will also hold a town hall in Elm City.ADVERTISEMENTCooper attributed the broadband advances to the pandemic shutdowns that made people more reliant on the internet.“It tossed us into the future by about a decade — we had to make something good out of something bad,” he said. He added that 1 million of the state’s residents have been on the wrong side of the digital divide, something the build-out will help to correct.Neither candidate in North Carolina’s U.S. Senate race — Democrat Cheri Beasley and Republican Ted Budd — was slated to appear at the events.Vilsack said that past trips show how broadband connectivity is starting to make a difference. While in Nevada this summer, he heard from people in the town of Lovelock who plan to use the improved internet to enhance their emergency responder services and tourism opportunities as well as help high school students who are earning college credit online.ADVERTISEMENT___Follow the AP’s coverage of the 2022 midterm elections at https://apnews.com/hub/2022-midterm-elections . And check out https://apnews.com/hub/explaining-the-elections to learn more about the issues and factors at play in the midterms.",Rural areas to get $759M in grants for high-speed internet
9,-1,-1_new_said_study_people,https://apnews.com/article/british-politics-technology-europe-boris-johnson-civil-service-b3f505a4ecf8bf11e4b89fea2113583d,"FILE - Britain's Foreign Secretary Liz Truss leaves a Cabinet meeting at 10 Downing Street in London, Tuesday, April 19, 2022. The British government insisted Sunday, Oct. 30, 2022 it has robust cybersecurity for government officials, after a newspaper reported that former Prime Minister Liz Truss’ phone was hacked while she was U.K. foreign minister. (AP Photo/Alastair Grant, File)FILE - Britain's Foreign Secretary Liz Truss leaves a Cabinet meeting at 10 Downing Street in London, Tuesday, April 19, 2022. The British government insisted Sunday, Oct. 30, 2022 it has robust cybersecurity for government officials, after a newspaper reported that former Prime Minister Liz Truss’ phone was hacked while she was U.K. foreign minister. (AP Photo/Alastair Grant, File)LONDON (AP) — The British government insisted Sunday it has robust cybersecurity for government officials, after a newspaper reported that former Prime Minister Liz Truss’ phone was hacked while she was U.K. foreign minister.The Mail on Sunday said that the hack was discovered when Truss was running to become Conservative Party leader and prime minister in the summer. It said the security breach was kept secret by then-Prime Minister Boris Johnson and the head of the civil service.The newspaper, citing unnamed sources, said Russian spies were suspected of the hack. It said the hackers gained access to sensitive information, including discussions about the Ukraine war with foreign officials, as well as private conversations between Truss and a political ally, former Treasury chief Kwasi Kwarteng.The U.K. government spokesperson declined to comment on security arrangements, but said it had “robust systems in place to protect against cyber threats,” including regular security briefings for ministers.ADVERTISEMENTOpposition parties demanded an independent investigation into the hack, and into the leak of the information to a newspaper.“Was Liz Truss’s phone hacked by Russia, was there a news blackout and if so why?” said Liberal Democrat foreign affairs spokesperson Layla Moran. “If it turns out this information was withheld from the public to protect Liz Truss’ leadership bid, that would be unforgivable.”Labour Party law-and-order spokesperson Yvette Cooper said “the story raises issues around cybersecurity.”“It’s why cybersecurity has to be taken so seriously by everyone across government, the role of hostile states,” she told Sky News. “But also the allegations about whether a Cabinet minister has been using a personal phone for serious government business, and serious questions about why this information or this story has been leaked or briefed right now.”",UK politicians demand probe into Liz Truss phone hack claim
10,-1,-1_new_said_study_people,https://apnews.com/article/business-kamala-harris-seattle-washington-pollution-16405c66d405103374d6f78db6ed2a04,"Vice President Kamala Harris speaks at an event highlighting the Biden-Harris Administration's investments in electric school buses at Lumen Field in Seattle on Wednesday, oct. 26, 2022. (Karen Ducey/The Seattle Times via AP)Vice President Kamala Harris speaks at an event highlighting the Biden-Harris Administration's investments in electric school buses at Lumen Field in Seattle on Wednesday, oct. 26, 2022. (Karen Ducey/The Seattle Times via AP)WASHINGTON (AP) — Nearly 400 school districts spanning all 50 states and Washington, D.C., along with several tribes and U.S. territories, are receiving roughly $1 billion in grants to purchase about 2,500 “clean” school buses under a new federal program.The Biden administration is making the grants available as part of a wider effort to accelerate the transition to zero-emission vehicles and reduce air pollution near schools and communities.Vice President Kamala Harris and Environmental Protection Agency Administrator Michael Regan announced the grant awards Wednesday in Seattle. The new, mostly electric school buses will reduce greenhouse gas emissions , save money and better protect children’s health, they said.As many as 25 million children ride yellow buses each school day, and they will have a healthier future with a cleaner fleet, Harris said.“We are witnessing around our country and around the world the effects of extreme climate,” she said. “What we’re announcing today is a step forward in our nation’s commitment to reduce greenhouse gases, to invest in our economy ... to invest in building the skills of America’s workforce. All with the goal of not only saving our children, but for them, saving our planet.″ADVERTISEMENTOnly about 1% of the nation’s 480,000 school buses were electric as of last year, but the push to abandon traditional diesel buses has gained momentum in recent years. Money for the new purchases is available under the federal Clean School Bus Program, which includes $5 billion from the bipartisan infrastructure law President Joe Biden signed last year.The clean bus program “is accelerating our nation’s transition to electric and low-emission school buses while ensuring a brighter, healthier future for our children,” Regan said.The EPA initially made $500 million available for clean buses in May but increased that to $965 million last month, responding to what officials called overwhelming demand for electric buses. An additional $1 billion is set to be awarded in the budget year that began Oct. 1.The EPA said it received about 2,000 applications requesting nearly $4 billion for more than 12,000 buses, mostly electric. Some 389 applications worth $913 million were accepted to support purchase of 2,463 buses, 95% of which will be electric, the EPA said. The remaining buses will run on compressed natural gas or propane.ADVERTISEMENTSchool districts identified as priority areas serving low-income, rural or tribal students make up 99% of the projects that were selected, the White House said. More applications are under review, and the EPA plans to select more winners to reach the full $965 million in coming weeks.Districts set to receive money range from Wrangell, Alaska, to Anniston, Alabama, and Teton County, Wyoming, to Wirt County, West Virginia. Besides the District of Columbia, big cities that won grants for clean school buses include New York, Dallas, Houston, Atlanta and Los Angeles.White House adviser Mitch Landrieu said he expects many buses to be delivered by the start of the next school year, with the remainder likely to be in service by the end of 2023. The billion dollars being spent this year — along with an additional $4 billon expected over the next four years — should “supercharge” a domestic manufacturing boom for electric school buses, said Landrieu, a former New Orleans mayor tapped by Biden to oversee spending in the massive infrastructure law.ADVERTISEMENT“These buses will be made in America — real jobs with real wages,″ Landrieu said in an interview. “We are going to ramp up manufacturing in this country.″Environmental and public health groups hailed the announcement, which comes after years of advocacy to replace diesel-powered buses with cleaner alternatives.“It doesn’t make sense to send our kids to school on buses that create brain-harming, lung-harming, cancer-causing, climate-harming pollution,″ said Molly Rauch, public health policy director for Moms Clean Air Force, an environmental group. “Our kids, our bus drivers and our communities deserve better.″","More kids to ride in âcleanâ school buses, mostly electric"
24,-1,-1_new_said_study_people,https://arstechnica.com/science/2022/10/myth-busted-formation-of-namibias-fairy-circles-isnt-due-to-termites/,"So-called ""fairy circles"" are bare, reddish-hued circular patches notably found in the Namibian grasslands and northwestern Australia. Scientists have long debated whether these unusual patterns are due to termites or to an ecological version of a self-organizing Turing mechanism. A few years ago, Stephan Getzin of the University of Göttingen found strong evidence for the latter hypothesis in Australia. And now his team has found similar evidence in Namibia, according to a new paper published in the journal Perspectives in Plant Ecology, Evolution and Systematics.""We can now definitively dismiss the termite hypothesis, as the termites are not prerequisite to form new fairy circles,"" Getzin told Ars. This holds both for Australian and Namibian fairy circles.As we've reported previously, Himba bushmen in the Namibian grasslands have passed down legends about the region's mysterious fairy circles. They can be as large as several feet in diameter. Dubbed ""footprints of the gods,"" it's often said they are the work of the Himba deity Mukuru, or an underground dragon whose poisonous breath kills anything growing inside those circles.Scientists have their own ideas, and over the years, two different hypotheses emerged about how the circles form. One theory attributed the phenomenon to a particular species of termite (Psammmotermes allocerus), whose burrowing damages plant roots, resulting in extra rainwater seeping into the sandy soil before the plants can suck it up—giving the termites a handy water trap as a resource. As a result, the plants die back in a circle from the site of an insect nest. The circles expand in diameter during droughts because the termites must venture farther out for food.AdvertisementThe other hypothesis—the one espoused by Getzin—holds that the circles are a kind of self-organized spatial growth pattern (a Turing pattern) that arise as plants compete for scarce water and soil nutrients. In his seminal 1952 paper, Alan Turing was attempting to understand how natural, non-random patterns emerge (like a zebra's stripes), and he focused on chemicals known as morphogens. He devised a mechanism involving the interaction between an activator chemical and an inhibitor chemical that diffuse throughout a system, much like gas atoms will do in an enclosed box.It's akin to injecting a drop of black ink into a beaker of water. Normally this would stabilize a system: the water would gradually turn a uniform gray. But if the inhibitor diffuses at a faster rate than the activator, the process is destabilized. That mechanism will produce a Turing pattern: spots, stripes, or, when applied to an ecological system, clusters of ant nests or fairy circles.In 2019, Getzin's team conducted a study of fairy circles in northwestern Australia, near an old mining town called Newman. The team dug more than 150 holes in almost 50 fairy circles in the region to collect and analyze soil samples, specifically to test the termite hypothesis. They also used drones to map larger areas of the continent to compare the gaps in vegetation typically caused by harvester termites in the region, with the fairy circles that sometimes form.The vegetation gaps caused by harvester termites were only about half the size of the fairy circles and much less ordered, so they didn't find any hard subterranean termitaria that would prevent the growth of grasses. But they did find high soil compaction and clay content in the circles, evidence for the contribution of heavy rainfall, extreme heat, and evaporation to their formation. ""Termite constructions can occur in the area of the fairy circles, but the partial local correlation between termites and fairy circles has no causal relationship,"" Getzin said at the time. ""So no destructive mechanisms, such as those from termites, are necessary for the formation of the distinct fairy circle patterns; hydrological plant-soil interactions alone are sufficient.”AdvertisementHaving effectively disproven the Australian termite origin hypothesis, Getzin turned his attention to specifically testing the termite hypothesis for Namibia, using a similar methodology. While his earlier work on Namibian fairy circles did not specifically address the investigations of plant roots, this new study shows that plant roots are not touched by insect herbivores.""For the first time, we went right after rainfall to the fairy circles and checked the new grasses for termite herbivory,"" Getzin told Ars. ""Our excavations demonstrate that termites did certainly not cause the death of the grasses. If you come too late to the fairy circles, the grasses are long dead and detritivores like termites may have already fed on the lignified grass. But they did not kill the grass. We are showing unambiguously that the grasses die before and completely independent of any termite action.""So what's next for Getzin? He believes more research is needed on the swarm intelligence of plants, likening plants to beavers in the sense that they can act as ""ecosystem engineers"" that modify their environment. ""Most people cannot believe this or are unwilling to believe that, because plants have no brains,"" said Getzin. ""But plants act similarly like the beaver as ecosystem engineers because their only way to survive is forming optimal, strictly geometric patterns""—in other words, Turing patterns.DOI: Perspectives in Plant Ecology, Evolution and Systematics, 2022. 10.1016/j.ppees.2022.125698 (About DOIs).","Myth, busted: Formation of Namibiaâs fairy circles isnât due to termites"
19,-1,-1_new_said_study_people,https://arstechnica.com/gadgets/2022/11/pantone-wants-15-month-for-the-privilege-of-using-its-colors-in-photoshop/,"If you want to use up-to-date versions of Adobe's Creative Cloud apps, you've already been paying subscription fees for years now. And if you want to use Pantone colors inside of Adobe's apps, it's about to get even more expensive. Starting this month, the Pantone color books in Adobe's apps are mostly going away, and continuing to use those colors in your files will require a new Pantone Connect extension.Using that extension is free once you've created an account, but using the full library of colors, creating unlimited color palettes, and ""a dozen more tools to create smarter, more impactful palettes"" will now require a subscription that will run $15 per month or $90 per year, on top of what you're already paying to use Adobe's apps in the first place. I could browse through colors using the basic version of the extension, but trying to browse and select most colors from most libraries prompted me to pay for a subscription.Strange as it might seem for a company to be able to ""own"" colors, that's an oversimplification of what Pantone does—it maintains a large library of reference colors and physical color samples used in print publishing and many design industries to ensure that colors look the way they're supposed to look, no matter what material they're being used on. If you want to see what a given color will look like when printed on a matte sheet of paper versus a glossy sheet of paper versus plastic versus cloth (among other things), and you want to know that the manufacturer or printer sees the exact same color you do, that's when Pantone colors can be useful. Different computer, tablet, and phone displays will also show different colors differently based on how the displays are calibrated and what colors they're capable of showing—Pantone colors and physical samples help to eliminate some guesswork and inconsistency.AdvertisementFun times ahead for #Adobe designers. Today, if you open a PSD (even one that's 20 years old) with an obscure PANTONE colour, it will remove the colour and make it black. Pantone want US$21/month for access, and Solid Coated goes behind the paywall in early November. pic.twitter.com/BUxzViYFaQ — Iain Anderson (@funwithstuff) October 28, 2022This change seems to be rolling out gradually. Some users have already encountered Photoshop error messages informing them of the change, and that Pantone colors in old Photoshop files are being replaced with black when they're opened in newer versions of the software. Adobe says that the Solid Coated and Solid Uncoated Pantone libraries will be removed ""after November 2022,"" leaving only the CMYK Coated, CMYK Uncoated, and Metallic Coated Pantone libraries behind.Ars asked Adobe about the color replacement issue, and were told the following by an Adobe spokesperson: ""We are currently looking at ways to lessen the impact on our customers. In the meantime, customers also have access to up to 14 extensive color books through Creative Cloud subscriptions.""On an M1 MacBook Air running the most current version of Photoshop, I can still access all Pantone color libraries as before, including the Solid Coated and Solid Uncoated libraries that are supposedly going away. (Adding insult to injury for Mac users, the current version of the Pantone Connect extension isn't Apple Silicon-compatible and requires launching the app in the slower Intel emulation mode.)To hear Pantone tell it, Adobe had not been updating the Pantone color libraries in its apps for more than a decade, which prompted the end of the previous licensing deal and the wholesale removal of the old libraries from Adobe's apps in favor of the Pantone Connect Extension. But communication around this issue has been muddled, with conflicting and changing dates for when the removal of the existing Pantone libraries begins and different pricing data based on the source you're looking at. The Pantone FAQ and coverage from earlier this year list an $8/month or $60/year subscription price for Pantone Connect, much lower than the prices listed on the plugin's product page.AdvertisementThat Pantone FAQ also claims that ""existing Creative Cloud files and documents containing Pantone Color references will keep those color identities and information,"" a statement that seems to be contradicted by the Adobe licensing error message. For its part, Adobe's FAQ says that versions of its apps released before August 2022 ""will continue to have all previous Pantone Color books pre-loaded and available.""In a pinch, this means you could use Creative Cloud's ""other versions"" feature to install an older version of your apps that can still see and work with Pantone colors as you could before (Photoshop 23.4, InDesign 17.3, and Illustrator 26.4 appear to be the most recent versions released before August 2022).This is only a stopgap; Adobe doesn't offer older versions of its apps indefinitely. But for people who don't use Pantone colors heavily or regularly, it can allow you to open and modify your files so you don't end up with blacked-out colors in your images. Others have suggested that manually copying these Pantone color libraries from older versions of the apps and re-adding them to the newer versions could also be a workaround for some users.We asked Adobe about all the contradictory information we've run into: the exact date users can expect to see these changes; whether old files will have their colors removed or whether they'll be unchanged; and whether any missing colors can be restored by installing the Pantone Connect plugin (something we can't test for because Pantone colors are still working fine on our end). The company had nothing else to share about these specific questions as of this writing.",Pantone wants $15/month for the privilege of using its colors in Photoshop
147,-1,-1_new_said_study_people,https://techcrunch.com/2017/02/21/hr-mcmaster-trump-national-security-flynn-replacement/,"A week after Michael Flynn’s abrupt fall from grace, President Trump will smooth things over with a national security adviser that at least some people can agree on.Called everything from a “warrior scholar” to the “rarest of soldiers,”, Army Lt. Gen. H.R. McMaster is an about-face from the divisive Flynn, who resigned amid the escalating controversy over his contact with Sergey Kislyak, Russian ambassador to the U.S.McMaster, often described as the army’s own futurist, holds a complex view on technology, cautioning against technological hubris as a solution to modern warfare. “Be skeptical of concepts that divorce war from its political nature, particularly those that promise fast, cheap victory through technology,” McMaster wrote in a 2013 op-ed in the New York Times titled “The Pipe Dream of Easy War.” He continued:“Wars like those in Afghanistan and Iraq cannot be waged remotely. Budget pressures and persistent fascination with technology have led some to declare an end to war as we know it. While emerging technologies are essential for military effectiveness, concepts that rely only on those technologies, including precision strikes, raids or other means of targeting enemies, confuse military activity with progress toward larger wartime goals.”That same characteristic deep perspective appears to be on display in his controversial but largely well-respected book, Dereliction of Duty, about the failing of military leaders, particularly the Joint Chiefs of Staff, during the Vietnam war. McMaster’s academic streak is just one of the traits that paints him in stark contrast to Flynn, who is widely regarded as ideologically driven, particularly by anti-Islamic sentiment.During an April 2015 symposium on Army innovation, McMaster expanded on the risk inherent in an overreliance on military technology. “The biggest risk that we have today is the development of concepts that are inconsistent with the enduring nature of war,” McMaster said. “What we see today is really an effort to simplify this complex problem of future war and to essentially make it a targeting exercise. The idea is that the next technology we develop is going to make this next war fundamentally different from all those that have gone before it.”At a defense conference in London a few months later, McMaster emphasized that traditional manpower can’t be ignored in favor of flashy technological advances that appear to provide short-term gains. “[There is a] delusion that… a narrow range of military technologies will be decisive in future war,” he said. “Technology is the element of our differential advantage over our enemies which is most easily transferred to our enemies.”McMaster is no technophobe, but he dismisses conceptions of the future of war that “cut against war’s political nature, war’s human natures, war’s uncertainty and war as a contest of wills.”Notably, he also really, really hates PowerPoint. “It’s dangerous because it can create the illusion of understanding and the illusion of control,” McMaster told the New York Times. “Some problems in the world are not bullet-izable.” (Good luck telling that to the commander-in-chief.)It’s too early to tell how McMaster will fit into Trump’s roiling inner circle, or perhaps the outermost circle of his concentric inner circles, but McMaster’s willingness to critique authority around issues of national security is likely to prove relevant.As Middle East scholar and former U.S. Army officer Andrew Exum writes in the Atlantic:“One thing that stands out in the book is the way in which McMaster criticized the poorly disciplined national security decision-making process in the Kennedy and Johnson administrations, and especially the way in which the Kennedy administration made national-security decisions by a small group of confidants without a robust process to serve the president.”It’s not hard to imagine how the Army’s big picture thinker might extend that criticism to a president who prefers to craft decisions through a small cluster of loyalists, incorporating little outside input. It remains to be seen if Trump will bring McMaster fully into the fold or if he’ll just freeze him out like so many other administration officials who have expressed dissent.Whatever role he ends up playing, McMaster will join Defense Secretary James Mattis and Homeland Security Secretary John Kelly to round out the trifecta of well-respected military leaders who have Trump’s ear.",Trumpâs new national security adviser is a futurist with warnings about technology
18,-1,-1_new_said_study_people,https://arstechnica.com/gadgets/2022/09/stadia-controllers-could-become-e-waste-unless-google-issues-bluetooth-update/,"Google's Stadia game-streaming service will die a nearly inevitable death early next year. Google is refunding players the cost of all their hardware and game purchases. But, so far, Google is also leaving Stadia players with controllers that, while once costing $70, will soon do less than a $20 Bluetooth gamepad.Stadia's controllers were custom-made to connect directly to the Internet, reducing lag and allowing for instant firmware updates and (sometimes painful) connections to smart TVs. There's Bluetooth inside the Stadia controller, but it's only used when you're setting up Stadia, either with a TV, a computer with the Chrome browser, or a Chromecast Ultra.The Google Store's page for the Stadia controller states in a footnote: ""Product contains Bluetooth Classic radio. No Bluetooth Classic functionality is enabled at this time. Bluetooth Classic may be implemented at a later date."" (Bluetooth Classic is a more traditional version of Bluetooth than modern low-energy or mesh versions.)That potential later date can't get much later for fans of the Stadia controller. Many cite the controller's hand feel and claim it as their favorite. They'd like to see Google unlock Bluetooth to make their favorite something more than a USB-only controller and avoid a lot of plastic and circuit board trash.""Now if you'd just enable Bluetooth on the controller, we could help the environment by not letting them become electronic waste,"" writes Roadrunner571 on one of many controller-related threads on the r/Stadia subreddit. ""They created trash and they at least owe it to me to do their best within reason to prevent millions of otherwise perfectly good controllers from filling landfills,"" another wrote.Many have called for Google, if they're not going to push a firmware update themselves to unlock the functionality, to open up access to the devices themselves, so the community can do it for them. That's often a tricky scenario for large companies relying on a series of sub-contracted manufacturers to produce hardware. Some have suggested that the full refunds give Google more leeway to ignore the limited function of their devices post-shutdown.AdvertisementYou can still plug the Stadia controller into the USB port on your Smart TV, computer, or gaming console and use it as a controller through a standard HID (Human Interface Device) connection. How-To Geek reports that it's working well on PCs and with Android devices but not great on Xbox or Playstation consoles. At least one Github project reportedly improves the Stadia controller's Windows function (as an Xbox controller). One intrepid Stadia fan, Parth Shah, had already cobbled together a ""Stadia Wireless"" Python hack to get the Stadia controller working ""wirelessly"": connected to a phone, then that phone connecting to a Windows PC over Wi-Fi, emulating a standard Xbox controller.Yet Shah is also active in the Stadia subreddit, asking for his creation to be made obsolete: ""Not having to go through all this trouble would be so amazing. Hopefully [G]oogle does something about it.""There's some precedent to pushing new firmware to old business ideas. Valve, makers of the Steam PC gaming store and assorted hardware connected to it, enabled Bluetooth Low-Energy on Steam Controllers just before its Steam Box and Steam Link hardware ambitions fizzled out. Valve had something else in mind for them, namely its Steam Link software on other platforms. But Valve made Steam Controllers viable for lots of other platforms and prevented them from ending up in, at best, e-waste sorting facilities.E-waste from abandoned hardware is an area where Google, along with many other large tech companies, is far more quiet than it is about carbon emissions, water, or even food waste. The company's pledge to create ""A circular Google"" states that the company believes that by ""incorporating circularity into our designs from inception, things created today can become the resources of tomorrow and enable reuse, repair, and recovery.""In this case, it seems like circularity, in the form of a standard Bluetooth controller, is sitting inside Stadia controllers. The reuse and recovery would be much appreciated by customers.",Stadia controllers could become e-waste unless Google issues Bluetooth update
144,-1,-1_new_said_study_people,https://techcrunch.com/2016/12/06/open-bionics-partners-with-nhs-for-a-feasibility-study-to-bring-bionic-hands-to-the-u-k-health-system/,"Open Bionics partners with NHS for a feasibility study to bring bionic hands to the U.K. health systemOpen Bionics is a U.K. startup making bionic hands for patients needing prosthetics and co-founder of the company Samantha Payne came onstage today at TechCrunch Disrupt London to tell us about a new deal between Open Bionics and the National Health Service (NHS) to bring new technologies to amputees.The deal involves a feasibility study with the NHS through SBRI Healthcare to see if Open Bionics can provide a multi-grip bionic hand to amputees for significantly less money, possibly saving the NHS millions of pounds — a significant development considering hospital-grade myoelectric hands and limbs can cost up to $100,000 in some cases, take a long time to get and don’t always fit well.Open Bionics hands and limbs can produce its robotic hands in a matter of days and for a few thousand dollars.Open Bionics came out of Disney’s TechStars accelerator program with a plan to dramatically lower the cost of prosthetics using a combination of open-sourced 3D printing software and robotic sensors.We visited Open Bionics during its time with Disney. You can see that interview here:The startup began developing robotic arms with fun themes for children such as Disney’s Frozen and Star Wars with the goal to help kids missing body parts to have something fun and enjoyable to show to their peers.Daniel Mellville, who later joined Payne on stage said he wanted the other kids to say to those wearing an Open Bionics prosthetic, “What is this, how does it work and why can’t I have one?”Open Bionics said it managed to keep afloat by winning pots of money set aside in government programs and through the Techstars accelerator in the last few years of its existence but is now raising its first seed round. A seed round will help the company to grow, conduct medical testing and the several mechanical and electrical engineers needed to carry out bigger ambitions.Though Payne didn’t want to give specifics on what the company is working on next, one of the endeavors she mentioned has to do something with video games.“In video games, people even choose to lose a limb,” Payne pointed out.Open Bionics also said it wants to start building lower limbs. “There’s a huge range of robotic tech and it could be made cheaper,” Payne said.[gallery ids=""1424630,1424611,1424610,1424609,1424608,1424607,1424606,1424605,1424604,1424603""]",Open Bionics partners with NHS for a feasibility study to bring bionic hands to the U.K. health system
299,0,0_solar_energy_wind_power,https://www.businessinsider.com/evs-nasa-charged-electric-car-5-minutes-tech-2022-10,"A cooling system developed by NASA for electronics in space could be used to help charge electric cars.The system could, in theory, allow electric cars to be charged in just five minutes, NASA said.The heat-transfer system can cool cables carrying high currents, potentially allowing super-fast charging without the risk of overheating.Sign up for our newsletter for the latest tech news and scoops — delivered daily to your inbox. Loading Something is loading. Thanks for signing up! Access your favorite topics in a personalized feed while you're on the go. download the app Email address By clicking ‘Sign up’, you agree to receive marketing emails from Insider as well as other partner offers and accept our Terms of Service and Privacy PolicyResearchers have found that NASA technology developed for use on the International Space Station could also be used to charge electric vehicles at a much faster rate than in currently possible.The complex cooling technique, which has been developed to help certain electrical systems in space maintain proper temperatures, can deliver almost five times the current of other electric vehicle chargers currently on the market, NASA said in a blog post.Higher electrical currents generate more heat, meaning the more current flowing, the higher likelihood of components overheating.NASA's complex heat transfer system, known as ""subcooled flow boiling,"" can cool cables carrying high charges, potentially allowing for a faster flow of electricity without the risk of components overheating, according to the blog post.In the blog, NASA said the system had been developed to help deliver ""nuclear fission power systems for missions to the Moon, Mars, and beyond; vapor compression heat pumps to support Lunar and Martian habitats; and systems to provide thermal control and advanced life support onboard spacecraft.""Current charging time for EVs range from less than 20 minutes at some public fast charging stations to days or hours when using at-home chargers.NASA claims the new heat transfer system could reduce the charging time at charging stations to just five minutes.NASA said in a blog post: ""Application of this new technology resulted in unprecedented reduction of the time required to charge a vehicle and may remove one of the key barriers to worldwide adoption of electric vehicles.""The US government has been ramping up investment in electric car charging to meet its target of installing 500,000 electric-car chargers across the country by 2030.Slow charging and unreliable charging stations have been an issue for some EV owners.",Electric vehicles could be charged within 5 minutes thanks to tech developed by NASA for use in space
81,0,0_solar_energy_wind_power,https://newatlas.com/energy/aeromine-rooftop-wind/,"Aeromine says its unique ""motionless"" rooftop wind generators deliver up to 50% more energy than a solar array of the same price, while taking up just 10% of the roof space and operating more or less silently. In independent tests, they seem legit.Distributed energy generation stands to play a growing part in the world's energy markets. Most of this currently comes in the form of rooftop solar, but in certain areas, wind could definitely play a bigger part. Not every spot is appropriate for a bladed wind turbine, though, and in this regard, University of Houston spinoff Aeromine Technologies has designed a very different, very tidy form of rooftop wind energy capture that looks like it could be a real game-changer.As with traditional wind turbines, size is key. So while Aeromine's wind energy boxes take up a relatively small footprint on your roof, they're still pretty bulky. The wings themselves are maybe 10 feet (3 m) high, at a rough guess, and looking at the latest imagery they're now sitting on top of boxes that might add another 6 ft (1.8 m) or more to their height – so they're no shrinking violets. On the other hand, they don't create the noise, or the constantly moving visual distraction of a regular, bladed turbine, so they may prove to be less unwelcome in populated areas.They work differently too – kind of like a set of race-car spoiler wings sandwiched together facing each other, with a round pole in between them. Angled into the wind, these stationary wings generate a low pressure vacuum in the center of the device, which sucks air through perforations either in the wings themselves or in the round pole, which also aids in accelerating the ambient airflow over the wings.(rendered image) the design places a couple of racecar spolier-like wings in a sandwich orientation, to create a low-pressure vacuum that sucks air through from below. The turbine is thus kept out of harm's way AeromineSo where's the turbine? Depending on the installation, it can either be at the bottom of that central pole, surrounded by a duct, or in more compact designs that sit right down on the roof instead of up on top of a box, the fan can be down in the roof of the building itself, in a pipe connecting either to that central pole or to hollow chambers in the perforated wings. Either way, the wings create a low-pressure zone, air is sucked through a tube to fill that low-pressure zone, and Aeromine places a relatively small, cheap internal propeller (perhaps 36 inches/91 cm in diameter) in that tube to run a generator.It's very quiet, very safe and very cheap to build; you don't need any fancy materials like carbon fiber, there's nothing special about the fan itself, and the whole thing comes apart for transport and a relatively simple construction process on site.Aeromine hasn't yet nominated a standard capacity for its devices in their latest iteration – indeed, we had to go digging to find much information at all about the device. But in a solution presented to the AFWERX Reimagining Energy challenge in January 2021, these units were each rated for 5 kW – pretty close to the output of a typical 21-panel, household rooftop solar system. Multiple units can of course be run along the leading edge of a building, spaced around 15 ft (4.6 m) apart, and each unit in this (now outdated) AFWERX challenge promised to generate around 14.3 MWh annually. Just for perspective, my 6.5-kW home rooftop solar system makes somewhere around 9 MWh a year.Aeromine Wind-Harvesting Unit, installed as a pilot trial on top of BASF's manufacturing facilities in Wyandotte, Michigan. We expect the large box at the bottom will disappear in commercial applications, with the air intake and turbine running below the roof line AeromineThe potential here is pretty clear; solar and wind work well in a complementary fashion. Solar's only generating during the sunniest hours, wind can be 24 hours but is totally dependent on conditions. The small rooftop footprint of an Aeromine system makes it possible to cover the rest of the roof in solar panels, then get some on-site battery storage happening and run a decent-sized business more or less off the grid.So what are the downsides? Well, these things need to be installed in spots where the wind direction is pretty constant, because they don't angle themselves to catch a breeze – and they probably never will, since they're designed to be such a cost-conscious machine. Their height might make them a visual or civic planning issue in some areas, and what's more, they'll cast shadows, which will block the sun from reaching rooftop PV panels unless the building is oriented such that the sun comes from one side and the wind from the other. So there's certainly going to be a limited number of places where they'll work optimally in a hybrid system.But that's about it at first glance. They're certainly cheaper, hardier, safer and less intrusive than windmill-style designs, and they offer a highly-accessible way to introduce reliable wind energy into a distributed power system.(Rendered image) Many Aeromine units can be run together, with proper spacing, and there's plenty of rooftop left for solar panels to run a hybrid system AeromineBefore getting too excited about any strange new wind energy tech, it's always worth revisiting Mike Barnard's excellent checklist to weed out dodgy wind power claims. Written in 2013, it's as relevant as ever today as more and more money flows into clean energy tech.Aeromine fares well against the Barnard test. With the University of Houston behind it, it has also submitted its gear to the gold-standard Sandia National Laboratories for testing – indeed Sandia has been involved directly in the development. It claims to harvest no more than 1/3 to 1/2 of the Betz limit of potential wind energy.A technical performance analysis prepared in partnership with Sandia, using wind tunnels at Texas Tech University, states that ""by sweeping a large area of wind with a reliable design, AeroMINEs overcome the challenges that have plagued other distributed wind solutions and have hindered distributed wind from playing a significant role in energy markets."" The report also notes a boost in energy extraction, coupled with some aerodynamic instability, when the airflow reaches the device from higher angles of attack.Aeromine motionless wind unitAeromine says that BASF is testing the system at its manufacturing plant in Wyandotte, Michigan – although it doesn't say how many units have been installed, or of what capacity. Hunting through the satellite overlays in Google Maps, it seems that the pilot test unit in the video above is situated right here, judging by the unique markings on the roof – but the satellite image isn't recent enough to show the wind power system, so we can't figure out much more.We expect this extra-tall unit sitting on a box is simply a temporary prototype designed to be super-easy to add and remove, where the final product will sit flush against the roof and have its piping and turbine installed under the surface. We wish Aeromine would be more forthcoming in its marketing.Either way, despite the limited information the company is willing to release at this point, this looks like it could be an impressive step forward for distributed rooftop wind energy. We hope Aeromine proves capable of scaling this tech up and making a serious contribution in the race to zero carbon.Check out a short video from Sandia below.R&D 100 Winner 2021: AeroMINE — Stationary Harvesting of Distributed Wind EnergySource: Aeromine via PR Newswire",Rooftop wind system delivers 150% the energy of solar per dollar
393,0,0_solar_energy_wind_power,https://www.independent.co.uk/tech/solar-power-record-perovskite-renewable-energy-b2191443.html,"For free real time breaking news alerts sent straight to your inbox sign up to our breaking news emails Sign up to our free breaking news emails Please enter a valid email address Please enter a valid email address SIGN UP I would like to be emailed about offers, events and updates from The Independent. Read our privacy notice Thanks for signing up to theBreaking News email {{ #verifyErrors }}{{ message }}{{ /verifyErrors }}{{ ^verifyErrors }}Something went wrong. Please try again later{{ /verifyErrors }}Researchers have passed the 30 per cent efficiency barrier with silicon solar cells for the first time by combining them with the so-called “miracle material” perovskite.A team from various universities and institutes in the Netherlands made the breakthrough with a tandem solar cell that compliments traditional silicon-based cells – which have an energy conversion efficiency of around 22 per cent – with the widely-acclaimed properties of perovskite.The researchers said that achieving greater than 30 per cent efficiency with the four-terminal tandem device marked “a big step in accelerating the energy transition” and would improve energy security by reducing fossil fuel dependency.“This type of solar cell features a highly transparent back contact that allows over 93 per cent of the near infrared light to reach the bottom device,” said Dr Mehrdad Najafi from the Netherlands Organisation for Applied Scientific Research (TNO).“This performance was achieved by optimizing all layers of the semi-transparent perovskite solar cells using advanced optical and electrical simulations as a guide for the experimental work in the lab.”Perovskite has been hailed for its potential to transform an array of industries, ranging from ultra-high-speed communications to renewable energy production.The researchers behind the latest solar cell record now hope to commercialise the technology to achieve a widespread roll-out.“Now we know the ingredients and are able to control the layers that are needed to reach over 30 per cent efficiency,” said Professor Gianluca Coletti, program manager of Tandem PV.“Once combined with the scalability expertise and knowledge gathered in the past years to bring material and processes to a large area, we can focus with our industrial partners to bring this technology efficiencies beyond 30 per cent into mass production.”The results were presented at the World Conference on Photovoltaic Energy Conversion (WCPEC-8) in Milan.",Solar power world record broken with âmiracle materialâ
472,0,0_solar_energy_wind_power,https://www.pv-magazine.com/2022/10/27/new-solar-capacity-10-times-cheaper-than-gas-says-rystad/,"Building new solar capacity in Europe could be 10 times cheaper than continuing to operate gas-fired power plants in the long-term, according to a new study by Norway-based research company Rystad Energy.Europe has seen skyrocketing gas prices since the drop in Russian gas exports, with spot prices on the Netherlands-based Title Transfer Facility (TTF) rising to an average €134 ($135.15)/MWh so far this year. Rystad forecasts prices will stabilize at around €31 per MWh by 2030, which puts the LCOE of existing gas-fired plants closer to €150 per MWh.“This is still three times more than the LCOE of new solar PV facilities. For gas-fired plants to continue being competitive, gas prices would need to fall closer to €17 per MWh and carbon prices would need to fall to €10 per tonne, which is currently unthinkable,” the company said in a statement.Popular content“Gas will continue to play an important role in the European energy mix for some time to come, but unless something fundamental shifts, then simple economics, as well as climate concerns, will tip the balance in favor of renewables,” said Carlos Torres Diaz, head of power at Rystad Energy.If gas funds were invested in renewables instead, Europe could replace gas with solar and onshore wind power generation by 2028, when total capacity would reach 333 GW, Rystad forecasts. This estimate assumes a weighted average capital cost for the technologies of €1.3 per watt, as well as a two-year pre-development phase.","New solar capacity 10 times cheaper than gas, says Rystad"
237,0,0_solar_energy_wind_power,https://techcrunch.com/2022/08/10/ford-locks-in-solar-energy-deal-with-dte-energy/,"Ford said Wednesday it has reached a deal with DTE Energy to power its electricity supply in Michigan with clean energy, a step toward its goal to become carbon neutral by 2050.The automaker’s deal with DTE, Michigan’s largest producer of renewable energy, will add 650 megawatts of new solar energy capacity in the state by 2025, allowing the carmaker to assemble each vehicle it makes there with renewable energy.Ford called the deal the largest-ever renewable energy purchase from a utility in the U.S. The arrangement will help Ford decarbonize its operations and meet its sustainability goals, including a target to power all of its global facilities with renewable energy by 2035.Ford said the purchase will help it cut its carbon dioxide emissions by up to 600,000 tons. Overall, Ford’s arrangement with DTE will increase Michigan’s solar capacity by 70%, according to the automaker.The announcement comes one day after Ford said it will raise the price of its electric F-150 Lightning pickup truck between $6,000 and $8,500 for new orders.“Due to significant material cost increases and other factors, Ford has adjusted MSRP starting with the opening of the next wave of F-150 Lightning orders,” a statement read.The entry-level Lightning will now retail for $46,974, while the top-tier “Platinum Extended Range” version starts at $96,874. The price increase will not apply to customers who have already ordered a truck and are awaiting delivery.Ford also said the truck’s standard range battery can now travel 240 miles, up from 230 miles, on a full charge.",Ford locks in solar energy deal with DTE Energy
471,0,0_solar_energy_wind_power,https://www.pv-magazine.com/2022/10/27/5-cooling-down-solar-modules-by-increasing-space-between-panel-rows/,"A research group led by the US Department of Energy's National Renewable Energy Laboratory (NREL) has proposed a new approach to utilize connective heat transfer for solar module cooling in large-scale solar power plants.Their modeling considers factors such as row spacing, panel height, and tilt angle. It also uses a length scale input to characterize the space through which air moves around or through the solar modules. In the standard models, by contrast, the length often used is a ratio of module dimensions, which ignores PV plant configuration.“The convective heat transfer curve was generated through computational flow simulations and wind tunnel experiments that allowed for convective heat transfer to be described for a lacunarity length scale value that describes the spacing of the entire PV array through a single length unit,” the scientists said, claiming that using the lacunarity length scale leads to 1.5% more accurate power production.Their techno-economic analysis considered 1 MW south-facing PV systems located in Phoenix, Arizona, with a fixed tilt angle of 30 degrees over different row spacings, or ground coverage ratio (GCR). The annual land lease cost was assumed at $0.054/m2. The row spacing of the PV plants was varied from two to 11 meters, corresponding to GCR values of 0.73 to 0.08.“Increasing spacing could enable more varieties of crops and more types of agricultural equipment to be utilized in agrivoltaic systems,” said Jordan Macknick, who leads a different NREL research project focused on agrivoltaics. “That could potentially make these spaced-out solar systems more cost-effective and compatible with larger-scale agriculture.”Popular contentThrough the modeling, the group ascertained that the optimal levelized cost of energy (LCOE) point was $0.29/kWh, with row spacing varying between 4.83 and 7.34 meters. With two-meter spacing, the LCOE was $0.33/kWh, and with 11 meters it was $0.36/kWh.The group found that the greatest LCOE improvements were registered in climates with low average annual ambient temperatures and moderate to high average annual wind speeds across the US. They presented the modeling in the study “Technoeconomic Analysis of Changing PV Array Convective Cooling Through Changing Array Spacing,” recently published in the IEEE Journal of Photovoltaics.Other recent proposals for using convective solar module cooling include packing PV panels in close proximity, and taking wind direction and module inclination into account.",Cooling down solar modules by increasing space between panel rows
292,0,0_solar_energy_wind_power,https://www.africanews.com/2022/09/15/undersea-power-cable-to-connect-egypt-to-europe-via-greece/,"How can Europe manage to secure abundant and cheaper electricity?One of the most ambitious projects in the planning is the interconnection of Europe with Egypt via Greece.An undersea cable that will carry 3,000 MW RES electricity and connect northern Egypt directly to Attica in Greece.The project has been undertaken by the Copelouzos Group, whose management met last week with the Egyptian leadership to speed up the procedures""By bringing 3,000 MW of clean energy to Europe, via Greece, we are helping Europe wean itself off Russia's fossil fuels and natural gas. Also, the green energy we will transport will be much cheaper than today's energy prices. You understand that this will help both Greek and European consumers"", said Ioannis Karydas, CEO of Renewables, Copelouzos Group.The so-called ""GREGY interconnection” is a 3.5 billion euro project that has been categorized as a Project of Common Interest (PCI) by the European Union.It will carry clean electricity produced in Egypt (or other African countries) through solar or wind parks.""Approximately one third (of the electricity that will come from Egypt) will be consumed in Greece, and mainly in Greek industries, another third will be exported to neighboring European countries and one third will be used in Greece, for the production of green hydrogen. The majority of this hydrogen will also be exported to neighboring European countries"", added Ioannis Karydas.Egypt has completed interconnections with Libya, Sudan and Saudi Arabia and aspires to become a major energy hub for SE Europe too.The ""GREGY interconnection” is expected to be operational in 7 to 8 years.",Undersea power cable to connect Egypt to Europe via Greece
115,0,0_solar_energy_wind_power,https://reneweconomy.com.au/australia-can-slash-emissions-81-pct-by-2030-using-existing-technologies-report-says/,"Australia could slash its emissions by 81 per cent by 2030 – almost double the 43% target recently legislated by the federal government – using little else but “off the shelf” technologies, a new report says.The Beyond Zero Emissions report argues that the vast bulk of Australia’s emissions reduction task can be met by a combination of six technologies, all available today: solar PV, wind, batteries, electric vehicles, heat pumps and electrolysers.BZE says the “ambitious but achievable” plan relies on ramping up the rollout rates of these technologies over the coming five years, in some cases quite significantly, and would be supported by targeted “carbon drawdown initiatives.”The fast-tracking of the rollout of these technologies required to hit the 81% emissions reduction mark would include a doubling of the current rate of deployment of solar panels, and EV uptake boosted to 14 times its current levels.The uptake of heat pumps for water heating and air conditioning would need to be increased by 37 times, the report says, in line with calls from electrification and energy efficiency advocates to ho hard at a household level to cut emissions.Two appliances per household“This plan sets the ambition, establishes the job potential and demonstrates the opportunity of acting without delay for industry and manufacturing, business and households,” the report says.“In the next five years, we need to install clean technology in our homes, vehicles and industries at a rate of about two units or appliances per household,” it adds.“We cannot afford to wait for new research and inventions, and we don’t need to.”Across households and businesses this looks something like, 10 million-plus units of hot water and air conditioning heat pumps, 2.9 million units of building efficiency technologies, 7,000 units of industrial-scale tech, such as heat pumps and electrolysers, and 3.8 million units of transport – mostly electric vehicles and chargers.On renewables and storage, which BZE says will be the foundation of success for its emissions “elimination” plan, the reports calls for the installation of about 6,000 wind turbines and 66 million solar panels over the coming five years.This would be joined by 67GWh of utility-scale and domestic batteries over the five-year period and 3,000 electrolysers to supply renewable hydrogen for industry.“It is doable”“This ambitious undertaking means installing more generation capacity and far more storage than the total of all types of generation capacity and storage in Australia today,” the report says.“It is doable: in 2021 alone Australia added 6.2GW of renewable generation. Doubling the 2021 rollout rate of renewable generation will realise the ambitions identified in this plan.”BZE chief Heidi Lee says that as well as using existing technologies, the plan’s target is supported by more than 50 companies who are “already getting on” with the job.“Australia has doubled its rollout of domestic solar generation over the past five years and our plan now requires us to double down on utility solar, wind and energy storage,” Lee says.“If we take this approach to other renewable technologies we won’t just meet our legislated emission reduction targets, we’ll go well beyond them.”On “carbon drawdown,” the plan looks to initiatives that remove carbon from the atmosphere, such as revegetation, the use of pyrolysis for biomass and the protection of ecosystems and existing carbon stock.","Australia can slash emissions 81 pct by 2030 using six existing technologies, report says"
276,0,0_solar_energy_wind_power,https://techxplore.com/news/2022-09-cobalt-free-cathode-lithium-ion-batteries.html,"Working with researchers at four U.S. national laboratories, Huolin Xin, UCI professor of physics & astronomy, has found a way to fabricate lithium-ion batteries without using cobalt, a rare, costly mineral extracted under inhumane conditions in Central Africa. Credit: Steve Zylius / UCIResearchers at the University of California, Irvine and four national laboratories have devised a way to make lithium-ion battery cathodes without using cobalt, a mineral plagued by price volatility and geopolitical complications.In a paper published today in Nature, the scientists describe how they overcame thermal and chemical-mechanical instabilities of cathodes composed substantially of nickel—a common substitute for cobalt—by mixing in several other metallic elements.""Through a technique we refer to as 'high-entropy doping,' we were able to successfully fabricate a cobalt-free layered cathode with extremely high heat tolerance and stability over repeated charge and discharge cycles,"" said corresponding author Huolin Xin, UCI professor of physics & astronomy. ""This achievement resolves long-standing safety and stability concerns around high-nickel battery materials, paving the way for broad-based commercial applications.""Cobalt is one of the most significant supply chain risks threatening widespread adoption of electric cars, trucks and other electronic devices requiring batteries, according to the paper's authors. The mineral, which is chemically suited for the purpose of stabilizing lithium-ion battery cathodes, is mined almost exclusively in the Democratic Republic of Congo under abusive and inhumane conditions.""Electric vehicle manufacturers are eager to curtail the use of cobalt in their battery packs not only for cost reduction but to counter the child labor practices used to mine the mineral,"" Xin said. ""Research has also shown that cobalt can lead to oxygen release at high voltage, causing damage to lithium-ion batteries. All of this points to a need for alternatives.""However, nickel-based cathodes come with their own problems, such as poor heat tolerance, which can lead to oxidization of battery materials, thermal runaway and even explosion. Although high-nickel cathodes accommodate larger capacities, volume strain from repeated expansion and contraction can result in poor stability and safety concerns.The researchers sought to address these issues through compositionally complex high-entropy doping using HE-LMNO, an amalgamation of transition metals magnesium, titanium, manganese, molybdenum and niobium in the structure's interior, with a subset of these minerals used on its surface and interface with other battery materials.Xin and his colleagues employed an array of synchrotron X-ray diffraction, transmission electron microscopy and 3D nanotomography instruments to determine that their zero-cobalt cathode exhibited an unprecedented volumetric change of zero during repeated use. The highly stable structure is capable of withstanding more than 1,000 cycles and high temperatures, which makes it comparable to cathodes with much lower nickel content.For some of these research tools, Xin collaborated with researchers at the National Synchrotron Light Source II, located at the U.S. Department of Energy's Brookhaven National Laboratory in New York. As a DOE Office of Science user facility, NSLS-II offered the team access to three of its 28 scientific instruments—called beamlines—to study the internal structure of the new cathode.""The combination of the different methods at NSLS II beamlines enabled the discovery of a trapping effect of oxygen vacancies and defects inside the material, which effectively prevents the crack formation in the HE-LMNO secondary particle, making this structure extremely stable during cycling,"" said co-author Mingyuan Ge, a scientist at NSLS-II.Added Xin: ""Using these advanced tools, we were able to observe the dramatically increased thermal stability and zero-volumetric-change characteristics of the cathode, and we've been able to demonstrate extraordinarily improved capacity retention and cycle life. This research could set the stage for the development of an energy-dense alternative to existing batteries.""He said the work represents a step toward achieving the dual goal of spurring the proliferation of clean transportation and energy storage while addressing environmental justice issues around the extraction of minerals used in batteries.More information: Huolin Xin, Compositionally complex doping for zero-strain zero-cobalt layered cathodes, Nature (2022). www.nature.com/articles/s41586-022-05115-z Journal information: Nature Huolin Xin, Compositionally complex doping for zero-strain zero-cobalt layered cathodes,(2022). DOI: 10.1038/s41586-022-05115-z",Researchers develop a cobalt-free cathode for lithium-ion batteries
116,0,0_solar_energy_wind_power,https://reneweconomy.com.au/huge-new-nickel-mine-aims-for-100-pct-renewables-with-worlds-biggest-renewable-micro-grid/,"The massive $1.7 billion West Musgrave nickel and copper project – given the green light by Australian mining company Oz Minerals late last week – will be a groundbreaking project for the sheer scale and influence of the renewable energy resources it proposes to harness.Oz Minerals aims to reach 100 per cent renewable power at the remote mine – located in Western Australia near the borders with South Australia and the Northern Territory – and will start off by providing more than 80 per cent of its power needs from what will be the world’s biggest renewable energy micro-grid.Modelling done so far indicates an optimal mix of around 60MW of solar and 90MW of wind, along with a sizeable battery – although the specifications of the battery have yet to be finalised.Oz Minerals believes this configuration will allow the mine and its processing plant to run on “diesel off” or “renewables only” for extended periods.The size of the battery will likely be decided by the extent to which the company can tailor the operators of the mine and processing plant to when the wind is blowing and the sun is shining, and how much of excess output can afford to be spilled.See this story for more information on that: Mopping up spilled energy: Mining giant looks to take next step to 100% renewablesLargest renewable micro grid in worldAll this will make it one of the largest operations in the world running on wind and solar only, and its “game changing” operations will be highly informative for the operators of Australia’s national grid which intend to follow the same path over the next decade.The recognition that wind and solar can play such a significant role even – or perhaps especially – in such a remote location has been one of the decisive factors helping OZ Minerals commit to one of the world’s lowest cost and lowest emission operations.The cost is important, and so are the emissions, given the Oz Minerals intends to “ride the global electrification wave” and target the surging demand for nickel and copper for electric vehicles, along with the boom in wind, solar and storage in the grid.The cost of powering such plants with diesel – or gas via a new pipeline – would have been prohibitive. And by focusing on renewables and solar, Oz Minerals has absolute clarity on its electricity costs for the life of the mine.The West Musgrave transport fleet will initially be powered by fossil fuels, but the company intends to switch to electrify the haul truck fleet at the first opportunity, likely to be the first engine change-out.It hopes to increase the penetration of renewables to reach net zero scope 1 emissions well before 2038, and expects technology advancements will help it make the leap from 80 per cent to full renewables, most likely in the areas of storage.Low cost and low emissionsOz Minerals has actually been looking at the renewables options for West Musgrave for many years – we first wrote about it in 2018 – because it knew then that powering the mine with fossil fuels wasn’t a viable option.It has long recognised that at such a remote location a big share of renewables was the key to helping it deliver one of the world’s lower cost and lowest emissions copper and nickel mining projects.The fact that it will supply a low emissions supply of nickel and copper will be important for the electric vehicle markets that it intends to tap into, because EV makes also want to ensure their supply chains are low carbon too.Oz Minerals is currently seeking bids from vendors before finalising the best mix of wind, solar, battery and diesel, and will then sign a long term power purchase agreement with an option to buy the energy component outright at a later date.The mine is being built on the land of the Ngaanyatjarra people.",Australiaâs huge new nickel mine will host worldâs biggest renewable micro-grid
444,0,0_solar_energy_wind_power,https://www.psu.edu/news/research/story/battery-tech-breakthrough-paves-way-mass-adoption-affordable-electric-car/,"UNIVERSITY PARK, Pa. — A breakthrough in electric vehicle battery design has enabled a 10-minute charge time for a typical EV battery. The record-breaking combination of a shorter charge time and more energy acquired for longer travel range was announced today (Oct. 12) in the journal Nature.“The need for smaller, faster-charging batteries is greater than ever,” said Chao-Yang Wang, the William E. Diefenderfer Professor of Mechanical Engineering at Penn State and lead author on the study. “There are simply not enough batteries and critical raw materials, especially those produced domestically, to meet anticipated demand.”In August, California’s Air Resources Board passed an extensive plan to restrict and ultimately ban the sale of gasoline-powered cars within the state. By 2035, the largest auto market in the United States will effectively retire the internal combustion engine.If new car sales are going to shift to battery-powered electric vehicles (EVs), Wang explained, they’ll need to overcome two major drawbacks: they are too slow to recharge and too large to be efficient and affordable. Instead of taking a few minutes at the gas pump, depending on the battery, some EVs can take all day to recharge.“Our fast-charging technology works for most energy-dense batteries and will open a new possibility to downsize electric vehicle batteries from 150 to 50 kWh without causing drivers to feel range anxiety,” said Wang, whose lab partnered with State College-based startup EC Power to develop the technology. “The smaller, faster-charging batteries will dramatically cut down battery cost and usage of critical raw materials such as cobalt, graphite and lithium, enabling mass adoption of affordable electric cars.”The technology relies on internal thermal modulation, an active method of temperature control to demand the best performance possible from the battery, Wang explained. Batteries operate most efficiently when they are hot, but not too hot. Keeping batteries consistently at just the right temperature has been major challenge for battery engineers. Historically, they have relied on external, bulky heating and cooling systems to regulate battery temperature, which respond slowly and waste a lot of energy, Wang said.Wang and his team decided to instead regulate the temperature from inside the battery. The researchers developed a new battery structure that adds an ultrathin nickel foil as the fourth component besides anode, electrolyte and cathode. Acting as a stimulus, the nickel foil self-regulates the battery’s temperature and reactivity which allows for 10-minute fast charging on just about any EV battery, Wang explained.“True fast-charging batteries would have immediate impact,” the researchers write. “Since there are not enough raw minerals for every internal combustion engine car to be replaced by a 150 kWh-equipped EV, fast charging is imperative for EVs to go mainstream.”The study’s partner, EC Power, is working to manufacture and commercialize the fast-charging battery for an affordable and sustainable future of vehicle electrification, Wang said.The other coauthors on the study are Teng Liu, Xiao-Guang Yang, Shanhai Ge and Yongjun Leng of Penn State and Nathaniel Stanley, Eric Rountree and Brian McCarthy of EC Power.The work was supported by the U.S. Department of Energy, the U.S. Department of Defense, the U.S. Air Force and the William E. Diefenderfer Endowment.",Battery tech breakthrough paves way for mass adoption of affordable electric car
402,0,0_solar_energy_wind_power,https://www.mckinsey.com/industries/electric-power-and-natural-gas/our-insights/renewable-energy-development-in-a-net-zero-world,"The rapid maturation of wind and solar power has been nothing short of astonishing. Not long ago, the development of new solar and wind farms was typically driven by small regional players, and the cost was significantly higher than that of a coal plant. Today, the cost of renewables has plummeted, and many solar and wind projects are undertaken by large multinational companies, which often also announce staggering development targets.Over the past decade, the growth of renewable energy has consistently and dramatically outperformed nearly all expectations (Exhibit 1). Upward corrections of estimates have become something of a ritual.Exhibit 1 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.comBut this growth story is just getting started. As countries aim to reach ambitious decarbonization targets, renewable energy—led by wind and solar—is poised to become the backbone of the world’s power supply. Along with capacity additions from major energy providers, new types of players are entering the market (Exhibit 2). Today’s fast followers include major oil and gas companies, which aim to shift their business models to profit from the increased demand for renewables and the electrification of vehicles, and private-equity players and institutional investors that make renewable energy a central component of their investment strategy. Leaders in the shipping industry are investing in renewables to enable the production of hydrogen and ammonia as zero-emission fuel sources; steel manufacturers are eyeing green hydrogen to decarbonize their steel production, with renewables providing the green electricity for the process. Car manufacturing companies are also striking renewable-energy deals to help power their operations and manufacturing, as well as making investments in wind and solar projects.Exhibit 2 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.comMcKinsey estimates that by 2026, global renewable-electricity capacity will rise more than 80 percent from 2020 levels (to more than 5,022 gigawatts). Of this growth, two-thirds will come from wind and solar, an increase of 150 percent (3,404 gigawatts). By 2035, renewables will generate 60 percent of the world’s electricity. But even these projections might be too low. Three years ago, we looked at advances made by renewable energy and asked, “How much faster can they grow?” The answer is: faster than you think they can.Three core capabilities for wind and solar developersThis race to build additional solar and wind capacity increases the pressure on developers to execute efficiently and heightens competition for finite resources. Still, the three winning capabilities we identified three years ago as important for building or expanding a renewables business are even more critical now. They form the bedrock required to tackle upcoming challenges:Value-chain excellence. As competition intensifies and government support for renewables subsides, strong capabilities across the entire value chain are the required cost of admission. For instance, gaining access to scarce amounts of attractive land will require differentiation in project origination and development. As margins squeeze and operators’ exposure to risk increases, ambitious companies will want to explore new, profitable offtake markets for their electricity, such as data centers or hydrogen electrolyzers for industrial production.Economies of scale and skill. Driven by the rapid scaling of the renewables industry, many players have built efficient operating models. However, finding employees with the necessary skills and capabilities, particularly in high-demand areas such as project development and engineering, is becoming a bottleneck for growth ambitions.Agile operating model. Agility and speed will be key in finding innovative ways to integrate partners and in establishing robust, high-performing supply chains. They will also enable businesses to shift resources quickly to the biggest value pools and respond to changes in the landscape, such as shifting regulations or price volatility.Four challenges that will define the new era of renewable energyLeveraging these capabilities as a strong foundation, successful renewables developers must navigate an increasingly complex and competitive landscape. Specifically, they will have to focus on and address four emerging challenges:A scarcity of top-quality land. Developers are in a constant scramble to identify new sites with increasing speed. Our analysis in Germany, a country aiming to nearly double its share of electricity coming from renewables by 2030, offers a glimpse into the constraints. Of the 51 percent of the country’s land that is potentially suitable for onshore wind farms, regulatory, environmental, and technical constraints eliminate all but 9 percent. Meeting capacity targets will mean adding wind turbines to 4 to 6 percent of the country, giving developers very little room for error.Meeting capacity targets will mean adding wind turbines to 4 to 6 percent of the country, giving developers very little room for error. A blue-collar and white-collar labor shortage. Across economies, the “Great Attrition” is making it difficult for companies to find and keep employees. Since April 2021, 20 million to 25 million US workers have quit their jobs, and 40 percent of employees globally say they are at least somewhat likely to leave their current position in the next three to six months. This environment presents a particularly acute challenge for industries such as renewable energy, where specific technical expertise and experience are crucial elements of success. For instance, our analysis suggests that between now and 2030, the global renewables industry will need an additional 1.1 million blue-collar workers to develop and construct wind and solar plants, and another 1.7 million to operate and maintain them. This includes construction laborers, electricians, truck and semitrailer drivers, and operating engineers.This environment presents a particularly acute challenge for industries such as renewable energy, where specific technical expertise and experience are crucial elements of success. For instance, our analysis suggests that between now and 2030, the global renewables industry will need an additional 1.1 million blue-collar workers to develop and construct wind and solar plants, and another 1.7 million to operate and maintain them. This includes construction laborers, electricians, truck and semitrailer drivers, and operating engineers. Supply chain pressures. The soaring cost of steel, manufacturing disruptions caused by extended lockdowns in China, and transportation backlogs at ports are already making it difficult for wind and solar developers to complete projects in their pipeline on time and on budget. Some of these pressures will abate as others move to the forefront. For instance, many of the raw materials needed to manufacture solar panels and wind turbines are projected to be in short supply. This includes nickel, copper, and rare earth metals such as neodymium and praseodymium, which are indispensable for the creation of magnets used in wind turbine generators.Pressure on profits and volatility of returns in the short term. The increasing number of players moving into the renewable-development space, combined with reduced levels of government support and higher costs of materials, technology, and financing, is putting pressure on returns. At the same time, an all-time-high price volatility creates uncertainty and market risk.Renewables developers will need to act decisively to prepare for these upcoming challenges. In a series of future articles, we provide detailed insights on each of these pressures and share potential ways players can take action.",Renewable-energy development in a net-zero world
17,0,0_solar_energy_wind_power,https://arstechnica.com/cars/2022/09/mercedes-f1-team-cut-its-freight-emissions-by-89-with-biofuel-switch/,"A switch from diesel to biofuel significantly reduced the Mercedes-AMG Formula 1 team's freight carbon emissions in a new test. The team made the switch for the final three European races of this season, using locally sourced hydrotreated vegetable oil (HVO)—made from food waste like fryer oil—to run 16 heavy trucks as they hauled the team between grand prix in Belgium, the Netherlands, and Italy.Over a distance of 870 miles (1,400 km), it says the use of HVO resulted in less carbon emissions—44,091 kg less to be specific, which is a decrease of 89 percent compared to normal fossil fuel diesel.The race cars are a rounding errorA push for greater fuel efficiency in Formula 1 has resulted in some fairly remarkable engineering. A current F1 powertrain is as complex as the sport has ever seen, combining comparatively tiny but extraordinarily efficient V6 gasoline engines with hybrid systems that recover energy under braking and from the turbocharger spinning.This is something that everyone involved is rightly proud of, but in terms of reducing the sport's carbon emissions, the fuel burned by the race cars is frankly irrelevant. F1 measures fuel by weight, and each car is allowed 220.5 lbs (100 kg) for the race. Being extremely generous, we can triple that to account for three hours of practice sessions plus qualifying, but that's still only 661 lbs (300 kg) per car, of which there are 20, and this year they'll race 23 times in total.AdvertisementBut each team flies about 80 people to each race. And they need a whole lot of equipment; obviously the race cars and all the tools to run them, but also massive mobile offices and hospitality suites for the European events, when everything travels by road freight. (For the rest of the calendar, all the teams' cars and equipment travel together, and the gin palaces stay behind.) As transport and freight therefore accounts for much more of the carbon emitted by the sport, it's clearly a good idea to try to reduce that.Mercedes first tried HVO in one of its trucks as it drove back from the Hungarian Grand Prix to the team's base in Brackley, UK. The real test began following this year's Belgian Grand Prix, when 16 trucks used the fuel to drive 186 miles (300 km) to the Dutch Grand Prix in Zandvoort, the Netherlands, then another 684 miles (1,100 km) to the Italian Grand Prix in Monza, Italy. The entire journey was fueled by locally sourced HVO, apart from the last 12 miles (20 km) ""due to supply challenges,"" the team says.Analyzing the data, it says that HVO saved 97,204 lbs (44,091 kg) of carbon dioxide emissions compared to diesel, an 89 percent reduction.""Sustainability is at the heart of our operations. Trialing the use of biofuels for our land freight is another example of our commitment to embed sustainability in every decision we make and action we take. We aim to be on the cutting edge of change and hope we can make the adoption of sustainable technology possible as we are all in the race towards a sustainable tomorrow,"" said team principal and CEO Toto Wolff.",Mercedesâ F1 team cut its freight emissions by 89% with biofuel switch
435,0,0_solar_energy_wind_power,https://www.offshore-energy.biz/worlds-1st-bulker-powered-by-wind-sails-into-port-of-newcastle/,"October 24, 2022, by Jasmina Ovcina MandraImage by Port of NewcastleThe world’s first bulk carrier to be partially powered by wind, the Shofu Maru, sailed into the Port of Newcastle this morning on its maiden voyage.The 100, 422 dwt bulker, owned by Japanese shipping company Mitsui O.S.K. Lines (MOL), was delivered on October 7, 2022. The vessel is the first of its kind and it heralds a new era in modern shipping as wind propulsion marks its return as a sustainable power source.Related Article Posted: about 1 month ago World’s 1st ship equipped with ‘Wind Challenger’ delivered Posted: about 1 month agoShofu Maru is fitted with the so-called Wind Challenger system, a telescoping hard sail that harnesses wind power to propel the vessel.The system converts wind energy directly to a propulsive force. The installation of the sail on merchant ships has the potential to significantly reduce fuel consumption, which in turn reduces GHG emissions by about 5-8%. The sail can be shortened and extended (55 meters in length) adapting automatically to the wind conditions on board as it can rotate 360 degrees.The vessel was greeted in the Port of Newcastle by a delegation of port officials and diplomatic dignitaries, including Consul-General of Japan in Sydney Tokuda Shuichi, Deputy Secretary of Safety, Environment and Regulation at Transport for NSW Tara McCarthy and MOL Managing Executive Officer for East/South East Asia and Oceania Nobuo Shiotsu.The world’s first bulk carrier to be partially powered by wind, the Shofu Maru, sailed into #Newcastle this morning on its maiden voyage.The vessel’s unique hard sail will reduce greenhouse gas emissions by around 5% on its Japan-Australia journey. pic.twitter.com/RXNEqO8zB8 — Port of Newcastle (@PortofNewcastle) October 24, 2022“Port of Newcastle is pleased to join Port Authority of New South Wales in welcoming the vessel to our beautiful (and rainy) city, alongside the Consul-General of Japan in Sydney Tokuda Shuichi, Deputy Secretary of Safety, Environment and Regulation at Transport for NSW Tara McCarthy and MOL Managing Executive Officer for East/South East Asia and Oceania Nobuo Shiotsu,” the port authority said.The 235-metre-long Shofu Maru will transport coal mainly from Australia, Indonesia, and North America as a dedicated vessel for Japan’s electric services company Tohoku Electric Power.The vessel will now be loaded with its coal shipment, before it departs Newcastle on Tuesday.MOL plans to build a second bulk carrier equipped with the Wind Challenger hard sail system at Oshima Shipbuilding. Under the plan, MOL Drybulk will operate the 62,900 dwt vessel, slated for delivery in 2024. Once completed, the vessel is chartered to transport wood pellets for Enviva, which specializes in the production of sustainable wood bioenergy.MOL is also looking into the feasibility of adopting rotor sails, an auxiliary wind propulsion system developed by UK’s Anemoi Marine Technologies on the bulker. The combined use of both the Wind Challenger and rotor sails is expected to slash GHG emissions by an average of 20%.Related Article Posted: 3 months ago MOL pens deal for 2nd Wind Challenger-fitted bulker Posted: 3 months agoExploration of the wind-assisted propulsion technology forms part of MOL’s overall decarbonization strategy. The MOL Group has set mid- to long-term targets to reduce GHG emissions intensity in marine transport by approximately 45% by 2035 (i.e. versus 2019) and achieve net zero by 2050.",Worldâs 1st bulker powered by wind sails into Port of Newcastle
22,0,0_solar_energy_wind_power,https://arstechnica.com/science/2022/09/us-installs-record-solar-capacity-as-prices-keep-falling/,"This week, the US Department of Energy's Berkeley Lab released its annual analysis of solar energy in the US. It found that nearly half the generating capacity was installed in the US during 2021 and is poised to dominate future installs. That's in part because costs have dropped by more than 75 percent since 2010; it's now often cheaper to build and operate a solar plant than it is to simply buy fuel for an existing natural gas plant.The analysis was performed before the passage of the Inflation Reduction Act, which contains many incentives and tax breaks that should expand solar's advantages in the coming years.Solar, by the numbersIn terms of large, utility-scale solar installs, the US added over 12.5 gigawatts of new capacity last year, bringing the total installed capacity to over 50 gigawatts. Texas led the way, with about a third of the total capacity added (3.9 GW) going online in the Lone Star State. Combined with residential and other distributed solar installations, solar alone accounted for 45 percent of the new generating capacity added to the grid last year.AdvertisementThat growth showed up in figures on how much energy solar supplies. Five states now receive more than 15 percent of their electricity from solar power, including Massachusetts and Vermont, with California receiving 25 percent of its electricity from the Sun.Solar's expansion has largely been driven by falling costs. The DOE estimates that the price of building a solar plant has been dropping by an average of about 10 percent a year, leading to a fall of over 75 percent since 2010. That has left prices averaging about $1.35 for each watt of capacity in 2021. Large-scale plants benefit the most, with projects over 50 megawatts costing about 20 percent less than those under 20 MW.The drop in prices is causing some somewhat odd trends, driven by the fact that it's becoming increasingly economical to install large facilities in states that don't get as much sun, like Maine, Michigan, and Wisconsin. As a result, the past several years have seen the average incoming energy at newly constructed facilities (measured as daily kilowatt-hours per square meter) drop by about 20 percent.That has helped cause a large spread in what's called the capacity factor, which is calculated by dividing the amount of energy produced at a facility by the maximum energy it could have generated if it produced 24 hours a day. The median capacity factor of solar plants in the US was 24 percent, but outliers were as low as 9 percent and as high as 35 percent. As prices continue to fall, this spread may become more pronounced, with more plants at the low end of the range.",US installs record solar capacity as prices keep falling
16,0,0_solar_energy_wind_power,https://arstechnica.com/cars/2022/09/how-electric-cars-could-rescue-the-us-power-grid/,"Last month, California finalized a rule that will ban the sale of new gas-powered cars, starting in 2035. Obviously, that’ll accelerate the adoption of electric vehicles and encourage other states to do the same. (Oregon has already followed California’s lead.) But less obviously, spurning carbon-spewing vehicles could help buttress the United States’ ancient, creaky electrical grids.Cars are no longer just modes of transportation; they are increasingly integrated into the larger energy infrastructure. If your EV is sitting in your garage fully charged (cars are typically parked 95 percent of the time) and you lose power, that big battery offers an opportunity to keep the lights on. And when there’s a sudden spike in demand for the grid—because everyone wants to turn on their AC during a heat wave or their heat during a deep freeze—utilities could pay homeowners for their excess battery power.This is known as bidirectional or vehicle-to-grid charging (aka V2G), and it’s “one of the legitimate game changers,” says Clifford Rechtschaffen, commissioner of the California Public Utilities Commission. “If all the EVs in the state plug in during these peak load times and feed power back to the grid, they’re acting as giant batteries. We could use them to greatly relieve stress on the grid during the periods of greatest need.”It’s still early days for V2G. More than 100 V2G pilots are scattered worldwide, though most are in Europe. California’s experimentation has been limited to small test programs. Still, more makers of cars and chargers are offering two-way charging, and experts think the concept could work on a large scale. Some 200 million electric vehicles could be on global roads by 2030, according to a recent estimate. California alone could have 14 million by 2035, the Natural Resources Defense Council estimates. If just local utilities could exploit all those batteries, they’d be able to power every home in the state for three days.AdvertisementWhen someone plugs in a car to charge it, alternating current (AC) power is converted into direct current voltage, which is stored inside the car’s battery. If the owner has a bidirectional charger, that DC power can be converted back to AC and added to the grid.Bidirectional chargers are far from common today and can be expensive, often requiring additional specialized hardware. Yet automakers and other companies are starting to roll them out to help EV owners contribute to the grid or to store and then convert power for their own purposes. Ford’s new electric F-150 can power a home for up to three days—a serious perk in the climate-change-wracked dystopia to come. Volkswagen has touted the bidirectional charging capabilities of its newest and upcoming EVs. Just this month, Nissan approved the first bidirectional charger for the all-electric Leaf, a car that has been sold in the US for almost 12 years.But utilities will likely play the biggest role in ushering in a new era of electricity grids, says Max Baumhefner, a senior attorney at the Natural Resources Defense Council. One easy way they can encourage EV drivers help the grid is by offering “time-of-use” rates, which make it cheaper for owners to charge at times when the grid is less taxed—for example, when most folks are asleep at night. After watching 10 years of success with these rates programs, Baumhefner has concluded that “if we give people a little nudge, they will respond.” This sort of strategy can actually keep costs down for all grid users by helping utilities use the infrastructure they already paid for more efficiently, and avoid making upgrades.The trick will be standardization, says Katie Sloan, vice president of customer programs and services at the utility Southern California Edison. As more people start sending battery power back to the grid, it would help if the various EVs and charging systems were technologically integrated. “It’s really analogous to what we saw in the solar industry,” says Sloan. “That was the first time we were moving from one-way power flow into homes really having bidirectional power flow.” Similarly, automakers, charging companies, and utilities need to work together to make use of EV batteries sitting in garages.",How electric cars could rescue the US power grid
64,0,0_solar_energy_wind_power,https://interestingengineering.com/innovation/china-worlds-largest-wind-farm,"Work on the project will begin “before 2025.” It will surpass the largest wind farm in the world once it is finished, according to Guangdong province officials.The Jiuquan Wind Power base in China, a huge facility with a 20 gigawatt capacity, presently holds the distinction of being the world’s largest wind farm.A city in the nearby Fujian Province earlier this year proposed a 1 trillion yuan ($138 billion) project that included 50 gigawatts of offshore wind.With more than 25 percent of the world's wind power capacity, China is claimed to be a world leader in wind energy.Offshore wind farm at low tide in sunset. silkwayrain/iStockCapacity: 13 million homesThe 10-kilometer-long farm, which will have thousands of strong wind turbines, will operate between 75 and 185 kilometers (47 and 115 miles) offshore.And because of the region's distinctive topographical features and windy location, these turbines will be able to run between 43 percent to 49 percent of the time, meaning 3,800 to 4,300 hours each year.A gigawatt is one billion watts, and 3 million solar panels are required to produce one gigawatt of power. 100 million LEDs or 300,000 typical European homes may each be powered by one gigawatt.The facility's 43.3 GW of power-generating capacity could supply electricity to 13 million households, which is equal to 4.3 billion LED lights, as per Euronews.Over 99 percent of Norway's electricity comes from hydropower plants with a 31 GW capacity, which is less than the new Chinese wind farm project. The record-breaking offshore farm would be bigger than all of the power plants in Norway combined.",China to break its own record: Worldâs new largest wind farm could power 13 million homes
253,0,0_solar_energy_wind_power,https://techcrunch.com/2022/09/12/tesla-solar-roof-new-powerwall-requirement/,"Want a Tesla Solar Roof? You may need to shell out for a Powerwall firstAt Tesla, the company’s stock price apparently isn’t the only thing that’s climbing lately.Powerwall sales are also poised to rise, now that the automaker’s big batteries are reportedly a prerequisite for Solar Roof installations.The requirement is new as of September, and it comes as Tesla ramps up Powerwall production at its Nevada factory, per Electrek. Tesla did not respond to a request for comment on the report. The change seemingly reflects the inroads the firm has made on its hefty backlog of Powerwall orders. Tesla pinned the pileup on the global chip shortage last year.Tesla markets its Powerwall batteries as a way for homeowners to rely less on the grid by storing solar energy “for use when the sun isn’t shining.” Under Elon Musk, the company has repeatedly hiked the price of the battery system, which will now set you back at least $11,500. Tesla said it crossed its 200,000th Powerwall installation back in May, 2021.Tesla’s solar division recently recorded its best quarter in four years. However, its progress on this front appears to hinge on conventional solar panels sales. Tesla mysteriously put most new Solar Roof installations on ice earlier this year, but according to Electrek, the automaker is “restarting scheduling for the fourth quarter.”Requiring Powerwall batteries may help Tesla as it attempts to scale its own virtual power plant, which it piloted last year. Tesla described the program as a way for Powerwall owners to “dispatch” their batteries “when the grid is in critical need of additional power” — and boy oh boy does the grid need more renewable energy, particularly in California.",Want a Tesla Solar Roof? You may need to shell out for a Powerwall first
117,0,0_solar_energy_wind_power,https://reneweconomy.com.au/solar-eliminates-nearly-all-grid-demand-as-its-powers-south-australia-grid-during-day/,"The combined forces of rooftop and utility scale solar met the equivalent of all of South Australia’s electricity demand for more than six hours on Sunday, as “operational demand” was nearly eliminated as it fell to a record low on the same day.The new benchmark was noted by the Australian Energy Market Operators, and some independent data analysts, and highlights – like the new demand lows in Western Australia – the rapidly changing nature of the country’s electricity grids.According to AEMO, the minimum operational demand (i.e. that not provided by rooftop solar and other distributed resources) fell to a new record low of 100MW at 1pm on Sunday.That was below the previous record set in November last year, and is headed towards the one time unthinkable benchmark of zero operational demand that AEMO expects to see in South Australia, possibly as early as this year.At the time, rooftop solar provided 92 per cent of the state’s power needs. That’s not a record, but according to data provider NemLog2, the combination of rooftop solar and utility scale solar reached a new peak of 116.7 per cent of demand (at least in a five minute trading period), about one hour later.This graph above, tweeted by AEMO late on Monday, highlights the role played by rooftop and utility scale solar during the day, accounting for more than 100 per cent of the state’s electricity demand between 10am until after 4pm on Sunday.The excess supply – including some wind and a very small amount of gas to provide “synchronous” support to the grid – was exported to Victoria, or stored in batteries.AEMO expects that the use of advanced battery inverters, and the new connection to NSW from 2025, will mean that at these times of excess wind and solar supply, no fossil fuel generation will be needed at all.The essential grid services will come from the advanced battery inverters, which will allow the grid to be the first at this scale to operate on just wind, solar, and battery storage.It’s been a dramatic transition over the last seven years, as energy analyst Dylan McConnell noted in his own tweet and graph that tracked the falls in operational demand during daylight hours over the last seven years.Over the last 12 months, wind and solar have provided an average of more than 65 per cent of state electricity demand, and is expected to average 100 per cent in the next five years.See also: Rooftop solar smashes demand and supply records in world’s biggest isolated grid",Solar eliminates nearly all grid demand as its powers South Australia during day
255,0,0_solar_energy_wind_power,https://techcrunch.com/2022/09/14/enverus-acquires-solar-planning-solution-ratedpower/,"Enverus, a company specialized in energy-focused software-as-a-service products, has announced that it has acquired RatedPower, a Spanish startup that helps you plan, design and optimize solar power plants. Terms of the deal remain undisclosed.Enverus works with all sorts of customers in the energy ecosystem focused both on renewables and oil and gas. In particular, the company has partnered with 98% of U.S. energy producers and 35,000 suppliers to offer real-time access to analytics. With these insights, energy-focused companies can benchmark cost and revenue more easily before committing to a new project.Customers include not only power, utilities and infrastructure companies, but also financial institutions working on energy projects. They can use Enverus to allocate capital based on production and distribution optimization.With RatedPower, Enverus is doubling down on renewable energy development as RatedPower focuses on utility-scale solar photovoltaic plants exclusively.“Much like our start decades ago, RatedPower is a trailblazer in digitalization, automation and efficiency, but in the rapidly growing solar market. Our common denominator is truth in data, strategic planning and optimizing efficiencies,” Enverus CEO Jeff Hughes said in a statement. “As of today, we have taken a major leap forward in adding to our solar planning capabilities enabling us to strategically advise customers where and how to design their plants to maximize production.”Engineers use RatedPower’s software platform — pvDesign — to study, analyze and design new projects for photovoltaic plants. Overall, RatedPower has been used to develop more than 43 GW in solar power projects across 20,000 projects in 160 countries.For instance, RatedPower can scan many different layout configurations and tell you which one will maximize the profitability and value of this solar plant project based on topography, meteorological data, horizon profile, and so on.“We were born with the vision of digitalizing the renewable energy industry and making solar the world’s main energy source to accelerate the energy transition,” RatedPower CEO Andrea Barber said in a statement. “Our mission aligns perfectly with Enverus and our combined strengths will place us in a privileged position to reach that vision while powering the new, global energy economy.”",Enverus acquires solar planning solution RatedPower
432,0,0_solar_energy_wind_power,https://www.npr.org/2022/10/30/1130239008/fires-from-exploding-e-bike-batteries-multiply-in-nyc-sometimes-fatally,"Fires from exploding e-bike batteries multiply in NYC — sometimes fatallyEnlarge this image toggle caption FDNY FDNYNEW YORK — Four times a week on average, an e-bike or e-scooter battery catches fire in New York City.Sometimes, it does so on the street, but more often, it happens when the owner is recharging the lithium ion battery. A mismatched charger won't always turn off automatically when the battery's fully charged, and keeps heating up. Or, the highly flammable electrolyte inside the battery's cells leaks out of its casing and ignites, setting off a chain reaction.""These bikes when they fail, they fail like a blowtorch,"" said Dan Flynn, the chief fire marshal at the New York Fire Department. ""We've seen incidents where people have described them as explosive — incidents where they actually have so much power, they're actually blowing walls down in between rooms and apartments.""Brooklyn: 374 East 9th Street @FDNY operating at a 3 Alarm Fire in a 3 story private dwelling with extention to adjoining building pic.twitter.com/oCs3VI39SQ — NYRRT (@NYRRT) April 21, 2022A fire in Brooklyn in April was traced to a faulty e-bike or e-scooter battery that ignited and gutted two houses.And these fires are getting more frequent.As of Friday, the FDNY investigated 174 battery fires, putting 2022 on track to double the number of fires that occurred last year (104) and quadruple the number from 2020 (44). So far this year, six people have died in e-bike-related fires and 93 people were injured, up from four deaths and 79 injuries last year.In early August, a 27-year-old Venezuelan immigrant, identified as Rafael Elias Lopez-Centeno, died after his lithium ion battery caught fire and ripped through the Bronx apartment where he was staying. Carmen Tiburcio, a neighbor, said Lopez's aunt told her he had tried to escape through the front door, but the bike was in the way. Instead, he took refuge in the bathroom, where he tried to fill up the bathtub with water to protect himself from the flames. But the smoke got to him, she said.""He didn't make it,"" Tiburcio said. ""His lungs were very bad.""Another danger to delivery workMany, if not most, of the fires in New York involve e-bike batteries owned by restaurant delivery workers, who work long shifts, traveling dozens of miles a day.""The bikes tend to get beat up, subjected to the elements,"" Flynn said. ""They're not really made for our streets.""The longer the batteries are used, the more time it takes to fully recharge them, and it can take up to 8 hours. That in turn makes it harder for owners to keep on eye on their batteries the whole time they are plugged in, which is key for safety.Enlarge this image toggle caption FDNY FDNYIn addition, new batteries are costly, and the temptation to opt for a less-expensive refurbished battery for much less money is great — especially for couriers who make an average of $12.21 an hour after expenses, according to a survey by Los Deliveristas Union, an advocacy and membership organization.Several e-bike owners interviewed by NPR in New York City said they were aware of the risks batteries posed, and took measures to reduce them.""A lot of guys have four, five, six bikes in their apartment and they swap out chargers for different bikes when it doesn't belong to that bike,"" said Rafael Cardanales, who lives on the Lower East Side. ""You can't just use any charger, you know.""Musfiqur Rahman said that when he first got into the delivery business, he bought two new Arrow brand batteries — for $550 each. He did it specifically to avoid fires.""As far as I know, this brand never get involved in this kind of incident,"" the 27-year-old Bangladeshi immigrant said.The FDNY says most batteries are so destroyed by fire when they inspect them that they can make no conclusions about which brand is safer than another.The FDNY has begun posting videos on social media warning about the dangers of recharging lithium ion batteries.Living in close quartersE-bike related fires have occurred elsewhere, such as London, San Francisco, Michigan and South Florida. But nowhere does concern for them appear to be as high as in New York, perhaps because of the prevalence of apartment living — and also the prevalence of ordering take-out.While restaurants sometimes store bikes overnight for employees, fewer people are now working for particular restaurants and many more for themselves, using apps like Door Dash or Uber Eats to connect with customers. And these couriers often don't have any other place to store and recharge their e-bikes except in their apartments.That, in turn, creates a fire hazard not just for the workers, but also for their neighbors. This summer, the New York City Housing Authority proposed banning e-bikes and batteries from its 2,600 buildings. But the proposal created an uproar, and officials have not gone through with it.Enlarge this image toggle caption Matthew Schuerman/NPR Matthew Schuerman/NPRCity councilmembers have proposed their own solutions. One bill, for instance, would ban the sale of used batteries within city limits. Another would require all batteries to be sold to be approved by a national testing service, such as Underwriters Laboratories. Mayor Eric Adams recently announced he would direct $1 million to create hubs for delivery workers with charging stations and other amenities — though they would likely be used during the day and not provide overnight charging.Councilmember Gale Brewer, who sponsored the legislation that would outlaw the sale of used batteries, says she recognizes that new batteries could be prohibitively expensive to delivery workers.""They do, you know, God's work, so to speak, because New Yorkers like to have food delivered,"" she said. ""So now the question is how do they get the new batteries that are not going to cause fires?""",Fires from exploding e-bike batteries multiply in NYC â sometimes fatally
12,0,0_solar_energy_wind_power,https://apnews.com/article/science-health-california-cancer-climate-and-environment-83c87000f5c52692431218842378a089,"In this 2022 image provided by PSE Healthy Energy, a gas stove is tested for benzene in California. Stoves in California homes are leaking the cancer-causing gas benzene, researchers found in a new study published on Thursday, Oct. 20, though they say more research is needed to understand how many homes have leaks. (PSE Healthy Energy via AP)In this 2022 image provided by PSE Healthy Energy, a gas stove is tested for benzene in California. Stoves in California homes are leaking the cancer-causing gas benzene, researchers found in a new study published on Thursday, Oct. 20, though they say more research is needed to understand how many homes have leaks. (PSE Healthy Energy via AP)Gas stoves in California homes are leaking cancer-causing benzene, researchers found in a new study published on Thursday, though they say more research is needed to understand how many homes have leaks.In the study, published in Environmental Science and Technology on Thursday, researchers also estimated that over 4 tons of benzene per year are being leaked into the atmosphere from outdoor pipes that deliver the gas to buildings around California — the equivalent to the benzene emissions from nearly 60,000 vehicles. And those emissions are unaccounted for by the state.The researchers collected samples of gas from 159 homes in different regions of California and measured to see what types of gases were being emitted into homes when stoves were off. They found that all of the samples they tested had hazardous air pollutants, like benzene, toluene, ethylbenzene and xylene (BTEX), all of which can have adverse health effects in humans with chronic exposure or acute exposure in larger amounts.ADVERTISEMENTOf most concern to the researchers was benzene, a known carcinogen that can lead to leukemia and other cancers and blood disorders, according to the National Cancer Institute .The finding could have major implications for indoor and outdoor air quality in California, which has the second highest level of residential natural gas use in the United States.“What our science shows is that people in California are exposed to potentially hazardous levels of benzene from the gas that is piped into their homes,” said Drew Michanowicz, a study co-author and senior scientist at PSE Healthy Energy, an energy research and policy institute. “We hope that policymakers will consider this data when they are making policy to ensure current and future policies are health-protective in light of this new research.”Homes in the Greater Los Angeles, the North San Fernando Valley, and the San Clarita Valley areas had the highest benzene in gas levels. Leaks from stoves in these regions could emit enough benzene to significantly exceed the limit determined to be safe by the California Office of Environmental Health Hazards Assessment.This finding in particular didn’t surprise residents and health care workers in the region who spoke to The Associated Press about the study. That’s because many of them experienced the largest-known natural gas leak in the nation in Aliso Canyon in 2015.ADVERTISEMENTBack then, 100,000 tons of methane and other gases, including benzene , leaked from a failed well operated by Southern California Gas Co. It took nearly four months to get the leak under control and resulted in headaches, nausea and nose bleeds.Dr. Jeffrey Nordella was a physician at an urgent care in the region during this time and remembers being puzzled by the variety of symptoms patients were experiencing. “I didn’t have much to offer them,” except to help them try to detox from the exposures, he said.That was an acute exposure of a large amount of benzene, which is different from chronic exposure to smaller amounts, but “remember what the World Health Organization said : there’s no safe level of benzene,” he said.ADVERTISEMENTKyoko Hibino was one of the residents exposed to toxic air pollution as a result of the Aliso Canyon gas leak. After the leak, she started having a persistent cough and nosebleeds and eventually was diagnosed with breast cancer, which has also been linked to benzene exposure . Her cats also started having nosebleeds and one recently passed away from leukemia.“I’d say let’s take this study really seriously and understand how bad (benzene exposure) is,” she said.___Follow Drew Costley on Twitter: @drewcostley .___The Associated Press Health and Science Department receives support from the Howard Hughes Medical Institute’s Department of Science Education. The AP is solely responsible for all content.","Study: Cancer-causing gas leaking from CA stoves, pipes"
21,0,0_solar_energy_wind_power,https://arstechnica.com/science/2022/09/the-pathway-to-90-clean-electricity-is-mostly-clear-the-last-10-not-so-much/,"The United States gets about 40 percent of its electricity from carbon-free sources, including renewables and nuclear, and researchers have a pretty good idea of how to cost-effectively get to about 90 percent.But that last 10 percent? It gets expensive, and there is little agreement about how to do it.A new paper in the journal Joule identifies six approaches for achieving that last 10 percent, including a reliance on wind and solar, a build-out of nuclear power, and development of long-term energy storage using hydrogen.This isn’t a matter of one pathway winning out over the others, said Trieu Mai, the paper’s lead author and senior energy researcher for the National Renewable Energy Laboratory in Golden, Colorado.“A 100 percent carbon-free power system will require a portfolio of resources,” he said. “But humility is needed to accept that we don’t know what the optimal mix to solving the last 10 percent” is going to be.The larger point, he said, is that researchers and industry need to be doing the work now to figure out which technologies are the most viable in order to meet the goal, set by the Biden administration, to get to net-zero emissions in the electricity sector by 2035.Here are the six options from the paper, along with what I see as their pluses and minuses:","The pathway to 90% clean electricity is mostly clear. The last 10%, not so much"
280,0,0_solar_energy_wind_power,https://techxplore.com/news/2022-10-retrofits-diesel-hydrogen.html,"The hydrogen-diesel direct injection dual-fuel system developed at UNSW enables a traditional diesel engine to be retrofitted to run as a hydrogen-diesel hybrid engine. Credit: Prof. Shawn KookEngineers from UNSW Sydney have successfully converted a diesel engine to run as a hydrogen-diesel hybrid engine—reducing CO 2 emissions by more than 85% in the process.The team, led by Professor Shawn Kook from the School of Mechanical and Manufacturing Engineering, spent around 18 months developing the hydrogen-diesel direct injection dual-fuel system that means existing diesel engines can run using 90% hydrogen as fuel.The researchers say that any diesel engine used in trucks and power equipment in the transportation, agriculture and mining industries could ultimately be retrofitted to the new hybrid system in just a couple of months.Green hydrogen, which is produced using clean renewable energy sources such as wind and solar, is much more environmentally friendly than diesel.And in a paper published in the International Journal of Hydrogen Energy, Prof. Kook's team show that using their patented hydrogen injection system reduces CO 2 emissions to just 90 g/kWh—85.9% below the amount produced by the diesel powered engine.""This new technology significantly reduces CO 2 emissions from existing diesel engines, so it could play a big part in making our carbon footprint much smaller, especially in Australia with all our mining, agriculture and other heavy industries where diesel engines are widely used,"" says Prof. Kook.""We have shown that we can take those existing diesel engines and convert them into cleaner engines that burn hydrogen fuel.""Being able to retrofit diesel engines that are already out there is much quicker than waiting for the development of completely new fuel cell systems that might not be commercially available at a larger scale for at least a decade.""With the problem of carbon emissions and climate change, we need some more immediate solutions to deal with the issue of these many diesel engines currently in use.""High-pressure hydrogen direct injectionThe UNSW team's solution to the problem maintains the original diesel injection into the engine, but adds a hydrogen fuel injection directly into the cylinder.The collaborative research, performed with Dr. Shaun Chan and Professor Evatt Hawkes, found that specifically timed hydrogen direct injection controls the mixture condition inside the cylinder of the engine, which resolves harmful nitrogen oxide emissions that have been a major hurdle for commercialisation of hydrogen engines.The Hydrogen-Diesel Direct Injection Dual-Fuel System has been developed by a team from the UNSW Engine Research Laboratory led by Professor Shawn Kook (right), and including Xinyu Liu (back left) and Jinxin Yang (front left). Credit: Prof. Shawn Kook""If you just put hydrogen into the engine and let it all mix together you will get a lot of nitrogen oxide (NO x ) emissions, which is a significant cause of air pollution and acid rain,"" Prof. Kook says.""But we have shown in our system if you make it stratified—that is in some areas there is more hydrogen and in others there is less hydrogen—then we can reduce the NO x emissions below that of a purely diesel engine.""Importantly, the new Hydrogen-Diesel Direct Injection Dual-Fuel System does not require extremely high purity hydrogen which must be used in alternative hydrogen fuel cell systems and is more expensive to produce.And compared to existing diesel engines, an efficiency improvement of more than 26% has been shown in the diesel-hydrogen hybrid.That improved efficiency is achieved by independent control of hydrogen direct injection timing, as well as diesel injection timing, enabling full control of combustion modes—premixed or mixing-controlled hydrogen combustion.The research team hope to be able to commercialize the new system in the next 12 to 24 months and are keen to consult with prospective investors.They say the most immediate potential use for the new technology is in industrial locations where permanent hydrogen fuel supply lines are already in place.That includes mining sites, where studies have shown that about 30% of greenhouse-gas emissions are caused by the use of diesel engines, largely in mining vehicles and power generators.And the Australian market for diesel-only power generators is currently estimated to be worth around $765 million.""At mining sites, where hydrogen is piped in, we can convert the existing diesel engines that are used to generate power,"" says Prof. Kook.""In terms of applications where the hydrogen fuel would need to be stored and moved around, for example in a truck engine that currently runs purely on diesel, then we would also need to implement a hydrogen storage system to be integrated into our injection system.""I do think the general technology with regards to mobile hydrogen storage needs to be developed further because at the moment that is quite a challenge.""More information: Xinyu Liu et al, Direct injection of hydrogen main fuel and diesel pilot fuel in a retrofitted single-cylinder compression ignition engine, International Journal of Hydrogen Energy (2022). Journal information: International Journal of Hydrogen Energy Xinyu Liu et al, Direct injection of hydrogen main fuel and diesel pilot fuel in a retrofitted single-cylinder compression ignition engine,(2022). DOI: 10.1016/j.ijhydene.2022.08.149",New system retrofits diesel engines to run on 90% hydrogen
13,0,0_solar_energy_wind_power,https://apnews.com/article/technology-electric-vehicles-climate-and-environment-92ab74779ba5589708e8211cc0df4663,"A 2024 Chevrolet Equinox EV 3LT is shown in Warren, Mich., Aug. 30, 2022. General Motors is preparing to roll out a $30,000 Chevy Equinox electric vehicle in the most popular part of the U.S. auto market. (AP Photo/Paul Sancya)A 2024 Chevrolet Equinox EV 3LT is shown in Warren, Mich., Aug. 30, 2022. General Motors is preparing to roll out a $30,000 Chevy Equinox electric vehicle in the most popular part of the U.S. auto market. (AP Photo/Paul Sancya)WARREN, Mich. (AP) — Even though battery costs are rising, auto companies are rolling out more affordable electric vehicles that should widen their appeal to a larger group of buyers.The latest came Thursday from General Motors, a Chevrolet Equinox small SUV with a starting price somewhere around $30,000 and a range-per-charge of 250 miles (400 kilometers). You can get range of 300 miles (500 kilometers) if you pay more.GM won’t release the exact price of the Equinox EV until closer to the date it goes on sale, about this time next year. But the SUV is at the low end of Edmunds.com’s list of prices for electric vehicles sold in the U.S., where the average cost of an EV is around $65,000.Hitting a price around $30,000 and a range per charge close to 300 miles is key to getting mainstream buyers to switch away from gasoline vehicles, industry analysts say.“You’re kind of at that sweet spot,” said Ivan Drury, director of insights for Edmunds.com. “You’re basically at the price point that everyone is clamoring for.”ADVERTISEMENTAuto industry analysts say that if the Equinox makes efficient use of interior space with plenty of cargo and passenger room, and if it is styled similar to current gas-powered small SUVs, it should be a hit in the most popular segment of the U.S. auto market. About 20% of all new vehicles sold in the U.S. are compact SUVs.“It’s a perfect vehicle for a lot of different users, whether it’s a small family, maybe an empty nester,” said Jeff Schuster, president of global forecasting for LMC Automotive, a Detroit-area consulting firm. “You’ve got space to haul things, but it’s easy to drive.”A $30,000 EV that checks all of the boxes is just a little above the price of a comparable small gas-powered SUV. The Toyota RAV4, the top seller in the segment and the top-selling vehicle in the U.S. that isn’t a pickup, starts at just over $28,000.Until the last few years, electric vehicles were either expensive and aimed at affluent luxury buyers, or cheaper but with limited travel ranges. For example, a base version of Tesla’s Model 3, the lowest-price model from the top-selling EV brand in the U.S., starts at more than $48,000. A larger Tesla Model X SUV starts at over $120,000.ADVERTISEMENTThe only EVs with starting prices under $30,000 (including shipping) now are versions of the Nissan Leaf and Chevrolet Bolt. Both are smaller than a typical gas-powered compact SUV. The Mini Cooper Electric, Mazda MX30 and Hyundai Kona Electric are in the $30,000s, according to Edmunds.Kia’s Niro EV, Hyundai’s Ioniq 5, Ford’s F-150 Lightning pickup, the Volkswagen ID.4, Kia EV6, Toyota b24x, Ford’s Mustang Mach E, Audi’s Q4 e-tron, the Subaru Solterra, Polestar 2, and Tesla Model 3 all have starting prices in the $40,000s.GM may find it difficult to keep the Equinox price around $30,000, largely because minerals such as lithium, copper, cobalt and nickel that are key components of batteries have been rising fast. There’s a finite number of mines and increasing demand as nearly all automakers introduce new EVs.ADVERTISEMENTDrury says that even if GM is able to keep the Equinox starting price around $30,000, demand likely will be high enough so the company builds mainly higher-priced versions. And some dealers have been marking up EVs beyond the automaker’s sticker price due to high demand. In the first half of the year, U.S. EV sales rose 68% from the same period a year ago, to nearly 313,000.Some EVs could get a whole lot cheaper in the U.S., too, with federal tax credits starting next year of up to $7,500 that are part of the Inflation Reduction Act . But meeting federal requirements may be difficult.The vehicles and batteries have to be assembled in North America, and the new law phases in requirements that battery minerals and parts have to come from the continent. Most minerals such as lithium, a key battery ingredient, are now imported from China and other countries.The Equinox checks the North American assembly box. It will be made in Mexico. The company won’t say where the battery will be made, but GM has announced three joint-venture battery factories in the U.S., including one that’s operating already in Warren, Ohio.ADVERTISEMENTFrom there, GM is working on meeting the other criteria for getting the tax credit. “We’re really working through the rules and the regulations right now,” said Steve Majoros, vice president of marketing for Chevrolet. “We think it’s all lining up nicely, but more details to come on that.”Majoros hinted that it may take a couple of years to meet all the government’s requirements to get the full credit as GM takes more control of its supply chain for EV parts.The Equinox EV, Majoros said, is longer, wider and a bit shorter than the gas versions of the same vehicle. GM used new interior packaging methods to create comparable passenger and cargo space to the gas Equinox, he said. The relatively small price difference between the two should get many customers to consider EV over gas, he said.“It does a lot of things right,” Majoros said. “So when it does that, the (sales) volume follows.”ADVERTISEMENTGM CEO Mary Barra has said the company will overtake Tesla as the nation’s largest seller of EVs by the middle of this decade. The Equinox EV is a step toward that.“We think it’s going to be one of the products that’s really going to help that mainstream adoption really take off in the marketplace,” Majoros said.",Cheaper electric vehicles coming despite high battery costs
278,0,0_solar_energy_wind_power,https://techxplore.com/news/2022-10-australia-intercontinental-power-grid.html,"Australia's Prime Minister Anthony Albanese (R) shakes hands with Singapore's Prime Minister Lee Hsien Loong during their meeting at Parliament House in Canberra on October 18, 2022.Australia touted a world-first project Tuesday that could help make the country a ""renewable energy superpower"" by shifting huge volumes of solar electricity under the sea to Singapore.Singapore Prime Minister Lee Hsien Loong met Australian counterpart Anthony Albanese in Canberra to ink a new green energy deal between the two countries.Albanese said the pact showed a ""collective resolve"" to slash greenhouse gas emissions through an ambitious energy project.He name-checked clean energy start-up Sun Cable, which wants to build a high-voltage transmission line capable of shifting huge volumes of solar power from the deserts of northern Australia to tropical Singapore.Sun Cable has said that, if successful, it would be the world's first intercontinental power grid.""If this project can be made to work—and I believe it can be—you will see the world's largest solar farm,"" Albanese told reporters.""The prospect of Sun Cable is just one part of what I talk about when I say Australia can be a renewable energy superpower for the world.""Lee said the green economy deal was the ""first such agreement of its kind"".""We hope that it will be a pathfinder for other countries simply to co-operate with one another to deal with what is a global problem.""Australia is one of the world's largest coal and gas exporters and has been frequently criticised on the global stage for its failure to make meaningful reductions in carbon emissions.Coal still plays a key role in domestic electricity production.© 2022 AFP",Australia backs plan for intercontinental power grid
277,0,0_solar_energy_wind_power,https://techxplore.com/news/2022-09-communication-less-scheme-microgrid-setup-recovery.html?utm_source=nwletter&utm_medium=email&utm_campaign=daily-nwletter,"NREL's communication-less microgrid method allows grid frequency to vary across a wider range than normal. Devices watch the frequency and adjust their power output according to the frequency's changes. Credit: NRELDuring a power outage or after a disaster, it is hard to beat the simplicity of a diesel generator. Just supply fuel and start it up—so easy, anyone could do it. Renewable microgrids, on the other hand, are not so simple, with their suite of controls, software, and asset coordination. But the beauty of renewables is that fuel is free and already available on-site, even in remote disaster areas.The National Renewable Energy Laboratory (NREL) has now published a description of the improvised controls that saved NREL during its own outage, which could make microgrids easy and low cost where they are needed most.The publication, titled ""Unleashing the Frequency: Multi-Megawatt Demonstration of 100% Renewable Power Systems with Decentralized Communication-less Control Scheme,"" describes a microgrid approach that sidesteps the central controller—an expensive and complicated component—and its reliance on communications, instead using native controls of battery, solar, and wind systems.""NREL's approach makes it possible to assemble devices into a microgrid without arduous configuration, relying on just renewable energy and amateur electrical experience—perfect for recoveries in a pinch,"" said Przemyslaw Koralewicz, NREL engineer and co-developer of the communication-less method.A low-cost recovery resourceWhen NREL experienced a surprise power outage, the laboratory had few options for recovery: No microgrid controller and no preconfigured setup. Just a large battery, solar panels, and wind turbines. Other campuses—or districts, neighborhoods, and homes—could find themselves in similar circumstances, and during an outage is no time to be fumbling with complicated configurations. Like NREL, communities can now implement a resilient microgrid off the cuff, using controls that exist on essentially any energy resource.While microgrids are an apparent answer for recovery and resilience, the costs of a controller present a barrier to communities. In 2019, NREL found that microgrid controllers have a mean cost of $155,000/megawatt, potentially putting resilient microgrids out of reach for vulnerable areas.Besides costs, controllers introduce a tangle of communications and system settings, often opaque, proprietary, and designed to suit particular scenarios. These features can be useful for minimizing energy use and costs, but recoveries often call for a quick-and-ready option. NREL's method prioritizes fail-safe startup, foregoing elaborate programs and communications in favor of exceedingly basic controls, while still allowing more advanced designs to be built on top.Developers of the communication-less microgrid method observe the NREL Flatirons Campus' battery energy storage systems. Credit: Dennis Schroeder, NRELHow does it work?NREL's scheme is decentralized—the devices do not exchange data or issue commands (i.e., they are ""communication-less""). Instead, devices self-regulate using system frequency as the common language. In short, a battery or other power source forms the grid by supplying power at a set frequency. Other generators like solar panels and wind turbines follow the grid by watching frequency and changing their power accordingly.The method is nothing too new—so-called ""droop"" controls are familiar in standard fossil fuel generators—which is part of the appeal. The NREL researchers showed that the method works with 100% renewable energy, can be scaled, and is feasible with most any energy device.What is innovative is that NREL's method frees the grid frequency from a tight 60 hertz (Hz). Unbound by mechanical rotation, the microgrid frequency can take a wider range. In fact, that range is precisely how the devices coordinate without communicating: As frequency rises past 60 Hz, generators reduce power. At even higher frequencies, the generators reduce their power further, rebalancing the frequency around 60 Hz. The system self-stabilizes, never overcharging the batteries or underserving the loads.If this sounds more difficult than a diesel generator, it might be. It still requires some device programming and parameter setting, which are detailed in the report.""NREL's method is the very first step in a design that could become the standard for fail-safe microgrids,"" Koralewicz said. ""Our communication-less method could be natively configured in future devices or possibly certified for easy access by operators. With standardized adoption, microgrids of any type—military bases, hospital backups, even networked districts—could count on an unfailing foundation to their day-to-day operations.""The frontier of renewable energy systemsAt the frontier of power systems, engineers are addressing the technical matters of operating the grid with mostly renewables. Outstanding questions relate to inverters, which are the power electronic devices that interface renewable energy to the grid, and particularly, how inverters can form the grid in ways that fossil-based resources traditionally have. The UNIFI consortium is taking on the inverter challenge with the combined effort of dozens of research institutions, led by NREL.NREL's communication-less method is one example of a grid-forming strategy, the sort that will be needed as systems push toward higher levels of renewables. In proposing the method, NREL is breaking ground into some of the trickier topics that confront UNIFI and power systems everywhere, such as how to handle grid protection and which controls should be essential on grid-forming devices. The authors approach these questions in the report, offering one direction that future energy systems can take on the path to decarbonization.","Communication-less scheme streamlines microgrid setup, simplifies recovery"
112,0,0_solar_energy_wind_power,https://pv-magazine-usa.com/2022/10/24/chinas-solar-cell-production-capacity-may-reach-600-gw-by-year-end/,"From pv magazine globalChina’s total annual solar cell and module production capacity may increase from 361 GW at the end of last year to up to 600 GW at the end of 2022, according to the Asia Europe Clean Energy (Solar) Advisory (AECEA).“Since January, 20 companies disclosed to expand module production totaling 380 GW, planned to be executed within the next few months or up to 1,5 years,” the analyst firm said, noting that most of this capacity relates to n-type modules produced with tunnel oxide passivated contacts (TOPCon) solar cells or panels based on cells with a heterojunction (HJT) design. “Reportedly, TOPCon related expansion plans exceed 220 GW, whereas HJT is nearing the 150 GW mark. As an example, recently one HJT company conducted an online pitching and according to them almost 800 people joined that call,” it added.So far this year, the output of polysilicon, wafers, cells and modules has already beaten the achievements of the Chinese PV industry in 2021 by some 50%. “By June, module shipments of the TOP 10 manufacturer crossed the 100 GW and by the end of September may have reached 140-150 GW (2021: 133 GW),” said the AECEA. “Just five of these top 10 have set shipment targets of between 183-205 GW.”Furthermore, the AECEA revealed that the country’s polysilicon capacity should grow from around 530,000 MT at the end of 2021 to up to 1.2 million MT in 2022, jumping to 2.5 million MT in 2023, and up to 4 million MT in 2024.“In the near term, the overall industrial landscape won’t fundamentally change. Incumbent companies are further consolidating their market positions through backward/forward integration,” the AECEA stated. “By and large, vertical integration remains their favored business model, which in times of external supply constraints or external supply dependencies has gained ever more weight.”",Chinaâs solar cell production capacity may reach 600 GW by year-end
14,0,0_solar_energy_wind_power,https://apnews.com/article/technology-germany-europe-berlin-climate-and-environment-75afb2820d22c3d53b136b2ab2841767,"FILE -- An electric car is charged at a charging station during a press tour of the plant of the German manufacturer Volkswagen AG (VW) in Zwickau, Germany, Tuesday, May 14, 2019. Germany’s government wants to massively expand the country’s charging network for electric cars with 6.3 billion euros ($6.17 billion) over the next three years as it expects more and more drivers to turn from combustion cars to more climate-friendly electromobility. (AP Photo/Jens Meyer, file)FILE -- An electric car is charged at a charging station during a press tour of the plant of the German manufacturer Volkswagen AG (VW) in Zwickau, Germany, Tuesday, May 14, 2019. Germany’s government wants to massively expand the country’s charging network for electric cars with 6.3 billion euros ($6.17 billion) over the next three years as it expects more and more drivers to turn from combustion cars to more climate-friendly electromobility. (AP Photo/Jens Meyer, file)BERLIN (AP) — Germany wants to massively expand the country’s charging network for electric cars, spending 6.3 billion euros ($6.17 billion) over the next three years as it expects more and more drivers to turn away from combustion cars to more climate-friendly vehicles.The country’s transportation minister on Wednesday presented a “master plan” for improving the charging infrastructure that had been passed by Chancellor Olaf Scholz’ cabinet earlier in the day.“We are not just any automotive location, but a leading one in the world. And that’s why it’s important to us that what we’re preparing succeeds well,” Volker Wissing told reporters in Berlin. “We need a forward-looking expansion of the nationwide charging infrastructure that meets demand and is user-friendly.”The share of electric vehicles in Germany grew 24.8% year-on-year to a total share of 14.6% of all newly registered automobiles, according to figures released by the country’s Federal Office for Motor Vehicles.ADVERTISEMENTThere are around 70,000 charging points in the country but only 11,000 of those are fast-chargers, the ministry said.That is not enough to sufficiently fulfill the current needs, and it will be even less so as the number of electric cars grows quickly. There is also a big difference in availability of charging points between big cities and rural areas, where it is even harder to find charging stations.The German government’s goal is to have 1 million publicly accessible charging points in the country by 2030.In order to boost the number of charging points, the federal government will, among other initiatives provide real estate, especially along highways, where new charging stations can be built. Private owners of electric cars will be offered subsidized plans to install solar energy panels at their homes to charge their cars overnight.Electric charging is also supposed to get more user-friendly with new digital offers showing drivers where they can charge their cars on the road or being able to check online how much the different charging points demand, the minister said.Another issue the government wants to tackle is getting the country’s electric grid ready for the increased demand as more people turn to electric cars.ADVERTISEMENT“We are expecting an exponential increase in registered vehicles with battery electric drive in the next few years and must prepare accordingly,” the minister said.Switching Germans from combustion-engine automobiles to electric cars plays a key role in achieving the government’s climate targets set for the transport sector.The transformation to electric cars has also been boosted by a mix of regulatory pressure, tax breaks, improving battery range, and a wider range of vehicles to purchase.Europe in general is leading the push into battery-powered cars as electric vehicles enter the mainstream and has promised to phase out internal combustion cars by 2035.But availability of charging points is a problem not just in Germany but almost everywhere across the continent.ADVERTISEMENT“Not only is there an insufficient number of electric charging points along the road networks in most European Union countries, but the vast majority of these do not charge quickly enough,” the European Automobile Manufacturers’ Association said.Or as the German minister said: “Electric mobility will only find acceptance if charging is as easy as refueling is today.”___Follow all AP stories on climate change issues at https://apnews.com/hub/climate-and-environment.",Germany to massively expand electric car charging network
229,0,0_solar_energy_wind_power,https://techcrunch.com/2022/08/05/online-only-home-solar-seller-bags-23m-pledging-dramatically-lower-prices/,"Project Solar doesn’t make solar panels, nor does it employ crews to plop them on roofs, but the startup argues it can shake up the residential solar business by steering clear of sales reps and automating parts of the ordering, design and installation process.Backed by $23 million in fresh Series A funding led by Left Lane Capital, Project Solar says it is on track to install 30 megawatts of solar this year, mostly in California and Texas, while apparently undercutting some of its competitors on price. The startup aims to top its 2022 installation figure by a factor of five in 2023.But first, some context: Tesla bailed on door knocking ages ago, only to see its marketshare slump. Sunrun, meanwhile, has more than 100 sales jobs listed on its site and is the top residential solar brand by marketshare. So why would cutting out salespeople work for Project Solar, which is currently nowhere near as active as the industry leader?For homeowners, the sell mostly comes down to price. The startup says it charges $2.20 per watt on average for installations, and as low as $1.63 after federal incentives. That’s about 25% cheaper than the 2021 national average of $2.94 per watt (before tax credits), according to the Solar Energy Industries Association. Whether it qualifies as “dramatically lower,” as the startup characterizes it, is more a matter of opinion. For folks who are especially handy, Project Solar also offers cheaper DIY options.Project Solar’s revenue comes from marking up equipment prices, for which it gets volume discounts. The company says cutting out salespeople saves it as much as a buck per watt, and from there it leans on its in-house software to trim the time needed for things like system design, permitting and coordinating contractors. For each sale, “there are about 40 … hours of work involved in the back-end process (not including the physical labor of install, which usually takes a crew of 3-4 people eight hours),” CEO Trevor Hiltbrand told TechCrunch. He added that the firm’s software includes “a tool that has reduced engineering/CAD time from three hours to 30 minutes.”Project Solar plans to use the Series A to expand in the Midwest and South, and to continue working on its software. Beyond Left Lane Capital, Project Solar declined to share who chipped in on the funding round, calling them “strategics within the industry.”","Online-only home solar seller bags $23M, pledging âdramatically lower pricesâ"
584,0,0_solar_energy_wind_power,https://techcrunch.com/2022/05/10/sencrop-predicts-weather-conditions-at-a-microclimate-level-for-farmers/,"French startupFrench startupWeather forecasting is a complicated industry as it requires a ton of data to create an accurate forecasting model. That’s why Sencrop is drawing inspiration from Netatmo or Waze and betting on crowdsourcing to improve its service.Weather forecasting is a complicated industry as it requires a ton of data to create an accurate forecasting model. That’s why Sencrop is drawing inspiration from Netatmo or Waze and betting on crowdsourcing to improve its service.Sencrop customers can buy their own connected stations to measure temperature, humidity, rainfall, wind speed and sunlight. These weather stations all contribute to Sencrop’s real-time data.Sencrop customers can buy their own connected stations to measure temperature, humidity, rainfall, wind speed and sunlight. These weather stations all contribute to Sencrop’s real-time data.And it’s been working well as the company has already sold over 20,000 weather stations. In France, there’s one weather station per 20 square kilometers on average. The company operates in 20 countries and has office in France, the Netherlands, the U.K., Germany, Spain and Italy.And it’s been working well as the company has already sold over 20,000 weather stations. In France, there’s one weather station per 20 square kilometers on average. The company operates in 20 countries and has office in France, the Netherlands, the U.K., Germany, Spain and Italy.Customers can see current weather conditions and export historical data. But they can also move around the map, set up alerts and select between multiple forecasting models to prepare for the next few days and anticipate agronomic risks. Sencrop can help you when it comes to water stress and irrigation management as well. There are multipleCustomers can see current weather conditions and export historical data. But they can also move around the map, set up alerts and select between multiple forecasting models to prepare for the next few days and anticipate agronomic risks. Sencrop can help you when it comes to water stress and irrigation management as well. There are multipleSencrop also provides multiple integrations with Decision Support Tools, such as Rimpro, VitiMeteo, fruitweb, 360 viti and Idroplan. This metric-driven way of growing fruits, cereals or vines should lead to better productivity and an improvement on the bottom line.Sencrop also provides multiple integrations with Decision Support Tools, such as Rimpro, VitiMeteo, fruitweb, 360 viti and Idroplan. This metric-driven way of growing fruits, cereals or vines should lead to better productivity and an improvement on the bottom line.In addition to JVP, other investors in today’s round include EIT Food, Stellar Impact, IRD Management and some of the company’s existing shareholders, such as Bpifrance, Demeter IM and NCI Waterstart.In addition to JVP, other investors in today’s round include EIT Food, Stellar Impact, IRD Management and some of the company’s existing shareholders, such as Bpifrance, Demeter IM and NCI Waterstart.“Sencrop’s mission is to democratize precision farming and reduce crop risks for farmers,” Sencrop co-founder and General Manager Martin Ducroquet said in a statement. “We have developed a unique microclimate technology which today allows more than 20,000 professionals — farmers, winegrowers, fruit growers, etc. — to access ultra-precise and ultra-local information for better daily monitoring of their crops and the risks on their plots.”“Sencrop’s mission is to democratize precision farming and reduce crop risks for farmers,” Sencrop co-founder and General Manager Martin Ducroquet said in a statement. “We have developed a unique microclimate technology which today allows more than 20,000 professionals — farmers, winegrowers, fruit growers, etc. — to access ultra-precise and ultra-local information for better daily monitoring of their crops and the risks on their plots.”Up next, the company wants to accelerate its international expansion, starting with North America.Up next, the company wants to accelerate its international expansion, starting with North America.",Sencrop predicts weather conditions at a microclimate level for farmers
579,0,0_solar_energy_wind_power,https://cosmosmagazine.com/technology/flow-battery-china/,"More on:More on:The world’s largest flow battery has opened, using a newer technology to store power.The world’s largest flow battery has opened, using a newer technology to store power.The Dalian Flow Battery Energy Storage Peak-shaving Power Station, in Dalian in northeast China, has just been connected to the grid, and will be operating by mid-October.The Dalian Flow Battery Energy Storage Peak-shaving Power Station, in Dalian in northeast China, has just been connected to the grid, and will be operating by mid-October.The vanadium flow battery currently has a capacity of 100 MW/400 MWh, which will eventually be expanded to 200 MW/800 MWh.The vanadium flow battery currently has a capacity of 100 MW/400 MWh, which will eventually be expanded to 200 MW/800 MWh.According to the Chinese Academy of Sciences, who helped develop the project, it can supply enough electricity to meet the daily demands of 200,000 residents.According to the Chinese Academy of Sciences, who helped develop the project, it can supply enough electricity to meet the daily demands of 200,000 residents.It will be used to smooth peaks and troughs in Dalian’s electricity demand and supply, making it easier to use solar and wind power.It will be used to smooth peaks and troughs in Dalian’s electricity demand and supply, making it easier to use solar and wind power.Battery power module. Credit: DICPBattery power module. Credit: DICPFlow batteries are a newer type of battery technology that operate by combining tanks of liquid electrolytes, rather than using static electrodes.Flow batteries are a newer type of battery technology that operate by combining tanks of liquid electrolytes, rather than using static electrodes.They use cheaper and more sustainable materials than lithium-ion batteries, and are longer-lasting: theoretically, vanadium flow batteries could charge andThey use cheaper and more sustainable materials than lithium-ion batteries, and are longer-lasting: theoretically, vanadium flow batteries could charge andThey’re also flame-resistant and thereforeThey’re also flame-resistant and thereforeThe tanks containing electrolyte for the flow battery. Credit: DICPThe tanks containing electrolyte for the flow battery. Credit: DICPFlow batteries aren’t as energy dense as lithium-ion batteries, however, meaning they’re unlikely to be viable alternatives inFlow batteries aren’t as energy dense as lithium-ion batteries, however, meaning they’re unlikely to be viable alternatives inInstead, most interest in flow batteries is on static applications, like the big grid-scale battery in Dalian.Instead, most interest in flow batteries is on static applications, like the big grid-scale battery in Dalian.Are you interested in the energy industry and the technology and scientific developments that power it? Then our new email newsletterAre you interested in the energy industry and the technology and scientific developments that power it? Then our new email newsletterAustralia’s first and largest flow battery is aAustralia’s first and largest flow battery is aAustralia’s biggest operating lithium-ion battery, by comparison, is currently the 300 MW/450 MWhAustralia’s biggest operating lithium-ion battery, by comparison, is currently the 300 MW/450 MWhHow does a flow battery differ from other batteries?How does a flow battery differ from other batteries?Start of tracking content syndication. Please do not remove this section as it allows us to keep track of republished articlesEnd of tracking content syndication",Worldâs biggest flow battery opens in China
369,0,0_solar_energy_wind_power,https://www.euronews.com/green/2022/10/11/major-milestone-for-greek-energy-as-renewables-power-100-of-electricity-demand,"Renewable energy met all of Greece’s electricity needs for the first time ever last week, the country’s independent power transmission operator IPTO announced.For at least five hours on Friday, renewables accounted for 100 per cent of Greece’s power generation, reaching a record high of 3,106 megawatt hours.Solar, wind and hydro represented 46 per cent of the nation’s power mix in the eight months to August this year, up from 42 per cent in the same period in 2021, according to Greece-based environmental think-tank The Green Tank.Green Tank called it, a ""record of optimism for the country's transition to clean energy, weaning off fossil fuels and ensuring our energy sufficiency.”""European countries like Greece are rapidly accelerating away from fossil fuels and towards cheap renewable electricity. The milestone reached by Greece proves that a renewables-dominated electricity grid is within sight,” Elisabeth Cremona, an analyst at energy think tank Ember, told Euronews Green.“This also clearly demonstrates that the electricity system can be powered by renewables without compromising reliability. But there remains more to do to ensure that renewables overtake fossil fuels in Greece's power sector across the whole year.""What’s the big picture for Greece’s energy transition?It’s a significant milestone in the history of the country's electricity system, and follows the bright news that renewables fully met the rise in global electricity demand in the first half of 2022.But Greece’s transition to clean energy hasn’t been entirely straightforward.Like other European countries, Greece has cut its reliance on Russian gas following the war in Ukraine by increasing liquefied natural gas (LNG) imports to meet its needs. It has also boosted coal mining, pushing back its decarbonisation plan.Using IPTO data, The Green Tank finds that renewables - excluding large hydro sources - surpassed all other energy sources, leaving fossil gas in second place as it decreased slightly for the first time since 2018.Greece aims to more than double its green energy capacity to account for at least 70 per cent of its energy mix by 2030. To help hit that target, the government is seeking to attract around €30 billion in European funds and private investments to upgrade its electricity grid.It plans to have 25 gigawatt of installed renewable energy capacity from about 10 gigawatt now but analysts say Athens might reach that target sooner.IPTO has been investing in expanding the country's power grid to boost power capacity and facilitate the penetration of solar, wind and hydro energy.",Major milestone for Greek energy as renewables power 100% of electricity demand
370,0,0_solar_energy_wind_power,https://www.euronews.com/green/2022/10/13/scientists-dream-up-a-massive-floating-solar-farm-in-space-heres-how-it-would-work,"It sounds like the stuff of science fiction - but Europe might one day be powered by giant floating solar panels orbiting the planet.The European Space Agency (ESA) has unveiled a plan to harvest the sun’s energy in space and beam it back down to Earth.The technology is still in the preliminary testing phase - but the end goal is the construction of a 2km long solar space farm, generating as much energy as a nuclear power plant.The farm would orbit an eye-watering 36,000 km above the Earth.“[Such a project] would ensure that Europe becomes a key player– and potentially leader – in the international race towards scalable clean energy solutions for mitigating climate change,” the ESA said in a statement.How will the space farm technology work?Solar power is one of the best sources of clean energy, but it’s currently held back by a few limitations. Panels can only harness power in the daytime, and even then, much of the sunlight is absorbed by the atmosphere on its journey to the ground.In Space, the sun’s beams are around ten times as intense as they are on Earth.The ESA have partnered with Airbus - a European multinational aerospace corporation - to develop ‘wireless power transmission’ to capture this 24-hour source of electricity and beam it down to us.The sun's rays are 10 times as strong in space, where they are not diluted in the atmosphere. ClearedThe technology is based on the transmission used by TV and communication satellites every day, Airbus engineer Nicolas Schneider explains.""We are not very far from a 4G antenna, except that what we want is not to radiate in all directions, we want to be very precise like a laser, in fact,” he says.“It’s a wave that can be directed to this receiving antenna which will then transform this wave into electricity.""The problem is scale. The satellite would have to be massive, so would be difficult to launch and build.But with technology evolving rapidly, the project could be a reality in coming decades. The ESA will discuss the Solaris project at its meeting in November.The satellite would be repaired by robots while in orbit, explains Gwenaëlle Aridon, hHead of the Robotics Laboratory at Airbus Defence and Space.""The robot will be able to come and repair a panel, remove it and put a new one on if necessary,” she said.“Having [robot] manipulators reboot arms would effectively reduce the cost of the operations that are carried out.”It may be a logistical challenge - but as energy crises shift and unfold, it could eventually be a game changer.A single solar power satellite of the planned scale would generate around two gigawatts of power, equivalent to a conventional nuclear power station or six million solar panels on earth.The resulting energy could power more than one million homes.",Scientists dream up a massive floating solar farm in space - here's how it would work
344,0,0_solar_energy_wind_power,https://www.engadget.com/gm-defense-ultium-prototype-battery-pack-military-dod-144524818.html,"General Motors, through its GM Defense subsidiary, will build a battery pack prototype for the Department of Defense to test and analyze. The agency's Defense Innovation Unit is seeking a scalable design that can be used in electrified versions of tactical military vehicles.The battery pack will be based on GM's Ultium platform, which it's using to power its own electric vehicles. Due to the type of battery cells it employs, Ultium is billed as a modular and scalable system that can be adapted to different needs, so it may just fit the bill for the military.GM said the military wants a light- to heavy-duty EV for use in garrison and operational environments in order to reduce fossil fuel use. As a result, that should reduce the military's carbon emissions.Turn on browser notifications to receive breaking news alerts from Engadget You can disable notifications at any time in your settings menu. Not now Turned on Turn onThis isn't the first partnership that GM Defense has forged with the military. In July, the company secured a deal with the US Army to provide an electric Hummer for testing. Last year, GM Defense president Steve duMont said the company would build an electric military vehicle prototype based on the Hummer EV.",GM will make an Ultium battery pack prototype for the US military
34,0,0_solar_energy_wind_power,https://electrek.co/2022/08/26/in-a-us-first-california-will-pilot-solar-panel-canopies-over-canals/,"In March 2021, Electrek reported that scientists published a feasibility study about the benefits of erecting solar panels over canals. That study is about to become a reality when a pilot project breaks ground in California.Solar over canalsAugust 26 update: Project Nexus in the Turlock Irrigation District, a $20 million project funded by the State of California, will break ground in mid-October at two locations:A 500-foot span of a canal in Hickman, east of ModestoA mile-long canal span in the nearby city of CeresJosh Weimer, Turlock Water & Power’s external affairs manager, said [via Reuters]:If this is something that works on these first two miles of Project Nexus that we’re doing, there’s the potential that this could scale to multiple locations.If all 4,000 miles of California’s canals were covered with solar panels, that could produce 13 gigawatts of renewable power. A gigawatt is enough to power 750,000 homes, so that would be enough power for 9.75 million households. For perspective, as of July 2021, there were 13.1 million households in California.March 1: The California Department of Water Resources, utility company Turlock Irrigation District (TID), Marin County, California-based water and energy project developer Solar AquaGrid, and The University of California, Merced, are partnering on Project Nexus – a “nod to the water-energy nexus paradigm gaining attention among public utilities.”India already has solar panels over canals, but the mile-long Project Nexus in California’s San Joaquin Valley will be the first of its kind in the US.TID explains what the pilot is:Project Nexus includes the installation of solar panel canopies over various sections of Turlock Irrigation District’s (TID) irrigation canals. Project Nexus will serve as a Proof of Concept to pilot and further study solar over canal design, deployment, and co-benefits on behalf of the State of California using TID infrastructure and electrical grid access. The project is anticipated to break ground in fall 2022 and be complete by the end of 2024.Project Nexus will provide the opportunity to see whether the solar panels reduce water evaporation as the result of midday shade and wind mitigation; create improvements to water quality throughreduced vegetative growth; reduce canal maintenance as a result of reduced vegetative growth; andof course, generate renewable electricity.Roger Bales, distinguished professor of engineering at the University of California, Merced, who is working on this project, wrote in the Conversation:Most of California’s rain and snow falls north of Sacramento during the winter, while 80% of its water use occurs in Southern California, mostly in summer. That’s why canals snake across the state – it’s the largest such system in the world. We estimate that about 1-2% of the water they carry is lost to evaporation under the hot California sun. In a 2021 study, we showed that covering all 4,000 miles of California’s canals with solar panels would save more than 65 billion gallons of water annually by reducing evaporation. That’s enough to irrigate 50,000 acres of farmland or meet the residential water needs of more than 2 million people. By concentrating solar installations on land that is already being used, instead of building them on undeveloped land, this approach would help California meet its sustainable management goals for both water and land resources.Read more: Solar panels and California’s canals could make a winning pairPhoto: Robin Raj, Citizen Group & Solar AquagridUnderstandSolar is a free service that links you to top-rated solar installers in your region for personalized solar estimates. Tesla now offers price matching, so it’s important to shop for the best quotes. Click here to learn more and get your quotes. — *ad.","In a US first, California will pilot solar-panel canopies over canals"
36,0,0_solar_energy_wind_power,https://electrek.co/2022/09/30/offshore-wind-seaweed-farms/,"Stockholm-headquartered renewable energy developer OX2 has signed letters of intent with Swedish edible seaweed companies Nordic SeaFarm and KOBB to explore the possibility of seaweed farming at one of OX2’s offshore wind farms.Seaweed and offshore windOX2’s Galatea-Galene huge 1.7 gigawatt offshore wind farm will be sited off Halland, a county on the western coast of Sweden. It’s named after two Greek sea nymphs, Galatea and Galene, and consists of two sub-areas around 15.5 miles (25 km) outside the cities of Falkenberg and Varberg.Galatea-Galene is expected to consist of up to 101 wind turbines and generate around 6 to 7 terawatt-hours of clean electricity per year. That’s the equivalent of the average annual electricity consumption of more than 1.2 million Swedish households. (There are 4.8 million households in Sweden, for perspective.)This offshore wind farm will be developed in a single phase. Construction is expected to commence in 2028 and enter into commercial operation in 2030.Simon Johansson, CEO of Nordic SeaFarm, and Benjamin Ajo, chairman of the board of KOBB, said in a joint statement [via Offshorewind.biz]:We see great opportunities, in collaboration with both the fishing industry and the wind power industry, to both maintain and create new jobs when we investigate the possibilities of creating a new industry in Sweden in the form of large-scale aquaculture. Developing the national food supply while [offshore wind] farms contribute to stopping the negative effects of climate change are more positive aspects.All seaweed needs to grow is saltwater and sunlight. It’s a superfood that’s rich in vitamins, minerals, fiber, and antioxidants, and particularly high in iodine, so it’s very nutritious. (Note that crispy seaweed in Chinese restaurants is actually cabbage.)It can be used to wrap sushi, in soups and salads, in snacks and instant noodles, and as livestock food.Seaweed also provides a source of food for marine life. In April, Electrek reported that a groundbreaking study found that the first US offshore wind farm has had no negative effect on fish and has even proven to be beneficial.Here’s a short video from Nordic SeaFarm that shows how the company grows and harvests seaweed for consumption:Electrek’s TakePairing seaweed farms and offshore wind farms seems like an inspired idea.Seaweed’s ability to absorb toxins and other contaminants from the sea make it environmentally friendly, but that’s not what humans want to consume. That’s where seaweed growers come in: they test the seaweed for safety and quality.Any multipurpose sustainable use of an offshore wind farm, particularly one that provides both clean energy and nutritious food that doesn’t require either fertilizer or fresh water to grow, is a win. It’s also another example of innovation that the clean energy revolution is bringing about in the climate change fight.Read more: This new innovation boosts wind farm energy output yet costs nothingUnderstandSolar is a free service that links you to top-rated solar installers in your region for personalized solar estimates. Tesla now offers price matching, so it’s important to shop for the best quotes. Click here to learn more and get your quotes. — *ad.",Could offshore wind sites host edible seaweed farms? The Swedes think so
37,0,0_solar_energy_wind_power,https://electrek.co/2022/10/04/renewables-global-electricity/,"Renewables met all of the rise in global electricity demand in the first half of 2022, preventing any growth in coal and gas generation, according to a new report published by London-based energy think tank Ember.The rise in wind and solar generation met over 75% of the demand growth in the first half of 2022, while hydro met the remainder, preventing a possible 4% increase in fossil-fuel generation and avoiding $40 billion in fuel costs and 230 Mt CO2 in emissions.Malgorzata Wiatros-Motyka, senior analyst at Ember, said:Wind and solar are proving themselves during the energy crisis. The first step to ending the grip of expensive and polluting fossil fuels is to build enough clean power to meet the world’s growing appetite for electricity.The report analyzes electricity data from 75 countries representing 90% of global electricity demand. It compares the first six months of 2022 to the first half of 2021 to show how the electricity transition has progressed.The report finds that global electricity demand grew by 389 terawatt hours (TWh) in the first half of 2022. Renewables – wind, solar, and hydro – increased by 416 TWh, slightly exceeding the rise in electricity demand.Wind and solar alone rose by 300 TWh, which was equal to 77% of the rise in global electricity demand. In China, the rise in wind and solar generation alone met 92% of its electricity demand rise. In the United States it was 81%, and in India it was 23%.Rise in fossil-fuel generation unchangedAs a result of the growth in renewables, fossil-fuel generation was almost unchanged (+5 TWh, +0.1%). Coal fell by 36 TWh (-1%) and gas by 1 TWh (-0.05%). This offset a slight rise in other fossil fuels (mainly oil) of 42 TWh. Consequently, global CO2 power sector emissions were unchanged in the first half of 2022 compared to the same period last year, despite the rise in electricity demand.Coal in the EU rose 15% only to cover a temporary shortfall in nuclear and hydro generation. Coal in India rose 10% because of a sharp rebound in electricity demand from lows early last year when the pandemic struck hardest. Globally, these rises were offset by coal-power falls of 3% in China and 7% in the United States.The growth in wind and solar prevented a 4% rise in fossil fuel electricity generation worldwide. In China, the growth in wind and solar enabled fossil fuel power to fall by 3%. Without this growth, fossil fuels would have risen by 1%. In India, fossil fuel power rose by 9%, but it would have been 12% without growth in wind and solar. In the United States, it slowed down the rise in fossil fuel power from 7% to just 1%. In the EU, fossil fuel power rose by 6%, but it would have been 16% without growth in wind and solar.Despite the halt in fossil-fuel generation in the first half of 2022, coal and gas generation increased in July and August. It leaves open the possibility that power sector emissions in 2022 may yet rise, following last year’s all-time high.Further, a new report from San Francisco-based NGO Global Energy Monitor found that approximately 89.6 gigawatts (GW) of gas plants in development globally, totaling 5,070 million metric tons of CO2e lifetime emissions if built, are coal-to-gas conversions or replacements.These conversions are proceeding despite data showing that gas projects are increasingly uncompetitive with renewables and are just as bad, if not worse, for the environment than coal.Wiatros-Motyka said:We can’t be sure if we’ve reached peak coal and gas in the power sector. Global power sector emissions are still pushing all-time highs when they need to be falling very quickly. And the same fossil fuels pushing us into a climate crisis are also causing the global energy crisis. We have a solution: Wind and solar are homegrown and cheap, and are already cutting both bills and emissions fast.Read more: Siemens Gamesa launches recyclable onshore wind turbine bladesUnderstandSolar is a free service that links you to top-rated solar installers in your region for personalized solar estimates. Tesla now offers price matching, so it’s important to shop for the best quotes. Click here to learn more and get your quotes. — *ad.",Renewables met 100% of the rise in global electricity demand in the first half of 2022
38,0,0_solar_energy_wind_power,https://electrek.co/2022/10/10/wind-turbine-24-hour-power-world-record/,"Siemens Gamesa’s 14-222 DD offshore wind turbine prototype has, according to the Spanish-German wind giant today, set a world record for the most power output by a single wind turbine in a 24-hour period: 359 megawatt-hours.UnderstandSolar is a free service that links you to top-rated solar installers in your region for personalized solar estimates. Tesla now offers price matching, so it’s important to shop for the best quotes. Click here to learn more and get your quotes. — *ad.This would be enough energy, according to the company, for a mid-sized electric vehicle – think a Tesla Model 3 – to drive around 1.12 million miles (1.8 million km).Siemens Gamesa’s huge wind turbine achieved this power output milestone only 10 months after it produced its first electricity and delivered it to the grid at the test center in Østerild, Denmark.The SG 14-222 DD is a 14 megawatt (MW) offshore wind turbine with a capacity of up to 15 MW with Power Boost.It features a 222-meter (728 feet) diameter rotor, 108-meter-long (354-feet-long) B108 blades that are cast in a single piece and can now be recycled, and a swept area of 39,000 square meters (419,792 square feet).The SG 14-222 DD can provide enough energy to power around 18,000 households annually.Siemens Gamesa writes:By increasing the rotor diameter to 222 meters with 108 meter-long blades, the SG 14-222 DD delivers more than 25% [annual energy production] AEP compared to its predecessor. With every new generation of our offshore direct drive turbine technology – which uses fewer moving parts than geared turbines – component improvements have enabled greater performance while maintaining reliability. We are able to reduce time to market of the SG 14-222 DD thanks to standardized processes and a fully developed supply chain. Enabling high volume production at low risk. The serial production is planned for 2024.In June, Siemens Gamesa was awarded a firm order for 60 of its SG 14-222 DD offshore wind turbines, which will be installed at the 882-megawatt (MW) Moray West offshore wind farm in Scotland. It will be the first installation of this model.Read more: The world’s most powerful wind turbine will make its debut in ScotlandPhoto: Siemens Gamesa",A turbine prototype just broke a 24-hour wind power world record
111,0,0_solar_energy_wind_power,https://pv-magazine-usa.com/2022/10/13/global-polysilicon-capacities-may-nearly-double-by-end-of-2023-to-536-gw/,"In its Q2 2022 PV Supplier Market Intelligence Program Report (SMIP), solar and storage supply technical advisory Clean Energy Associates (CEA) said polysilicon production capacity may reach 295 GW by the end of 2022, and 536 GW year’s end 2023.CEA said it expects this production capacity to far exceed solar installations next year. While this may suggest supply problems could begin to be alleviated, CEA said module capacity expansions are slowing. Many manufacturers are instead expanding cell production capacity, catering to the trend of n-type TOPCon and HJT manufacturing.Ingot capacity grew almost 30 GW this quarter, most of which can be attributed to two facilities bringing 23 GW online.Wafer capacity decreased slightly, said CEA, primarily due to a major provider retiring its multi-crystalline wafer capacity. The 17 suppliers covered in the report boosted cell capacity by 22% in Q2 2022, bringing 47 GW of capacity online, reaching a total of 262 GW.Module production reached 324 GW in Q2. CEA forecasts this may expand 20% by the end of the year, reaching 400 GW.Only seven suppliers covered by the CEA report are fully vertically integrated from ingot to module production, with most others operating at the cell and module link in the supply chain. “With growing merchant wafer options, there is little need for most suppliers to expand upstream,” said CEA.The report said suppliers are working to optimize wafer sizes after the industry standardized 210 mm (G12) and 182 mm (M10) module dimensions. The “182 mm Plus” (182P) has increased wafer heights to further reduce “white space” caused by intercell gaps to achieve up to 5 W of additional output, said CEA. The “210 mm Reduced” (210R) reduced wafer widths for niche rooftop applications at the expense of power output. CEA said it expects new module sizes for the residential solar market to be introduced.Many analysts have predicted China will break 100 GW of installations this year. CEA expects slightly lower installations in China during 2022 due to high module prices impacting utility-scale projects. It said many investment decisions have been deferred as projects could not meet their internal rate of return thresholds.Most of the polysilicon supply chain originates in China. Outside of China, production capacities are 11 GW of ingot, 42 GW of cell, and 50 GW of module capacity. By end of 2023, these capacities are expected to expand to 23 GW, 73 GW, and 74 GW, respectively.“Policy uncertainties continue to defer expansion plans of suppliers as they remain cautious due to lingering policy uncertainty in the United States surrounding the Uyghur Forced Labor Prevention Act and anticircumvention investigation,” said CEA in the report.",Global polysilicon capacities may nearly double by end of 2023 to 536 GW
335,0,0_solar_energy_wind_power,https://www.cnn.com/2022/10/02/us/solar-babcock-ranch-florida-hurricane-ian-climate/index.html,"CNN —Anthony Grande moved away from Fort Myers three years ago in large part because of the hurricane risk. He has lived in southwest Florida for nearly 19 years, had experienced Hurricanes Charley in 2004 and Irma in 2017 and saw what stronger storms could do to the coast.Grande told CNN he wanted to find a new home where developers prioritized climate resiliency in a state that is increasingly vulnerable to record-breaking storm surge, catastrophic wind and historic rainfall.What he found was Babcock Ranch — only 12 miles northeast of Fort Myers, yet seemingly light years away.Babcock Ranch calls itself “America’s first solar-powered town.” Its nearby solar array — made up of 700,000 individual panels — generates more electricity than the 2,000-home neighborhood uses, in a state where most electricity is generated by burning natural gas, a planet-warming fossil fuel.The streets in this meticulously planned neighborhood were designed to flood so houses don’t. Native landscaping along roads helps control storm water. Power and internet lines are buried to avoid wind damage. This is all in addition to being built to Florida’s robust building codes.Some residents, like Grande, installed more solar panels on their roofs and added battery systems as an extra layer of protection from power outages. Many drive electric vehicles, taking full advantage of solar energy in the Sunshine State.Climate resiliency was built into the fabric of the town with stronger storms in mind.So when Hurricane Ian came barreling toward southwest Florida this week, it was a true test for the community. The storm obliterated the nearby Fort Myers and Naples areas with record-breaking surge and winds over 100 mph. It knocked out power to more than 2.6 million customers in the state, including 90% of Charlotte County.But the lights stayed on in Babcock Ranch.“It certainly exceeded our expectations of a major hurricane,” Grande, 58, told CNN.A damaged building is seen in Babcock Ranch after Hurricane Ian. Courtesy Nancy ChorpenningAn uprooted tree in Babcock Ranch after Hurricane Ian. Courtesy Nancy ChorpenningSign up for CNN’s Life, But Greener newsletter.The storm uprooted trees and tore shingles from roofs, but other than that Grande said there is no major damage. Its residents say Babcock Ranch is proof that an eco-conscious and solar-powered town can withstand the wrath of a near-Category 5 storm.“We have proof of the case now because [the hurricane] came right over us,” Nancy Chorpenning, a 68-year-old Babcock Ranch resident, told CNN. “We have water, electricity, internet — and we may be the only people in Southwest Florida who are that fortunate.”Grande said Hurricane Ian came through southwest Florida “like a freight train.” But he wasn’t afraid that he would lose everything in a storm, like he was when he lived in Fort Myers.“We’re very, very blessed and fortunate to not be experiencing what they’re experiencing now in Sanibel Island and Fort Myers Beach,” Grande said. “In the times that we’re living in right now with climate change, the beach is not the place to live or have a business.”Solar successSyd Kitson, a former professional football player for the Green Bay Packers and Dallas Cowboys, is the mastermind behind Babcock Ranch. Kitson envisioned it to be an eco-conscious and innovative neighborhood that is safe and resilient from storms like Ian.The ranch broke ground in 2015 with the construction of the solar array — which was built and is run by Florida Power and Light — and its first residents moved into the town in 2018. Since then, the array has doubled in size and thousands of people have made Babcock their home.Around 700,000 solar panels power Babcock Ranch. Dennis Axer/Alamy“It’s a great case study to show that it can be done right, if you build in the right place and do it the right way,” said Lisa Hall, a spokesperson for Kitson, who also lives in Babcock Ranch.“Throughout all this, there’s just so many people saying, ‘it worked, that this was the vision, this is the reason we moved here,’” Hall told CNN.Perhaps the highest endorsement for the city is that it is now a refuge for some of Ian’s hardest-hit victims. The state opened Babcock Neighborhood School as an official shelter, even though it didn’t have the mandated generator. The solar array kept the lights on.Some of Chorpenning’s friends who live on Sanibel Island — which is now cut off from the mainland after Ian’s devastating storm surge severed the causeway — came to shelter at a friend’s house at Babcock Ranch. It will be a while before they can go back, she said.“They’re going to be renting a place over here for a while, while they figure out what’s going to happen out there,” she said. “I joked that we may be the only people in southwest Florida whose property value just increased.”Even Kitson chose to ride out the storm in Babcock to see how the community would fare in the hurricane. Kitson declined CNN’s request for an interview; Hall said he is focused on helping neighboring communities rebuild.“He was there during the storm; he said, ‘where else would I be?’” Hall said. “We built it to be resilient and as much as you plan and think you’ve done the right thing, you don’t know until you put it to the test.”Babcock Ranch has sold more than 2,000 homes, according to the neighborhood's website. Dennis Stephenson/AlamyAs utilities scramble to restore power across the state, Babcock residents say September storms showed that America’s energy infrastructure is not well-equipped to handle worsening extreme weather events. Hurricane Fiona ravaged Puerto Rico’s power grid when it made landfall there on September 18. Now, Ian has left millions of people in the dark in Florida.Babcock residents say their neighborhood is a model for urban development in a climate change-ravaged future.“It’s not what it was 20 or 25 years ago; the storms are getting bigger and bigger, and it’s no surprise, because the warnings have all been there,” Grande said. “I think Babcock Ranch’s future has gotten even brighter.”",This 100% solar community endured Hurricane Ian with no loss of power and minimal damage
39,0,0_solar_energy_wind_power,https://electrek.co/2022/10/13/miners-cut-co2-emissions-in-half-switching-electric-vehicles/,"While you may notice more electric vehicles on the road today than ever before, the technology is now making its way to miners, the companies extracting the critical minerals needed to build EVs, ensure adequate global food supply, etc. A new contract to supply battery electric vehicles to the Jansen potash project (potentially the world’s largest potash mine) expects to slash carbon emissions in half compared to its peers. What if we could apply this technology to miners in the EV industry, creating a full circle sustainable supply chain?BHP’s Jansen potash project is expected to be the largest of its kind, with initial capacity forecasts of 4.3 to 4.5 Mtpa.Potash is often found in fertilizer and is a rich source of potassium, a mineral essential for our health. Since potassium is not produced by the body naturally, it must be consumed through food. Potash is the most commonly used potassium fertilizer, but over 70% is based on conventional underground mining that uses heavy-duty equipment to extract it.Although underground mining can release half the CO2 emissions of open-pit mining, the company is taking it further by introducing several battery electric vehicles.Normet Canada, the mining company behind the project, was awarded a new contract to use battery electric vehicles at the Jansen potash project, including 10 underground EV loaders and one electric tethered loader.With these heavy-duty electric vehicles, BHP says the Jansen project is forecast to reduce CO2 emissions by 50% compared to the Saskatchewan potash mine.The electric vehicle deliveries are expected to begin in March 2023 and run through 2024, which will work perfectly as the miners aim to start production in 2026.Like potassium is critical for humans, electric vehicles also require certain minerals such as lithium and nickel. These miners are looking for ways to build on the momentum electric vehicles are establishing in reducing carbon emissions, completing a complete sustainability supply chain. Several companies are now working to make this a reality.Normet electric mining equipment (Source: Normet)Miners using electric vehicles can complete the sustainability supply chainAlmost every automaker has set its intentions to scale production of its electric vehicles to meet the growing demand for zero-emission cars while setting carbon reduction targets.For example, GM is going all-electric by 2035, Ford looks to produce 2 million EVs annually by 2026, and Volkswagen plans to be fully electric in Europe by 2035, while North America follows.Tesla, only selling electric cars, continues breaking records with 343,000 EVs delivered in Q3 on its way to producing 20 million annually.However, getting to these numbers will require mining. A few companies have already begun working to build a sustainable EV supply chain.Snow Lake Lithium outlined its plans in February to develop the world’s first all-electric lithium mine, one of the most critical minerals used to build EV batteries. The mining company’s CEO said at the time if you are going to mine for these resources that will be used to protect the environment, then obtaining them must also be done in a sustainable matter.The company is working with Meglab, an electrical equipment provider and mining solutions company, to make the dream a reality.There are operations around the world working to reduce emissions in the mining sector. In July 2021, Opibus (now ROAM) converted a Toyota Land Cruiser fitted with an electric powertrain to use a mining vehicle.Miners using electric vehicles can significantly reduce the heat and carbon exposure they typically experience with diesel-powered equipment. Furthermore, EV mining technology can cost less as it requires less ventilation and cooling.The point is, as the auto industry transitions to electric vehicles, companies are figuring out ways to build on the momentum EVs are establishing in reducing emissions. If miners get on board, it will come full circle, creating a complete sustainable EV supply chain.",Miners are cutting CO2 emissions in half by switching to electric vehicles for extracting critical minerals
216,0,0_solar_energy_wind_power,https://techcrunch.com/2022/07/14/gridtential-thinks-the-og-renewable-battery-chemistry-is-ripe-for-disruption/,"Lithium-ion batteries are one of the driving forces behind the transition away from fossil fuels, but the rechargeable battery that started it all — lead-acid — has received very little attention.Invented over 160 years ago, lead-acid batteries have been upgraded a few times, but today’s cells are largely unchanged from those sold in the ’70s and ’80s. Gridtential, a Santa Clara-based startup, is betting there’s plenty of room for improvement. It’s developed a silicon plate that can replace up to 35% of the lead in a traditional battery while improving its charging rate fivefold and quadrupling its lifespan.Gridtential today announced partnerships with two players in the lead-acid market, Hammond Group, which makes battery materials and additives, and Wirtz Manufacturing, which makes equipment to deposit battery material on a substrate.",Gridtential thinks the OG renewable battery chemistry is ripe for disruption
391,0,0_solar_energy_wind_power,https://www.independent.co.uk/tech/nasa-battery-electric-planes-b2199312.html,"For free real time breaking news alerts sent straight to your inbox sign up to our breaking news emails Sign up to our free breaking news emails Please enter a valid email address Please enter a valid email address SIGN UP I would like to be emailed about offers, events and updates from The Independent. Read our privacy notice Thanks for signing up to theBreaking News email {{ #verifyErrors }}{{ message }}{{ /verifyErrors }}{{ ^verifyErrors }}Something went wrong. Please try again later{{ /verifyErrors }}Nasa has invented a new type of high-performance battery that researchers claim could be used to power fully electric airplanes.The US space agency made the breakthrough following investigations into solid-state batteries, which hold more energy and are lighter than industry-standard lithium-ion batteries.Solid-state batteries also perform better in stressful environments, as they are less prone to overheating, fire and loss of charge over time, however they typically cannot discharge energy at the same rate as li-ion batteries.Until now, this has made them unsuitable for powering large electronics, such as electric vehicles, as they require batteries capable of discharging their energy an incredibly fast rate.This issue was solved by researchers at Nasa’s Solid-state Architecture Batteries for Enhanced Rechargeability and Safety (SABERS) unit, who were able to increase the battery’s discharge rate by a factor of 10 using innovative new materials that have yet to be used in batteries.A novel vertical-stack design also allowed SABERS researchers to create a solid-state battery capable of powering objects at a capacity of 500 watt-hours per kilogram – roughly double that of an electric car.“The possibilities are pretty incredible,” said Rocco Viggiano, principal investigator for SABERS at Nasa’s Glenn Research Center in Cleveland.“We’re starting to approach this new frontier of battery research that could do so much more than lithium-ion batteries can... Not only does this design eliminate 30 to 40 per cent of the battery’s weight, it also allows us to double or even triple the energy it can store, far exceeding the capabilities of lithium-ion batteries that are considered to be state of the art.”The new battery has already generated interest from government, industry and academia, with SABERS researchers partnering with several organisations to continue developing the technology.The ultimate aim is to use it to pioneer a new era of sustainable aviation that eliminates carbon emissions and noise from planes.Nasa has already made significant progress in this area through its experimental X-57 aircraft, nicknamed Maxwell, which replaces fuel-powered propellers with an all-electric propulsion system.",Nasa invents âincredibleâ battery for electric planes
319,0,0_solar_energy_wind_power,https://www.cnbc.com/2022/09/23/electric-vehicle-ev-sales-set-to-hit-an-all-time-high-in-2022-iea-says.html,"Tesla electric cars photographed in Germany on March 21, 2022. According to the International Energy Agency, electric vehicle sales are on course to hit an ""all-time high"" this year. Sean Gallup | Getty Images News | Getty ImagesElectric vehicle sales are on course to hit an all-time high this year, but more work is needed in other sectors to put the planet on course for net-zero emissions by 2050, according to the International Energy Agency.In an announcement accompanying its Tracking Clean Energy Progress update, the IEA said there had been ""encouraging signs of progress across a number of sectors"" but cautioned that ""stronger efforts"" were required to put the world ""on track to reach net zero emissions"" by the middle of this century. The TCEP, which is published yearly, looked at 55 parts of the energy system. Focusing on 2021, it analyzed these components' progression when it came to hitting ""key medium-term milestones by the end of this decade,"" as laid out in the Paris-based organization's net-zero pathway. On the EV front, the IEA said global sales had doubled in 2021 to represent nearly 9% of the car market. Looking forward, 2022 was ""expected to see another all-time high for electric vehicle sales, lifting them to 13% of total light duty vehicle sales globally."" The IEA has previously stated that electric vehicle sales hit 6.6 million in 2021. In the first quarter of 2022, EV sales came to 2 million, a 75% increase compared to the first three months of 2021.The IEA said both EVs and lighting — where more than 50% of the worldwide market is now using LED tech — were ""fully on track for their 2030 milestones"" in its net-zero by 2050 scenario. Despite the outlook for EVs, the IEA separately noted that they were ""not yet a global phenomenon. Sales in developing and emerging countries have been slow due to higher purchase costs and a lack of charging infrastructure availability."" Overall, the rest of the picture is a more challenging one. The IEA noted that 23 areas were ""not on track"" with a further 30 deemed as needing more effort. ""Areas not on track include improving the energy efficiency of building designs, developing clean and efficient district heating, phasing out coal-fired power generation, eliminating methane flaring, shifting aviation and shipping to cleaner fuels, and making cement, chemical and steel production cleaner,"" the IEA said. The shadow of 2015's Paris Agreement looms large over the IEA's report. Described by the United Nations as a ""legally binding international treaty on climate change,"" the accord aims to ""limit global warming to well below 2, preferably to 1.5 degrees Celsius, compared to pre-industrial levels."" Cutting human-made carbon dioxide emissions to net-zero by 2050 is seen as crucial when it comes to meeting the 1.5 degrees Celsius target.","EV sales to hit all-time high in 2022, IEA says, but more work needed to put world on net-zero path"
40,0,0_solar_energy_wind_power,https://electrek.co/2022/11/01/solar-will-become-10-times-cheaper-than-gas-in-europe-study/,"It would be 10 times more expensive to operate gas-fired power plants in the long term than to build new solar PV capacity in Europe, according to a new study from Oslo-based energy research company Rystad Energy.Natural gas is presently very expensive in Europe, due to the plunge in Russian gas exports.Rystad Energy reports that “spot prices on the Netherlands-based Title Transfer Facility (TTF) gas hub, the main reference for Western Europe, have risen from an average of €46 per megawatt-hour (MWh) in 2021 to €134 per MWh so far this year – an increase of 187%.” And gas will be needed for power generation through the winter.But more than 50 gigawatts (GW) of new solar and wind capacity are expected to be commissioned in 2023, along with up to 30 GW of nuclear capacity currently undergoing maintenance that French multinational utility EDF is planning to bring back online.Rystad Energy forecasts that TTF prices will stabilize at around €31 per MWh by 2030, which puts the levelized cost of electricity (LCOE) of existing plants closer to €150 per MWh. That’s still three times more than the LCOE of new solar PV facilities. For gas-fired plants to continue to be competitive, gas prices would need to fall closer to €17 per MWh and carbon prices would need to fall to €10 per metric ton, which is, according to Rystad, “currently unthinkable.”Carlos Torres Diaz, head of power at Rystad Energy, said:Gas will continue to play an important role in the European energy mix for some time to come, but unless something fundamental shifts, then simple economics, as well as climate concerns, will tip the balance in favor of renewables.The study also found that by 2028, new clean energy generation capacity installed using money that would otherwise have been spent on gas generation would reach 333 GW – enough to generate 663 TWh of electricity. Renewable power generation would be enough to replace forecast gas-fired generation that same year.By 2050, new renewable energy capacity would be generating more than 2,000 TWh:This growth in generation only considers output from new capacity developed using ‘funds from gas’ and is on top of Rystad Energy’s base case forecast, which anticipates that 2,385 GW of solar PV and wind capacity, and 520 GW of utility scale batteries are installed by 2050.Read more: Greece runs on 100% renewables for the first time on recordPhoto: EDPUnderstandSolar is a free service that links you to top-rated solar installers in your region for personalized solar estimates. Tesla now offers price matching, so it’s important to shop for the best quotes. Click here to learn more and get your quotes. — *ad.",Solar will become 10 times cheaper than gas in Europe â study
222,0,0_solar_energy_wind_power,https://techcrunch.com/2022/07/26/sono-motors-reveals-final-design-of-its-solar-charging-sion-ev/,"Sono Motors unveiled this week the final production design of the Sion EV, a solar electric vehicle that’s been in the making since the Munich-based startup launched in 2016.It’s been a long and bumpy road for Sono Motors, a journey that involved a couple of crowdfunding campaigns, a near insolvency and a debut on the Nasdaq exchange as a publicly traded company.The solar-powered fruits of those rather tumultuous labors were shown off at its bustling “Celebrate the Sun” Community event. The Sion solar-powered EV took center stage. However, another product called the Solar Bus Kit, a series of solar panels that are designed to be retrofitted onto 12-meter public buses, suggests the company has more ambitious plans beyond one passenger vehicle.The question, of course, is can Sono Motors produce and sell the EV at volume? And how?Sono aims to begin deliveries of the Sion in the second half of 2023 to customers in Germany, Austria and Switzerland. The company did not provide guidance on how many Sions would be delivered next year. The only metric it has shared is that it expects to make 43,000 Sions per year with a production capacity of 257,000 over seven years, according to a company spokesperson.However, even the most well-funded EV startups have had trouble making it to production recently. Sono has had to combat a range of challenges since going public in November last year, from a plummeting stock price to switching manufacturing partners, and its path forward to production and delivery will likely continue to be bumpy given current market and supply chain uncertainty.Let’s break down what this company is up to:The Sion’s final production designThe Sion is a compact, five-door, family-friendly hatchback that will sell for €25,126 (~$25,628 at today’s conversion). Its outer shell will consist of 456 integrated solar half-cells that will collect power from the sun and enable self-sufficiency on shorter journeys. Of course, the vehicle will still use a traditional charger to refuel, but the constant drip feed of solar should be enough to take care of most urban commutes, the company says.The car’s 54 kWh lithium iron phosphate battery has a range of about 190 miles. Sono expects the energy generated by the solar cells to extend that by an average of 70 miles, and up to 152 miles, each week. In addition, the Sion is built with bidirectional charging technology, which allows commuters to use energy stored in the battery (~11 kW) to power homes or other electronic devices.[gallery ids=""2360114,2360119,2360115""]Other enhancements to the final design’s exterior and interior includes a sleeker, cleaner-looking vehicle, according to Sono.The exterior has new headlights, including a new daylight strip, rear lights, a rear camera, a bottom sideline design and a revamped charging lid in front. There are also new door handles, which photos show have the words “made to be shared” — a nod to Sono’s hope to get this car into fleets.Sono says paid reservations have increased from 13,000 last November to more than 19,000. That could mean a net sales volume of €415 million if all reservations result in sales. With an average down payment of €2,225, Sono could have over €42 million in the bank to help it get to production. (By the way, if those numbers — 19,000 x €25,126 — don’t quite add up, it’s because the price of the Sion has shifted over time and earlier backers are grandfathered into their original prices, says a company spokesperson.) To bring costs down, Sono Motors has relied on outsourcing its vehicle’s manufacture and distribution. Previously, Sono planned to partner with NEVS, the Swedish EV manufacturer that acquired the assets of the bankrupt Saab Automobile in 2012. But NEVS’s parent firm, Evergrande, is deep in debt and is still trying to sell NEVS, which is still struggling to put its Saab 9-3 into production. , which manufactures in Finland. Sono switched gears in April and signed a contract with a new partner, Valmet Automotive The solar bus kit While Sono’s solar bus kit can’t turn a diesel-pumping bus into an eco-friendly hybrid, it can take some of the emissions out of the equation. The kit, which has been standardized to work with “virtually all European bus fleets,” can save up to 1,500 liters of diesel and up to four tons of CO 2 per bus per year, according to the company. Its total size of about eighrt square-meters of solar panels provide around 1.4 kW at peak installation. That solar energy can be used to power a bus’ subsystems such as HVAC. Sono said bus fleet operators stand to see a potential payback time of about three to four years, depending on the sunniness of days in operation and fuel prices.At CES 2021, Sono said it would license its solar body panel technology to other companies as an additional source of revenue, and this new bus kit is a part of that move to diversify the business.Since going public, Sono says it has ongoing letters of intent, pilots and prototypes for 19 companies that are implementing the company’s solar technology on a variety of vehicle architectures, like buses, trailers, trucks and electric transporters. For example, Sono recently piloted a solar bus trailer in Munich to provide backup power for public transport. Sono has also partnered with the Reefer Group, a manufacturer of refrigerated semi-trailers, to build a solar trailer for testing.The company said its B2B solar business is already generating revenue, more details of which will be shared during the company’s earnings call in September. Sono’s stock has plummeted since its debut, losing close to 93% of its value, so the company will definitely need to earn another source of revenue as it revs up for production next year.Why Sono stock has taken a hitWhen Sono Group, the parent company of Sono Motors, began trading, it opened at $20.06 after the IPO was initially priced at $15. Shares hit a high of $38.74 before the market closed on its first day. At the time of this writing, Sono Group is trading at $2.64, a slide that has continued even after the company revealed the production version of the Sion.It seems the buzz around EV startups has started to fizzle.In May, Sono closed its previously announced underwritten follow-on offering of 10 million ordinary shares, with an additional 1.5 million shares available for the underwriters — which include Cantor Fitzgerald and B. Riley Securities — at an even greater discount.Sono said it gained $40 million in gross proceeds from this offering, which it used to help fund the start of production of its Sion, but investors are likely to see the offering as one that devalues shares to an even greater extent.",Sono Motors reveals final design of its solar-charging Sion EV
567,0,0_solar_energy_wind_power,https://www.zdnet.com/article/this-nasa-space-tech-could-make-your-ev-charge-faster-too/,"Image: Getty Images/Jung GettyFully charging an electric vehicle (EV) battery can take 10 hours at home, but now NASA has given details of a cooling system that could cut the time taken to get a full charge to just five minutes.NASA's technology could be a game-changer for the EV industry, eliminating one of its main obstacles to adoption – that five-minute charge would put charging EVs on a par with the time it takes to fill the tank of a petrol-powered car.The answer to five-minute charging could be found in the ""Flow Boiling and Condensation Experiment (FBCE)"", a project NASA's Biological and Physical Sciences Division is sponsoring Purdue University to develop and focuses on coolants in the charging cable between the EV's inlet and the battery.Also: The 5 best electric cars: Plus, the cheapest EV availablePurdue researchers demonstrated a technique called ""sub-cooled flow boiling"" that improves the effectiveness of heat transfer, which could help EVs on Earth but is primarily intended for the International Space Station.Unfortunately, owners of Tesla and other EVs shouldn't count on having a five-minute full charge any time soon because it requires capacities, measured in amperes or amps, far exceeding what's available today. However, when that is available, NASA's technology could provide temperature controls that those charging systems would require.As NASA explains, a five-minute charge requires a charging system that provides a current at 1,400 amps. But, as Purdue points out, Tesla's V3 Supercharger – which is the apex of today's EV chargers – doesn't exceed 600 amps. Purdue's prototype last year demonstrated 2,400 amps.Hence the eventual need for a cooling system beyond today's capabilities that can handle 1,400 amps. Using NASA's FBCE, Purdue researchers, led by Dr. Issam Mudawar, pumped non-electrically conducting liquid coolant through a charging cable to capture heat from the current-carrying conductor.""Subcooled flow boiling allows Mudawar's team to deliver 4.6 times the current of the fastest available electric vehicle chargers on the market today by removing up to 24.22 kilowatts of heat,"" NASA explains.""Application of this new technology resulted in unprecedented reduction of the time required to charge a vehicle and may remove one of the key barriers to worldwide adoption of electric vehicles,"" it said.Faster charging times ought to drive higher adoption of EVs, which are part of the world's answer to achieving net-zero carbon emissions. A recent report from the International Energy Agency said sales of EVs reached 6.6 million in 2021, up from 120,000 in 2012, and accounted for 9% of all vehicle sales.Chinese consumers bought 3.3 million new EVs, many of which were two- and three-wheelers, while Europeans bought 2.3 million EVs, and US consumers bought 630,000. Still, in the US, EV sales doubled their share in 2021 year-on-year to 4.5%. China was also rolling out charging faster than most regions.Source: Purdue University","This NASA space tech could make your EV charge faster, too"
568,0,0_solar_energy_wind_power,https://thehill.com/policy/energy-environment/3685583-nasa-suggests-new-space-cooling-technology-could-charge-electric-cars-in-5-minutes/,"NASA has suggested an experimental cooling system it is funding could ultimately allow electric vehicle users to charge their cars within five minutes.The agency said a team led by a Purdue University professor has developed the “subcooled flow boiling” technology for experimentation, with the hope it can control future systems’ temperatures in space.“A team sponsored by NASA’s Biological and Physical Sciences Division is developing a new technology that will not only achieve orders-of-magnitude improvement in heat transfer to enable these systems to maintain proper temperatures in space, but will also enable significant reductions in size and weight of the hardware,” NASA“What’s more, this same technology may make owning an electric-powered car here on Earth easier and more feasible,” the post continued.NASA indicated achieving such a feat of charging electric vehicles within five minutes would require chargers to provide current at 1,400 amperes, far higher than currently available technology.Most chargers currently available support currents less than 150 amperes, while some of the most advanced chargers on the market deliver currents up to 520 amperes, the post noted.But NASA said Purdue University’s developmental cable can provide currents of up to 2,400 amperes by removing heat through the new technology, which would deliver charging at 4.6 times the rate of the fastest charger currently available.“Application of this new technology resulted in unprecedented reduction of the time required to charge a vehicle and may remove one of the key barriers to worldwide adoption of electric vehicles,” NASA wrote.President Biden has emphasized a shift to electric vehicles as a significant component of his climate initiatives, but the proposals have been met with criticism among some in the GOP, who haveQuestions have also been raised about whether the U.S. electrical grid could even handle a hard shift toward electric vehicles.The Inflation Reduction Act, a party-line reconciliation package passed over the summer, includes billions in funding for electric vehicle tax credits and other financial incentives.It also includes a $7.5 billion investment to build a network of charging stations across the U.S.“The great American road trip is going to be fully electrified,” Biden",NASA suggests new space cooling technology could charge electric cars in 5 minutes
314,1,1_plastic_robots_robot_robotics,https://www.cbsnews.com/news/plastic-recycling-failed-concept-us-greenpeace-study-5-percent-recycled-production-up/,"Why recycling plastic isn't as effective as you think, according to industry criticsRecycling plastic isn't as effective as you thinkRecycling plastic isn't as effective as you thinkWashington — Plastic recycling rates are declining even as production shoots up, according to a Greenpeace USA report out Monday that blasted industry claims of creating an efficient, circular economy as ""fiction.""Titled ""Circular Claims Fall Flat Again,"" the study found that of 51 million tons of plastic waste generated by U.S. households in 2021, only 2.4 million tons were recycled, or around five percent. After peaking in 2014 at 10 percent, the trend has been decreasing, especially since China stopped accepting the West's plastic waste in 2018.Virgin production — of non-recycled plastic, that is — meanwhile is rapidly rising as the petrochemical industry expands, lowering costs.""Industry groups and big corporations have been pushing for recycling as a solution,"" Greenpeace USA campaigner Lisa Ramsden told AFP.""By doing that, they have shirked all responsibility"" for ensuring that recycling actually works, she added. She named Coca-Cola, PepsiCo, Unilever and Nestle as prime offenders.According to Greenpeace USA's survey, only two types of plastic are widely accepted at the nation's 375 material recovery facilities.The first is polyethylene terephthalate (PET), which is commonly used in water and soda bottles; and the second is high density polyethylene (HDPE), seen in milk jugs, shampoo bottles and cleaning product containers. These are numbered ""1"" and ""2"" according to a standardized system in which there are seven plastic types.But being recyclable in theory doesn't mean products are being recycled in practice.The report found that PET and HDPE products had actual reprocessing rates of 20.9 percent and 10.3 percent, respectively — both down slightly from Greenpeace USA's last survey in 2020.Plastic types ""3"" through ""7"" — including children's toys, plastic bags, produce wrappings, yogurt and margarine tubs, coffee cups and to-go food containers — were reprocessed at rates of less than five percent.Despite often carrying the recycling symbol on their labels, products that use plastic types ""3"" through ""7"" fail to meet the Federal Trade Commission classification of recyclable.This is because recycling facilities for these types aren't available to a ""substantial majority"" of the population, defined as 60 percent, and because the collected products are not being used in the manufacturing or assembly of new items.According to the report, there were five main reasons why plastic recycling is a ""failed concept.""First, plastic waste is generated in vast quantities and is extremely difficult to collect — as becomes clear during what the report called ineffective ""volunteer cleanup stunts"" funded by nonprofits such as ""Keep America Beautiful.""Second, even if it were all collected, mixed plastic waste cannot be recycled together, and it would be ""functionally impossible to sort the trillions of pieces of consumer plastic waste produced each year,"" the report said.Third, the recycling process itself is environmentally harmful, exposing workers to toxic chemicals and itself generating microplastics.Fourth, recycled plastic carries toxicity risks through contamination with other plastic types in collection bins, preventing it from becoming food-grade material again.Fifth and finally, the process of recycling is prohibitively expensive.""New plastic directly competes with recycled plastic, and it's far cheaper to produce and of higher quality,"" said the report.Ramsden called on corporations to support a Global Plastics Treaty, which United Nations members agreed to create in February, and move toward refill and reuse strategies.""This isn't actually a new concept — it's how the milkman used to be, it's how Coca-Cola used to get its beverages to people. They would drink their beverage, give the glass bottle back, and it would be sanitized and reused,"" she said.Some countries are leading the way, including India, which recently banned 19 single-use plastic items. Austria has set reuse targets of 25 percent by 2025 and at least 30 percent by 2030 for beverage packaging, while Portugal has also set the 30 percent by 2030 goal. Chile is moving to phase out single-use cutlery and mandating refillable bottles.","Plastic recycling a ""failed concept,"" study says, with only 5% recycled in U.S. last year as production rises"
580,1,1_plastic_robots_robot_robotics,https://techcrunch.com/2022/05/10/robotics-guru-rodney-brooks-and-veos-clara-vu-discuss-human-robot-interaction-at-tc-sessions-robotics-2022/,"Robots have transformed automation across industries such as agtech, automotive, logistics, manufacturing and warehousing. Yet even the most advanced robots typically work in restricted workcells away from people due to safety concerns.Robots have transformed automation across industries such as agtech, automotive, logistics, manufacturing and warehousing. Yet even the most advanced robots typically work in restricted workcells away from people due to safety concerns.The field of human-robot interaction (HRI) offers the potential for robots with enough cognitive smarts to work effectively and safely alongside humans in places such as factory floors. The rise of the collaborative robot — or cobot — is well underway, with a projected market value ofThe field of human-robot interaction (HRI) offers the potential for robots with enough cognitive smarts to work effectively and safely alongside humans in places such as factory floors. The rise of the collaborative robot — or cobot — is well underway, with a projected market value ofWe’re thrilled to announce that two roboticists at the forefront of HRI — Rodney Brooks, founder and CTO of Robust.AI (and co-inventor of Roomba, the popular household robot), along with Clara Vu, co-founder and CTO of Veo Robotics — will join us on stage at atWe’re thrilled to announce that two roboticists at the forefront of HRI — Rodney Brooks, founder and CTO of Robust.AI (and co-inventor of Roomba, the popular household robot), along with Clara Vu, co-founder and CTO of Veo Robotics — will join us on stage at atOur conversation will cover the current state of HRI, the challenges of developing robots with “common sense,” and the technologies required for people and robots to work in close proximity. Of course, we’ll ask what they’re up to right now and what’s coming down the road.Our conversation will cover the current state of HRI, the challenges of developing robots with “common sense,” and the technologies required for people and robots to work in close proximity. Of course, we’ll ask what they’re up to right now and what’s coming down the road.We’ll also ask how long will it be before cobots become part of the everyday work environment. And what kind of training infrastructures do we need to create as more efficient cobots replace human workers in physically demanding or repetitive menial jobs?We’ll also ask how long will it be before cobots become part of the everyday work environment. And what kind of training infrastructures do we need to create as more efficient cobots replace human workers in physically demanding or repetitive menial jobs?Robotics legend Rodney Brooks founded Robust.AI in 2019 with a mission to build a first-of-its-kind industrial-grade cognitive platform for robots. The goal is to make robots that are smarter, safer, more robust, context aware and collaborative. Such robots could function reliably in construction, eldercare, households and other highly complex environments.Robotics legend Rodney Brooks founded Robust.AI in 2019 with a mission to build a first-of-its-kind industrial-grade cognitive platform for robots. The goal is to make robots that are smarter, safer, more robust, context aware and collaborative. Such robots could function reliably in construction, eldercare, households and other highly complex environments.In 2020, Robust.AI raised a $15 million Series A.In 2020, Robust.AI raised a $15 million Series A.In addition to his roles at Robust.AI, Brooks, an award-winning computer scientist, taught and held directorships at MIT. He was also founder and CTO at Rethink Robotics and iRobot.In addition to his roles at Robust.AI, Brooks, an award-winning computer scientist, taught and held directorships at MIT. He was also founder and CTO at Rethink Robotics and iRobot.Veo Robotics, an industrial automation company founded in 2016, created FreeMove, a comprehensive 3D safeguarding system for industrial robots that powers dynamic human-robot collaboration. In other words, it turns run-of-the mill industrial robots into machines that respond to humans.Veo Robotics, an industrial automation company founded in 2016, created FreeMove, a comprehensive 3D safeguarding system for industrial robots that powers dynamic human-robot collaboration. In other words, it turns run-of-the mill industrial robots into machines that respond to humans.When human skill and creativity join forces with the strength and speed of robots, the result is a flexible human-robot interaction, which would help manufacturers adjust to continuous, rapidly changing market demands.When human skill and creativity join forces with the strength and speed of robots, the result is a flexible human-robot interaction, which would help manufacturers adjust to continuous, rapidly changing market demands.In 2019, the company raised a $15 million Series A.In 2019, the company raised a $15 million Series A.As Veo Robotic’s co-founder and CTO, Clara Vu leads the engineering team and developments of the computer vision-powered sensing and intelligence used by four of the biggest industrial robot companies in the world: FANUC, Yaskawa, ABB and Kuka.As Veo Robotic’s co-founder and CTO, Clara Vu leads the engineering team and developments of the computer vision-powered sensing and intelligence used by four of the biggest industrial robot companies in the world: FANUC, Yaskawa, ABB and Kuka.With more than two decades of robotics experience, Vu has developed multiple products from inception to market. She began her career at iRobot programming robots for oil well exploration; she then moved on to interactive toys and the Roomba.With more than two decades of robotics experience, Vu has developed multiple products from inception to market. She began her career at iRobot programming robots for oil well exploration; she then moved on to interactive toys and the Roomba.Prior to Veo, Vu was co-founder and director of software development for Harvest Automation, the makers of mobile robots for agricultural automation.Prior to Veo, Vu was co-founder and director of software development for Harvest Automation, the makers of mobile robots for agricultural automation.Don’t miss a fascinating conversation with Rodney Brooks and Clara Vu, two roboticists on the cutting edge of human-robot interaction, about the reality and potential of humans and robots working side-by-side.Don’t miss a fascinating conversation with Rodney Brooks and Clara Vu, two roboticists on the cutting edge of human-robot interaction, about the reality and potential of humans and robots working side-by-side.TC Sessions: Robotics 2022TC Sessions: Robotics 2022",Rodney Brooks and Clara Vu will discuss human-robot interaction at TC Sessions: Robotics 2022
100,1,1_plastic_robots_robot_robotics,https://nypost.com/2022/10/26/terrifying-video-shows-chinese-robot-attack-dog-with-machine-gun-dropped-by-drone/,"A Chinese military contractor created a video showing off its terrifying new military technology, revealing a robot attack dog that can be dropped off by a drone.The video that was initially released on the verified Weibo account of “Kestrel Defense Blood-Wing,” a page affiliated with a Chinese defense contractor, shows a drone hovering over a building and then dropping off a robot on the roof. After the drone flies away, the robot gets up on four legs and then begins to scan for targets around the building with what appears to be some sort of automatic weapons attached to its back.According to a report from WarZone, the weapon mounted on the robot dog is possibly a Chinese QBB-97 light machine gun, which is capable of firing 650 rounds per minute at an effective range of 400 meters.A description of the robot posted by Kestrel Defense Blood-Wing and automatically translated to English boasts that the weapon can easily “launch a surprise attack.”“War dogs descending from the sky, air assault, Red Wing Forward heavy-duty drones deliver combat robot dogs, which can be directly inserted into the weak link behind the enemy to launch a surprise attack or can be placed on the roof of the enemy to occupy the commanding heights to suppress firepower. And ground troops [can] conduct a three-dimensional pincer attack on the enemy in the building,” the description reads.Video shows a drone hovering over a building and then dropping off a robot on the roof. Blood-WingPrevious 1 of 4 Next The weapon mounted on the robot dog is possibly a Chinese QBB-97 light machine gun. The robot gets up on four legs and then begins to scan for targets around the building. Red Wing Forward heavy-duty drones deliver combat robot dogs. The machine is capable of firing 650 rounds per minute at an effective range of 400 meters.The Chinese technology and other similar weapons have so far been designed to be operated by a human at the controls, though military analysts fear that systems in which robots designed to operate autonomously are in development and could soon be deployed on battlefields.Autonomous weapons systems would be particularly deadly for opposing forces, with the military that develops them being able to drop them deep behind enemy lines into areas that had previously been too difficult to reach or too dangerous to deploy human soldiers.",Terrifying video shows Chinese robot attack dog with machine gun dropped by drone
315,1,1_plastic_robots_robot_robotics,https://www.cbsnews.com/news/robots-pick-strawberries-california/,"California produces about 90% of the nation's strawberries, but severe drought and worker shortages are threatening the fruit. One company is hoping to change that with the power of robots.Eric Adamson's company is behind a strawberry robotic revolution. He said they're programmed to think on their own, with cameras that sense texture and color.""People think robots have been around forever, but they're actually very, very new, especially robots that make decisions and are autonomous,"" Adamson said.They work in a hydroponic field, which is a type of farming that can use up to 90% less water than traditional methods.As good as they are, though, they're hardly foolproof.""We expect we'll make mistakes and we'll expect things will break,"" he said.Adamson said the robots pick with 95% accuracy.And it's not just the robots that are learning. Jeanpol Rodriguez, who used to work in the fields, now manages the robots picking strawberries. He said he didn't know anything about robotics before entering this new role.""The robot is doing the job. I'm like — I'm cool!"" Rodriguez told CBS News.Adamson said this is a way that ""we can create jobs with higher wages and with higher skill development.""Adamson said his goal is to expand beyond just strawberry picking.""We hope to have hundreds and hundreds of robots around the world's leading farms, picking table grapes, peppers, cucumbers, blackberries, raspberries,"" he told CBS News.","""The robot is doing the job"": Robots help pick strawberries in California amid drought, labor shortage"
348,1,1_plastic_robots_robot_robotics,https://www.engadget.com/smart-buoys-protect-whales-211616043.html,"Whales face numerous threats from humans, not the least of which are ship collisions — the World Sustainability Organization estimates 18,000 to 25,000 animals die each year. There may be a technological way to minimize those deaths, however. Reuters reports Chile's government and the MERI Foundation have deployed the first smart buoy from the Blue Boat Initiative, an effort to both safeguard whales and track undersea ecosystems. The device, floating in the Gulf of Corcovado 684 miles away from Chile, alerts ships to nearby blue, humpback, right and sei whales to help avoid incidents.The technology uses oceanographic sensors and AI-powered Listening to the Deep Ocean Environment (LIDO) software to determine a waterborne mammal's type and location. It also checks the ocean's health by monitoring oxygen levels, temperature and other criteria. That extra data could help study climate change and its impact on sea life.The Blue Boat Initiative currently aims to install six or more buoys to protect whales across the gulf. In the long term, though, project members hope to blanket the whales' complete migratory route between Antarctica and the equator. This could reduce collisions across the creatures' entire habitat, not to mention better inform government decisions about conservation and the environment.The technology may be as important for humans as for the whales. On top of their roles in delicately balanced ecosystems, whales both help capture CO2 and redistribute heat through ocean currents. The more these animals are allowed to flourish, the better the ocean is at limiting global warming and its harmful effects.",Smart buoy 'hears' the sea to protect whales against ship collisions
102,1,1_plastic_robots_robot_robotics,https://petapixel.com/2022/10/04/white-house-releases-blueprint-for-artificial-intelligence-bill-of-rights/,"On behalf of President Joe Biden, the White House has released five principles that it believes should guide the design, use, and deployment of automated systems to protect the American public in the age of artificial intelligence (AI).Called The Blueprint for an AI Bill of Rights, the White House hopes that it will serve as a guide to protect people from real threats to society that are caused by leaning heavily on automated, AI-driven systems.“Automated systems have brought about extraordinary benefits, from technology that helps farmers grow food more efficiently and computers that predict storm paths, to algorithms that can identify diseases in patients. These tools now drive important decisions across sectors, while data is helping to revolutionize global industries,” the Office of Science and Technology Policy writes.“[But] this important progress must not come at the price of civil rights or democratic values.”The White House outlines five principles that it believes should be used as a guideline whenever automated systems have the potential to meaningfully impact the public’s rights, opportunities or access to critical needs: Safe and Effective Systems, Algorithmic Discrimination Protections, Data Privacy, Notice and Explanation, and Human Alternatives, Consideration, and Fallback.This AI Bill of Rights is supported by the American Civil Liberties Union, which published a statement in conjunction with and in support of its announcement.“Just as our Constitution’s Bill of Rights protects our most basic civil rights and liberties from the government, in the 21st century, we need a ‘bill of rights’ to protect us against the use of faulty and discriminatory artificial intelligence that infringes upon our core rights and freedoms,” ReNika Moore, director of the American Civil Liberties Union’s Racial Justice Program, says.“Unchecked, artificial intelligence exacerbates existing disparities and creates new roadblocks for already-marginalized groups, including communities of color and people with disabilities. When AI is developed or used in ways that don’t adequately take into account existing inequities or is used to make decisions for which it is inappropriate, we see real-life harms such as biased, harmful predictions leading to the wrongful arrest of Black people, jobs unfairly denied to women, and disparate targeting of children of color for removal from their families. The Blueprint for an AI Bill of Rights is an important step in addressing the harms of AI.”The Blueprint for an Artificial Intelligence Bill of Rights is designed to cover all AI-driven technology and automated systems that have the potential to impact the American public’s civil rights and liberties, equal opportunity, or access to critical resources or services. All five principles are described in detail on the White House website. The full blueprint can also be downloaded from the government website along with a guide on how to apply it to varying AI solutions.Image credits: Header photo licensed via Depositphotos.",White House Releases Blueprint for Artificial Intelligence Bill of Rights
383,1,1_plastic_robots_robot_robotics,https://www.globalconstructionreview.com/scientists-can-now-build-structures-with-swarms-of-flying-drones/,"“We’ve proved that drones can work autonomously and in tandem to construct and repair buildings, at least in the lab,” said research leader Prof. Mirko Kovac. “Our solution is scalable and could help us to construct and repair buildings in difficult-to-reach areas in the future.”",Scientists can now build structures with swarms of flying drones
159,1,1_plastic_robots_robot_robotics,https://techcrunch.com/2018/06/25/darpa-ground-x-wheel-design/,"As part of its Ground X-Vehicle Technologies program, DARPA is showcasing some new defense vehicle tech that’s as futuristic as it is practical. One of the innovations, a reconfigurable wheel-track, comes out of Carnegie Mellon University’s National Robotics Engineering Center in partnership with DARPA. The wheel-track is just one of a handful of designs meant to improve survivability of combat vehicles beyond just up-armoring them.As you can see in the video, the reconfigurable wheel-track demonstrates a seamless transition between a round wheel shape and a triangular track in about two seconds, and the shift between its two modes can be executed while the vehicle is in motion without cutting speed. Round wheels are optimal for hard terrain while track-style treads allow an armored vehicle to move freely on softer ground.According to Ground X-Vehicle Program Manager Major Amber Walker, the tech offers “instant improvements to tactical mobility and maneuverability on diverse terrains” — an advantage you can see on display in the GIF below.While wheel technology doesn’t sound that exciting, the result is visually impressive and smooth enough to prompt a double-take.The other designs featured in the video are noteworthy as well, with one offering a windowless navigation technology called Virtual Perspectives Augmenting Natural Experiences (V-PANE) that integrates video from an array of mounted LIDAR and video cameras to recreate a real-time model of a windowless vehicle’s surroundings. Another windowless cockpit design creates “virtual windows” for a driver, with 3D goggles for depth enhancement, head-tracking and wraparound window display screens displaying data outside the all-terrain vehicle in real time.",DARPA design shifts round wheels to triangular tracks in a moving vehicle
88,1,1_plastic_robots_robot_robotics,https://news.mit.edu/2022/microparticles-oscillating-current-1013,"Taking advantage of a phenomenon known as emergent behavior in the microscale, MIT engineers have designed simple microparticles that can collectively generate complex behavior, much the same way that a colony of ants can dig tunnels or collect food.Working together, the microparticles can generate a beating clock that oscillates at a very low frequency. These oscillations can then be harnessed to power tiny robotic devices, the researchers showed.“In addition to being interesting from a physics point of view, this behavior can also be translated into an on-board oscillatory electrical signal, which can be very powerful in microrobotic autonomy. There are a lot of electrical components that require such an oscillatory input,” says Jingfan Yang, a recent MIT PhD recipient and one of the lead authors of the new study.The particles used to create the new oscillator perform a simple chemical reaction that allows the particles to interact with each other through the formation and bursting of tiny gas bubbles. Under the right conditions, these interactions create an oscillator that behaves similar to a ticking clock, beating at intervals of a few seconds.“We're trying to look for very simple rules or features that you can encode into relatively simple microrobotic machines, to get them to collectively do very sophisticated tasks,” says Michael Strano, the Carbon P. Dubbs Professor of Chemical Engineering at MIT.Strano is the senior author of the new paper, which appears today in Nature Communications. Along with Yang, Thomas Berrueta, a Northwestern University graduate student advised by Professor Todd Murphey, is a lead author of the study.Collective behaviorDemonstrations of emergent behavior can be seen throughout the natural world, where colonies of insects such as ants and bees accomplish feats that a single member of the group would never be able to achieve.“Ants have minuscule brains and they do very simple cognitive tasks, but collectively they can do amazing things. They can forage for food and build these elaborate tunnel structures,” Strano says. “Physicists and engineers like myself want to understand these rules because it means we can make tiny things that collectively do complex tasks.”In this study, the researchers wanted to design particles that could generate rhythmic movements, or oscillations, with a very low frequency. Until now, building low-frequency micro-oscillators has required sophisticated electronics that are expensive and difficult to design, or specialized materials with complex chemistries.The simple particles that the researchers designed for this study are discs as small as 100 microns in diameter. The discs, made from a polymer called SU-8, have a platinum patch that can catalyze the breakdown of hydrogen peroxide into water and oxygen.When the particles are placed at the surface of a droplet of hydrogen peroxide on a flat surface, they tend to travel to the top of the droplet. At this liquid-air interface, they interact with any other particles found there. Each particle produces its own tiny bubble of oxygen, and when two particles come close enough that their bubbles interact, the bubbles pop, propelling the particles away from each other. Then, they begin forming new bubbles, and the cycle repeats over and over.“One particle by itself stays still and doesn’t do anything interesting, but through teamwork, they can do something pretty amazing and useful, which is actually a difficult thing to achieve at the microscale,” Yang says.",Tiny particles work together to do big things
118,1,1_plastic_robots_robot_robotics,https://scitechdaily.com/for-the-first-time-a-robot-has-learned-to-imagine-itself/,"A robot created by Columbia Engineers learns to understand itself rather than the environment around it.Our perception of our bodies is not always correct or realistic, as any athlete or fashion-conscious person knows, but it’s a crucial factor in how we behave in society. Your brain is continuously preparing for movement while you play ball or get dressed so that you can move your body without bumping, tripping, or falling.Humans develop our body models as infants, and robots are starting to do the same. A team at Columbia Engineering revealed today that they have developed a robot that, for the first time, can learn a model of its whole body from scratch without any human aid. The researchers explain how their robot built a kinematic model of itself in a recent paper published in Science Robotics, and how it utilized that model to plan movements, accomplish objectives, and avoid obstacles in a range of scenarios. Even damage to its body was automatically detected and corrected.The robot watches itself like an infant exploring itself in a hall of mirrorsThe researchers placed a robotic arm within a circle of five streaming video cameras. The robot watched itself through the cameras as it undulated freely. Like an infant exploring itself for the first time in a hall of mirrors, the robot wiggled and contorted to learn how exactly its body moved in response to various motor commands. After about three hours, the robot stopped. Its internal deep neural network had finished learning the relationship between the robot’s motor actions and the volume it occupied in its environment.“We were really curious to see how the robot imagined itself,” said Hod Lipson, professor of mechanical engineering and director of Columbia’s Creative Machines Lab, where the work was done. “But you can’t just peek into a neural network, it’s a black box.” After the researchers struggled with various visualization techniques, the self-image gradually emerged. “It was a sort of gently flickering cloud that appeared to engulf the robot’s three-dimensional body,” said Lipson. “As the robot moved, the flickering cloud gently followed it.” The robot’s self-model was accurate to about 1% of its workspace.A technical summary of the study. Credit: Columbia EngineeringSelf-modeling robots will lead to more self-reliant autonomous systemsThe ability of robots to model themselves without being assisted by engineers is important for many reasons: Not only does it save labor, but it also allows the robot to keep up with its own wear-and-tear, and even detect and compensate for damage. The authors argue that this ability is important as we need autonomous systems to be more self-reliant. A factory robot, for instance, could detect that something isn’t moving right, and compensate or call for assistance.“We humans clearly have a notion of self,” explained the study’s first author Boyuan Chen, who led the work and is now an assistant professor at Duke University. “Close your eyes and try to imagine how your own body would move if you were to take some action, such as stretch your arms forward or take a step backward. Somewhere inside our brain we have a notion of self, a self-model that informs us what volume of our immediate surroundings we occupy, and how that volume changes as we move.”Self-awareness in robotsThe work is part of Lipson’s decades-long quest to find ways to grant robots some form of self-awareness. “Self-modeling is a primitive form of self-awareness,” he explained. “If a robot, animal, or human, has an accurate self-model, it can function better in the world, it can make better decisions, and it has an evolutionary advantage.”The researchers are aware of the limits, risks, and controversies surrounding granting machines greater autonomy through self-awareness. Lipson is quick to admit that the kind of self-awareness demonstrated in this study is, as he noted, “trivial compared to that of humans, but you have to start somewhere. We have to go slowly and carefully, so we can reap the benefits while minimizing the risks.”Reference: “Fully body visual self-modeling of robot morphologies” by Boyuan Chen, Robert Kwiatkowski, Carl Vondrick and Hod Lipson, 13 July 2022, Science Robotics.DOI: 10.1126/scirobotics.abn1944The study was funded by the Defense Advanced Research Projects Agency, the National Science Foundation, Facebook, and Northrop Grumman.The authors declare no financial or other conflicts of interest.",A swarm of tiny robots could soon brush and floss your teeth for you
372,1,1_plastic_robots_robot_robotics,https://www.euronews.com/next/2022/10/06/amazon-and-alphabet-unit-wing-begin-drone-deliveries-in-parts-of-the-us,"A drone lands and drops off a bag of snacks in the backyard of a Texas house. The bottle of soda has condensation beads on it – it’s still chilled.Drone deliveries have finally become a reality in parts of the United States, and for some users, they arrive in less than 15 minutes.Several drone delivery services including Amazon, Alphabet-owned Wing and Israeli start-up Flytrex have started operating in the country after receiving a green light from the Federal Aviation Administration.Wing says it delivers by drone in about 10 minutes and that its services can help reduce road congestion.""It's more environmentally friendly. These are all electric drones that use very little power,"" said Jacob Demmitt, Wing's marketing and communication manager in the US.""It's also significantly cheaper. You can have a pilot sitting in downtown Dallas, watching flights all over the entire metro (area) someday"".Tiffany Bokhari lives in Frisco, Texas. She first tested Wing's drone delivery app out of curiosity, and she did it again when she needed a band-aid.It's now the third time a drone brings her the light goods she ordered from her smartphone – in this case, snacks and that cold soda.""I think it's better to be delivered by drone versus car because it's helpful for the environment, you know. It saves gas, and money, and saves the environment,"" she said.For now, Wing’s 4.5 kg drone can load food and small products weighing up to 1 kg.Supporters of airborne drop-offs claim that electric drones are more efficient and safer than conventional deliveries done by fossil fuel-powered cars.""Drones don't get tired. They don't try to text while driving. They don't drink and drive…You just get much better service,"" Flytrex CEO Yariv Bash told AFP.For more on this story, watch the video in the media player above.",Amazon and Alphabet unit Wing begin drone deliveries in parts of the US
294,1,1_plastic_robots_robot_robotics,https://www.axios.com/2022/10/06/boston-dynamics-pledges-weaponize-robots,"Several robotics companies, including Boston Dynamics, are pledging not to support the weaponization of their products and are calling for others in the industry to do the same, according to a letter shared first with Axios.Why it matters: Robots, like drones before them, have a wide range of peaceful and even life-saving uses, but can be turned into war-fighting machines, too.Details: The open letter highlights the erosion of consumer trust in robots as among the reasons not to allow them to be used as weapons.""We believe that adding weapons to robots that are remotely or autonomously operated, widely available to the public, and capable of navigating to previously inaccessible locations where people live and work, raises new risks of harm and serious ethical issues,"" the companies said in the letter.The companies pledged not to add weapons technology themselves or to support others doing so. And ""when possible"" they said they will review customers' plans in hopes of avoiding those who would turn the robots into weapons, in addition to exploring technical features that could prevent such use.In addition to Boston Dynamics, five other firms signed on to the commitment: Agility Robotics, ANYbotics, Clearpath Robotics, Open Robotics and Unitree Robotics.What they're saying: Boston Dynamics CEO Robert Playter said, in an e-mailed statement: ""We are concerned about recent increases in makeshift efforts by individuals attempting to weaponize commercially available robots... For this technology to be broadly accepted throughout society, the public needs to know they can trust it. And that means we need policy that prohibits bad actors from misusing it.""Go deeper: Here's the full text of the letter:An Open Letter to the Robotics Industry and our Communities,General Purpose Robots Should Not Be WeaponizedWe are some of the world’s leading companies dedicated to introducing new generations of advanced mobile robotics to society. These new generations of robots are more accessible, easier to operate, more autonomous, affordable, and adaptable than previous generations, and capable of navigating into locations previously inaccessible to automated or remotely-controlled technologies. We believe that advanced mobile robots will provide great benefit to society as co-workers in industry and companions in our homes.As with any new technology offering new capabilities, the emergence of advanced mobile robots offers the possibility of misuse. Untrustworthy people could use them to invade civil rights or to threaten, harm, or intimidate others. One area of particular concern is weaponization. We believe that adding weapons to robots that are remotely or autonomously operated, widely available to the public, and capable of navigating to previously inaccessible locations where people live and work, raises new risks of harm and serious ethical issues. Weaponized applications of these newly-capable robots will also harm public trust in the technology in ways that damage the tremendous benefits they will bring to society. For these reasons, we do not support the weaponization of our advanced-mobility general-purpose robots. For those of us who have spoken on this issue in the past, and those engaging for the first time, we now feel renewed urgency in light of the increasing public concern in recent months caused by a small number of people who have visibly publicized their makeshift efforts to weaponize commercially available robots.We pledge that we will not weaponize our advanced-mobility general-purpose robots or the software we develop that enables advanced robotics and we will not support others to do so. When possible, we will carefully review our customers’ intended applications to avoid potential weaponization. We also pledge to explore the development of technological features that could mitigate or reduce these risks. To be clear, we are not taking issue with existing technologies that nations and their government agencies use to defend themselves and uphold their laws.We understand that our commitment alone is not enough to fully address these risks, and therefore we call on policymakers to work with us to promote safe use of these robots and to prohibit their misuse. We also call on every organization, developer, researcher, and user in the robotics community to make similar pledges not to build, authorize, support, or enable the attachment of weaponry to such robots. We are convinced that the benefits for humanity of these technologies strongly outweigh the risk of misuse, and we are excited about a bright future in which humans and robots work side by side to tackle some of the world’s challenges.Signed,Boston DynamicsAgility RoboticsANYboticsClearpath RoboticsOpen RoboticsUnitree Robotics",Exclusive: Boston Dynamics pledges not to weaponize its robots
108,1,1_plastic_robots_robot_robotics,https://phys.org/news/2022-10-scientists-material-plastic-metal.html,"A group of scientists with the University of Chicago have discovered a way to create a material that can be made like a plastic, but conducts electricity more like a metal. Above, members of the Anderson lab at work. Credit: John Zich/University of ChicagoScientists with the University of Chicago have discovered a way to create a material that can be made like a plastic, but conducts electricity more like a metal.The research, published Oct. 26 in Nature, shows how to make a kind of material in which the molecular fragments are jumbled and disordered, but can still conduct electricity extremely well.This goes against all of the rules we know about for conductivity—to a scientist, it's kind of like seeing a car driving on water and still going 70 mph. But the finding could also be extraordinarily useful; if you want to invent something revolutionary, the process often first starts with discovering a completely new material.""In principle, this opens up the design of a whole new class of materials that conduct electricity, are easy to shape, and are very robust in everyday conditions,"" said John Anderson, an associate professor of chemistry at the University of Chicago and the senior author on the study. ""Essentially, it suggests new possibilities for an extremely important technological group of materials,"" said Jiaze Xie (Ph.D. '22, now at Princeton), the first author on the paper.'There isn't a solid theory to explain this'Conductive materials are absolutely essential if you're making any kind of electronic device, whether it be an iPhone, a solar panel, or a television. By far the oldest and largest group of conductors is the metals: copper, gold, aluminum. Then, about 50 years ago, scientists were able to create conductors made out of organic materials, using a chemical treatment known as ""doping,"" which sprinkles in different atoms or electrons through the material.This is advantageous because these materials are more flexible and easier to process than traditional metals, but the trouble is they aren't very stable; they can lose their conductivity if exposed to moisture or if the temperature gets too high.But fundamentally, both of these organic and traditional metallic conductors share a common characteristic. They are made up of straight, closely packed rows of atoms or molecules. This means that electrons can easily flow through the material, much like cars on a highway. In fact, scientists thought a material had to have these straight, orderly rows in order to conduct electricity efficiently.Then Xie began experimenting with some materials discovered years ago, but largely ignored. He strung nickel atoms like pearls into a string of of molecular beads made of carbon and sulfur, and began testing.To the scientists' astonishment, the material easily and strongly conducted electricity. What's more, it was very stable. ""We heated it, chilled it, exposed it to air and humidity, and even dripped acid and base on it, and nothing happened,"" said Xie. That is enormously helpful for a device that has to function in the real world.But to the scientists, the most striking thing was that the molecular structure of the material was disordered. ""From a fundamental picture, that should not be able to be a metal,"" said Anderson. ""There isn't a solid theory to explain this.""Xie, Anderson, and their lab worked with other scientists around the university to try to understand how the material can conduct electricity. After tests, simulations, and theoretical work, they think that the material forms layers, like sheets in a lasagna. Even if the sheets rotate sideways, no longer forming a neat lasagna stack, electrons can still move horizontally or vertically—as long as the pieces touch.The end result is unprecedented for a conductive material. ""It's almost like conductive Play-Doh—you can smush it into place and it conducts electricity,"" Anderson said.The scientists are excited because the discovery suggests a fundamentally new design principle for electronics technology. Conductors are so important that virtually any new development opens up new lines for technology, they explained.One of the material's attractive characteristics is new options for processing. For example, metals usually have to be melted in order to be made into the right shape for a chip or device, which limits what you can make with them, since other components of the device have to be able to withstand the heat needed to process these materials.The new material has no such restriction because it can be made at room temperatures. It can also be used where the need for a device or pieces of the device to withstand heat, acid or alkalinity, or humidity has previously limited engineers' options to develop new technology.The team is also exploring the different forms and functions the material might make. ""We think we can make it 2D or 3D, make it porous, or even introduce other functions by adding different linkers or nodes,"" said Xie.More information: John Anderson, Intrinsic glassy-metallic transport in an amorphous coordination polymer, Nature (2022). DOI: 10.1038/s41586-022-05261-4. www.nature.com/articles/s41586-022-05261-4 Journal information: Nature",Scientists discover material that can be made like a plastic but conducts like a metal
338,1,1_plastic_robots_robot_robotics,https://www.collisionrepairmag.com/ev-av-report-loblaws-deploys-completely-driverless-trucks-in-toronto-magna-takes-its-slice-of-the-autonomous-pizza/,"Toronto, Ontario — In this weekly electric and autonomous vehicle report, a grocery giant drops driverless trucks in Toronto; Magna deploys autonomous pizza delivery robots and the federal government awards more money to Quebec mining efforts.Hey, who’s driving that truck?In last week’s EV/AV Report, we covered Loblaws’ partnership with autonomous vehicle company Gatnik. Together, the two companies have deployed a fleet of autonomous delivery trucks across the Greater Toronto Area.The trucks were initially operating with a safety driver behind the wheel⁠—but that precaution has since been removed, in what the partners have deemed “a first for Canada.”Gatnik claims that its delivery vehicles⁠ now operate at Level 4 autonomy, just shy of the almighty autonomous Level 5 goal.The companies have been allowed to operate driverless vehicles on public roads thanks to a pilot project enacted by the Ontario government in 2016.Pizza sans peopleMagna International has taken its slice of the metaphorical autonomous pizza.The Aurora, Ont.-based auto parts supplier has unveiled a new pilot program⁠—a pizza delivery robot⁠—making its way around Detroit.The new micro-mobility solution offers retailers end-to-end, last-mile autonomous delivery opportunities around both restaurant orders and e-commerce products, though Magna sees more potential for the autonomous delivery system.“Expanding into the growing world of new mobility is a key part of our ‘Go Forward’ strategy that takes Magna beyond its existing technical strength in automotive and vehicle systems, and into entirely new markets and business models,” said Matteo Del Sorbo, executive vice president for Magna International and the global lead for Magna New Mobility.Earmarked for EVsThe federal government announced a $222 million investment last week, with the funds allotted to Rio Tinto’s efforts to increase the production of critical minerals for EVs and batteries.Prime Minister Justin Trudeau said the funds derive from the Strategic Innovation Fund and will allow the company to increase the production of critical minerals such as lithium, titanium and scandium.Rio Tinto has pledged to reduce its greenhouse gas emissions by 70 percent⁠—the equivalent of removing 145,000 cars from the road, it said. The company also plans to quadruple its production of scandium, which can be used in EVs; titanium and lithium, used in electric car batteries.",EV/AV Report: Loblaws deploys completely driverless trucks in Toronto; Magna takes its slice of the autonomous pizza
322,1,1_plastic_robots_robot_robotics,https://www.cnbc.com/2022/10/02/how-deere-plans-to-build-a-world-of-fully-autonomous-farming-by-2030.html,"Deere's autonomous 8R is the culmination of nearly two decades of work in automation, data analytics, GPS-guidance, internet-of-things connectivity and software engineering.Can John Deere become one of the leading AI and robotics companies in the world alongside Tesla and Silicon Valley technology giants over the next decade?That notion may seem incongruous with the general perception of the 185-year-old company as a heavy-metal manufacturer of tractors, bulldozers and lawnmowers painted in the signature green and yellow colors.But that is what the company sees in its future, according to Jorge Heraud, vice president of automation and autonomy for Moline, Illinois-based Deere, a glimpse of which was showcased at last January's Consumer Electronics Show in Las Vegas, where Deere unveiled its fully autonomous 8R farm tractor, driven by artificial intelligence rather than a farmer behind the wheel.The autonomous 8R is the culmination of Deere's nearly two decades of strategic planning and investment in automation, data analytics, GPS guidance, internet-of-things connectivity and software engineering. While a good deal of that R&D has been homegrown, the company also has been on a spree of acquisitions and partnerships with agtech startups, harvesting know-how as well as talent.""This comes from our realization that technology is going to drive value creation and increase productivity, profitability and sustainability for farmers,"" Heraud said.While Deere made a big splash at CES and intrigued the investment community, Stephen Volkmann, equity research analyst at Jefferies, said, ""We are very, very, very early in this process.""""The total global fleet of autonomous Deere tractors is less than 50 today,"" he added. And even though Deere's goal is to have a fully autonomous farming system for row crops in place by 2030, Volkmann said, ""in Wall Street time, that's an eternity.""For the time being, Deere is creating value and profits with well-established automated systems that can be retrofitted to its existing tractors, such as GPS-based self-steering and precision seeding that measures how deep and far apart to plant. Those steps have to be in place, Volkmann said, before you can put full autonomy around them.The autonomous 8R represents a giant leap in current agtech, not to mention the marketing benefit. ""Prior to its introduction at CES, everybody thought [full autonomy] was pie in the sky,"" said Scott Shearer, chair of the department of food, agricultural and biological engineering at Ohio State University.Around the world, Shearer said, there are probably 30 different autonomous tractor projects in the works, though none are commercially available. ""But when Deere, with 60% of the tractor market share in North America, comes out with one, that's when reality sets in,"" Shearer said.That reality reflects Deere's autonomy strategy. ""The AI we use involves computer vision and machine learning,"" Heraud said, science that was well underway at Silicon Valley startup Blue River Technology, which Deere bought in 2017 for $305 million — a deal that also brought on Blue River co-founder and CEO Heraud. Blue River's ""see and spray"" robotics platform utilizes dozens of sophisticated cameras and processors to distinguish weeds from crop plants when applying herbicides.Attached to the autonomous tractor are six pairs of stereo cameras that can ""see"" an obstacle in the field — whether it's a rock, a log or a person — and determine its size and relative distance. Images captured by the cameras are passed through a deep neural network that classifies each pixel in approximately 100 milliseconds and decides whether the tractor should keep moving or stop.""We've curated hundreds of thousands of images from different farm locations and under various weather and lighting conditions,"" Heraud said, ""so that with machine learning, the tractor can understand what it's seeing and react accordingly. This capability also allows the farmer, instead of being in the tractor, to operate it remotely while doing something else.""Heraud was referring to autonomous driving, another piece of Deere's agtech puzzle that came together when it purchased Bear Flag Robotics last year for $250 million. Also a Silicon Valley startup, launched in 2017, Bear Flag's autonomous navigation system can be retrofitted onto existing tractors. In the case of Deere's 8R model, a tractor which first went on the market in 2020, the latest version with autonomous capabilities uses technology from Blue River.Since the CES rollout, Deere has acquired AI assets from two other agtech pioneers. In April, Deere formed a joint venture with GUSS Automation, which has devised semi-autonomous orchard and vineyard sprayers. Using AI and IoT, multiple GUSS (Global Unmanned Spray System) sprayers can be remotely controlled by a single operator, running up to eight sprayers simultaneously from a laptop. GUSS can detect trees and determine how much to spray on each one, regardless of height or canopy size.A month later, Deere announced the acquisition of numerous patents and other intellectual property from AI startup Light, according to The Robot Report. Light's depth-perception platform improves upon existing stereo-vision systems by using additional cameras, mimicking the structure of a human eye to enable more accurate 3D vision. Deere plans to integrate Light's platform into future versions of its autonomous farm equipment.To keep a close eye on other agtech R&D, Deere has established a Startup Collaborator program to test innovative technologies with customers and dealers without a more formal business relationship. ""The hope is that they find the diamonds before they become obvious to [competitors] and keep them in the fold,"" Volkmann said. Among the current crop are Four Growers, a Pittsburgh-based startup providing robotic harvesting and analytics for high-value crops, starting with greenhouse tomatoes, and Philadelphia-based Burro, which is producing small, autonomous robots that can assist farm workers with various conveyance tasks.Not surprisingly, Deere's biggest competitors have been developing automation and autonomy for its farm machinery, too. AGCO, whose brands include Massey Ferguson and Fendt, ""has been automating farming operations since the mid-1990s,"" said Seth Crawford, senior vice president and general manager of the Duluth, Georgia-based company's precision agriculture and digital division. ""We're at a stage we call supervised autonomy, where we still have someone in the cab of the machine,"" he said. ""The buzz is around fully autonomous operations, but where farmers are willing to pay for automation is feature by feature.""Whereas Deere is focused on adding full autonomy to its own farm equipment, AGCO is eying the wider retrofit market, Crawford said. ""In summer 2023, we'll have a performance-enhancing retrofit kit available for multiple brands of machines,"" he said. ""Where others say we bring you autonomy with a half-million-dollar tractor,"" he said, alluding to the price tag of Deere's 8R, ""we have kits that allow you to do that with your existing fleet. We see a huge opportunity with the installed base, where farmers want to adopt technology to enhance their outcomes, and yet don't want to flip their entire fleet and make that massive investment.""In 2016, Case IH, a subsidiary of CNH Industrial, headquartered in London, rolled up to the Farm Progress Show with what it called the Autonomous Concept Vehicle. The sleek prototype tractor, minus a driver's cab, hinted at the view of autonomy at the time. Fast forward six years, to September's Farm Progress Show, where Case IH unveiled its Trident 5550 autonomous applicator.",How John Deere plans to build a world of fully autonomous farming by 2030
558,1,1_plastic_robots_robot_robotics,https://www.vox.com/recode/2022/10/13/23402334/weaponized-robots-boston-dynamics-drones,"A Twitter post featuring a video of a robot dog firing a gun that’s racked up nearly 120,000 likes since July. Videos of Ukrainian soldiers apparently modifying off-the-shelf drones to airdrop weapons. An art project featuring Spot, the Boston Dynamics robot most known for viral dancing videos, outfitted with a paintball gun.These kinds of videos are all over the internet. They demonstrate the kind of scary scenarios that six top robot manufacturers, including Boston Dynamics, probably had in mind when they published a letter last week promising not to weaponize their products. While robots are becoming increasingly accessible to consumers, these companies warned, people might try to turn them into weapons meant to harm people. To prevent this from happening, the companies promised to review what customers want to do with their commercial robots before selling them (“when possible”) and to look into developing technologies that might reduce the risk of this happening in the first place.Sign up for the newsletter Recode Discover how the digital world is changing — and changing us — with Recode’s weekly newsletter. Thanks for signing up! Check your inbox for a welcome email. Email (required) Oops. Something went wrong. Please enter a valid email and try again. By submitting your email, you agree to our Terms and Privacy Notice . You can opt out at any time. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. For more newsletters, check out our newsletters page Subscribe“[W]e do not support the weaponization of our advanced-mobility general-purpose robots,” the companies wrote. “[W]e now feel renewed urgency in light of the increasing public concern in recent months caused by a small number of people who have visibly publicized their makeshift efforts to weaponize commercially available robots.”The robots available to the general public are still somewhat expensive, and they’re not as common as the other kinds of commercial technologies that people can buy (namely, drones). Still, this letter serves as a reminder that the risk of weaponization isn’t exactly zero, and that it’s an issue that robot makers are already worried about. At the same time, though, these companies left plenty of caveats in their declaration, and they’ve kept the door open to continue selling robots to law enforcement and the military. They’re also far from the only manufacturers making these kinds of technologies, which are slowly entering the mainstream.“You don’t necessarily want to be seen by the public as producing a good and then intentionally having it be used for military purposes,” explains Erik Lin-Greenberg, a professor who studies how emerging military technologies impact conflict at MIT. “Whether or not that kind of statement is actually going to have any impact on how these systems are being used I think is another question. These are essentially just off-the-shelf technologies.”An international campaign called Stop Killer Robots has urged people to push back against the development of autonomous weapons, and has highlighted how racism, sexism, and dehumanization can be built into these technologies. One former New York City Council member, Ben Kallos, proposed a law banning police from acquiring any kind of armed robot last year after the New York Police Department started trialing a Boston Dynamics robot. (That pilot was called off after backlash.) The Electronic Frontier Foundation, a digital rights organization, has called for banning law enforcement from using autonomous or remote-controlled robots.Even some unconventional efforts have caught the attention of robot manufacturers. In 2021, an art collective called MSCHF purchased a Spot, Boston Dynamics’ nearly $75,000 robot dog, for a demonstration the group called “Spot’s Rampage.” The project involved attaching a paint gun to the robot and then inviting people from around the world to remotely operate the “weapon.” Though the paintball gun-armed robot eventually broke down, Boston Dynamics — which has offered the same robot model to police departments and militaries in the past — was not happy, and said the project misrepresented how its robot “is being used to benefit our daily lives.”Though off-the-shelf robots are still somewhat rare, drones have become more commonplace, and demonstrate how consumer technologies can be weaponized. Amid the war in Ukraine, some soldiers have turned to off-the-shelf drones and used them to drop ammunition, including grenades and weapons meant to target tanks. Cartels in Mexico have similarly used drones to carry and detonate explosives. Terrorist groups and other non-state actors can also retrofit these relatively simple technologies to their advantage, explains Kerry Chávez, an instructor at Texas Tech University and a project administrator at the university’s peace, war, and social conflicts laboratory.“A lot of them are just the hobbyist and commercial models, even some homemade ones,” Chávez told Recode. “Even if you cut off a supply chain from one vector, they can just activate another one.”The US Bureau of Alcohol, Tobacco, Firearms, and Explosives did not respond to a request for comment as to how common it knows weaponizing drones to be in the US, but we do know that it’s happened. In 2015, an 18-year-old in Connecticut stirred global outrage, and an investigation, after he fired a handgun attached to a homemade drone. In 2020, a Pennsylvania man was sentenced to prison after, among other crimes, using a drone to drop explosives to “terrorize” a woman that he used to date. The Federal Aviation Administration has pursued legal action in at least one case concerning a weaponized drone. Operating a drone with a dangerous weapon attached is illegal and comes with up to a $25,000 fine, according to agency spokesperson Rick Breitenfeldt.What we’ve already seen with drones might make companies getting ahead of the weaponization of more advanced, consumer robots seem like a good idea. But there are critical caveats. For one thing, the companies seem to acknowledge that they alone won’t be able to stop the misuse of their tech, and they’re already asking the government for help. At the same time, these technologies can still be used for other types of harm, like surveilling people or funneling weapons across borders. Earlier this year, Canadian police caught a drone carrying almost a dozen handguns from the US after it crashed into a tree in the southern part of Ontario.And there’s the biggest caveat of all: These companies limited their pledge to “general-purpose” robots, but noted that “we are not taking issue with existing technologies that nations and their government agencies use to defend themselves and uphold their laws.”This story was first published in the Recode newsletter. Sign up here so you don’t miss the next one!",The race to stop weaponized robots
160,1,1_plastic_robots_robot_robotics,https://techcrunch.com/2018/08/10/new-material-design-stores-energy-like-an-eagle/,"Auxetics are materials that store energy internally rather than bulging out. In this way they can store more energy when squeezed or struck and disperse it more regularly. Historically, however, these materials have had sharp corners that could break easily with enough pressure. Now researchers at Queen Mary University of London and University of Cambridge have discovered a way to use auxetics in a more efficient and less fragile way. In this way you can create systems that store energy and release it mechanically multiple thousands of times.“The exciting future of new materials designs is that they can start replacing devices and robots. All the smart functionality is embedded in the material, for example the repeated ability to latch onto objects the way eagles latch onto prey, and keep a vice-like grip without spending any more force or effort,” said Queen Marry University’s Dr. Stoyan Smoukov. For example, a robot using this system can close its hand over and object and keep it closed until its time to let go. There is no need to continue sending power to the claw or hand until it is time to open up and drop the object.“A major problem for materials exposed to harsh conditions, such as high temperature, is their expansion. A material could now be designed so its expansion properties continuously vary to match a gradient of temperature farther and closer to a heat source. This way, it will be able to adjust itself naturally to repeated and severe changes,” said Eesha Khare, an undergrad who worked on the project.The project used 3D printing to make small clips that grab a toothed actuator. To release the energy, you pull on the opposite sides of the object to release the teeth. While the entire thing looks quite simple the fact that this object stores energy without bulging is important. The same technology can be used to “grab” bullets as they strike armor, resulting in better durability.",New material design stores energy like an eagle
103,1,1_plastic_robots_robot_robotics,https://petapixel.com/how-steve-sasson-invented-the-digital-camera/,"Steve Sasson is an electrical engineer who invented the digital camera while working for Kodak. The Rochester, New York, company, which had made its fortune by selling photographic film and paper for most of the 20th century, did not think that Sasson’s digital camera had any place in photography, and that lack of foresight ironically put Kodak out of business.A Young Engineer Finds a Job at KodakIn July 2022, Kodak announced that it is repurposing some of the expensive, high-tech machines used to manufacture its photography film to produce batteries for electric vehicles (EV).In June 1973, Sasson (born 1950) graduated with a master’s degree from Rensselaer Polytechnic Institute in upstate New York. The same month he landed his first [and last] job at Kodak.Kodak was not typically hiring electrical engineers. They were hiring chemical engineers and mechanical engineers because cameras were, of course, mechanical, and photo processing equipment required those two disciplines. It became pretty clear then that the bulk of the unit cost of manufacturing a consumer camera was going to electronic and electrical components like film advances, exposure controls, and flash controls. All of these were being implemented electronically, putting a new emphasis on hiring electrical engineers.“When I interviewed there, I went to several locations at Kodak,” Sasson tells PetaPixel. “And when I went for my end-of-the-day summary, they asked me which area I was most interested in, and I was interested in the research lab. They took on all kinds of problems and challenges, and of course, I liked that. It was interdisciplinary with mechanical engineers, physicists, and mathematicians with an engaging environment, and that’s why I chose it to start my career.”Playing with a CCD SensorSasson started work at the applied research laboratory in the apparatus division. This laboratory was involved with anything to do with equipment and had a far-ranging charter. It helped resolve technical problems and did research into new ideas.One day Sasson’s boss came up with a work proposal.“He was leaning against the file cabinet in my office while offering me a choice of two projects to work on as I was between jobs,” says Sasson. “He said, ‘you could do modeling for exposure control for XL movie cameras or look at this new charge-coupled device. I’m curious how it works or what we could do with it.’“I was in the electronics group — as I was an electronics engineer. It was a new type of device that we had not worked with before. It had to do with a two-dimensional exposure surface, much like a film, but it was all electronic.“I said I would be interested in working with the charge-coupled device based on my experience at college. He said, ‘Great get one and play with it and see if there is anything useful or what we might be able to do with it’.”A charge-coupled device (CCD) is an integrated circuit containing an array of coupled capacitors — one that, along with CMOS, have become widely used as digital camera imaging sensors.Inventing the Digital Camera in Two YearsSasson went to work feverishly on the CCD and created a crude, largish representation of a digital camera.“I stole the lens from our used parts bin on the manufacturing floor of the XL movie cameras,” explains Sasson. “Because the super eight movie format was larger than the active area of the CCD, I could place this in the film plane of the XL movie camera assembly, and it would work.The camera had only an electronic shutter and no mechanical shutter, with one shutter speed of 1/20s (or 50 milliseconds). It had an infrared (IR) blocking filter in front of it as it was very sensitive to IR, and that was a problem with incandescent lighting.Sasson added a tape assembly, although he had thought of building a memory card to record the image, and it became functional in December of 1975. The digital camera was invented — Sasson had made his major discovery almost within two years of starting work at Kodak.“The reaction I got from Kodak management was one of curiosity and skepticism as it did not feel like a major invention,” says Sasson. “There was not a real feeling that we had invented something. The feeling was that this was a very scary look at what could be possible in the future. As the company’s entire business model was focused around sensitized goods, proposing that they not use any of that was not popular.”Kodak was hesitant to cannibalize its cash cow businesses in film photography with an unproven new technology.“I would walk in with my crazy-looking camera, which was about the size of a toaster, but I could hold it,” Sasson says. “I took pictures of people in the room before I said anything. Then I took out the tape that the digital information was stored on and put it in the playback unit when a picture popped up on the TV screen. And that generally got everybody’s attention.”“Indeed, they didn’t ask me how this worked,” Sasson says. They simply asked me why anybody would want to take a picture this way when there was nothing wrong with conventional photography. Nobody [including his bosses at Kodak] was asking me to develop this camera. In fact, nobody even knew that I was developing this camera.“So, the dialogue in those many meetings in the spring and summer and winter of 1976 centered around whether this could be a viable form of [photography]. If it ever was, how long would it take? What would it take to require it to get there? We probably had some of the most in-depth digital photography discussions in that conference room in 1976.”Kodak management may have been reluctant to dive headfirst into digital photography, but Sasson himself continued down the rabbit hole that would one day become the ubiquitous way photographs are made.“I was fascinated with the idea of filmless photography and that I wouldn’t have to use consumables to take photos. I worked in digital photography from that day in 1975 till I retired. So, I have worked in digital photography longer than anybody, which has motivated me a lot.”The young inventor was asked all the time [at Kodak] when he thought this would be viable for consumers. And, of course, he did not know.“You get desperate when you ask questions that you cannot answer. I called the research labs and asked how many pixels I needed to make an equivalent of a film quality 110 film photograph. It was the worst consumer format I could picture, so I went for the lowest level. They said a million pixels, 2 million if you want color. So, I had 10,000 pixels in black and white, and I had to get to 2,000,000 in color. I used Moore’s law and came up with 15-20 years. It turned out to be not a bad prediction for all the wrong reasons.“I didn’t know everything that would happen in between, but we launched our first digital camera 18 years later. So wasn’t a bad prediction, just a lucky one.”The First Digital Portrait is No MoreThe first portrait was of a technician, Joy Marshall, working at the teletype a few doors down from the lab.“She knew us as the crazy guys in the back lab. There was just nothing to take a picture of in our lab. So, I picked up the camera, walked a short way down the hall, saw her there, and asked if I could take a shot of her.“It was an odd-looking device with all the electronic things hanging out, a lens on the front of it, a viewfinder. She sat there, and I took a head and shoulders shot of her, and that was the first snapshot taken with a digital camera of a person.”The first digital photographer took out the camera back and removed the tape. He put it in the playback machine and what he saw on the TV was her head and shoulder-length hair, which was black. That was well defined, but her face was completely static, wholly unrecognizable. She had followed him back and was standing in the entranceway of the lab when the picture came up.“Needs work,” she said, before turning around and walking away.“I had designed the playback unit,” remembers Sasson. “Each pixel was digitized to four bits, with all four being zero when it was black and all four being ones when they were white. I had serially encoded the bits, put them on the tape, and read them off the software I had written, which was a very rudimentary microprocessor at the time.“I had accidentally reversed the order of the bits in my head. So, if all the bits were zero and black, it did not make any difference what the order was, so they showed up as values. If they were all white with lighter tones, it didn’t make any difference and they showed up as white.“That’s why the black hair and white background made sense, so we could see that the image was correct geometrically. But all the in-between tones represented by variations of ones and zeros were reversed, so it did not make any sense.“It took about an hour to figure that out. Then I reversed some wires, and it was easier to reverse wires than change the software then, and the pictures showed up in proper form.“The blacks and whites were all in the correct position, but any in-between tones of the four bits with the 16 levels of gray were mixed up. You could see that her face and the background were all right. We were pleased with that. It showed us that all the pixels were going in the right places, which was a big part of the success. But the continuous part of the image was screwed up.“We had worked for a year developing details of this imaging chain. Everything had to be created from scratch. There was nothing I could lift from anything. We had to develop the circuitry, build it into the camera on the playback unit test, and then connect it to the whole system to see if we could get that portion of the signal chain to work.”Unfortunately, that first digital portrait photo was immediately lost to history.“We didn’t save this image,” Sasson says. “I didn’t take note of it other than that I was thrilled that the camera worked, and we spent the next six months trying to improve the performance. If he had saved that image, it would probably have had some historical significance [and a lot of money as the first digital Mona Lisa!].Image Size and StorageThe file size of these first digital images was 10,000 pixels. It was a hundred pixels on a line, and 100 lines, with each pixel, digitized to four bits. If you put it into bytes, it would be 5,000 bytes or 5 KB (kilobytes).The images were recorded on Phillips 300-foot cassette tapes. Typically, only two photos were saved on each tape, but they could have stored about 30 in terms of length and bit density. However, you quickly run out of tapes, so they are erased and reused.It was an ongoing experiment, constantly trying to improve the quality. They were not thinking of some historical milestone. Their main concern was to make it work better and faster. Sometimes, the camera would not work mainly because the CCD is unstable.The Pushback at KodakThe photo, it was presumed then, would have to be viewed on an electronic screen, which was a TV screen. The resolution and color rendition of a photo print was far better than an NTSC signal on the TV at the time. Prints have been with us for 100 years, and it was a great way to preserve photos, store them, retrieve them, put them into books — obviously, there was a big business there. They also felt that people were happy with that approach, so suggesting that they would store and retrieve images electronically was entirely alien.So, it wasn’t just the fidelity of the image. It was also the convenience and the structure and infrastructure with it.“But there was a lot of pushback on it,” says the inventor. “Which I thought was odd because we had the slide business with people sitting down and looking at slides projected on the wall, but that didn’t make much difference.“That was the pushback [that it did not have enough detail], but it wasn’t the only pushback. Even if you had excellent resolution, the restriction would have been in the technical limitations in the NTSC or in the case of Europe PAL.“So, it was right to push back on that, 110 film was not a Kodachrome slide, so if you were taking pictures with a 110 camera, you would get a resolution far inferior to a 35mm slide.”A Calculator or a Camera“The question that was often asked is how could this become a consumer device?” remembers Sasson. “In fact, would consumers even accept a digital camera-oriented device?“I had to go to analogies when I was presenting. I said, think of the future camera as a calculator that had just been coming out. HP came out with their HP 35 a few years before, and others were coming out with calculators. That was the only digital product that was somewhat of a consumer product.“Consumers were starting to use calculators a little, and so I said think of a calculator with a lens. That’s how I envisioned the camera, getting small enough to be like a calculator.“I said consumers would accept a calculator if it could be improved to the point where it could process all these images. Put the lens and the CCD in, then that calculator could also be a camera, and the size of the calculator is tiny compared to what I was showing.“That was the vision I suggested in the 1976 meetings. It was a comprehensive attempt without much data to back it up.”This was roughly a decade before the Macintosh computer was unleashed on the world and about a quarter century before Windows XP appeared. To Kodak management, the word “digital” was risky and unknown.“They [Kodak] did not like the word ‘digital’,” says Sasson. “A completely digital product made it seem more distant, as odd as that seems today, because there were no consumer Digital Products.“Digital as a technology seemed esoteric, a bit complicated, lots of wires, integration was relatively small, microprocessors were just coming out, but they were very awkward to work with and didn’t have a good reputation. The fact that it was digital made it even more distant, futuristic, complicated, and risky.“When I wrote a technical report, and when I applied for the patent granted in 1978, I mentioned it as electronics still camera, not a digital still camera, for that reason.Here’s the technical report Sasson wrote, titled “A Hand-held Electronic Still Camera and its Playback System”:Where is the Camera Today?“The camera itself I have kept; it still exists. It’s up in Kodak now,” says the inventor. “It was in the Smithsonian for several years. They brought it back and have it in the museum at Kodak. I kept the camera. I shouldn’t have. It was paid for with R&D dollars. You’re supposed to destroy all that stuff when you’re done with it. Tax thing.“But I kept the camera because it was too cool to throw away, and nobody cared, so I just kept it. I even lost it once but got it back again. If you see the back of the camera, it says Please Return to Steve Sasson.“When it started to get popular again in the early 2000s, people were surprised that I still had it. Of course, it is a historical document, so it’s a historical artifact, but nobody cared about the camera then. In that camera was a tape. There is nothing, unfortunately, on that tape.”Recognition for Inventing the Digital CameraOn November 17, 2009, U.S. President Barack Obama awarded Sasson the National Medal of Technology and Innovation at a ceremony in the East Room of the White House. This is the highest honor awarded by the U.S. government to scientists, engineers, and inventors“I was a fortunate guy to be representing the men and women of Kodak … to be the person that gets this notoriety is extremely humbling,” says Sasson.Before Obama put the medal around Sasson’s neck, the president turned and looked at the photographers in the back of the East Room of the White House and joked with them, “this picture better be good.”The president evidently likes to take pictures, and met with him privately and had a good conversation about photography. He learned that the president would steal Pete Souza’s camera to take photographs when Souza was not looking.Looking Back at the Birth of an Industry“I was not trying to take fantastic [images] as I was limited by the device I was working with,” Sasson says about his pioneering work in digital photography. “It was the only one of its kind and was brand-new. What I was trying to demonstrate was a photographic system.“This idea of taking [photos] without consuming anything. The only thing that would be consuming would be joules of energy. My idea was the sort of thing I wanted to suggest that consumers could, at some point in the future, take pictures without consuming film, paper, or any of the processing associates. And you could do it right away. That was the idea. So, the idea was let’s create a photographic system and all elements of it. Still, none of the elements were good in image quality compared to the present photographic system.“I thought my idea would be usable if I could get to 2 million pixels. In my view, I was thinking of in the 2 million – 3 million range, and once we reach that point, the arguments that I had in 1976 would be resolved. It was an acceptable image but low [quality] in consumer photography.“Now I had no idea that starting in ’97 and ’98, CCDs and eventually CMOS imagers would advance at about a million pixels a year. That was the rate of improvement that I did not think would happen.”All these years later, Sasson bemoans the fact that Kodak missed the boat when it could have seized first-mover advantage in digital cameras.“I was sad about [Kodak filing for bankruptcy.] I saw it coming, and Kodak had a resistant attitude towards it. For obvious reasons, they couldn’t make nearly the money from digital that they had made from conventional photography, and I saw this for many years.“I retired from Kodak in 2009 because I thought they would probably go away. I didn’t want them to, but it was just looking that way. Eventually, I didn’t know what had happened. I could just see that there could have been a different outcome if they could have taken a different approach. Again, it’s tough and very hard to accept that change in your fundamental business model.”About the author: Phil Mistry is a photographer and teacher based in Atlanta, GA. He started one of the first digital camera classes in New York City at The International Center of Photography in the 90s. He was the director and teacher for Sony/Popular Photography magazine’s Digital Days Workshops. You can reach him here.Image credits: Photographs courtesy the Eastman Kodak Company",How Steve Sasson Invented the Digital Camera
337,1,1_plastic_robots_robot_robotics,https://www.cnn.com/2022/10/24/world/search-and-rescue-rats-apopo-hnk-spc-intl/index.html,"CNN —Buildings don’t collapse very often – but when they do, it’s catastrophic for those trapped inside. Natural disasters like earthquakes and hurricanes can level entire towns, and for the search and rescue teams trying to find survivors, it’s a painstaking task.But an unlikely savior is being trained up to help out: rats.The project, conceived of by Belgian non-profit APOPO, is kitting out rodents with tiny, high-tech backpacks to help first responders search for survivors among rubble in disaster zones.“Rats are typically quite curious and like to explore – and that is key for search and rescue,” says Donna Kean, a behavioral research scientist and leader of the project.In addition to their adventurous spirit, their small size and excellent sense of smell make rats perfect for locating things in tight spaces, says Kean.The rats are currently being trained to find survivors in a simulated disaster zone. They must first locate the target person in an empty room, pull a switch on their vest that triggers a beeper, and then return to base, where they are rewarded with a treat.While the rodents are still in the early stages of training, APOPO is collaborating with the Eindhoven University of Technology to develop a backpack, which is equipped with a video camera, two-way microphone, and location transmitter to help first responders communicate with survivors.“Together with the backpack and the training, the rats are incredibly useful for search and rescue,” says Kean.Rat packAPOPO has been training dogs and rats at its base in Tanzania in the scent detection of landmines and tuberculosis for over a decade. Its programs use African Giant Pouched Rats, which have a longer lifespan in captivity of around eight years compared to the four years of the common brown rat.Life-saving rats — Tanzania-based NGO Apopo trains giant African pouched rats to sniff out land mines and detect tuberculosis -- two scourges that have had a tremendously negative impact on the African landscape. Tanzania-based NGO Apopo trains giant African pouched rats to sniff out land mines and detect tuberculosis -- two scourges that have had a tremendously negative impact on the African landscape. Courtesy Apopo Clearing Mozambique — In 2006, Apopo started testing rats on the mine fields in Mozambique, a country that at that time was one of the worst affected by landmines, thanks mainly to a civil war that ended in 1992. In 2006, Apopo started testing rats on the mine fields in Mozambique, a country that at that time was one of the worst affected by landmines, thanks mainly to a civil war that ended in 1992. Courtesy Apopo Almost mine-free — Since then, Apopo has cleared the country of 6,693 landmines, 29,934 small arms and ammunition, and 1,087 bombs. It is on track to clear Mozambique of landmines by the year's end. Since then, Apopo has cleared the country of 6,693 landmines, 29,934 small arms and ammunition, and 1,087 bombs. It is on track to clear Mozambique of landmines by the year's end. Courtesy Apopo Working with humans — Mine detection rats take nine months to a year to train. The rats are socialized when they are four-weeks-old so that they are comfortable working with humans. Mine detection rats take nine months to a year to train. The rats are socialized when they are four-weeks-old so that they are comfortable working with humans. Courtesy Apopo For the love of bananas — The rats are then conditioned with clicker training, so that they associate the sound of a click with a reward (usually peanuts or bananas). They are then introduced to a target scent (TNT or positive TB samples). The rats are then conditioned with clicker training, so that they associate the sound of a click with a reward (usually peanuts or bananas). They are then introduced to a target scent (TNT or positive TB samples). Courtesy Apopo Sandbox training — Mine-detection rats are then trained in a sandbox, where they are charged with sniffing out TNT-stuffed tea balls. Mine-detection rats are then trained in a sandbox, where they are charged with sniffing out TNT-stuffed tea balls. Courtesy Apopo Testing their know-how — In the final stage of training, mine-detection rats demonstrate their abilities at a training field at Morgoro, Tanzania -- the second largest in the world. It has over 1,500 mines, over 14 types are used during different training stages. In the final stage of training, mine-detection rats demonstrate their abilities at a training field at Morgoro, Tanzania -- the second largest in the world. It has over 1,500 mines, over 14 types are used during different training stages. Courtesy Apopo Speedy work — After a rat detects a mine, a manual deminer extracts the device. A single rat can clear 200 square feet in under an hour. It will take a manual deminer working alone about 50 hours to clear the same space. After a rat detects a mine, a manual deminer extracts the device. A single rat can clear 200 square feet in under an hour. It will take a manual deminer working alone about 50 hours to clear the same space. Courtesy Apopo An affordable solution — Because rats are small, they are also cheaper to transport and store than dogs -- who are traditionally employed to sniff out mines. In Africa, they are a cheaper option, because they are plentiful and easy to train. Each Apopo rat costs about $7,600 to train (a third the price it costs to train a dog). Because rats are small, they are also cheaper to transport and store than dogs -- who are traditionally employed to sniff out mines. In Africa, they are a cheaper option, because they are plentiful and easy to train. Each Apopo rat costs about $7,600 to train (a third the price it costs to train a dog). Courtesy Apopo Stuck in the lab — Apopo also trains rats to sniff out TB. Currently, the NGO's rodents are screening TB samples in Dar es Salaam, Tanzania and Maputo, Mozambique. Apopo also trains rats to sniff out TB. Currently, the NGO's rodents are screening TB samples in Dar es Salaam, Tanzania and Maputo, Mozambique. Briana Marie Going global — In addition to its work in Mozambique, Apopo has participated in mine-clearing projects in a number of countries, including Angola, Cambodia, Thailand, Vietnam and Lao. Here, one of Apopo's training supervisors works with a Cambodian trainer. In addition to its work in Mozambique, Apopo has participated in mine-clearing projects in a number of countries, including Angola, Cambodia, Thailand, Vietnam and Lao. Here, one of Apopo's training supervisors works with a Cambodian trainer. Courtesy Apopo Adopt a fuzzy friend — In order to raise funds, Apopo has launched an adopt-a-rat program, which allows participants to sponsor a 'hero rat'. In order to raise funds, Apopo has launched an adopt-a-rat program, which allows participants to sponsor a 'hero rat'. Courtesy Apopo Prev NextWhile the search and rescue project only officially launched in April 2021, when Kean joined the team, APOPO had been trying to get the idea off the ground for years but lacked funding and a search and rescue partner to support it. But when volunteer search and rescue organization GEA approached APOPO in 2017 about the possibility of using rats in its missions, the team began exploring the idea.A key component to the search and rescue mission was the technology to allow first responders to communicate with victims via the rats. APOPO didn’t have this – until electrical engineer Sander Verdiesen got involved.Looking to “apply technology to improve lives” during his master’s studies at Eindhoven University of Technology, Verdiesen interned with APOPO in 2019 and was tasked with creating the first prototype of the rat backpack, to help rescuers get a better idea of what was going on inside disaster zones.The prototype consisted of a 3D-printed plastic container with a video camera that sent live footage to a receiver module on a laptop, while also saving a high-quality version on an SD card. It attached to the rats with a neoprene vest, the same material that’s used for scuba suits.APOPO trains the rats at its base in Tanzania, in rooms that replicate the debris and rubble of disaster zones. © APOPOVerdiesen flew to Tanzania to test out the equipment and says that initially, the rats “didn’t really know how to deal with it” but adapted quickly. “By the end, they were just running around with the backpack on, no problem at all,” he adds.Big challenges for tiny techWith the backpacks working “better than expected,” Verdiesen continued to refine the design even after his internship ended, as a volunteer.But sizing down technology and adapting it for disaster zones hasn’t been easy.GPS can’t penetrate the dense rubble and debris of collapsed buildings, says Verdeisen. An alternative is the Inertial Measurement Unit, a location tracker used in the heels of firefighters’ boots.“If you’re walking, your foot is going to be still every step or so – that’s where you can recalibrate. With the rats, we’re yet to find that,” he says. Other engineers are working on similar projects, so he’s hopeful they can find a solution.The backpack currently contains a video camera, but APOPO hopes to also include a location tracker and two-way microphone. APOPOVerdeisen is also trying to pack more technology into the next version, such as a two-way microphone, while reducing its size. Weighing around 140 grams (4.9 ounces), the prototype was twice as heavy as originally intended – although Verdeisen says that bulkiness was more of an issue, at 10 centimeters long (3.9 inches) and 4 centimeters deep (1.6 inches).“The rats were walking up against something that they would normally be able to go under, and suddenly they can’t anymore,” he explains.To make it “as small as possible” without losing any functionality, Verdeisen plans to integrate everything onto a single printed circuit board, which will free up more space. This upgraded version of the backpack should be ready later this year, and he hopes one day it can help first responders “to locate somebody that would otherwise not be rescued.”Rodents to the rescueMeanwhile, in Tanzania, Kean is increasing the complexity of the rats’ training environment, “to make it more like what they might encounter in real life.” That includes adding industrial sounds like drilling to mimic real emergencies.So far, the results are promising: from her observations, Kean says the rats are responding well to the increasingly difficult simulations: “They have to be super confident in any environment, under any conditions, and that’s something that these rats are naturally good at.”Handled from birth, the rats are exposed to a variety of environments, sights, sounds, and people as part of a “habituation process,” which makes their gradual exposure to more extreme situations less stressful, according to Kean.As animals are at the center of APOPO’s projects and missions, welfare is a priority. The animals are trained in 15-minute sessions five days a week, and live alone or with same-sex siblings in home cages, which is also where they live out their days once they retire from working life.Behavioral research scientist Donna Kean (pictured) says the rats are friendly, sociable, and easy to work with. © APOPOEating a diet of fresh fruit and vegetables, they also get daily playtime in a custom-built playroom – although, for the search and rescue rats, training is very similar, “just with a little bit of direction,” says Kean.The program is still in development, but Kean estimates it will take at least nine to 12 months to train each rat.For the next stage of training, Kean says the team will create “levels to mimic multiple floors of a collapsed building” and move closer to “real world scenarios.” Once the rats are confident in more complex environments, the project will move to Turkey, where GEA is based, for further preparation in more realistic environments. If that goes well, then the rats would potentially enter real-life situations.For now, though, Kean and the team in Tanzania are focused on getting the rats through their first phase of training – and hopefully one day, into the field.“Even if our rats find just one survivor at a debris site, I think we would be happy to know it’s made a difference somewhere,” says Kean.",Rats with backpacks could help rescue earthquake survivors
371,1,1_plastic_robots_robot_robotics,https://www.euronews.com/next/2022/09/16/this-japanese-start-up-has-designed-a-high-speed-motorcycle-that-can-fly-for-40-minutes,"Learning to ride a bike is one of our earliest achievements in life. But how about learning to fly your bike?The world's first flying bike was revealed on Thursday at the Detroit Auto Show, meaning someday soon we might ride the drone-like vehicle instead of cars.The XTURISMO hoverbike is capable of flying for 40 minutes and reaching speeds of up to 100 km/h.Thad Szott, the co-chair of the Motor City's auto show, was one of the first to take a test drive.""I mean, it's awesome! Of course, you have a little apprehension, but I was just so amped. I literally had goosebumps and feel like a little kid,"" he said.Szott sees great potential for the XTURISMO in the future.""I think we'll start with delivering goods and services, maybe by drone. It's ready for today's world if you go airport to airport. But I'm anxious to see when we can go neighbourhood to neighbourhood. So, let's get it worked out,"" he said.The Japanese start-up behind the flying motorcycle, AERWINS Technologies, first started out making drones and unmanned aerial vehicles.The hoverbike is already on sale in Japan and a smaller version will be available in the US in 2023 for $777,000 (€775,400).Shuhei Komatsu, the company’s CEO, told Reuters that the company hopes to get the cost down to $50,000 (€49,890) for a smaller electric model by 2025.For more on this story, watch the video in the media player above.",This Japanese start-up has designed a high-speed motorcycle that can fly for 40 minutes
133,1,1_plastic_robots_robot_robotics,https://techcrunch.com/2013/06/26/bendlay-3d-is-a-bendable-printing-filament-that-you-can-use-to-make-clear-flexible-straps-and-bands/,"BENDLAY 3D Is A Bendable Printing Filament That You Can Use To Make Clear, Flexible Straps And Bands[youtube=http://www.youtube.com/watch?feature=player_embedded&v=VtYbYr19GVg]Kai Parthy is a German engineer who creates odd printing filaments for 3D printers. His previous projects, LayWoo-d3 and Laybrick, are two non-warping plastics that offer wood and brick-like consistencies when extruded. Oddly, LayWoo-d3 actually smells like wood when printed.Now he’s created a bendable printing filament called BENDLAY that is 91% transparent and remains “bendable” after printing.One of the problems with ABS plastic is that it can split and warp as it is formed and it isn’t quite food safe. It is also very brittle and will “whiten” when bent, resulting in a messy final object. This filament is made of stretchy Butadiene, a form of synthetic rubber. It is foodsafe and can be used for clear bottles and containers and works well for flexible straps.While ABS can be used to create flexible items like bracelets, this material will truly bend without breaking, allowing for hinges and other mechanical parts to be built into other, stiffer parts.It costs $42 a roll, which is about right for a pound or so of 3mm filament, and comes on 750 gram rolls. It should work with almost any extruder-based device but, sadly, doesn’t smell like fresh rubber.via 3DPrintingIndustry","BENDLAY 3D Is A Bendable Printing Filament That You Can Use To Make Clear, Flexible Straps And Bands"
187,1,1_plastic_robots_robot_robotics,https://techcrunch.com/2020/06/15/cmu-demonstrates-nanoscale-technology-that-causes-plants-to-absorb-nutrients-with-nearly-100-efficiency/,"Spraying plants with fertilizers and pesticides is typically a highly lossy affair — as little as 1% of the substances currently used in industrial and food production farming is actually taken up by the plant, while the rest leaches off into the soil. A new technology demonstrated for the first time by Carnegie Mellon University’s Greg Lowry and his team reverses that — causing a plant to absorb molecules with up to 99% efficiency, meaning only 1% is wasted.There are efficiency improvements, and then there are technology demonstrations that could completely upend current methods of doing things — like this one. Lowry’s research, which has been demonstrated as outlined in a peer-review publication now available in Nanoscale Communications, makes use of nanoparticles to coat molecular substances that you would want to be absorbed by a plant. These could include nutrients designed to optimize growth and crop yields, for instance, or pesticides that could protect them from destructive bugs and infestations.We covered this work last year, when it was still just at the pre-demonstration stage — now, Lowry’s team has shown that you can indeed engineer nanoparticles specifically to target pores on the surface of a leaf. Essentially, it’s like custom-creating Lego blocks for receptors on the leaf’s surface and then tying the nutrients you want to deliver to those custom Lego blocks for a perfect fit.This demonstration bears out the team’s hypothesis, which sets the stage for potential further development and, eventually, commercial application. The biggest potential commercial use of this technology could be in pesticides, as it’s estimated that as much as 40% of potential crop yield is still lost to plant disease that’s preventable with effective use of pesticides that can block them from entering through pores on leaves. They could also improve absorption of plant food and fertilizer designed to stimulate growth, and potentially these two uses could be combined into a single “dose” of nanoparticles that can do double duty, with great potential to increase plant and crop output.",CMU demonstrates nanoscale technology that causes plants to absorb nutrients with nearly 100% efficiency
481,1,1_plastic_robots_robot_robotics,https://www.reuters.com/technology/want-fries-with-that-robot-makes-french-fries-faster-better-than-humans-do-2022-10-04/,"PASADENA, Calif., Oct 4 (Reuters) - Fast-food French fries and onion rings are going high-tech, thanks to a company in Southern California.Miso Robotics Inc in Pasadena has started rolling out its Flippy 2 robot, which automates the process of deep frying potatoes, onions and other foods.A big robotic arm like those in auto plants - directed by cameras and artificial intelligence - takes frozen French fries and other foods out of a freezer, dips them into hot oil, then deposits the ready-to-serve product into a tray.Flippy 2 can cook several meals with different recipes simultaneously, reducing the need for catering staff and, says Miso, speed up order delivery at drive-through windows.“When an order comes in through the restaurant system, it automatically spits out the instructions to Flippy,"" Miso Chief Executive Mike Bell said in an interview."" ... It does it faster or more accurately, more reliably and happier than most humans do it,” Bell added.Miso said it took five years to develop Flippy and recently made it commercially available.The robot's name comes from Flippy, an earlier robot designed to flip burgers. But once Miso's team finished that machine, they realized there was a much tighter bottleneck at the fry station, particularly late at night.[1/4] The Flippy 2 robot takes fries out of a vat of oil at a lab of manufacturer Miso Robotics Inc in Pasadena, California, U.S. September 27, 2022, in this screen grab from a REUTERS video. Sandra Stojanovic/REUTERS TV via REUTERS 1 2 3 4Bell said Flippy 2 makes a splash - at first.“When we put a robot into a location, the customers that come up and order, they all take pictures, they take videos, they ask a bunch of questions. And then the second time they come in, they seem not to even notice it, just take it for granted,"" he said.Miso engineers can watch Flippy 2 robots working in real time on a big screen, enabling them to help troubleshoot any problems that crop up. A number of restaurant chains have adopted the robotic fry cook, including Jack in the Box in San Diego, White Castle in the Midwest and CaliBurger on the West Coast, Bell said.Bell said three other big U.S. fast-food chains have put Flippy 2 to work, but says they're hesitant to advertise because of sensitivities about perceptions that robots are taking jobs away from humans.“The task that the humans are most happy to offload are tasks like the fry station. ... They're delighted to have the help so they can do other things,"" Bell said.Miso Robotics has around 90 engineers, who tinker with prototypes or work on computer code. One of its next projects is Sippy, a drink-making robot which will take an order from a customer, pour drinks, put lids on them, insert a straw and group them together.Bell said that some day, people will ""walk into a restaurant and look at a robot and say, 'Hey, remember the old days when humans used to do that kind of thing?’""And those days ... it's coming. ... It's just a matter of ... how quick.”Reporting by Phil Lavelle and Sandra Stojanovic; editing by Jonathan OatisOur Standards: The Thomson Reuters Trust Principles.","Robots are making French fries faster, better than humans"
492,1,1_plastic_robots_robot_robotics,https://www.seas.harvard.edu/news/2022/10/tentacle-robot-can-gently-grasp-fragile-objects,"If you’ve ever played the claw game at an arcade, you know how hard it is to grab and hold onto objects using robotics grippers. Imagine how much more nerve-wracking that game would be if, instead of plush stuffed animals, you were trying to grab a fragile piece of endangered coral or a priceless artifact from a sunken ship.Most of today’s robotic grippers rely on embedded sensors, complex feedback loops, or advanced machine learning algorithms, combined with the skill of the operator, to grasp fragile or irregularly shaped objects. But researchers from the Harvard John A. Paulson School of Engineering and Applied Sciences (SEAS) have demonstrated an easier way.Taking inspiration from nature, they designed a new type of soft, robotic gripper that uses a collection of thin tentacles to entangle and ensnare objects, similar to how jellyfish collect stunned prey. Alone, individual tentacles, or filaments, are weak. But together, the collection of filaments can grasp and securely hold heavy and oddly shaped objects. The gripper relies on simple inflation to wrap around objects and doesn’t require sensing, planning, or feedback control.The research was published in the Proceedings of the National Academy of Sciences (PNAS).“With this research, we wanted to reimagine how we interact with objects,” said Kaitlyn Becker, former graduate student and postdoctoral fellow at SEAS and first author of the paper. “By taking advantage of the natural compliance of soft robotics and enhancing it with a compliant structure, we designed a gripper that is greater than the sum of its parts and a grasping strategy that can adapt to a range of complex objects with minimal planning and perception.”Becker is currently an Assistant Professor of Mechanical Engineering at MIT.The gripper’s strength and adaptability come from its ability to entangle itself with the object it is attempting to grasp. The foot-long filaments are hollow, rubber tubes. One side of the tube has thicker rubber than the other, so when the tube is pressurized, it curls like a pigtail or like straightened hair on a rainy day.The curls knot and entangle with each other and the object, with each entanglement increasing the strength of the hold. While the collective hold is strong, each contact is individually weak and won’t damage even the most fragile object. To release the object, the filaments are simply depressurized.",Tentacle robot can gently grasp fragile objects
51,1,1_plastic_robots_robot_robotics,https://gizmodo.com/human-composting-green-burial-california-1849558091,"In a few years , people in California will have a new choice for what to do with their loved ones’ bodies after death: put them in their garden.This weekend, Gov. Gavin Newsom signed a bill into law that makes human composting legal in the state beginning in 2027. The bill, AB-351, makes California the fifth state to allow human composting since it was first legalized in Washington in 2019 (Oregon, Colorado, and Vermont are the other places where you can make yourself into mulch).“AB 351 will provide an additional option for California residents that is more environmentally-friendly and gives them another choice for burial,” Assemblymember Cristina Garcia, who sponsored the bill, said in a release. “With climate change and sea-level rise as very real threats to our environment, this is an alternative method of final disposition that won’t contribute emissions into our atmosphere.”AdvertisementHuman beings cause more than enough trouble while we’re alive, but the practices we’ve developed to handle our bodies after death are also pretty bad for the environment. Burying a dead body takes about three gallons of embalming liquid per corpse—stuff like formaldehyde, methanol, and ethanol—and about 5.3 million gallons total gets buried with bodies each year. Meanwhile, cremation creates more than 500 pounds (227 kilograms) of carbon dioxide from the burning process of just one body, and the burning itself uses up the energy equivalent of two tanks of gasoline. In the U.S., cremation creates roughly 360,000 metric tons of carbon dioxide each year.It’s a no-brainer, then, to think of greener alternatives. The most common process for human composting—and the one laid out in the new California law—is called natural organic reduction, which involves leaving the body in a container with some wood chips and other organic matter for about a month to let bacteria do its work. The resulting mulch (yep, it’s human body mulch) is then allowed to cure for a few more weeks before being turned over to the family. E ach body can produce about a cubic yard of soil, or around one pickup truckbeds’ worth. According to Garcia’s release, this process will save about a metric ton of CO2 per body.Seattle-based company Recompose, which is mentioned in Garcia’s press release, was the first officially licensed human composting service to open in the U.S. after Washington state legalized the practice. In the release, Recompose’s founder, Katrina Spade, who invented the natural organic reduction process and was a key part of the legalization drive in Washington, said the company hopes to expand its services to California soon.",Human Composting Now Legal in California
212,1,1_plastic_robots_robot_robotics,https://techcrunch.com/2022/06/27/uks-magical-mushroom-company-uses-mycelium-to-replace-plastic-packaging/,"Global plastic waste has more than doubled, and 40% of that waste comes from packaging. Luckily there is no shortage of sustainable packaging startups in Europe. Just take a look: Circleback (Germany), Recup (Germany), Sourceful (U.K.), one • five (Germany), Shellworks (U.K.), Woola (Estonia), Papkot (France), Biotic (Israel), FunCell (France) and Traceless (Germany).The latest to join the ranks is Magical Mushroom Company (MMC). It’s now raised a £3 million seed round led by Ecovative Design LLC with participation by Dale Vince, founder of Ecotricity (a green energy company in the U.K.); Robert Del Naja (activist); and Marcus Watson, co-founder of Adoreum Partners, who brought 30 other investors.The investment will be used to fund the opening of its first raw material production plant.MMC’s solution is a direct replacement for plastic-based packaging such as polystyrene and cardboard. It does this by combining agricultural waste with mycelium — the root structure of a mushroom. The result, claims the company, is biodegradable (in 45 days), durable and comparable in price to traditional packaging derived from fossil fuels like polystyrene.Launched in 2019, MMC’s clients include Raymarine, BA Components, Castrads, Ffern, Selfridges, Lush, Seedlip and ID Watch, among many others.Paul Gilligan, CEO and founder at Magical Mushroom Company, said in a statement: “We have just eight years to meet the UN’s Sustainable Development Goals and businesses have a crucial role to play – but they need viable and cost-effective solutions that significantly reduce the carbon footprint across their entire supply chain. We’re proud to be creating value from waste and unlocking the potential of mycelium.”",UKâs Magical Mushroom Company uses mycelium to replace plastic packaging
134,1,1_plastic_robots_robot_robotics,https://techcrunch.com/2015/08/17/new-stretchy-electronics-will-help-us-stay-healthy-and-safe/,"Researchers at the Air Force Research Laboratory at Wright-Patterson Air Force Base are working on a new form of electronics encased in a stretchy, bendable casing that allows you to wrap leads around body parts and even sense strain and stress in buildings, airplanes, and other vehicles. The product of the Department of Defense’s Flexible Hybrid Electronics Manufacturing Innovation Institute, the material can be used in any situation where flexibility and high pressure are the norm.“Basically, we are using a hybrid technology that mixes traditional electronics with flexible, high-performance electronics and new 3-D printing technologies,” said Benjamin J. Leever, Ph.D. “In some cases, we incorporate ‘inks,’ which are based on metals, polymers and organic materials, to tie the system together electronically. With our technology, we can take a razor-thin silicon integrated circuit, a few hundred nanometers thick, and place it on a flexible, bendable or even foldable, plastic-like substrate material.”The team is using liquid gallium alloys injected into the substrate to carry electricity. Because the alloys are never exposed to air you efficiently prevent oxidation. The material can then be “integrated into complex curved surfaces, such as an airplane’s wing, or even a person’s skin.” The team also sees some value in the material for weapons. For example, a slice of this material connected to a bomb could survive the initial impact with a target to control the final payload detonation. In short, it acts as a tough skin and sensor for highly sensitive situations.",New Stretchy Electronics Will Help Us Stay Healthy And Safe
478,1,1_plastic_robots_robot_robotics,https://www.reuters.com/technology/1990s-relic-floppy-disks-get-second-life-california-warehouse-2022-10-20/,"LAKE FOREST, Calif., Oct 20 (Reuters) - It has been two decades since their heyday, but one bulk supplier of the iconic 3.5-inch floppy disk used to store data in 1990s says business is still booming.Tom Persky runs floppydisk.com, a California-based online disk recycling service that takes in new and used disks before sending them onto a reliable customer base - he reckons he sells about 500 disks a day.Who buys floppy disks in an age when more sophisticated storage devices like CD-ROMS, DVDs and USB flash drives have been made increasingly obsolete by internet and cloud storage? Those in the embroidery, tool and die, and airline industry, especially those involved in aircraft maintenance, says Persky.""If you built a plane 20 or 30 or even 40 years ago, you would use a floppy disk to get information in and out of some of the avionics of that airplane,"" said 73-year-old Persky.At his warehouse, shelves are packed with bright green, orange, blue, yellow or black disks sent from around the world. At one end sits a large magnetic machine with a conveyor belt that wipes out information on disks, while another machine slaps labels on them.The warehouse also holds 8-inch floppy disks - an even older storage medium - including one labeled as containing the 1960 John F. Kennedy and Richard Nixon U.S. presidential debate.[1/4] Tom Persky, owner of floppydisk.com, uses older computers to format the disks that he sources from second-hand websites and eBay at his warehouse in Lake Forest, California, U.S., October 6, 2022 in this screengrab from a Reuters TV video. REUTERS/Alan Devall 1 2 3 4Despite being a relic in the modern world, Persky says floppy disks have several redeeming qualities.""Floppy disks are very reliable, very stable, a very well understood way to get information in and out of a machine,"" he says. ""Plus, they have the additional feature of not being very hackable.""Persky ended up in the floppy disk business after working in software development for a tax company in the 1990s that duplicated its software onto floppy disks. He says he fell in love with the business and took it on after it was spun off.But he is not expecting it to survive another 20 years.""When I see the 'save' icon, I see a floppy disk. But most people just see the 'save' icon,"" Persky said.""I'll be here for as long as people continue to want to have these disks. But it's not forever.""(This story has been corrected to fix term to 'tool and die' in paragraph 3.)Writing by Deepa Babington; Editing by Lisa ShumakerOur Standards: The Thomson Reuters Trust Principles.","A 1990s relic, floppy disks get second life at California warehouse"
475,1,1_plastic_robots_robot_robotics,https://www.reuters.com/lifestyle/science/meet-japans-cyborg-cockroach-coming-disaster-area-near-you-2022-09-21/,"SAITAMA, Japan, Sept 22 (Reuters) - If an earthquake strikes in the not too distant future and survivors are trapped under tonnes of rubble, the first responders to locate them could be swarms of cyborg cockroaches.That's a potential application of a recent breakthrough by Japanese researchers who demonstrated the ability to mount ""backpacks"" of solar cells and electronics on the bugs and control their motion by remote control.Kenjiro Fukuda and his team at the Thin-Film Device Laboratory at Japanese research giant Riken developed a flexible solar cell film that's 4 microns thick, about 1/25 the width of a human hair, and can fit on the insect's abdomen.The film allows the roach to move freely while the solar cell generates enough power to process and send directional signals into sensory organs on the bug's hindquarters.The work builds upon previous insect-control experiments at Nanyang Technological University in Singapore and could one day result in cyborg insects that can enter hazardous areas much more efficiently than robots.""The batteries inside small robots run out quickly, so the time for exploration becomes shorter,"" Fukuda said. ""A key benefit (of a cyborg insect) is that when it comes to an insect's movements, the insect is causing itself to move, so the electricity required is nowhere near as much.""[1/5] A Madagascar hissing cockroach, mounted with a ""backpack"" of electronics and a solar cell that enable remote control of its movement, is pictured during a photo opportunity at the Thin-Film Device Laboratory of Japanese research institution Riken in Wako, Saitama Prefecture, Japan September 16, 2022. REUTERS/Kim Kyung-Hoon 1 2 3 4 5Fukuda and his team chose Madagascar hissing cockroaches for the experiments because they are big enough to carry the equipment and have no wings that would get in the way. Even when the backpack and film are glued to their backs, the bugs can traverse small obstacles or right themselves when flipped over.The research still has a long way to go. In a recent demonstration, Riken researcher Yujiro Kakei used a specialized computer and wireless Bluetooth signal to tell the cyborg roach to turn left, causing it to scramble in that general direction. But when given the ""right"" signal, the bug turned in circles.The next challenge is miniaturising the components so that the insects can move more easily and to allow for mounting of sensors and even cameras. Kakei said he constructed the cyborg backpack with 5,000 yen ($35) worth of parts purchased at Tokyo's famed Akihabara electronics district.The backpack and film can be removed, allowing the roaches to go back to life in the lab's terrarium. The insects mature in four months and have been known to live up to five years in captivity.Beyond disaster rescue bugs, Fukuda sees broad applications for the solar cell film, composed of microscopic layers of plastic, silver, and gold. The film could be built into clothing or skin patches for use in monitoring vital signs.On a sunny day, a parasol covered with the material could generate enough electricity to charge a mobile phone, he said.($1 = 143.3100 yen)Writing by Rocky Swift Editing by Mark HeinrichOur Standards: The Thomson Reuters Trust Principles.","Meet Japan's cyborg cockroach, coming to disaster area near you"
48,1,1_plastic_robots_robot_robotics,https://futurism.com/the-byte/cyborg-cockroaches-remote-controlled,"Rise of the cyborg cockroaches.Cyborg CockroachAn international team of scientists has created cyborg cockroaches, with electronics wired to their nervous systems that allow them to be remote controlled.The researchers fitted wireless control modules powered by rechargeable batteries to the backs of Madagascar cockroaches, which can grow up to 2.4 inches long.By stimulating each of the cockroaches' cerci, which are appendages that act like sensory nerves, via their tiny backpacks, the scientists were able to tell them where to go from a distance — a wild showcase of cutting-edge tech that they say could one day be harnessed to aid during search and rescue missions or help monitor the environment.Solar BackpackWhile these aren't aren't the first cyborg cockroaches, the team write in a new paper published in the journal npj Flexible Electronics that these new ones have a number of innovations that allowed them to assume control for longer periods of time.Specifically, a tiny and extremely thin solar cell also ensured that the battery stays charged, allowing the researchers to remote control the cockroaches for extended periods of time.""The body-mounted ultrathin organic solar cell module achieves a power output of 17.2 microwatts, which is more than 50 times larger than the power output of current state-of-the-art energy harvesting devices on living insects,"" lead author Kenjiro Fukuda, a senior research scientist at Riken University, said in a statement.Urban RescueBut the cockroaches aren't being dispatched during search and rescue missions any time soon.""The current system only has a wireless locomotion control system, so it's not enough to prepare an application such as urban rescue,"" Fukuda told CNET. ""By integrating other required devices such as sensors and cameras, we can use our cyborg insects for such purposes.""According to the researcher, the same technology could be applied to beetles and cicadas as well.It's a fun and futuristic vision: an army of remotely controlled cyborg insects that can infiltrate hard to reach locations or monitor crops.But scientists will have to advance the tech carefully — nobody wants to risk a cyborg cockroach uprising.READ MORE: Scientists Create Cyborg Cockroaches Controlled by Solar-Powered Backpacks [CNET]More on cyborg insects: Tiny Cyborg Drone Navigates Using Surgically Removed Moth Antenna","Scientists Wire Chip to Cockroaches' Nervous System, Allow Them to Be Remote Controlled"
190,1,1_plastic_robots_robot_robotics,https://techcrunch.com/2020/11/16/sequoia-backed-recycling-robot-maker-amp-robotics-gets-its-largest-purchase-order/,"AMP Robotics, the manufacturer of robotic recycling systems, has received its largest purchase order from the publicly traded North American waste handling company Waste Connections.The order, for 24 machine learning-enabled robotic recycling systems, will be used on container, fiber and residue lines across numerous materials recovery facilities, the company said.The AMP technology can be used to recover plastics, cardboard, paper, cans, cartons and many other containers and packaging types reclaimed for raw material processing.The tech can tell the difference between high-density polyethylene and polyethylene terephthalate, low-density polyethylene, polypropylene and polystyrene. The robots can also sort for color, clarity, opacity and shapes like lids, tubs, clamshells and cups — the robots can even identify the brands on packaging.So far, AMP’s robots have been deployed in North America, Asia and Europe, with recent installations in Spain and across the U.S. in California, Colorado, Florida, Minnesota, Michigan, New York, Texas, Virginia and Wisconsin.In January, before the pandemic began, AMP Robotics worked with its investor, Sidewalk Labs on a pilot program that provided residents of a single apartment building representing 250 units in Toronto with detailed information about their recycling habits.Working with the building and a waste hauler, Sidewalk Labs transported the waste to a Canada Fibers material recovery facility where trash is sorted by both Canada Fibers employees and AMP Robotics. Once the waste is categorized, sorted and recorded, Sidewalk communicates with residents of the building about how they’re doing in their recycling efforts.Sidewalk says that the tips were communicated through email, an online portal and signage throughout the building every two weeks over a three-month period.For residents, it was an opportunity to have a better handle on what they can and can’t recycle and Sidewalk Labs is betting the information will help residents improve their habits. And for folks who don’t want their trash to be monitored and sorted, they could opt out of the program.Recyclers like Waste Connections should welcome the commercialization of robots tackling industry problems. Their once-stable business has been turned on its head by trade wars and low unemployment. About two years ago, China decided it would no longer serve as the world’s garbage dump and put strict standards in place for the kinds of raw materials it would be willing to receive from other countries. The result has been higher costs at recycling facilities, which actually are now required to sort their garbage more effectively.At the same time, low unemployment rates are putting the squeeze on labor availability at facilities where humans are basically required to hand-sort garbage into recyclable materials and trash.AMP Robotics is backed by Sequoia Capital, BV, Closed Loop Partners, Congruent Ventures and Sidewalk Infrastructure Partners, a spin-out from Alphabet that invests in technologies and new infrastructure projects.",Sequoia-backed recycling robot maker AMP Robotics gets its largest purchase order
520,1,1_plastic_robots_robot_robotics,https://www.theguardian.com/technology/2022/oct/07/killer-robots-companies-pledge-no-weapons,"Several robot production companies have pledged not to support the weaponization of their general purpose robots and have encouraged other companies to follow suit.In an open letter, six leading robotics firms promised not to add weapons to their general use technology and said they would oppose others doing so.We need policy that prohibits bad actors from misusing it“We believe that adding weapons to robots that are remotely or autonomously operated, widely available to the public and capable of navigating to previously inaccessible locations where people live and work, raises new risks of harm and serious ethical issues,” read the open letter, first reported by Axios.“We also call on every organization, developer, researcher and user in the robotics community to make similar pledges not to build, authorize, support, or enable the attachment of weaponry to such robots.”The letter was signed by Boston Dynamics, Agility Robotics, ANYbotics, Clearpath Robotics, Open Robotics and Unitree Robotics.Co-signers also pledged to review applications to buy their robots to prevent possible weaponization and to investigate technological features that could be weaponized in future.“To be clear, we are not taking issue with existing technologies that nations and their government agencies use to defend themselves and uphold their laws,” the letter said.‘The benefits for humanity of these technologies strongly outweigh the risk of misuse.’ Photograph: Annegret Hilse/ReutersIn a statement to Axios, Boston Dynamics said it was concerned about attempts made to weaponize commercially available robots, adding that such developments could further erode public trust in technology.“For this technology to be broadly accepted throughout society, the public needs to know they can trust it,” the statement said. “And that means we need policy that prohibits bad actors from misusing it.”Emergency departments have used Boston Dynamics’s “Spot robot” – a dog-like machine – to survey situations, NPR reported. The company has said the robot was not designed for surveillance or as a replacement for human police officers.In their open letter, the six robotics companies said they were “convinced that the benefits for humanity of these technologies strongly outweigh the risk of misuse, and we are excited about a bright future in which humans and robots work side by side to tackle some of the world’s challenges”.",Top robot companies pledge not to add weapons to their tech to avoid harm risk
191,1,1_plastic_robots_robot_robotics,https://techcrunch.com/2020/11/19/biomaterials-are-coming-to-pantyhose/,"In a deal that has potentially big implications for the sustainability of consumer packaged goods, biomaterial manufacturing technology developer Genomatica and the massive nylon material manufacturer Aquafil have partnered on a new demonstration scale facility.Nylon-6 is used to make everything from toothbrush bristles to pantyhose and industrial materials like carpeting and other heavy-duty fabrics.The material will be used to develop renewable products and showcase goods that can be brought to market as more companies look to clean up their supply chains and make products that have fewer negative consequences for the environment at the end of their life.The deal is a 50-fold expansion of previous production levels for Genomatica and represents a significant expansion of Genomatica’s capabilities.The textile industry is a $960 billion business, and it’s one of the most polluting in the world — both in terms of chemical treatments and greenhouse gas emissions. According to data cited by the World Economic Forum, the textile industry accounts for 1.2 billion tons of carbon dioxide equivalent per-year — nearly as much as the auto industry. Nylon production alone is responsible for about 60 million tons of greenhouse gas emissions per year, according to the companies.The multi-year agreement with European-based Aquafil expands on the two companies’ existing relationship. Earlier this year the two companies produced the first ton of bio-nylon-6 precursor material at a pilot scale. Now, the move to a demonstration scale plant will give Genomatica the ability to move ahead with supply agreements to certain brand partners.Clothing maker Far Eastern New Century uses Genomatica’s products in its clothes, and other partnerships are in the works, the company said.Genomatica is backed by Casdin Capital, Viking Global Investors, which continues as Genomatica’s largest shareholder, and organism engineering partner Ginkgo Bioworks.“Bio-nylon is positioned to replace a material that’s used in millions of applications every day,” said Christophe Schilling, Genomatica CEO. “Our research shows that despite health and economic turmoil, 56% of Americans still want brands to prioritize sustainability. With this scale, Genomatica is offering our brand partners a key way to meet their sustainability objectives, differentiate themselves, and meet surging consumer demand.”Aquafil is building the plant in Slovenia, where the Genomatica biological precursor material will be converted into bio-nylon-6 yarns, films and engineered plastics.",Genomaticaâs expanded Aquafil partnership brings biomaterials to more consumer goods
132,1,1_plastic_robots_robot_robotics,https://techcrunch.com/2009/01/26/avenger-laser-weapon-knocks-uav-out-of-the-air/,"Gosh, these lasers grow up so darn quick! It seems just yesterday that the Avenger’s predecessor, the Advanced Tactical Laser, was being tested on those cute little Hercules transports. And then all that fussing about the Free Electron Laser!Pretty soon they’re going to be taking it for its first ride on a 747 — you’re going to want to have the camera ready! And then take out the SD card once you’re done and swallow it because they’re going to need to confiscate your camera, sir. Orders.Can you say “plausible deniability?” I knew you could!",Avenger Laser weapon knocks UAV out of the air
539,1,1_plastic_robots_robot_robotics,https://www.theverge.com/2022/9/30/23374729/tesla-bot-ai-day-robot-elon-musk-prototype-optimus-humanoid,"Tesla CEO Elon Musk revealed a prototype of a humanoid ""Optimus"" robot that shares some AI software and sensors with its cars' Autopilot driver assistance features. At the start of Tesla's 2022 AI Day presentation, Musk acknowledged that they had ""a guy in a suit"" last year but promised something much more impressive today.According to Musk, this prototype can do more than what was shown live, but ""the first time it operated without a tether was tonight on stage."" Musk predicted it could hit a price of ""probably less than $20,000"" and later, in a Q&A session, explained that Tesla is very good at building the AI and the actuators necessary for robotics based on the experience of producing drive units for electric cars. Musk said that would help it get capable robots into production and start off by testing them within its factories.He claimed that the difference between Tesla's design and other ""very impressive humanoid robot demonstrations"" is that Tesla's Optimus is made for mass production in the ""millions"" of units and to be very capable. As he said that, a team of workers moved a non-walking prototype offstage behind him.Tesla “Optimus” robot prototype at AI Day 2022 Image: Tesla1 / 6Initially, the back doors of the stage opened to reveal a deconstructed Optimus that Tesla calls ""Bumble C"" that walked forward and did a ""raise the roof"" dance move. Musk admitted that they wanted to keep it safe, not make too many moves on stage, and have it ""fall flat on its face."" (Best to avoid another Cybertruck sledgehammer incident if you can.)Afterward, the company showed a few video clips of the robot doing other tasks like picking up boxes.Then Tesla's team brought out another prototype showing a ""very close to production"" version of Optimus with its body fully assembled but not fully functional — it was held up on a stand and waved to the audience, showing the range of motion of its wrist and hand. Musk claimed this unit (that was walked out and eventually rolled off by a team of workers) still contains actuators, battery pack, and everything else but ""wasn't quite ready to walk.""A “close to production” prototype Optimus robot waves to the crowd. Image: TeslaThey revealed that the initial robot presented was developed in just the past six months. Discussing hurdles they have to address in getting it from the prototype to a working design, they hope to ""get this done within the next few months... or years.""Engineers hope to clear additional design hurdles “within the next few months... or years.”It contains a 2.3kWh battery pack, runs on a Tesla SoC, and has Wi-Fi and LTE connectivity. Demonstrations focused on addressing the robot's joints, like its hands, wrists, or knees, showed how they processed data for each joint, then looked for the common areas in each design to find a method using only six different actuators. The human-like hands are a ""Biologically Inspired Design"" that engineers say will make them more suitable for picking up objects of various shapes and sizes, holding a 20-point bag, and having a ""precision grip"" on small parts.Tesla's Autopilot software was moved from its cars to the bot and retooled to work in the new body and environment. Tesla motion captured people doing real-world tasks like lifting a box and then using inverse kinematics, repeats the movements using Optimus. Then ""online motion adaptation"" is applied to make it so these tasks aren't so rigid and can be manipulated to take into account an unstructured environment.Tesla Optimus robot breakdown of CPU, battery pack, actuators, and hands. Image: Tesla""It'll be a fundamental transformation for civilization as we know it,"" said Musk. He continues to say that Optimus has the potential of ""two orders of magnitude"" of potential improvement of economic output.Musk first announced the ""Tesla Bot"" at last year's AI Day, promising it would be ""friendly"" and potentially revolutionize the company's assembly line and manufacturing business.Musk had warned his fans not to expect the prototype to look like the glossy black-and-white rendering first shown at last year's event. But there's been no shortage of hype, with Musk calling the robot ""the most important product development we're doing this year"" and predicting that it will have the potential to be ""more significant than the vehicle business over time.""Future applications could include cooking, gardening, or even ""catgirl"" sex partners; Musk has said while claiming that production could start as soon as next year.In the days leading up to AI Day, robotics experts warned against buying too much into Musk's claims. They've noted that other companies are much further along in developing robots that can walk, run, and even jump — but none are claiming to be close to replacing human labor.Tesla's history is littered with fanciful ideas that never panned out — like a solar-powered Supercharger network, battery swapping, or robotic snake-style chargers — so it's anyone's guess as to whether a production-ready Tesla Bot will ever see the light of day. But the company is where it is today because of Musk's sheer will. And the reveal of a prototype version of the robot is sure to bolster Musk's claims of Tesla as ""the world's biggest robotics company.""",Tesla CEO Elon Musk unveils prototype humanoid Optimus robot
186,1,1_plastic_robots_robot_robotics,https://techcrunch.com/2020/04/24/replacing-plastic-with-plant-pulp-for-sustainable-packaging-attracts-a-billionaire-backer/,"In a small suburb of Melbourne, two entrepreneurs are developing a technology that could mean big changes for the packaging industry.Stuart Gordon and Mark Appleford are the co-founders of Varden, a company that has developed a process to take the waste material from sugarcane and convert it into a paper-like packaging product with the functional attributes of plastic.Their technology managed to grab the attention of — and $2.2 million in funding from — Horizons Ventures, the venture capital fund managing the money of Li Ka-shing, one of the world’s wealthiest men.It’s an opportune time to launch a novel packaging technology, as the European Union has already instituted a ban on single-use plastic items, which will go into effect in 2021. Taking their lead, companies like Nestlé and Walmart have pledged to use only sustainable packaging for products beginning in 2025.The environmental toll that packaging takes on the earth’s habitats is already a concern for many, and the urgency to find a solution is only mounting with consumers and businesses actually producing more waste in the rush to change consumer behavior and socially distance as a result of the COVID-19 global pandemic.“I like technologies that focus on carbon reductions,” said Chris Liu, Horizons Ventures’ representative in Australia.A longtime tech and product executive who had stints at Intel and Fjord, a digital design studio, Liu relocated to Australia recently and has actually taken himself off the grid.Living in Western Australia, the climate emergency was brought directly to the top of Liu’s mind when the wildfires, which raged through the country, came within two kilometers of his new home.For Mark Appleford, it wasn’t so much the fires as it was the garbage that kept washing up on the shores of his beloved beaches.Over beers at a barbecue he began talking to his eventual co-founder, Stuart Gordon, about the environmental problem they’d solve if they had the ability to change things. They settled on plastics.Working in Appleford’s laundry room they started developing the technology that would become Varden. That early laundry room-work in 2015 led to a small seed round and the company’s long slog to get an initial product in the hands of test customers.Finagling some time with the New Zealand manufacturer Fisher and Paykel, the two co-founders put together an early prototype of their coffee pods made from sugarcane bagasse, a waste byproduct of the sugar feedstock.“We worked backwards through customers to supply chain, which led us to material selection, which was something that would allow us to create a product that people understood,” said Gordon.The production process has evolved to fit inside a 40-foot container that holds the firm’s machine, which takes agricultural waste and converts that waste into packaging.Instead of using rollers like a paper mill, Varden’s technology uses a thermoform to mold the plant waste into a product that has the same properties as plastic.It removes a complicated step that’s been essential to the current crop of bioplastics, which use bacteria to convert plant waste into plastic substitutes that are then sold to the industry.“It looks like paper… you can tear it in half and it sounds like paper when you rip it, and you can throw it in the bin,” said Appleford.Gordon said that the company’s containers are outperforming commodity based plastics. And the first target for replacement, the founders said, is coffee capsules.“We went for coffee because it’s the hardest,” said Appleford.It’s also a huge market, according to the company. Varden estimates there are more than 20 billion coffee pods consumed every year.With the new money, Varden will begin manufacturing at scale to meet initial demand from pilot customers and is hoping to expand its product line to include medical blister packs in addition to the coffee pods.“A pilot plant on the products we’re looking at is a pilot plant that can generate 20 million units a year,” said Gordon.Both men are hoping that their product — and others like it — can usher in a generation of new sustainable packaging materials that are better for the environment at every stage of their life cycle.“The next generation of packaging will be better… there are plant-based flexibles for your salads, for your potato chips… [But] the next generation of molded packaging is us… bioplastic will ultimately go.”",Replacing plastic with plant pulp for sustainable packaging attracts a billionaire backer
425,1,1_plastic_robots_robot_robotics,https://www.newswise.com/articles/engineers-discover-new-process-for-synthetic-material-growth-enabling-soft-robots-that-grow-like-plants,"Newswise — An interdisciplinary team of University of Minnesota Twin Cities scientists and engineers has developed a first-of-its-kind, plant-inspired extrusion process that enables synthetic material growth. The new approach will allow researchers to build better soft robots that can navigate hard-to-reach places, complicated terrain, and potentially areas within the human body.The paper is published in the Proceedings of the National Academy of Sciences of the United States of America (PNAS), a peer-reviewed, multidisciplinary, high-impact scientific journal.“This is the first time these concepts have been fundamentally demonstrated,” said Chris Ellison, a lead author of the paper and professor in the University of Minnesota Twin Cities Department of Chemical Engineering and Materials Science. “Developing new ways of manufacturing are paramount for the competitiveness of our country and for bringing new products to people. On the robotic side, robots are being used more and more in dangerous, remote environments, and these are the kinds of areas where this work could have an impact.”Soft robotics is an emerging field where robots are made of soft, pliable materials as opposed to rigid ones. Soft growing robots can create new material and “grow” as they move. These machines could be used for operations in remote areas where humans can’t go, such as inspecting or installing tubes underground or navigating inside the human body for biomedical applications.Current soft growing robots drag a trail of solid material behind them and can use heat and/or pressure to transform that material into a more permanent structure, much like how a 3D printer is fed solid filament to produce its shaped product. However, the trail of solid material gets more difficult to pull around bends and turns, making it hard for the robots to navigate terrain with obstacles or winding paths.The University of Minnesota team solved this problem by developing a new means of extrusion, a process where material is pushed through an opening to create a specific shape. Using this new process allows the robot to create its synthetic material from a liquid instead of a solid.“We were really inspired by how plants and fungi grow,” said Matthew Hausladen, first author of the paper and a Ph.D. student in the University of Minnesota Twin Cities Department of Chemical Engineering and Materials Science. “We took the idea that plants and fungi add material at the end of their bodies, either at their root tips or at their new shoots, and we translated that to an engineering system.”Plants use water to transport the building blocks that get transformed into solid roots as the plant grows outward. The researchers were able to mimic this process with synthetic material using a technique called photopolymerization, which uses light to transform liquid monomers into a solid material. Using this technology, the soft robot can more easily navigate obstacles and winding paths without having to drag any solid material behind it.This new process also has applications in manufacturing. Since the researchers’ technique only uses liquid and light, operations that use heat, pressure, and expensive machinery to create and shape materials might not be needed.“A very important part of this project is that we have material scientists, chemical engineers, and robotic engineers all involved,” Ellison said. “By putting all of our different expertise together, we really brought something unique to this project, and I’m confident that not one of us could have done this alone. This is a great example of how collaboration enables scientists to address really hard fundamental problems while also having a technological impact.”The research was funded by the National Science Foundation.In addition to Ellison and Hausladen, the research team included University of Minnesota Department of Chemical Engineering and Materials Science researchers Boran Zhao (postdoctoral researcher) and Lorraine Francis (College of Science and Engineering Distinguished Professor); and University of Minnesota Department of Mechanical Engineering researchers Tim Kowalewski (associate professor) and Matthew Kubala (graduate student).Watch a video of a soft growing robot navigating a tortuous path.Watch a video explaining the idea behind the plant-inspired research.","Engineers discover new process for synthetic material growth, enabling soft robots that grow like plants"
526,1,1_plastic_robots_robot_robotics,https://www.thenationalnews.com/travel/news/2022/08/23/worlds-first-floating-pod-homes-launched-in-panama-starting-at-295000/,"Panama will be home to the world's first community of floating SeaPods, with the inaugural pod now in the water at Linton Bay Marina in Colon.Ocean Builders, a company specialising in innovative marine technology, has officially launched what it says are the first floating eco-restorative pod homes in the world.Perched three metres above sea level on the Caribbean coast of Panama, the futuristic units are designed to accommodate two people and are on sale now, with prices ranging from $295,000 to $1.5 million.By December, the first overnight guests will be able to bed down in the pods, and 100 fully-owned units will be ready for full-time residents by summer next year.A second batch of more than 1,000 of the pods will go into production next year.Designed by Dutch architect Koen Olthuis, the futuristic SeaPods are geared toward climate-conscious travellers who want to live on the water, but don’t want to give up the luxuries of modern living.Futuristic living on the water from $295,000Master bedrooms come with epic views over the ocean. Photo: Ocean BuildersWith minimalist decor, curved walls and an ultra-sleek design, the SeaPod flagship model looks like something from The Jetsons. It is spread over 77 square metres of space that's split across three levels.There's a master bedroom with epic views, a living room and kitchen, a bathroom and an outdoor patio. Wraparound panoramic windows give residents 360-degree unobstructed ocean views.Wooden teak flooring, colour-changing lights and wireless charging stations are available in each SeaPod, which also comes with automated black-out blinds and sliding panel windows.Entirely customisable, there's the option to add climbing walls, greenhouses, skylights, hot tubs and patio gardens.Cutting-edge technology is at the heart of Panama's newest waterfront destination, with each pod using software to control lighting, internal air temperature, water pressure and more.Guests wear a smart ring that can be used to control technology settings, as well as being used to unlock doors, open windows and turn on music and mood lighting. Wireless charging stations are built into several of the pod's surfaces so that chargers and wires for phones become a thing of the past.Daily drone delivery and new marine habitatsDaily essentials such as groceries and medicine are flown in by drones that have been designed to easily withstand open ocean conditions. Self-driving boats remove rubbish, compost and recycling and, when they're not on duty, work to clean up the surrounding ocean waters by expelling trash to help mitigate human impact on the ocean even beyond the community.The SeaPods are designed so that each one can function as a new habitat for marine life. Photo: Ocean BuildersSustainability is a key focus for Ocean Builders and pods are designed to be respectful of their surrounding environment.The pods have been designed to become marine habitats for fish and other creatures, and artificial intelligence cameras will monitor what goes on beneath the waves. Marine detection technology is available to alert pod residents when dolphins, whales or their other favourite sea creatures are nearby.Ocean Builders officially launched three models on Monday. They include the original SeaPod, the GreenPod for land living and the EcoPod, a more affordable option of each version.The single-level SeaPod Eco is slightly smaller than the flagship version and laid out on a single floor, but includes the same principles, technology options and customisable design features.12 futuristic cities being built around the world, from Saudi Arabia to China — in pictures","Worldâs first floating pod homes launched in Panama starting at $295,000"
193,1,1_plastic_robots_robot_robotics,https://techcrunch.com/2020/11/23/recycling-robotics-company-amp-robotics-could-raise-up-to-70-million-sources-say/,"AMP Robotics, the recycling robotics technology developer backed by investors including Sequoia Capital and Sidewalk Infrastructure Partners, is close to closing on as much as $70 million in new financing, according to multiple sources with knowledge of the company’s plans.The new financing speaks to AMP Robotics’ continued success in pilot projects and with new partnerships that are exponentially expanding the company’s deployments.Earlier this month the company announced a new deal that represented its largest purchase order for its trash-sorting and recycling robots.That order, for 24 machine learning-enabled robotic recycling systems with the waste-handling company Waste Connections, was a showcase for the efficacy of the company’s recycling technology.That comes on the back of a pilot program earlier in the year with one Toronto apartment complex, where the complex’s tenants were able to opt into a program that would share with the building’s renters recycling habits monitored by AMP Robotics in an effort to improve their recycling behavior.The potential benefits of AMP Robotic’s machine learning-enabled robots are undeniable. The company’s technology can sort waste streams in ways that traditional systems never could and at a cost that’s far lower than most waste-handling facilities.As TechCrunch reported earlier, the tech can tell the difference between high-density polyethylene and polyethylene terephthalate, low-density polyethylene, polypropylene and polystyrene. The robots can also sort for color, clarity, opacity and shapes like lids, tubs, clamshells and cups — the robots can even identify the brands on packaging.AMP’s robots already have been deployed in North America, Asia and Europe, with recent installations in Spain and across the U.S. in California, Colorado, Florida, Minnesota, Michigan, New York, Texas, Virginia and Wisconsin.At the beginning of the year, AMP Robotics worked with its investor, Sidewalk Labs, on a pilot program that provided residents of a single apartment building representing 250 units in Toronto with detailed information about their recycling habits. Sidewalk Labs is transporting the waste to a Canada Fibers material recovery facility where trash is sorted by both Canada Fibers employees and AMP Robotics.Once the waste is categorized, sorted and recorded, Sidewalk communicates with residents of the building about how they’re doing in their recycling efforts.It was only last November that the Denver-based AMP Robotics raised a $16 million round from Sequoia Capital and others to finance the early commercialization of its technology.",Recycling robotics company AMP Robotics could raise up to $70M
97,1,1_plastic_robots_robot_robotics,https://news.sky.com/story/worms-saliva-found-to-break-down-plastic-in-major-pollution-breakthrough-12712583,"A worm could be the answer to solving the problem of what to do about one of the commonest forms of plastic pollution.Spanish researchers have found that chemicals in the saliva of the wax worm can break down polyethylene, a particularly hard-wearing material.Their research found that exposing the plastic to the creature's saliva caused it to degrade as much in a single hour as several years worth of normal exposure to the elements.Wax worms, the larvae of the wax moth, usually feed on the tough wax bees use to make honeycombs and are actually considered pests by beekeepers.The study, published in the journal Nature Communications, discovered that two enzymes in the worm's saliva - which it uses to break down the wax - also break down the plastic.For plastic to degrade, oxygen needs to penetrate the plastic's molecules, known as polymers, a process known as oxidation.The research found that the enzymes in the saliva caused this process to occur in a matter of hours without any need to pre-treat the plastic by exposing it to heat or radiation.Polyethylene is the most widely used plastic in the world and responsible for vast amounts of pollution.First created in 1933, it is inexpensive, hard wearing and doesn't interact with food, making it widely used.AdvertisementSpecifically designed to be hard to break down, it can remain intact for decades.Please use Chrome browser for a more accessible video player 0:38 A shoreline in Guatemala is completely awash with plastic waste.Enzymes have been produced syntheticallyHowever, this breakthrough could be set to change that with molecular biologist Federica Bertocchini of the Spanish National Research Council (CSIC), who led the study, saying it was ""changing the paradigm of plastic biodegradation"".She went on to explain that they had not only found out which enzymes break down the plastic, but had also managed to produce them synthetically, avoiding the need to use billions of wax worms to do the job.Read more:'Jaw-dropping' amount of plastic thrown away by British households every year revealedMicroplastics found in human blood for first time after scientists make 'concerning finding'Doing it that way would have several practical drawbacks and would also generate a large amount of carbon dioxide as the worms metabolise the polyethylene.Plastic use has skyrocketed over the past 30 years, with hundreds of millions of tons ending up as waste every year, and less than 10% of that being recycled.In March this year, the United Nations approved a landmark agreement to create the world's first global plastic pollution treaty after talks in Nairobi, with the goal of having a legally binding deal finalised by 2024.Subscribe to ClimateCast on Spotify, Apple Podcasts, or Spreaker.",Worm's saliva found to break down plastic in major pollution breakthrough
431,1,1_plastic_robots_robot_robotics,https://www.npr.org/2022/10/24/1131131088/recycling-plastic-is-practically-impossible-and-the-problem-is-getting-worse,"Recycling plastic is practically impossible — and the problem is getting worseEnlarge this image toggle caption Laura Sullivan/NPR Laura Sullivan/NPRThe vast majority of plastic that people use, and in many cases put into blue recycling bins, is headed to landfills, or worse, according to a report from Greenpeace on the state of plastic recycling in the U.S.The report cites separate data published this May which revealed that the amount of plastic actually turned into new things has fallen to new lows of around 5%. That number is expected to drop further as more plastic is produced.Greenpeace found that no plastic — not even soda bottles, one of the most prolific items thrown into recycling bins — meets the threshold to be called ""recyclable"" according to standards set by the Ellen MacArthur Foundation New Plastic Economy Initiative. Plastic must have a recycling rate of 30% to reach that standard; no plastic has ever been recycled and reused close to that rate.""More plastic is being produced, and an even smaller percentage of it is being recycled,"" says Lisa Ramsden, senior plastic campaigner for Greenpeace USA. ""The crisis just gets worse and worse, and without drastic change will continue to worsen as the industry plans to triple plastic production by 2050.""Waste management experts say the problem with plastic is that it is expensive to collect and sort. There are now thousands of different types of plastic, and none of them can be melted down together. Plastic also degrades after one or two uses. Greenpeace found the more plastic is reused the more toxic it becomes.New plastic, on the other hand, is cheap and easy to produce. The result is that plastic trash has few markets — a reality the public has not wanted to hear.Trent Carpenter, the general manager of Southern Oregon Sanitation, says when they told customers a couple years ago that they could no longer take any plastic trash other than soda bottles and jugs — like milk containers and detergent bottles — people were upset. They wanted to put their strawberry containers, bags, yogurt cups and all manner of plastic trash in their recycling bin.""We had to re-educate individuals that a great deal of that material is ending up in a landfill,"" Carpenter said. ""It's not going to a recycling facility and being recycled. It's going to a recycling facility and being landfilled someplace else because [you] can't do anything with that material.""That message has been difficult for the public to absorb with so many different bins in public spaces, and their own communities telling them to put their plastic in recycling containers.Carpenter says they wanted to be transparent with their customers and tell them the truth, unlike companies that continue to tell customers that plastic, such as bags and containers, is being turned into new things.""Politically it's easier to just say 'Gosh, we're going to take everything and we think we can get it recycled,' and then look the other way,"" Carpenter said of the other companies. ""That's greenwashing at its best.""Greenpeace found a couple facilities are trying to reprocess cups and containers — sometimes called ""number 5s"" because of the markings on the containers. But the numbers are low. While 52% of recycling facilities in the U.S. accept that kind of plastic, the report found less than 5% of it is actually repurposed — and the rest is put into a landfill.Similarly, the National Association for PET Container Resources, an industry trade group, found in 2017 that only 21 percent of the plastic bottles collected for recycling were turned into new things.The low reprocessing rates are at odds with plans from the oil and gas industry. Industry lobbyists say they plan to recycle every piece of plastic they make into something new by 2040. In interviews with NPR, industry officials were unable to explain how they planned to reach a 100 percent recycling rate.An NPR investigative report found in 2020 that industry officials misled the public about the recyclability of plastic even though their own reports showed they knew as early as the 1970s and 1980s that plastic could not be economically recycled.The American Chemistry Council, an industry lobby group, initially did not respond to NPR's request for comment on the Greenpeace report. After publication, Joshua Baca, vice president of plastics for the group, sent an email to NPR calling Greenpeace's views ""misleading, out of touch and misguided.""He said the industry believes it is ""on the cusp of a circularity revolution"" when it comes to recycling plastic by ""scaling up sortation, advanced recycling, and new partnerships that enable used plastic to be remade again and again.""Environmentalists and lawmakers in some states are now pushing for legislation that bans single use plastics, and for ""bottle bills"" which pay customers to bring back their plastic bottles. The bills have led to successful recycling rates for plastic bottles in places like Oregon and Michigan, but have faced steep resistance from plastic and oil industry lobbyists.""The real solution is to switch to systems of reuse and refill,"" Ramsden said. ""We are at a decision point on plastic pollution. It is time for corporations to turn off the plastic tap.""After years of embracing plastic recycling, many environmental groups say they hope the public will finally see plastic for what they say it is — trash — and that people will ask themselves if there is something else they could be using instead.",Recycling plastic is practically impossible â and the problem is getting worse
429,1,1_plastic_robots_robot_robotics,https://www.npr.org/2022/10/06/1127227605/boston-dynamics-robots-pledge-against-weapons,"Some leading robot makers are pledging not to weaponize themEnlarge this image toggle caption Patricia De Melo Moreira/AFP via Getty Images Patricia De Melo Moreira/AFP via Getty ImagesBoston Dynamics and five other robotics companies have signed an open letter saying what many of us were already nervously hoping for anyway: Let's not weaponize general-purpose robots.The six leading tech firms — including Agility Robotics, ANYbotics, Clearpath Robotics, Open Robotics and Unitree — say advanced robots could result in huge benefits in our work and home lives but that they may also be used for nefarious purposes.""Untrustworthy people could use them to invade civil rights or to threaten, harm, or intimidate others,"" the companies said.""We believe that adding weapons to robots that are remotely or autonomously operated, widely available to the public, and capable of navigating to previously inaccessible locations where people live and work, raises new risks of harm and serious ethical issues,"" they added.The firms pledged not to weaponize their ""advanced-mobility general-purpose robots"" or the software that makes them function. They also said they would try to make sure their customers didn't weaponize the companies' products.They companies said they don't take issue with ""existing technologies"" that governments use to ""defend themselves and uphold their laws.""According to Boston Dynamics' website, police and fire departments are using the company's dog-like robot Spot to assess risky situations, but the firm says Spot is not designed for surveillance or to replace police officers.There have been growing calls across the globe to curb the use of autonomous weapons systems — which operate on their own and don't involve a human operator — and the Stop Killer Robots campaign says nearly 100 countries and a majority of people oppose autonomous weapons.But a meeting of the United Nations Convention on Certain Conventional Weapons last year failed to reach a consensus governing the use of so-called killer robots, due in part to objections from countries working on such technologies including the U.S, the UK and Russia, CNBC reported.",Some leading robot makers are pledging not to weaponize them
65,1,1_plastic_robots_robot_robotics,https://interestingengineering.com/innovation/farming-robot-lasers-200000-weeds-per-hour,"Proven benefits for weed cropping“We’ve proven the effectiveness of our laserweeding technology and the immense benefits it offers farmers, including healthier crops and soil, decreased herbicide use, and reduced chemical and labor costs,” said Carbon Robotics CEO and Founder, Paul Mikesell.“To best serve farmers’ needs, we’ve adapted the design of our product, but will still leverage our proven laserweeding technology. Our mission has always been to provide farmers with the most effective tools, and the strong demand for LaserWeeders is evidence we’re helping them solve a serious problem.”The new machine features 30 industrial CO2 lasers, more than 3 times the lasers in Carbon Robotics’ self-driving Autonomous LaserWeeder. Growers who use the device are seeing up to 80 percent savings in weed management costs, with a break-even period of two to three years.Better yet, the new model integrates effortlessly into existing farming infrastructures while covering more ground and solving problems associated with spraying, hand weeding, and mechanical weeding.The machine also boasts sophisticated artificial intelligence technology that enables the robot to instantly identify, target, and eliminate weeds using thermal energy while continuously rolling. Furthermore, it features a lighting system that enables the LaserWeeder to operate day or night in almost all weather conditions.""Grimmway Farms is dedicated to protecting natural resources and the environment while optimizing crop yield and soil health,” said Jeff Morrison, director of innovation & new technology at Grimmway Farms.","This new farming robot uses lasers to kill 200,000 weeds per hour"
442,1,1_plastic_robots_robot_robotics,https://www.popsci.com/environment/ocean-plastic-clean-marine-organisms/,"Every year, more than 14 million tons of plastic pollute the ocean and threaten the life of various marine species. About 80 percent of all marine debris is plastic, which demonstrates the extent of global plastic pollution.Boat builders, sailors, and engineers have developed technological innovations like the Seabin to minimize all sorts of litter floating in the ocean. These mechanical cleanup inventions are fixed-point devices designed to separate and remove marine debris from various bodies of water. They work by sucking water from the surface and intercepting floating debris or lifting trash from the water onto a conveyor belt that gathers everything in a dumpster.However, they might have a limited benefit in reducing plastic pollution. Research shows the devices may even capture unknowing marine organisms, which is a problem because they threaten marine life.The rate of waste generation exceeds the rate of litter cleanupA recent Marine Pollution Bulletin study examined a Seabin in the Southwest United Kingdom and found that it captured an average of 58 litter items per day, mainly consisting of polystyrene balls, plastic pellets, and plastic fragments. The authors also found that the device caught one marine organism—like sand eels, brown shrimps, and crabs—for every 3.6 items of litter captured (or roughly 13 marine organisms per day), half of which were dead upon retrieval.The marine organisms may be attracted to the device to forage or seek refuge. Their mortality rate also appeared to increase with retention time in the machine. Some died due to being captured, possibly under the weight of the surrounding material, says Florence Parker-Jurd, study author and research assistant in the International Marine Litter Research Unit at the University of Plymouth in the United Kingdom.“At its current stage of development, the study found that in the environment examined, the quantity or mass of litter being removed by the device was minimal when considered alongside the risk of by-catch,” says Parker-Jurd. She adds that manual cleaning efforts with nets from pontoons tend to be more efficient and less resource intensive than the Seabin in environments like marinas, harbors, and ports, even though it was designed to operate in these locations.“Technological innovations have a part to play in reducing marine litter, particularly in coastal environments where they can complement existing cleanup efforts,” says Parker-Jurd. “This study has highlighted the need for robust, formal evaluations of such devices, especially given the increasing use and geographic spread of the Seabin and similar devices.”[Related: A close look at the Great Pacific Garbage Patch reveals a common culprit.]Although the study only formally evaluated one device, similar issues may apply to other marine cleaning devices. Things like the lack of an escape route, long periods of operation, and the time out of the water to separate marine life from organic matter and litter before it returns to the water can all contribute to the entrapment of marine organisms, says Parker-Jurd.Moreover, the current capacity of technological efforts to reduce plastic collection is limited in comparison to the extent of the plastic pollution problem. “Though there are no estimates of the overall removal of plastic and other debris from these devices, there is near consensus among experts that the magnitude of trash collected pales in comparison to the amount of waste that enters our environment,” says Meagan Dunphy-Daly, director of the Duke University Marine Laboratory Scholars Program. She was not involved in the study.There haven’t been many scientific studies on the effectiveness of various technologies in removing plastic pollution from the environment—or their rate of marine by-catch—but self-reported effectiveness is often higher than peer-reviewed reports on the efficacy, says Dunphy-Daly. Weather, current, and the location of the device deployment have to be considered when it comes to the effectiveness of cleanup technologies outside their pilot phases.Dutch non-profit The Ocean Cleanup went under fire recently for the heap of plastic debris they cleaned from the Great Pacific Garbage Patch, which some experts say was too clean for plastics that were supposedly in the water for years. The organization argued that there was no visible build-up of algae and barnacles because the water in the garbage patch lacked nutrients. Most of the plastic floated above the water, but conservation experts also refuted that.“Further studies need to evaluate the types of marine life being captured in these devices to determine population-level effects and weigh the risks and benefits of using these cleanup technologies,” says Dunphy-Daly.Technology must go hand-in-hand with reducing plastic production and useDeveloping and implementing technologies to reduce litter is only part of the solution. When there’s an oil spill, you don’t just focus on removing the oil from the surface of the water—you stop the leak and clean it up, says Dunphy-Daly.The leak has undoubtedly continued in the case of global plastic pollution. She adds that combating it requires a comprehensive approach that targets all stages of the plastic life cycle, from reducing overall production to cleaning up what has entered the environment.That said, the invention of cleanup devices effectively draws attention to the problem of marine litter. Last year, Coldplay partnered with The Ocean Cleanup and sponsored an Interceptor, a watercraft or vessel intended to remove plastic from rivers before they reach the ocean.[Related: Horrific blobs of ‘plastitar’ are gunking up Atlantic beaches.]“Hopefully, by generating public interest with these technologies, we can also gain support for targeting other life stages of plastic and reduce overall plastic pollution,” says Dunphy-Daly.A 2021 report from the National Academies of Sciences, Engineering, and Medicine argued that recycling processes and infrastructures are insufficient to manage the gross amount of plastic waste produced. The authors recommended several interventions to reduce waste generation, like establishing a national cap on virgin plastic production and a ban on specific disposable plastic products.Mechanical marine cleaning devices can shape perceptions around the issue of marine litter and potentially create a reliance on technological solutions to environmental problems. Therefore, these sorts of interventions should continue to be evaluated, says Parker-Jurd. According to a 2022 Societies paper, there is excessive optimism around technology and scientific advancement. Still, the man-made problems of the planet cannot be solved by modern and efficient technology alone.Even though the invention of cleanup devices is unlikely to alleviate one’s responsibility for waste and litter completely, evidence of their psychological impacts is currently lacking and should still form a crucial part of future research, says Parker-Jurd. She adds, “our primary focus should remain on implementing a systematic change in the way we produce, use, and dispose of plastics.”",Ocean plastic âvacuumsâ are sucking up marine life along with trash
70,1,1_plastic_robots_robot_robotics,https://interestingengineering.com/innovation/tiny-robots-brush-floss-teeth,"The development could be particularly useful for those who lack the manual dexterity to clean their teeth effectively themselves.A cumbersome, challenging process“Routine oral care is cumbersome and can pose challenges for many people, especially those who have hard time cleaning their teeth” said in the statement Hyun (Michel) Koo, a professor in the Department of Orthodontics and divisions of Community Oral Health and Pediatric Dentistry in Penn’s School of Dental Medicine and co-corresponding author on the study.“You have to brush your teeth, then floss your teeth, then rinse your mouth; it’s a manual, multistep process. The big innovation here is that the robotics system can do all three in a single, hands-free, automated way.”“Nanoparticles can be shaped and controlled with magnetic fields in surprising ways,” added Edward Steager, a senior research investigator in Penn’s School of Engineering and Applied Science and co-corresponding author.“We form bristles that can extend, sweep, and even transfer back and forth across a space, much like flossing. The way it works is similar to how a robotic arm might reach out and clean a surface. The system can be programmed to do the nanoparticle assembly and motion control automatically.”How the microbots work. Penn Engineering TodayRevolutionizing the good old fashioned toothbrushThe team is set to revolutionize the traditional but old-fashioned toothbrush. “The design of the toothbrush has remained relatively unchanged for millennia,” explained Koo.While adding electric motors elevated the basic “bristle-on-a-stick format,” the fundamental concept has remained the same. “It’s a technology that has not been disrupted in decades.”",A swarm of tiny robots could soon brush and floss your teeth for you
532,1,1_plastic_robots_robot_robotics,https://www.theverge.com/2022/10/14/23401381/great-lakes-plastic-pollution-cleanup-tech-robots,"In the murky waters of Lake Ontario just off the Toronto harbor, a stream of trash inches toward a round, tubular-looking device floating in the water. A piece of white styrofoam bumps up against the device’s lip. Then, in one fluid motion, it tumbles over the edge. With tendrils of marine plants circling the waste, it looks like the styrofoam could have entered a portal to an undersea world. Instead, the device is a gateway to a less mystical — yet vital — destination: the garbage dump.“It’s basically like a floating trash can,” says Chelsea Rochman, professor of ecology and evolutionary biology at the University of Toronto, who has worked with a team at the university to capture trash in Lake Ontario with bins like these since 2019. Powered from shore, the device, called a Seabin, uses a motor to create a vortex that gently pulls in floating waste from a 160-foot radius and then stores the trash in an attached basket.Across the Great Lakes, which stretch from Duluth, Minnesota, to the border between the United States and Canada in northern New York, dozens of Seabins now work alongside stormwater filters in a cross-border project dubbed the Great Lakes Plastic Cleanup. In mid-September, they were also joined by aquatic waste-collection drones and beach-cleaning roving robots — all to remove some of the 22 million pounds of plastic that enter the lakes each year and help researchers like Rochman understand the Great Lakes waste problem.People can’t remove waste 24 hours a day like the devices can“We know that the amount of litter we have out there needs more power than the people power that we have,” Rochman explains. Though local groups have organized beach cleanups for decades, people can’t remove waste 24 hours a day like the devices can, nor can they pick up the tiny pieces that machines are able to capture.Standing on the shore of Lake Ontario, with Toronto’s streetcars rattling by, Rochman points out the overflowing municipal trash bin along the sidewalk — one of several sources of the trash. Municipal sewage systems, industrial spills, stormwater runoff, recreational boating and beach waste, and agricultural debris all wind up in the lakes as well. In one bin, toothbrushes, tampon applicators, dental flossers, shoe strings, eyeglasses, food scraps, and syringes are entwined in the tendrils of marine plants. Between the leaves, tiny flecks of plastic poke out.Bright colored plastic floats among leaves and other debris in one of the Great Lakes Image: Great Lakes Plastic CleanupIn the lakes, which 40 million people rely on as their primary drinking water source, this waste breaks down, turning into microscopic pieces of plastic and debris, which are then eaten by fish, sucked into surrounding water treatment plants, or pulled to shore or out into the ocean. When plastic is consumed by fish, it can release chemicals like dyes and flame retardants, irritating and potentially damaging their digestive systems. In big sport fish, like lake trout or salmon, Rochman expects to find hundreds of pieces of plastic. Microplastics have also been found in drinking water in the region, where many water treatment plants are ill-equipped to filter the tiny pieces out. (The risk of consuming microplastics for humans remains unclear, though researchers continue to investigate the potential problem.)Once captured by researchers like Rochman, each piece of trash becomes another data point. Each day during the summer, students haul out the bins to count, classify, and dispose of their contents. “They know how many cigarette butts we collect, how many straws we collect, how many foam containers we collect,” Rochman says. Some days, the catch is more surprising — students have counted slices of deli meat, old shoes, and, once, a coconut in Seabins this summer.The Seabins capture 28 grams of waste, on average, each day. “It’s going to sound like a small number because plastic is light,” says Rochman. That weight translates to a couple hundred to 2,000 pieces of microplastic, along with multiple pieces of larger waste. This summer, Rochman expects her team to remove the amount of plastic equivalent to 7,000 plastic water bottles — and that’s only in the 12 bins the university oversees, which make up just a fraction of the devices deployed at 45 marinas across the Great Lakes region.From the northern shores of Lake Superior in Thunder Bay, Ontario, to the harbor of Buffalo, New York, just a short drive from Niagara Falls, Seabins like the ones in the Toronto harborfront are deployed at 44 other locations, typically in operation from May to November. These bins are monitored not by researchers but by marina owners or local organizations. Partners at the participating sites weigh and dispose of the bins’ contents as they fill up and perform full waste characterizing audits five to 10 times each year. Many marinas also have catch basin baskets installed, called LittaTraps, that sit inside stormwater drains to capture waste before it enters the lake system. Between 2020 and 2021, the project’s technology captured over 74,000 pieces of trash, a number that the team expects will increase as they continue outreach to marinas and municipalities in the region.A promotional photo of the beach-cleaning BeBot. Image: The Searial CleanersIn September, a waste collecting drone and beach-cleaning robot also joined the project’s fleet of trash catching technology. The devices, built by French waste capture technology company The Searial Cleaners, collect waste from lakes and beaches, working both via remote control and autonomously. The roving robots are also key public engagement tools, says Claire Touvier, the company’s chief executive. “That’s why this robot needed to be sexy and cool and fun, and to also have a cool name — these are extremely efficient tools when it comes to raising awareness,” she says.Still, the technology remains a reactive approach. Robots can help clean up the lakes, but human choices about how much plastic to produce, consume, and throw out are at the core of the Great Lakes trash problem. Changing them will be key to any long-term solution, says Melissa De Young, policy and programs director at the Canadian nonprofit Pollution Probe, one of the project’s main funders. “We’re doing what we can to remove plastic from the water, but we know that the technologies alone are not going to solve the problem,” she explains. “The data that we’re collecting is really critical because it provides, first, an understanding of the extent of the problem.”If large macroplastics wind up in capture devices in a certain area, for example, that can indicate communities nearby may lack easy access to disposal facilities or may be uninformed on why proper waste disposal is important. Alternatively, if small plastic pieces used to build other products, called preproduction pellets, or nurdles, are more common, that can indicate that somewhere upstream, a manufacturer may be improperly disposing of its trash.The PixieDrone collects plastic. Image: The Searial CleanersThe captured waste then informs the group’s approach to local solutions, whether that means starting a new educational campaign, meeting with policymakers, or advocating for new industry mandates. Last year, the team consulted on a new Ontario law that requires the foam used to build floating docks for cottages and marinas to be fully enclosed so it does not break down into the water. The group has also contributed to proposed legislation to include mandates for filters on washing machines to prevent microfibers from entering the sewage system in Ontario and stronger laws regarding preproduction plastic disposal in Illinois.“When we go to government policymakers, when we go to industry in the region, when we go to others to say, ‘Listen, we’ve got a problem here and we need to fix it,’ having that localized data, having that regional data, really aids us in those conversations in terms of capturing people’s attention and really motivating them to do something,” says Mark Fisher, head of the Council for the Great Lakes, a binational organization that also funds the project.“We don’t want to have to have trash traps in the water forever.”Other researchers in the region are hopeful about the new technology in the lakes, too. No initiative is going to be able to pick up 22 million pounds of plastic out of the Great Lakes every year, but a project that can motivate public and political action can have magnifying results, explains Timothy Hoellein, a biology professor at Loyola University Chicago who has worked on separate lake cleanup projects but is not involved in this one. When it comes to the Seabins, “Their individual footprint is pretty small,” Hoellein says. “But on a collective basis, it could really make a difference.”As the strategy garners success in the region, its lessons have begun to reach far beyond the shores of the Great Lakes. Rochman and the team at the University of Toronto have partnered with the nonprofit environmental group Ocean Conservancy to found the International Trash Trap Network, which works with groups from Fiji to Florida to help create more trash trapping strategies. Wherever trash traps capture waste, data collection follows.It’s all part of the goal of achieving a future where freshwater sources, like the Great Lakes, are no longer dumping zones for waste, Rochman says. “We don’t want to have to have trash traps in the water forever,” she adds.",How a fleet of robots could help solve the Great Lakes plastic pollution problem
69,1,1_plastic_robots_robot_robotics,https://interestingengineering.com/innovation/startup-3d-print-homes-recyclable-plastics,"""The construction sector is the largest global consumer of raw materials, responsible for approximately 11 percent of the world's total carbon emissions. Our responsibility to our customers and future generations is to use the most sustainable practices imaginable,"" said Ross Maguire, the CEO of Azure, in April.Azure also unveiled what it called the world's first 3D printed ""backyard studio"" made with recycled plastic materials in the same month.The plastic 3D printed studios and accessory dwelling units (ADUs) - meaning legal and regulatory terms for a secondary house or apartment that shares the building lot of a larger, primary home - are now available for preorder as the startup prepares to ramp up its production line in the Culver City neighborhood of Los Angeles.Azure uses recyclable plastics to build homes. AzureFaster, cheaper, and sustainable homes with 3D printingAzure sticks up for building homes 70 percent faster and 30 percent cheaper than ""traditional home construction methods."" As Business Insider says, most 3D home builders use a form of mixed or pure concrete to build a home. However, Azure is ""saying goodbye"" to this by using sustainable materials.Azure's printing materials consist of the waterproof plastic polymer generally found in plastic bottles and packaging food, according to the startup.""Our supply chain should never be short in our lifetime,"" told Ross Maguire to Business Insider.""We have created production efficiencies not only by capitalizing on the advances in 3D printing but by creating a design and process that is completed in only 20 hours. When compared with conventional construction, we produce the entire structural skeleton, the exterior sheathing, the water control barrier, the exterior finish, the passageways for utilities, and the grounding for interior finishes, in a fraction of the time and cost. By revolutionizing a new age of the home building with our sustainable, automated, and exact production processes, we see a very, very exciting future ahead,"" further added Maguire in April to describe the startup's goal.",This startup 3D prints tiny homes from recyclable plastics
326,2,2_meta_facebook_users_said,https://www.cnbc.com/2022/10/21/snap-shares-continue-to-plunge-on-disappointing-q3-revenue.html,"Shares of Snap fell 28% Friday after investors continued to digest the company's third-quarter earnings report that was released Thursday night. Shares are now trading at levels not seen since February 2019.The company posted an unexpected profit, but revenue missed estimates slightly, coming in at $1.13 billion versus the $1.14 billion expected, according to a Refinitiv survey of analysts.The social media company has suffered as a result of the struggling online advertising market. Apple 's data privacy update in 2021 has limited the ability of social media companies to track users online, which has continued to hurt the company.Bernstein analyst Mark Shmulik downgraded the stock Friday morning to market perform from outperform and reduced his price target to $9 from $15.In a note to investors, Shmulik signaled there's still hope for the company: ""SNAP's untapped potential remains, yet we're unlikely to see near-term inflection. Winning back investor and our own confidence will take time.""Barclays was more optimistic in its analysis of Snap's performance, reiterating an overweight rating on the stock after saying that Snap ""has a long history of overcoming challenging transitions.""The company has managed to continue to grow in popularity, with daily active users increasing 19% year over year in the third quarter.Shares of Snap are down about 77% year to date.— CNBC's Michael Bloom contributed to this report.","Snap shares plunge nearly 30%, closing at lowest since early 2019"
325,2,2_meta_facebook_users_said,https://www.cnbc.com/2022/10/15/meta-horizon-worlds-metaverse-losing-users-falling-short-of-goals.html?__source=iosappshare%7Ccom.apple.UIKit.activity.CopyToPasteboard,"Horizon Worlds, Meta 's flagship metaverse for consumers, is failing to meet internal performance expectations, according to The Wall Street Journal, which reviewed internal company documents.Meta initially aimed to reach 500,000 monthly active users in Horizon Worlds by the end of the year, but the current figure is less than 200,000, according to the report. Additionally, the documents showed that most users didn't return to Horizon after the first month on the platform, and the number of users has steadily declined since spring, the Journal said.Only 9% of worlds are visited by at least 50 people, and most are never visited at all, according to the report.The report comes as the company's stock falls, user numbers decline and advertisers cut spending. Meta shares are down 62% so far this year.Meta rebranded from Facebook last year in order to reflect the company's ambitions beyond social media. CEO Mark Zuckerberg has specifically been interested in building out the metaverse, which is a virtual world that allows users to work and play together.As a result, Meta created Horizon Worlds, which is a network of virtual spaces where users can engage with one another as avatars. Individuals can access Horizon through Meta's Quest virtual-reality headsets.In an effort to drum up some excitement around the metaverse, Zuckerberg unveiled his company's newest virtual reality headset, dubbed the Meta Quest Pro, at Meta's Connect conference Tuesday. The device costs $1,500 and contains new technologies, such as an advanced mobile Snapdragon computer chip.A Meta spokesman told The Wall Street Journal that the company continues to make improvements to the metaverse, which was always meant to be a multiyear project. Representatives for Meta didn't immediately respond to CNBC's request for comment.Meta has said it will release a web version of Horizon for mobile devices and computers this year, but the spokesman didn't have any launch dates to disclose.Read the full Journal report here.","Meta documents show main metaverse is losing users and falling short of goals, report says"
522,2,2_meta_facebook_users_said,https://www.theguardian.com/technology/2022/oct/31/online-age-verification-system-could-create-honeypot-of-personal-data-and-pornography-viewing-habits-privacy-groups-warn,"In the wake of the Optus and Medibank data breaches, digital rights groups are urging the federal government to rule out requiring identification documents as part of any online age-verification system, warning it could create a honeypot of people’s personal information and pornography-viewing habits.The eSafety commissioner, Julie Inman Grant, is developing an online safety “roadmap”, outlining a way to prevent minors from accessing adult content online by ensuring host sites have verified the ages of users.The commissioner’s report was initially due to the government in December, however, the deadline has now been extended to March next year. Stakeholders were informed of the delay in reporting last week.A variety of options for age verification has been offered during the roadmap’s development, including the use of third party companies, individual sites verifying ages using ID documents or credit card checks, and internet service providers or mobile phone operators being used to check users’ ages.Digital rights groups say almost all approaches to age verification will have some level of privacy and security risk.“Following the Optus and Medibank breaches, millions of people are now acutely aware of the dangers of collecting and storing large amounts of our personal information,” Samantha Floreani, program lead at Digital Rights Watch said.“Age verification is a terrible combination of being invasive and risky, while also being ineffective for its purported purpose.“Methods that are less privacy-invasive are easily bypassed by tech-savvy kids, and those that may be more likely to work at restricting access to pornography create massive and disproportionate privacy and digital security risks.”There was the potential for a new honeypot of people’s identities and porn-viewing habits if these systems were pursued, Floreani said.“The consequences of a breach of such a system would be devastating,” she said.Sign up to Guardian Australia's Morning Mail Free daily newsletter Our Australian morning briefing email breaks down the key national and international stories of the day and why they matter Privacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy . We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply.Electronic Frontiers Australia chair, Justin Warren, said EFA has long warned about the privacy and security risks of such a policy.“A government that claims to be interested in evidence-based policy would listen and act on our advice. Failure to do so suggests that the motivations for increased surveillance and control are ideological,” he said.A spokesperson for the communications minister, Michelle Rowland, said the Albanese government “supports restricting Australian children from viewing online pornography” but said questions about the roadmap were best directed to the eSafety commissioner.“The eSafety commissioner is progressing a complex body of work with a wide range of divergent stakeholder views and issues including privacy and security,” the spokesperson said.A spokesperson for the eSafety commissioner said privacy and cyber security issues were important considerations in developing the roadmap and have been extensively explored as part of the consultation process.Other groups have called for an effective ban on online pornography. Anti-porn group Collective Shout called for all pornography to be treated under the same classification as child sexual abuse material or terrorism material, which would be required to be removed or blocked in Australia.Some companies have already begun implementing age-verification procedures. Google, for example, since March estimates a person’s age using information gathered on that account, such as their search history. If ultimately the company needs to see ID documents, Google has said it deletes those documents after verification.","Online age-verification system could create âhoneypotâ of personal data and pornography-viewing habits, privacy groups warn"
328,2,2_meta_facebook_users_said,https://www.cnbc.com/2022/10/27/elon-musk-now-in-charge-of-twitter-ceo-and-cfo-have-left-sources-say.html,"Tesla CEO Elon Musk is now in charge of Twitter , CNBC has learned.Twitter CEO Parag Agrawal and finance chief Ned Segal have left the company's San Francisco headquarters and will not be returning, sources said. Vijaya Gadde, the head of legal policy, trust and safety, was also fired, The Washington Post reported.Musk had until Friday to complete his $44 billion acquisition of Twitter or face a court battle with the company.The billionaire tweeted ""the bird is freed"" in an apparent reference to the takeover being completed.Twitter did not immediately respond to a request for comment.In April, Twitter accepted Musk's proposal to buy the social media service and take it private. However, Musk soon began sowing doubt about his intentions to follow through with the agreement, alleging the company failed to adequately disclose the number of spam and fake accounts on the platform.When Musk said he was terminating the deal, Twitter sued the billionaire, alleging he ""refuses to honor his obligations to Twitter and its stockholders because the deal he signed no longer serves his personal interests.""","Elon Musk now in charge of Twitter, CEO and CFO have left, sources say"
530,2,2_meta_facebook_users_said,https://www.theregister.com/2022/11/02/windows_11_statcounter/,"Much of the Windows world has yet to adopt Microsoft's latest desktop operating system more than a year after it launched, according to figures for October collated by Statcounter.Just 15.44 percent of PCs across the globe have installed Windows 11, meaning it gained 1.83 percentage points in a month. This compares to the 71.29 percent running Windows 10, which fell marginally from 71.88 percent in September.Windows 7 is still hanging on with a tenuous grip, in third place with 9.61 percent, Windows 8.1 in fourth with 2.45 percent, plain old Windows 8 with 0.69 percent, and bless its heart, Windows XP with 0.39 percent because of your extended family.In total, Windows has almost 76 percent of the global desktop OS market followed by OS X with 15.7 percent and Linux with 2.6 percent.Android comprised 42.37 percent of total operating system market share, with Windows trailing on 30.11 percent, iOS on 17.6 percent, OS X on 6.24 percent, and Linux on 1.04 percent.Statcounter is a web analytics service with tracking code installed on 1.5 million websites, recording billions of page views for each site. AdDuplex collates stats from Microsoft Store apps that contain the AdDuplex SDK (estimated to be 5,000 apps) and it claimed that Windows 11 had 23 percent market share in June.Microsoft releases activation data to PC markers but this is not made public.It almost didn't happen...Windows 11 launched on October 5, 2021. It is the OS most never thought would happen after Microsoft said some six years earlier: ""With Windows 10, the experience will evolve and get even better over time. We'll deliver new features when they're ready, not waiting for the next major release. We think of Windows as a Service.""One reason for the new release was improved security and reliability, Microsoft said, including hardware root of trust via Trusted Platform Module (TPM) 2.0, Secure Boot, hypervisor-protected code integrity, and hardware-enforced stack protection.The release also managed to alienate a bunch of folk due to its hardware requirements. Microsoft decided that Windows 11 would not install on devices that lack a recent TPM-equipped CPU, and while there is a workaround it's an imperfect solution.According to Lansweeper data, some 42.76 percent of 27 million PCs it tested across 60,000 organizations failed the CPU test, and it said this was forcing users who didn't want to upgrade hardware to stick with Windows 10.Most corporate enterprises have yet to migrate to Windows 11 – typically they wait for 18 months after an OS has launched before upgrading business computers. The entire PC ecosystem will be waiting with bated breath to see if that happens next year or whether businesses will want to sweat assets for longer in a nod to uncertain economic times. ®",Windows 11 runs on fewer than 1 in 6 PCs
45,2,2_meta_facebook_users_said,https://fortune.com/2022/10/18/mark-zuckerberg-meta-avatars-video-chat-zoom-fatigue/,"At least digital humanoids don’t get Zoom fatigue—yet.During the Meta Connect 2022 live keynote last week, CEO Mark Zuckerberg discussed his new plans for Meta to bring avatars—uncanny digital stand-ins for human workers—to video chats.They would be customized to match a person’s exact skin tone, hairstyle, and outfit choices. According to Zuckerberg, an entirely virtual roundtable meeting would consist of you and your coworkers’ avatars chatting in something like a “third mode” between fully camera-on and camera-off.“You can still express yourself and react, but you’re not on-camera, so it’s kind of like a better camera-off mode,” he said.The social media giant invested $10 billion in building the metaverse last year, a digital space where users can interact with experiences and other people using VR technology. Zuckerberg revealed the video chat avatar feature in the key note after announcing partnerships with several companies, including one with Microsoft chairman and CEO Satya Nadella that would bring Microsoft apps to Meta Horizon Workrooms—the VR metaverse rooms where workers’ avatars meet—to create “a unified, digital office we think can make distributed work so much better.”As Intelligencer’s John Herrman points out, all of this could be a strategy to diversify Meta’s business—but it could also be a play at acknowledging execs’ challenges with remote work and trying to rectify them. The opportunity for a “better camera-off mode” just might be an answered prayer for the bosses unhappy with the remote workers who tend to join meetings with their web cameras off.Is seeing still believing?Proximity bias, which describes bosses tending to prefer workers they can see in person, has long been proven. It also may explain why managers who are used to commandeering a physical office would be thrilled if they could see their workers—even if that required them to wear an elaborate headset that costs as much as a Peloton.A 20,000-person survey by Microsoft itself found that bosses are still regularly questioning their remote employees’ productivity levels. Some have even taken draconian measures to ensure that their ideal level of productivity is met. Per August research from the New York Times, eight out of the 10 largest private employers in the U.S. track productivity metrics, including active online time, incidence of keyboard pauses, how long it takes to write an email, and even individual keystrokes.Zuckerberg’s enthusiasm about metaverse meetings, and the support from a tech sector heavyweight like Nadella, may speak to exactly this kind of “productivity paranoia.”But some experts are wary of a full-scale pivot to the metaverse. “We would have to carefully attend to the physical implications of headsets,” Roshni Raveendhran, assistant professor at the University of Virginia’s Darden School of Business, told Fortune last year. “Like if it harms our eyesight or implicates our brain functions; we don’t know any of these things now, and we won’t know until there’s more of a continual usage pattern. We need to pay attention to some of those before we go into full-scale adoption.”The metaverse is unlikely to be as all-encompassing as Zuckerberg hopes, says Cathy Hackl, a futurist and metaverse expert. For instance, meetings that hinge on deeper bonding or team building, such as new hire orientations or holiday parties, are still best done in person. “Your company can’t treat you to a cocktail virtually,” she told Fortune.And with even the most advanced VR devices, Hackl added, she hits her limit around the 45-minute mark. “I don’t think I could wear a headset for a six-hour video call.”",Mark Zuckerberg has a $10 billion plan to make it impossible for remote workers to hide from their bosses
329,2,2_meta_facebook_users_said,https://www.cnbc.com/2022/10/27/meta-is-no-longer-one-of-the-20-biggest-us-companies.html,"Sixteen months after Facebook crossed $1 trillion in market cap, joining an exclusive club consisting of Apple , Microsoft , Alphabet and Amazon , its parent company Meta is worth less than Home Depot and barely more than Pfizer and Coca-Cola .Far from Facebook's Big Tech days, Meta is no longer among the 20 most valuable U.S. companies after the stock sank 23% on Thursday. The company has shed 70% of its value this year and 74% since the stock peaked in September 2021, totaling over $730 billion in market cap lost. It's trading at its lowest since early 2016, when Barack Obama was still president.The stunning collapse of Meta's share price is reminiscent of the dot-com bust days, but far bigger in terms of value erased from a single company. The slide began late last year as signs of a sputtering economy started to emerge, and accelerated in early 2022 after the company said Apple's privacy change to iOS would result in a $10 billion revenue hit this year.Founder and CEO Mark Zuckerberg has been unable to stop the bleeding and only seems to be making matters worse. Since changing the company name to Meta a year ago Friday, Zuckerberg has said its future is the metaverse, a virtual universe of work, play and education. But investors just see it as a multibillion-dollar money pit, while the core advertising business shrinks — Facebook is forecasting a third consecutive drop in revenue for the fourth quarter.",Facebook used to be a Big Tech giant â now Meta isn't even in the top 20 most valuable U.S. companies
333,2,2_meta_facebook_users_said,https://www.cnet.com/tech/services-and-software/study-shows-30-of-people-are-redoing-google-searches/,"Almost 30% of people are having to redo their Google searches, either by refining or extending queries, according to research published earlier this month by SEMRush, an online marketing software company.SEMRush took data from 20,000 anonymous users who made 455,368 unique searches. It then looked at how long it took them to make a subsequent action. For over 70% of users, it took less than 15 seconds to make a secondary click, meaning they most likely found the website or answer they were looking for. Almost 30% of users, however, were refining, redoing or extending their searches in some way, suggesting that for some, answers weren't effectively percolating to the top.This 30% number comes from 9.7% of users who engaged in a ""Google Click,"" meaning they clicked on images or something in a carousel after making a query. For these people, they may have actually found what they were looking for. Another 17.9% of users made modifications to ""Google Keyword,"" or ways to modify their original query. This totals to 27.6%, which was then rounded up by SEMRush.Satisfaction wasn't something measured in this study, just click behaviors after making a Google Search. It's possible a person could have been happy with an initial result and might have wanted to rephrase to investigate further.Keyword changes happened more often on mobile, at 29.3% versus 17.9% on desktop, SEMRush found. It suggests people in need of quick information might be looking for answers on Google rather than clicking through to a website. Since the study didn't survey users about their experience, it's impossible to say exactly why someone on mobile is more often redoing or refining their searches. Typos on a small screen could be a culprit.On desktop, the study also found that 25.6% of results were ""zero clicks."" This means a person didn't click on a link after making a query. It could mean they refined their search, or that they found the answer they were looking for without clicking on a link to a website. The latter could spell trouble for the billions of sites that rely on traffic to bring in ad sales -- while less clicks is better for people looking for quick answers, it's detrimental to the many news and information sites creating that content.""Google Search sends billions of clicks to websites every day, and we've sent more traffic to the open web every year since Google was first created,"" said Danny Sullivan, Google's public liaison for Search. ""It's not unusual that people conduct a search without knowing exactly what they are looking for, then refine that search after seeing results and our refinement options (like related searches) to ultimately find what they need.""Complaints over Search's faltering reliability continue to come up in online discussions and articles. From Reddit threads to pieces in The Atlantic, people say they're fighting a battle against websites trying to game Google's search engine optimization and the company's own system of filtering results. Search also remains Google's most valuable product, with it controlling over 92% of online search market share, helping the company drive ad revenue.Some users say they're now using short-form video platform TikTok to find the answers they're looking for instead of Google. It might be why Google is integrating more TikTok-like features in Search and why it spent $100 million to buy an AI avatar startup.Alphabet, Google's parent company, reported $69 billion in revenue this past quarter, with $39.5 billion coming from ""Google Search and Other."" Even then, Google's earnings came short of analyst estimates.","Almost 30% of People Redo or Refine Google Searches, Study Says"
331,2,2_meta_facebook_users_said,https://www.cnbc.com/2022/11/01/twitter-reportedly-limits-employee-access-to-content-moderation-tools-.html,"In this photo illustration, the image of Elon Musk is displayed on a computer screen and the logo of twitter on a mobile phone in Ankara, Turkiye on October 06, 2022.Elon Musk's Twitter has taken away certain content moderation and policy enforcement tools from some employees ahead of the U.S. midterm elections, according to Bloomberg News.The move affects most employees who are part of Twitter's Trust and Safety organization, Bloomberg reported on Tuesday, citing unnamed sources. The staffers are unable to address and discipline user accounts that violate Twitter's rules around hate speech and misinformation unless they involve harm, the report said.Twitter is still using automated content moderation tools and third-party contractors to prevent the spread of misinformation and inflammatory posts while Twitter employees review high-profile violations, Bloomberg said.Twitter didn't immediately respond to CNBC's request for comment. Yoel Roth, Twitter head of safety, reacted to the Bloomberg News in a tweet.""This is exactly what we (or any company) should be doing in the midst of a corporate transition to reduce opportunities for insider risk,"" he wrote. ""We're still enforcing our rules at scale.""On Friday, after closing his acquisition of Twitter, Musk said he plans to form a ""content moderation council,"" without disclosing specifics such as who would be a part of it or what it would do. The Tesla CEO added that he would not make any ""major content decisions"" or reinstate previously banned accounts before the council begins its work.WATCH: Musk's Twitter takes on content moderation",Twitter reportedly limits employee access to content moderation tools as midterm election nears
534,2,2_meta_facebook_users_said,https://www.theverge.com/2022/10/28/23427577/apple-union-maryland-letter-benefits-contract,"Organizers at Apple’s Towson Town Center store in Maryland claim that the company isn’t telling the whole truth when it comes to withholding benefits from workers at the location. As the company’s first retail location to unionize in the US pushes to negotiate a contract, workers say it’s making it difficult for them to bargain for their benefits.In a letter addressed to Tim Cook, the negotiating committee says they’re disappointed to learn the company won’t be offering workers at the location some new health and education benefits that are rolling out to other retail employees. The union also says that Apple has been spreading “misinformation” by saying workers would have to bargain for those benefits to be included in their contract.“Apple management has not yet provided our union with any details about the new benefits”“There is crucial context missing in this communication around the process of change within a unionized store and the fact that we can, and we will include these (and any new benefits) in our collective bargaining contract proposal,” says the letter, which you can read in full below. However, the union also claims that Apple has made it difficult to bargain for those benefits by not sharing “any details” about them.Apple didn’t respond to The Verge’s multiple requests for comment on the union’s accusations.The union, known as IAM CORE (CORE stands for Coalition of Organized Retail Employees, and the organization is partnered with the International Association of Machinists and Aerospace Workers), won its union election by an almost two to one margin in June. Since then, workers at other locations say the company has continued to oppose unionization efforts, with the Communications Workers of America filing complaints about Apple’s behavior in New York and Oklahoma.Notably, the reports about Apple withholding benefits came out days before the Oklahoma store was scheduled to hold its union election, which IAM CORE’s letter says was a “calculated” move. If it was, it didn’t work: workers at Penn Square store in Oklahoma City voted to unionize in a 56-32 vote.Still, there are still ongoing union drives at Apple stores in New York and Atlanta, where the threat of withheld benefits could sway votes or even stall the process of holding an election entirely. The union involved with the campaign in Atlanta canceled the vote in May, saying Apple had made it impossible to hold a fair election.Earlier this month, Bloomberg broke the news about Apple’s push to withhold benefits at Towson and provided some details about exactly what workers might be missing out on; the list included a free Coursera subscription, prepaid tuition at some colleges (versus the reimbursement model Apple usually uses), and new healthcare plan options. The publication cited Harvard Law School professor Benjamin Sachs, who said that there was nothing stopping the company from offering those benefits to unionized employees.Wilma Liebman, a chairperson for the National Labor Relations Board, told Bloomberg that the company’s move to block benefits could be a violation of labor law, saying it was “hard to see how they could come up with a legitimate reason for the timing other than to influence the outcome of the election.” According to the NLRB’s site, employers also aren’t allowed to “refuse to furnish information the union requests that is relevant to the bargaining process.”",Appleâs first unionized workers say the company is withholding new benefits
336,2,2_meta_facebook_users_said,https://www.cnn.com/2022/10/21/tech/facebook-tiktok-misinfo-ads/,"New York CNN Business —Facebook and TikTok failed to block advertisements with “blatant” misinformation about when and how to vote in the US midterms, as well as about the integrity of the voting process, according to a new report from human rights watchdog Global Witness and the Cybersecurity for Democracy Team (C4D) at New York University.In an experiment, the researchers submitted 20 ads with inaccurate claims to Facebook, TikTok and YouTube. The ads were targeted to battleground states such as Arizona and Georgia. While YouTube was able to detect and reject every test submission and suspend the channel used to post them, the other two platforms fared noticeably worse, according to the report.TikTok approved 90% of ads that contained blatantly false or misleading information, the researchers found. Facebook, meanwhile, approved a “significant number,” according to the report, though noticeably less than TikTok.The ads, submitted in both English and Spanish, included information falsely stating that voting days would be extended and that social media accounts could double as a means of voter verification. The ads also contained claims designed to discourage voter turnout, such as claims that the election results could be hacked or the outcome was pre-decided.The researchers withdrew the ads after going through the approval process, if they were approved, so the ads containing misinformation were not shown to users.“YouTube’s performance in our experiment demonstrates that detecting damaging election disinformation isn’t impossible,” Laura Edelson, co-director of NYU’s C4D team, said in a statement with the report. “But all the platforms we studied should have gotten an ‘A’ on this assignment. We call on Facebook and TikTok to do better: stop bad information about elections before it gets to voters.”In response to the report, a spokesperson for Facebook-parent Meta said the tests “were based on a very small sample of ads, and are not representative given the number of political ads we review daily across the world.” The spokesperson added: “Our ads review process has several layers of analysis and detection, both before and after an ad goes live.”A TikTok spokesperson said the platform “is a place for authentic and entertaining content which is why we prohibit and remove election misinformation and paid political advertising from our platform. We value feedback from NGOs, academics, and other experts which helps us continually strengthen our processes and policies.”Google said it has “developed extensive measures to tackle misinformation on our platforms, including false claims about elections and voting procedures.” The company added: “We know how important it is to protect our users from this type of abuse – particularly ahead of major elections like those in the United States and Brazil – and we continue to invest in and improve our enforcement systems to better detect and remove this content.”While limited in scope, the experiment could renew concerns about the steps taken by some of the biggest social platforms to combat not just misinformation about candidates and issues but also seemingly clear cut misinformation about the process of voting itself, with just weeks to go before the midterms.TikTok, whose influence and scrutiny in US politics has grown in recent election cycles, launched an Elections Center in August to “connect people who engage with election content to authoritative information,” including guidance on where and how to vote, and added labels to clearly identify content related to the midterm elections, according to a company blog post.Last month, TikTok took additional steps to safeguard the veracity of political content ahead of the midterms. The platform began to require “mandatory verification” for political accounts based in the United States and rolled out a blanket ban on all political fundraising.“As we have set out before, we want to continue to develop policies that foster and promote a positive environment that brings people together, not divide them,” Blake Chandlee, President of Global Business Solutions at TikTok, said in a blog post at the time. “We do that currently by working to keep harmful misinformation off the platform, prohibiting political advertising, and connecting our community with authoritative information about elections.”Meta said in September that its midterm plan would include removing false claims as to who can vote and how, as well as calls for violence linked to an election. But Meta stopped short of banning claims of rigged or fraudulent elections, and the company told The Washington Post those types of claims will not be removed for any content involving the 2020 election. Looking forward, Meta has banned US ads that “call into question the legitimacy of an upcoming or ongoing election,” including the midterms, according to company policy.Google also took steps in September to protect against election misinformation, elevating trustworthy information and displaying it more prominently across services including search and YouTube.The big social media companies typically rely on a mix of artificial intelligence systems and human moderators to vet the vast amount of posts on their platforms. But even with similar approaches and objectives, the study is a reminder that the platforms can differ wildly in their content enforcement actions.According to the researchers, the only ad they submitted that TikTok rejected contained claims that voters had to have received a Covid-19 vaccination in order to vote. Facebook, on the other hand, accepted that submission.","Facebook and TikTok are approving ads with âblatantâ misinformation about voting in midterms, researchers say"
27,2,2_meta_facebook_users_said,https://arstechnica.com/tech-policy/2022/10/metas-value-plunges-more-than-65-billion-amid-falling-sales-rising-costs/,"Investors wiped more than $65 billion from Meta’s market capitalization on Wednesday after the Facebook owner reported another quarter of declining revenues and failed to convince investors that big bets on the metaverse and artificial intelligence were paying off.Shares in Meta dropped 19 percent in after-hours trading as the world’s largest social media platform joined other Big Tech groups in warning that an economic slowdown was hammering its advertising businesses as brands spend less on marketing.On top of the wider macroeconomic woes, Meta faces a confluence of challenges, including rising competition for its Instagram platform from rivals such as short-form video app TikTok and difficulties in targeting and measuring advertising because of Apple’s privacy policy changes.The company said it expected revenue in the current quarter to be in the range of $30 billion to $32.5 billion, compared with analysts’ expectations of $32.2 billion.Net income in the third quarter fell 52 percent to $4.4 billion, below consensus estimates for $5 billion, according to S&P Capital IQ. Meanwhile, revenues fell 4 percent to $27.71 billion, the slowest pace of growth since going public in 2012, after a 1 percent decline last quarter. That was slightly better than analysts’ estimates for a 5 percent drop.Mark Zuckerberg, Meta founder and chief executive, warned the company faced “near-term challenges on revenue” but said “the fundamentals are there for a return to stronger revenue growth.”On a call with analysts, he doubled down on his biggest bets including developing a short-form video format to rival TikTok, business messaging, and the metaverse. He tried to reassure investors that investments in these areas would pay off in the long term.“I appreciate the patience and I think that those who are patient and invest with us will end up being rewarded,” he said, arguing that the company was doing “leading work” on the metaverse that would be “of historical importance.”AdvertisementMeta’s disappointing earnings came amid a broader sell-off of Big Tech stocks. Shares of Google parent Alphabet fell more than 9 percent on Wednesday after it reported an unexpectedly severe slowdown in its core search ads business, while Snap’s stock plunged last week after it posted its slowest pace of growth since going public in 2017.Meta, which expanded headcount rapidly during the pandemic, has faced investor scrutiny for spending heavily on Zuckerberg’s vision of building a digital avatar-filled world known as the metaverse. Like other virtual and augmented reality projects Meta is working on, this is not expected to generate returns for many years.Revenues from Reality Labs, its metaverse unit, nearly halved in the third quarter to $285 million, while losses were $3.7 billion compared with $2.6 billion a year ago. The company said it expected operating losses in the unit to “grow significantly year-over-year” in 2023.“Meta is on shaky legs when it comes to the current state of its business,” said Debra Aho Williamson, an analyst at Insider Intelligence. “Zuckerberg’s decision to focus his company on the future promise of the metaverse took his attention away from the unfortunate realities of today.”The company estimated 2022 total expenses would be in the range of $85 billion to $87 billion, narrowing from its prior outlook of $85 billion to $88 billion. However, it anticipated 2023 expenses in the range of $96 billion to $101 billion despite recently seeking to cut costs and freeze most hiring.The company said it was “making significant changes across the board to operate more efficiently” and had “increased scrutiny on all areas of operating expenses.”But it warned “these moves… will take time to play out” and that some attempts to find savings, like shrinking its office space as more employees work from home, would result in “incremental costs in the near term.”Zuckerberg told analysts that investment in its artificial intelligence capabilities contributed to a surge in capital expenditure but that the technology would help boost views of its short-form video format.Analysts also raised concerns about mounting expenses. “Summing up how investors are feeling right now is that there are just too many experimental bets versus proven bets on the core,” said Brent Thill, an analyst at Jefferies.© 2022 The Financial Times Ltd. All rights reserved. Not to be redistributed, copied, or modified in any way.","Metaâs value plunges more than $65 billion amid falling sales, rising costs"
374,2,2_meta_facebook_users_said,https://www.express.co.uk/life-style/science-technology/1687324/WhatsApp-down-chat-app-offline-cant-send-messages,"We use your sign-up to provide content in ways you've consented to and to improve our understanding of you. This may include adverts from us and 3rd parties based on our understanding. You can unsubscribe at any time. More infoUPDATE FOUR - Friday 10pm: The issues which affected WhatsApp and other Meta apps earlier today have now been resolved.Speaking to Express.co.uk, a spokesperson said: ""Earlier today, a configuration change caused some people to have trouble accessing our products. We fixed the issue as quickly as possible for everyone who was impacted, and we apologise for any inconvenience.""According to independent outage monitor Down Detector, Meta's apps were experiencing issues for around an hour before problems were fixed.UPDATE THREE - Friday: WhatsApp has been hit with its second major outage in the space of a few days, with the hugely popular chat app going down on a Friday evening in the UK.Independent outage monitor Down Detector recorded a huge spike in WhatsApp down reports tonight, which began surging in around 8.30pm UK time.At the time of writing Down Detector has registered a peak of almost 10,000 reports of WhatsApp down.The second WhatsApp outage of this week coincided with reported problems with Meta's other apps - Messenger, Instagram and Facebook.WhatsApp is the world's most popular chat app and is used by over two billion people worldwide each month.UPDATE TWO - Tuesday 10am: WhatsApp is finally coming back online after almost two hours of issues. Most users should now start seeing their chats being sent and received via the popular platform. It's unclear what has caused the problems but things seem to be working again.ORGINAL STORY: If you're trying and failing to send a WhatsApp message this morning you are not alone. The hugely popular service, which boasts over 2 billion users worldwide, has been hit by some serious gremlins which have left thousands of fans unable to use the platform or speak to friends and family. When trying to chat, many are simply seeing the clock icon on their messages which means they haven't been sent.",WhatsApp down: Chat app hit by second huge outage in the space of a few days
3,2,2_meta_facebook_users_said,https://9to5mac.com/2022/10/27/apple-cant-make-enough-iphone-14-pros/,"Apple’s fiscal Q4 earnings are out and while the company saw another record for revenue, there were some slight misses on expectations. CEO Tim Cook shared more details on what challenges the company is seeing like trying to keep up with iPhone 14 Pro demand.Speaking with CNBC’s Steve Kovach, Cook shared more on the situation with iPhone supply and demand:“We’re constrained right now on the 14 Pro and Pro Max and have been from the day we launched…and so obviously we’re chasing supply there and trying to get as much supply as we can to solve the demand.”That lines up with previous reports we’ve seen:Apple slightly missed expectations on Q4 iPhone revenue. But an important detail to remember is there was only just over a week of iPhone 14 and 14 Pro sales included in the period. Even so, Apple was able to grow YoY iPhone revenue by almost 10% and overall revenue for the quarter was up 8%.Cook said another major factor for its Q4 performance was the foreign exchange rates:“The foreign exchange headwinds were over 600 basis points for the quarter,” Cook told CNBC’s Steve Kovach. “So it was significant. We would have grown in double digits without the foreign exchange headwinds.”On the whole, Cook was proud of the company’s performance noting it “countered the industry trends on the phone if you look at third party estimates…”Read more on Apple’s Q4 results:FTC: We use income earning auto affiliate links. More.Check out 9to5Mac on YouTube for more Apple news:",Tim Cook says Apple canât yet make enough iPhone 14 Pros to meet demand
521,2,2_meta_facebook_users_said,https://www.theguardian.com/technology/2022/oct/24/tiktok-election-misinformation-voting-politics,"In the final sprint to the US midterm elections the social media giant TikTok risks being a major vector for election misinformation, experts warn, with the platform’s huge user base and its design making it particularly susceptible to such threats.Preliminary research published last week from digital watchdog Global Witness and the Cybersecurity for Democracy team at New York University suggests the video platform is failing to filter large volumes of election misinformation in the weeks leading up to the vote.TikTok approved 90% of advertisements featuring election misinformation submitted by researchers, including ads containing the wrong election date, false claims about voting requirements, and rhetoric dissuading people from voting.TikTok has for several years prohibited political advertising on the platform, including branded content from creators and paid advertisements, and ahead of midterm elections has automatically disabled monetization to better enforce the policy, TikTok’s global business president, Blake Chandlee, said in a September blog post. “TikTok is, first and foremost, an entertainment platform,” he wrote.But the NYU study showed TikTok “performed the worst out of all of the platforms tested” in the experiment, the researchers said, approving more of the false advertisements than other sites such as YouTube and Facebook.The findings spark concern among experts who point out that – with 80 million monthly users in the US and large numbers of young Americans indicating the platform is their primary source of news – such posts could have far reaching consequences.Yet the results come to little surprise, those experts say. During previous major elections in the US, TikTok had far fewer users, but misinformation was already spreading widely on the app. TikTok faced challenges moderating misinformation about elections in Kenya and the war in Ukraine.And the company, experts say, is doing far too little to rein in election lies spreading among its users.“This year is going to be much worse as we near the midterms,” said Olivia Little, a researcher who co-authored the Media Matters report. “There has been an exponential increase in users, which only means there will be more misinformation TikTok needs to proactively work to stop or we risk facing another crisis.”A crucial testWith Joe Biden himself warning that the integrity of American elections is under threat, TikTok has announced a slew of policies aimed at combatting election misinformation spreading through the app.The company laid out guidelines and safety measures related to election content and launched an elections center, which “connect[s] people who engage with election content” to approved news sources in more than 45 languages.“To bolster our response to emerging threats, TikTok partners with independent intelligence firms and regularly engages with others across the industry, civil society organizations, and other experts,” said Eric Han, TikTok’s head of US safety, in August.In September, the company also announced new policies requiring government and politician accounts to be verified and said it would ban videos aimed at campaign fundraising. TikTok added it would block verified political accounts from using money-making features available to influencers on the app, such as digital payments and gifting.TikTok has laid out guidelines and launched an elections center to combat misinformation on the app. Photograph: Nicolas Asfouri/AFP/Getty ImagesStill, experts have deep concerns about the spread of election falsehoods on the video app.Those fears are exacerbated by TikTok’s structure, which makes it difficult to investigate and quantify the spread of misinformation. Unlike Twitter, which makes public its Application Programming Interface (API), software that allows researchers to extract data from platforms for analysis, or Meta, which offers its own internal search engine called Crowdtangle, TikTok does not offer tools for external audits.However, independent research as well as the platform’s own transparency reports highlight the challenges it has faced in recent years moderating election-related content.TikTok removed 350,000 videos related to election misinformation in the latter half of 2020, according to a transparency report from the company, and blocked 441,000 videos containing misinformation from user feeds globally.The internet non-profit Mozilla warned in the run-up to Kenya’s 2022 election that the platform was “failing its first real test” to stem dis- and misinformation during pivotal political moments. The non-profit said it had found more than 130 videos on the platform containing election-related misinformation, hate speech and incitement against communities before the vote, which together gained more than 4m views.“Rather than learn from the mistakes of more established platforms like Facebook and Twitter, TikTok is following in their footsteps,” Mozilla researcher Odanga Madung wrote at the time.Why TikTok is so vulnerable to misinformationPart of the reason TikTok is uniquely susceptible to misinformation lies in certain features of its design and algorithm, experts say.Its For You Page, or general video feed, is highly customized to users’ individual preferences via an algorithm that’s little understood, even by its own staff. That combination lends itself to misinformation bubbles, said Little, the Media Matters researcher.“TikTok’s hyper-tailored algorithm can blast random accounts into virality very quickly, and I don’t think that is going to change anytime soon because it’s the reason it has become such a popular platform,” she said.Meanwhile, the ease with which users’ remix, record and repost videos – few of which have been fact-checked – allows misinformation to spread easily while making it more difficult to remove.TikTok’s video-exclusive content brings up additional moderation hurdles, as artificial intelligence processes may find it more difficult to automatically scrape video content for misinformation compared to text.A report from Harvard’s Shorenstein Center on Media found that design features on the app make it an easy pathway for misinformation. Photograph: Paula Bronstein/APSeveral recent studies have highlighted how those features have exacerbated the spread of misinformation on the platform. When it comes to TikTok content related to the war in Ukraine, for example, the ability to “remix media” without fact-checking it has made it difficult “even for seasoned journalists and researchers to discern truth from rumor, parody and fabrication”, said a recent report from Harvard’s Shorenstein Center on Media.That report cited other design features in the app that make it an easy pathway for misinformation, including that most users post under pseudonyms and that, unlike on Facebook, where users’ feeds are filled primarily with content from friends and people they know, TikTok’s For You Page is largely composed of content from strangers.Some of these problems are not unique to TikTok, said Marc Faddoul co-director of Tracking Exposed, a digital rights organization investigating TikTok’s algorithm.Studies have shown that algorithms across all platforms are optimized to detect and exploit cognitive biases for more polarizing content, and that any platform that relies on algorithms rather than a chronological newsfeed is more susceptible to disinformation. But TikTok is the most accelerated model of an algorithmic feed yet, he said.At the same time, he added, the platform has been slow in coming to grips with issues that have plagued its peers like Facebook and Twitter for years.“Historically, TikTok has characterized itself as an entertainment platform, denying they host political content and therefore disinformation, but we know now that is not the case,” he said.Young user base is particularly at riskExperts say an additional cause for concern is a lack of media literacy among TikTok’s largely young user base. The vast majority of young people in the US use TikTok, a recent Pew Research Center report showed. Internal data from Google revealed in July that nearly 40% of Gen Z – the generation born between the late 1990s and early 2000s – globally uses TikTok and Instagram as their primary search engines.In addition to being more likely to get news coverage from social media, Gen Z also has far higher rates of mistrust in traditional institutions such as the news media and the government compared with past generations, creating a perfect storm for the spread misinformation, said Helen Lee Bouygues, president of the Reboot Foundation, a media literacy advocacy organization.“By the nature of its audience, TikTok is exposing a lot of young children to disinformation who are not trained in media literacy, period,” she said. “They are not equipped with the skills necessary to recognize propaganda or disinformation when they see it online.”Donald Trump supporters share a laugh after filming a dance for TikTok at his campaign event in Macon, Georgia, on 16 October 2020. Photograph: Dustin Chambers/ReutersThe threat is amplified by the sheer amount of time spent on the app, with 67% of US teenagers using the app for an average of 99 minutes per day. Research conducted by the Reboot Foundation showed that the longer a user spends on an app the less likely they are to distinguish between misinformation and fact.To enforce its policies, which prohibit election misinformation, harassment, hateful behavior, and violent extremism, TikTok says it relies on “a combination of people and technology” and partners with factcheckers to moderate content.The company directed questions to this blogpost regarding election misinformation measures, but declined to share how many human moderators it employs.Bouygues said the company should do far more to protect its users, particularly young ones. Her research shows that media literacy and in-app nudges towards fact-checking could go a long way when it comes to combating misinformation. But government action is needed to force such changes.“If the TikToks of the world really want to fight fake news, they could do it,” she said. “But as long as their financial model is keeping eyes on the page, they have no incentive to do so. That’s where policymaking needs to come into play.”",âWe risk another crisisâ: TikTok in danger of being major vector of election misinformation
368,2,2_meta_facebook_users_said,https://www.eurogamer.net/microsoft-will-keep-call-of-duty-on-sony-platforms-as-long-as-theres-a-playstation-out-there-to-ship-to,"Xbox boss Phil Spencer has made Microsoft's plainest promise yet around the future of Call of Duty on PlayStation platforms.Speaking to the Same Brain Youtube channel, Spencer pledged to keep releasing Call of Duty games on Sony's consoles ""as long as there's a PlayStation out there to ship to"".The future of Call of Duty on PlayStation has become a contentious topic for regulators such as the UK's Competition and Markets Authority (CMA), which is currently scrutinising Microsoft's planned $68n takeover of COD publisher Activision Blizzard.Watch on YouTube Eurogamer Newscast: Will Konami succeed bringing Silent Hill back from the dead?Microsoft has repeatedly said it will keep releasing Call of Duty games for the forseeable future - and previously promised Call of Duty would remain on PlayStation ""at least several more years"" beyond Sony's existing deal with Activision Blizzard. Still, regulators have questioned how long this will actually last, if and when Activision Blizzard is owned by Microsoft.Here's Spencer's latest quote on the subject in full:""We're not taking Call of Duty from PlayStation... That's not our intent,"" Spencer said. ""Our intent is not to do that and as long as there's a PlayStation out there to ship to, our intent is that we'll continue to ship Call of Duty on PlayStation - similar to what we've done with Minecraft since we owned that.""We've expanded the places where people can play Minecraft, we haven't reduced the places. And it's been good, it's been good for the Minecraft community - in my opinion - and we want to do the same when we think where Call of Duty can go over the years.""Another issue for regulators has been the competitive advantage Microsoft might gain from including Call of Duty in its Game Pass subscription service.If it did so, Microsoft could still keep releasing Call of Duty on PlayStation via its typical £70 upfront price, while alternatively offering it at no further cost to Xbox owners who have Game Pass already.""For Xbox itself, players who have invested in our console, the biggest addition you're going to see is some great games coming to Game Pass,"" Spencer continued, without mentioning Call of Duty specifically by name. ""This isn't going to be about pulling those communities off of those other platforms. But I want it to be a great place to see those games.""In September, PlayStation and Xbox traded blows over the future of Call of Duty following the announcement by the UK's CMA that it would further investigate Microsoft's $68bn Activision Blizzard takeover attempt.""Giving Microsoft control of Activision games like Call of Duty"" had ""major negative implications"", Sony said at the time.In response, Microsoft fired back: ""It makes zero business sense for Microsoft to remove Call of Duty from PlayStation given its market leading console position.""This latest promise from Spencer comes as the deal faces intense scrutiny, ahead of a final ruling from the UK's CMA in spring 2023.","Microsoft will keep Call of Duty on Sony platforms ""as long as there's a PlayStation out there to shipÂ to"""
330,2,2_meta_facebook_users_said,https://www.cnbc.com/2022/11/01/snap-meta-stocks-pop-after-fcc-commissioner-floats-us-tiktok-ban.html,"Shares of U.S. social media companies Snap and Meta spiked on the news that a Federal Communications Commissioner said the U.S. government should ban TikTok.""I don't believe there is a path forward for anything other than a ban,"" Republican Commissioner Brendan Carr told Axios in an interview.Snap shares rose 3.4% and Meta shares were up 2.2% Tuesday.The comments from Carr, one of four current commissioners at the Democrat-led agency, do not necessarily signal any pending actions against TikTok.","Snap, Meta shares pop after FCC commissioner says U.S. should ban TikTok"
550,2,2_meta_facebook_users_said,https://www.vice.com/en/article/epzkne/facebooks-monopoly-is-imploding-before-our-eyes,"For years, the definition of success for many tech employees has been getting a job at a FAANG company (Facebook, Amazon, Apple, Netflix, Google). Amazon, Apple, Microsoft, Facebook, and Google, meanwhile, are often the five major companies people think of when they think of ""big tech.""But there is evidence that Facebook—once a dominant monopoly rightly blamed for all sorts of societal ills—is on the precipice of dropping out of this group through years of sheer mismanagement, a failure to innovate, setting money on fire in pursuit of a metaverse that seemingly no one wants, a vulnerable business model that Apple is squarely taking aim at, and upstart competitors like TikTok that the company seemingly has no answer for. What seemed impossible just a year or two ago—that Facebook will become just another tech company, more or less—now seems like a very real possibility.AdvertisementIn a little over one year, the company has shed nearly $800 billion of its market capitalization, with the lion's share of that coming these past eight months. To be clear, the company is one of the biggest tech firms in existence, with billions of people regularly using its products and a still growing user base, and yet, by the definition of one proposed antitrust bill, has sat below the market capitalization of what counts as “Big Tech” for months.The company’s pivot to the metaverse, complete with a name change (Meta Platforms Inc.) and a soulless PR campaign featuring chief executive Mark Zuckerberg’s sickly digital avatar, has resulted in it hemorrhaging money, while its core products—Facebook, Instagram, and WhatsApp—all seem to have very real vulnerabilities. Reality Labs, Facebook's metaverse fantasy team, burned through $4.5 billion in 2019, $6.62 billion in 2020, and $10.19 billion in 2021 (that’s over $21 billion).In a February 2022 earnings call, chief financial officer David Wehner said those operating losses would ""increase meaningfully"" this year. And they have. Another $9.4 billion in losses have been realized in just the last three quarters, bringing Reality Labs’ operating losses to north of $31 billion. On this week's third quarter earnings call, Wehner warned that they ""anticipate that Reality Labs operating losses will grow significantly year-over year."" Meta's stock has fallen about 70 percent this year.AdvertisementBy all indicators, the metaverse is a wasteland devoid of any souls save those who are too zealous or too well compensated to realize admit how stupid it is. For now. While Zuckerberg’s main contribution has been to add legs to his company’s avatars and ship out nausea-inducing headsets needed to access this realm, he promises that this new world he’s building should be ready in 10 to 15 years.Zuckerberg's obsession with the metaverse is one major problem, but there are fundamental issues plaguing the company's core business that suggest Meta isn't going to just be able to effortlessly maintain the massive money printing factories that are Facebook and Instagram, and there is even reason to worry about WhatsApp's future as the world's most popular messenger.Schooled By a Real Monopoly: AppleFacebook’s core advertising business is flashing some warning signs thanks to another, more competent monopoly that has long been a thorn in its side: Apple.In a Q1 earnings call, Facebook warned that Apple’s 2021 privacy changes to its iOS operating system—which makes it harder for third parties like Facebook to harvest data to target users—would be ""a pretty significant headwind for our business"" to the tune of $10 billion in advertiser revenue this year. In a Q2 earnings call, Zuckerberg warned of ""an economic downturn that will have a broad impact on the digital advertising business."" Sure enough, over the past four quarters, Facebook's ad revenue has faltered: $33.67 billion (Q4 ‘21), $26.998 billion (Q1 ‘22), $28.152 billion (Q2 ‘22), and $27.2 billion (Q3 ‘22), with first-ever year-over-year declines reported these last two quarters.AdvertisementFor investors looking to generate excess profits on trades and investments, all of this is part of a dark and dreary picture. Facebook's revenue has declined for two consecutive quarters, costs and expenses are surging, operating margin is spiraling downwards, net income has been cut substantially, and so investors have abandoned ship and brought the share price down nearly 70 percent this year.Earlier this week, Apple announced yet another change that would also hit Facebook. Apple said it would consider buying ads within the Facebook app to be a ""digital purchase"" subject to the App Store's 30 percent commission. It's too early to say how much this will affect Facebook, but it's not good. This is especially important for a few reasons: Facebook makes more money per user in North America than it does from any other region, and Apple's iPhone is now used by more Americans than Android is. It's also gaining market share around the world. iPhone owners are also, on average, more wealthy and thus it can be more expensive to target them with ads.The rise of the iPhone in the U.S. and, more importantly, around the world may also, eventually, be problematic for Facebook if WhatsApp users begin to migrate toward iMessage and other messaging apps.An Advertising PlatformBeyond an excuse to indulge in schadenfreude, should you or anyone else you know care about Zuckerberg losing $100 billion of his net worth?AdvertisementAs Malcolm Harris points out in a terrific NYMag piece, there are some people thinking about all of this through the lense of ""technofeudalism,"" which argues that capitalist firms have leveraged monopolies into extensive data extractivist and rentier schemes. In this telling, Facebook is all-powerful and its march towards omnipotence inevitable—but on closer examination, we might realize this sounds remarkably like Silicon Valley’s self-mythology that doesn't track with how things have actually ended up.""Facebook is much less than what the technofeudalists make it out to be,” Harris writes. “It's an advertising platform that wrings pennies out of users' scrap time—attention that would otherwise go to waste, at least from the capitalist perspective.""For a long time the heart of Meta was Facebook, its advertising platform masquerading as a social network. Its major moves to reach for digital monopoly status beyond this came in the form of acquisitions or clones of competitors' products. Instagram was acquired for $1 billion in 2012, Oculus VR in 2014 for $2 billion, and WhatsApp for $19 billion in 2014.Its dozens of acquisitions have not only worked to support its core offerings, but eliminate competition or buyout talent—and when that fails, cloning a competitor service has been an option. Most notably, Facebook has offered clones in the form of Portal—an Amazon Echo clone that was recently killed off for consumers—and Reels, a TikTok clone that users and advertisers have struggled with. Reels has proven to be a disaster, with Instagram users spending less than 10 percent of the time watching Reels as TikTok users spend on their platform. None of Facebook's clones have been very successful since Instagram Stories, which was introduced all the way back in 2016. Quite simply, by many metrics, Facebook is getting its ass kicked by TikTok.AdvertisementMonopolize the metaverse or fade to irrelevanceWith Facebook core still incredibly popular worldwide but increasingly feeling like a bloated piece of garbage whose power users in the United States are aging (and, is, specifically, not being used by American teens) Instagram is regularly held up as being a lesser disaster of a platform, albeit one whose most popular and famous users are actively revolting against it. With all this going on, Facebook is now leaning on the last of its monopoly-seeking acquisitions that it thinks might still have legs: virtual reality, which is turning into a gigantic money pit.This is a stunning change of circumstances for a company that once threw its weight around with confidence and attempted to colonize as much of life outside of the Facebook app as possible. In pursuit of capitalist monopolies in various sectors, there exists a long list of projects Facebook has poured its endless resources into. Facebook has sought to radically change aspects of our lives with decisions about how specific platforms will operate, precisely because it has leveraged economic power into other forms.AdvertisementAt one point, Facebook even tried to monopolize the global monetary system with Libra, a global cryptocurrency backed by a basket of currencies and assets (e.g. a stablecoin), and Calibra, a digital wallet for said stablecoin. Immediately, regulators worldwide expressed concerns Libra would compete with sovereign currencies and undermine their authority on designing and implementing monetary policy. Part of Facebook's pitch was that it was too big to fail—or be broken up—in the context of a geopolitical struggle against China and its technology firms. Facebook promised Libra would extend the power of the U.S. dollar, and even downgraded its plans to a U.S.-backed stablecoin (Diem) coupled with a smaller wallet (Novi). Stil, the plan was laid into by Congress, quietly killed by financial authorities, sold for scraps to a bank, and the project’s head slunk out the back.What was the right way to understand Libra? A technofeudalist might have read it as another milestone on its inevitable march towards omnipotence. The project was announced with a coalition of dozens of corporations and non-profit organizations, it was being pushed by a chief executive with inordinate influence in Washington and Wall Street, and by a company with billions of users. And yet it was smothered in the crib.Evgeny Morozov—founder of The Syllabus and one of the main critics of the technofeudal model—offered a much simpler rationale that speaks to how the company has flailed for unassailable monopolies as its core product lagged: Facebook wanted to create another core business. It was interested in finance because Chinese tech giants showed payments and communications systems complement one another well; to compete in foreign markets with established Chinese firms it would need to offer its own payment-communication system; by aggressively moving against Chinese firms it could skirt regulatory roadblocks by framing itself as a strategic asset in a tech Cold War with China. Morozov wrote that Libra would have also helped the social network turn a greater profit.Advertisement“Yes, Facebook would need to pay something to its users—but, in turn, it would also be able to charge them for its services,"" he wrote. ""As long as all such transactions are conducted in a currency under its implicit control—and if Facebook succeeds in convincing its users that their data, on its own, has far less value than the services it supplies—it would not necessarily be such a bad outcome for the company.”So, Facebook was first and foremost pursuing a strategy to diversify its business while insulating it from antitrust scrutiny. It was denied that opportunity, but this doesn’t diminish the very real need for that pivot—especially as antitrust scrutiny has increased in the years since Libra was first proposed. Facebook doubling down on the metaverse, despite the infeasibility of the project and despite declines in advertising revenue, suggest lethargy as much as ambition. We’re seeing the reformulation of a necessary but desperate gambit by the company to do something which will allow it to preserve a key role in the digital economy, with or without advertisers. Finance was the first attempt, a depressing digital simulacrum of the real world is the second one.Facebook Hasn't Fallen YetEvery time Zuckerberg was trotted out in front of Congress, he was adamant that Facebook is not a monopoly, and that it faces plenty of competition on the internet. It was hard to imagine, at the time, that Zuckerberg would shift from being a weirdo obsessed with dominating social media to become a weirdo obsessed with lighting cash on fire in pursuit of becoming the premiere place to play virtual ping pong with a heavy computer strapped to your face. It was also hard to predict that this obsession would utterly tank his company.But just because Facebook appears to be in actual, real trouble for the first time in its history does not mean this slow decline to become just another advertising giant is inevitable, nor does it mean that we can forgive and forget its monopolistic behaviors and endeavors. Facebook is still a gigantic force that has spread an endless amount of disinformation and misinformation worldwide, a hugely important platform, and a monopolistic company; this cannot be waved away simply because the company is grossly incompetent.Perhaps Facebook's most monopolistic endeavor was Free Basics, a program to provide ""free"" internet access to people in developing countries—free, as long as the ""internet"" they were accessing was Facebook. The legacy of Free Basics and the simple fact that huge parts of the global population still interact with Facebook or Facebook-owned platforms as their only access to ""the internet"" is deeply concerning and remains dangerous.Within this understanding of Facebook, though, there’s reason to pause and celebrate. For one, while this is still a juggernaut that can and will throw its weight around at great cost to us and great profit to itself, it’s also a fragile and withering one that has to contend with investors who don’t care about Zuckerberg’s next three Five-Year Plans for competing with China and building a core non-advertising business line. There is a possible near future, if we're not already there, where Meta is just another company rather than a world-shaping monolith, having been outfoxed and outclassed by more competent monopolies and wrecked by the hubris of its chief executive.Secondly, regulators seem to be wise to this plan, or at least elements of it: the FTC has already sought to block Facebook acquisitions of companies that might help it build the metaverse it so desperately needs to work at this point. Finally, Meta's failure is another chance to spur people to think about and advocate for alternatives to the technological offerings we have today, and to prevent Facebook from recementing its stranglehold on our culture or upstarts from recreating it. What sort of communication, payments, and social media platforms do we actually want—especially if we don’t design them with advertiser revenue as the core concern? What sort of technologies should be allowed to flourish and what sorts should be prohibited?",Facebookâs Monopoly Is Imploding Before Our Eyes
25,2,2_meta_facebook_users_said,https://arstechnica.com/tech-policy/2022/09/fcc-advances-plan-to-require-blocking-of-spam-texts-from-bogus-numbers/,"The Federal Communications Commission today released a plan to require mobile carriers to block a wide range of illegal text messages.""In this Notice of Proposed Rulemaking (NPRM), we propose to require mobile wireless providers to block illegal text messages, building on our ongoing work to stop illegal and unwanted robocalls,"" the FCC order said. ""Specifically, we propose to require mobile wireless providers to block texts, at the network level, that purport to be from invalid, unallocated, or unused numbers, and numbers on a Do-Not-Originate (DNO) list."" These texts ""are highly likely to be illegal,"" the FCC said.The NPRM seeks public comment on the plan. Once the NPRM is published in the Federal Register, there will be 30 days for comments and another 15 days for reply comments. After that, the FCC can draft new requirements for mobile carriers and set up a final vote.""The American people are fed up with scam texts, and we need to use every tool we have to do something about it,"" FCC Chairwoman Jessica Rosenworcel said. ""Recently, scam text messaging has become a growing threat to consumers' wallets and privacy. More can be done to address this growing problem and today we are formally starting an effort to take a serious, comprehensive, and fresh look at our policies for fighting unwanted robotexts.""In addition to seeking comment on the proposed rules, the FCC order seeks comment more generally on the problem of spoofed text messages. ""We also seek comment on the extent to which spoofing is a problem with regard to text messaging today and whether there are measures the Commission can take to encourage providers to identify and block texts that appear to come from spoofed numbers,"" the FCC said.AdvertisementThe FCC also asked for comment ""on applying caller ID authentication standards to text messaging."" Caller ID authentication is already required for phone calls on the Internet Protocol portions of voice networks.Curious vote timing for year-old itemThe timing of the NPRM's release is curious because commissioners could have voted on it any time in the past 11.5 months. The item was circulated by Rosenworcel to commissioners on October 18, 2021, according to the FCC's list of items on circulation.When the FCC chair circulates an item, commissioners can vote whenever they're ready. The NPRM, ""Targeting and Eliminating Unlawful Text Messages,"" was the oldest one on the list of circulated items.An Axios report today said the vote was finally concluded shortly after a reporter asked why it was taking so long. ""The Federal Communications Commission approved a long-delayed proposal to crack down on spam texts Friday night after Axios asked agency members why it hadn't moved on the issue,"" the article said, noting that it ""had been awaiting a vote at the FCC for nearly a year.""The spam text NPRM was approved Friday and released publicly today. The vote was reportedly 4-0, meaning both Democrats and both Republicans approved.We asked Rosenworcel's office and the other commissioners today for details on when each member voted. Democratic Commissioner Geoffrey Starks ""voted on this item well before Friday,"" Starks' office told Ars.A spokesperson for Rosenworcel said the chair's office defers to commissioners on whether they want to disclose the timing of their votes, but also said the robotext proposal has had Rosenworcel's ""strong support"" since she proposed it.Republican FCC member Brendan Carr didn't say when he voted, but he told Ars in an email, ""This is a good item, and I'm hoping the FCC moves quickly to an order on it. I can tell you that I was not dragging [my] feet on it.""AdvertisementIt's not clear when Republican Nathan Simington cast a vote. We'll update this article if we get new information on the vote timing. The FCC still lacks a Democratic majority due to Senate inaction on Biden nominee Gigi Sohn.Robotexts a big and growing problemComplaints about spam texts are rising. The NPRM said the FCC ""received approximately 14,000 consumer complaints about unwanted text messages, representing an almost 146 percent increase from the number of complaints the year before."" Complaints rose to 15,300 in 2021, and 8,500 in the first six months of 2022.""Unwanted text messages present the same problems as unwanted calls—they invade consumer privacy, and are vehicles for consumer fraud and identity theft,"" the FCC said.Robocalls are still a bigger problem in terms of overall complaints, but the FCC noted that text message-based scams present some additional harms not seen with robocalls. The NPRM described how spam texts use phishing and malware to scam victims:Texts can include links to well-designed phishing websites that appear identical to the website of a legitimate company and fool a victim into providing personal or financial information. Texted links can also load unwanted software, including malware that steals passwords and other credentials, onto a device. Scam texts, like scam calls, may involve illegal caller ID spoofing, i.e., falsifying the caller ID information that appears on the called party's phone with the intent to defraud, cause harm, or wrongfully obtain something of value. In 2020, scammers stole over $86 million through spam texting fraud schemes. The median amount stolen from consumers in such scams was $800.The FCC issued a consumer alert about the rise in robotext scams on July 28. The FCC's Consumer Advisory Committee last month finalized a report on the state of text messaging, including information on the sources of illegal and unwanted text messages and tactics used by bad actors to defraud consumers.",FCC advances plan to require blocking of spam texts from bogus numbers
312,2,2_meta_facebook_users_said,https://www.cbsnews.com/amp/news/meta-stock-down-earnings-700-billion-in-lost-value/,"Facebook parent Meta Platforms is making a huge investment in virtual reality, but its actual reality is looking like a real disaster.Meta shares tumbled 24% on Thursday to its lowest level in nearly four years following an earnings report that one Wall Street analyst described as a ""train wreck."" It's a far cry from the company's position nearly a year ago, when CEO Mark Zuckerberg on October 28, 2021, announced with great fanfare that Facebook was changing its name to Meta Platforms to emphasize its focus on the ""metaverse.""Last fall, Facebook was still riding high: Its market value reached a peak of more than $1 trillion in September 2021. Revenue and profits were surging as advertisers flocked to Facebook and Instagram to reach their billions of users.Click here to view related media. click to expandTo be sure, practically the entire tech industry has taken a beating this year, but Meta's stock plunge has far outpaced the overall sector, with its shares down 67% from a year earlier compared with the tech-heavy Nasdaq's 31% slide over the same period. Meta's plunge translates into an eye-popping loss of about $700 billion in market value.On Thursday, Meta's market value sank to $268 billion, down from more than $1 trillion in September of 2021. The shares regained some ground on Friday morning, rising $1.72, or about 1.8%, to $99.66 per share.The company's travails raise questions about its all-in bet on the metaverse, as well as whether the social media company could suffer the fate of other major businesses whose gambles on the future failed to pay off. In the near-term, Meta's core Facebook business is facing challenges as the economy slows and advertisers trim spending.""Meta's results last night was an absolute train wreck that speaks to pervasive digital advertising doldrums ahead for Zuckerberg & Co. as they make the risky and head scratching bet on the metaverse,"" Wedbush analyst Dan Ives said in a report.The hit to Meta has also whittled down Zuckerberg's personal fortune, since most of his wealth stems from his 13% stake in the social media company. He is worth $37.7 billion as of October 27, according to Bloomberg Billionaires Index, having lost almost $88 billion in wealth during the past 12 months.Here are three key issues slamming Meta shares and deepening questions about its longer-term prospects.$9.4 billion in metaverse lossesOn a Wednesday conference call to discuss Meta's latest earnings, Zuckerberg told investors he is ""pretty confident this is going in a good direction.""Investors aren't convinced. The company is making what amounts to a wildly expensive bet on its ability to transform into a virtual reality behemoth and whether that technology can power the next phase in Meta's growth.Although such strategic pivots can take years for big companies to execute — as it did for IBM and Microsoft as they morphed from selling hardware to software — the early returns for Meta have been grim. For the first nine months of the year, Meta lost $9.4 billion on its metaverse unit, Reality Labs. It expects the unit to have ""significantly"" wider operating losses in 2023, the company said on Wednesday.Investors are skeptical because, at least so far, consumers aren't exactly flocking to the fledgling metaverse. Unlike the longer time-lines for building businesses common in Silicon Valley, Wall Street values companies based on near-term returns rather than hazier projections that stretch years into the future.Horizon Worlds, Meta's new virtual space, trimmed its goal for monthly active users to 280,000 from 500,000, but the space is attracting fewer than 200,000, the Wall Street Journal reported earlier this month.""[I]nvestors should remain on the sidelines as it will take many years before progress in the metaverse can be truly monetized,"" Angelo Zino, senior equity analyst CFRA Research, told investors in a research note.Slower Facebook growthBy comparison, Facebook had a massive base of 1.98 billion active daily users on average for September — a 3% increase from a year ago.That may seem respectable, but it's far from the huge growth Facebook experienced in earlier years. And the slower growth comes after Facebook in February said it had lost users for the first time in its history.The social media juggernaut, Meta's huge moneymaker, is battling challenges from upstarts like TikTok, which is grabbing younger consumers.Advertising challengesMeta's lifeblood is the advertising revenue booked by Facebook, Instagram and WhatsApp, with businesses eager to reach their billions daily users. But its ad revenue fell in the most recent quarter, with sales drooping 3.7% and adding to investor concerns.Meta announces its first hiring freeze, signaling tech slowdownOn the ad front, Meta faces a double whammy. An economic slowdown means that advertisers are cutting spending, with the company on Wednesday pointing to an ""uncertain and volatile macroeconomic landscape"" for ads. The company is also grappling with the impact of Apple's privacy changes to apps that run on its devices. That change means consumers can ask apps to not track them, and which Facebook has said will cost it $10 billion this year.","Meta's value has plunged by $700 billion. Wall Street calls it a ""train wreck."""
273,2,2_meta_facebook_users_said,https://techcrunch.com/2022/10/31/elon-musk-vine-reboot-poll/,"New Twitter owner Elon Musk has signalled a potential interest in reviving Vine, the social video app Twitter gave up on six years ago.In the last few hours the self styled “Chief Twit”) has tweeted a yes/no poll to his 112.4 million followers — pithily positing: “Bring back Vine?”.The question — whether serious product idea or another Elon flight of fancy/’trollercoaster’ tweet — quickly drew millions of votes (most in favor) and thousands of responding tweets. This engagement in turn garnered a bit of additional attention from Musk who responded to some of his followers’ questions/suggestions on the social video topic.In response to a tweet from YouTuber, Mr Beast — which jokily referenced TikTok — Musk struck a serious tone, asking: “What could we do to make it better than TikTok?”He also took the time to publicly agree with another Twitter user — who had chimed in to opine that “video shouldn’t be a separate app” but should rather be “within Twitter”. That observation earned the ‘Tesla Owners Silicon Valley’ account an ‘100’ emoji reply from Musk, to signal (apparent) total approval of the idea that any Vine/video revival should be a feature inside Twitter, not a standalone product.So make of that what you will.The Vine revival poll (plus Musk’s wider engagement to the chatter it generated) could imply his attention to the topic of video is, at least, genuine — although it’s anyone’s guess whether a Vine reboot is actually being seriously considered, or what that would even mean beyond reviving a brand name if his plan would include a major feature set makeover. (Twitter’s press team did not respond to requests for comment.)It’s equally plausible Musk is doing the equivalent of throwing spaghetti at a wall and seeing which/if bits stick. Or he’s just bored in the small hours and thought he’d bounce another shower thought off-of his fanbase. And/or fancied indulging in a spot of ‘lazy web’ brainstorming outsourcing.As ever where Elon is concerned, the usual caveats apply.Still, Vine’s demise was a pretty self-defeating chapter in Twitter’s company history — so a reboot could offer Musk the chance to rewrite that particular bit of the script and claim incoming hero status with former Vine fans.Bring back Vine? — Elon Musk (@elonmusk) October 31, 2022At the time of writing, more than 2.8M votes had been cast in Musk’s poll about bringing back Vine, with a strong majority (69.4%) voting for a return — although the poll still has 14 hours left to run.Twitter acquired the short form video platform Twitter back in 2013. But, in typical clown car fashion, the company ended up squandering the opportunity to build the fledgling social video platform into a TikTok-style juggernaut, after then-CEO Jack Dorsey opted to ditch the app a few short years later.The shuttering of Vine earned Twitter the baldest of subtweets from Vine’s founder, Rus Yusupov — who had this straight-up warning to other entrepreneurs at the time: “Don’t sell your company!”Ironically enough (given it’s *Twitter* that’s now been sold), Yusupov has chipped into the Musk-initiated chatter around reviving Vine — offering up the (non-serious?) suggestion that Vine should have “69 second videos”.69 second videos — Rus (@rus) October 31, 2022While Vine launched with hyper short videos (of up to 6 seconds), the max upload length was later expanded to 140 seconds — and social video behemoth TikTok has gone way beyond that and even began letting its users upload videos of up to 10mins long earlier this year — so 69 second videos is probably just an attempt to grab Musk’s attention (with a puerile joke), not a serious product suggestion.Regardless, the interaction does indicate that Yusupov is watching what’s unfolding at Twitter with interest.And that’s relevant because this is also not his first foray into tweeting at Musk about Vine: His public timeline remains topped with an earlier tweet — from April 25 — when he posted a photo of himself with the (then) would-be Twitter owner, asking Musk a (rhetorical?) question: “Hey @elonmusk was this meeting about saving Vine? I forget… ”Public response from Musk to that tweet there came none. But the Chief Twit is king of keeping everyone else guessing over what he’s actually going to do next — so, again, you can’t read much/anything into his public silence back then. (Musk did also try to wiggle out of buying Twitter entirely prior to agreeing to closing the deal last week; ergo, there have been a fair few cycles of ups and downs even just over these past few months.)Hey ⁦@elonmusk⁩ was this meeting about saving Vine? I forget…. pic.twitter.com/FgmxIfS8Vx — Rus (@rus) April 25, 2022If Yusupov and Musk have been talking about a Vine reboot, the former’s response to another Twitter user’s question last month — when author Eli Pariser asked the Twitterverse for “the smart take on why Vine died even though it was basically TikTok” — might offer a few clues about what any discussions might have focused on.The Vine founder responded to Pariser by lamenting over not having built the right features “in time” — likely linked to another blindspot he conceded around not understanding the importance of building on trends like lip-sync video — as well as blaming the app’s demise on a failure to help creators monetize.","Chief Twit, Elon Musk, signals interest in reviving Vine"
552,2,2_meta_facebook_users_said,https://www.vice.com/en/article/pkgma8/police-are-using-dna-to-generate-3d-images-of-suspects-theyve-never-seen?utm_source=reddit.com,"On Tuesday, the Edmonton Police Service (EPS) shared a computer generated image of a suspect they created with DNA phenotyping, which it used for the first time in hopes of identifying a suspect from a 2019 sexual assault case. Using DNA evidence from the case, a company called Parabon NanoLabs created the image of a young Black man. The composite image did not factor in the suspect’s age, BMI, or environmental factors, such as facial hair, tattoos, and scars. The EPS then released this image to the public, both on its website and on social media platforms including its Twitter, claiming it to be “a last resort after all investigative avenues have been exhausted.”AdvertisementThe EPS’s decision to produce and share this image is extremely harmful, according to privacy experts, raising questions about the racial biases in DNA phenotyping for forensic investigations and the privacy violations of DNA databases that investigators are able to search through.In response to the EPS’s tweet of the image, many privacy and criminal justice experts replied with indignation at the irresponsibility of the police department. Callie Schroeder, the Global Privacy Counsel at the Electronic Privacy Information Center, retweeted the tweet, questioning the usefulness of the image: “Even if it is a new piece of information, what are you going to do with this? Question every approximately 5'4"" black man you see? ...that is not a suggestion, absolutely do not do that.”“Broad dissemination of what is essentially a computer-generated guess can lead to mass surveillance of any Black man approximately 5'4"", both by their community and by law enforcement,” Schroeder told Motherboard. “This pool of suspects is far too broad to justify increases in surveillance or suspicion that could apply to thousands of innocent people.”The victim of the case only had a limited description of the suspect, “describing him as 5’4”, with a black toque, pants and sweater or hoodie” and “as having an accent,” making for a vague, indistinguishable profile.“Releasing one of these Parabon images to the public like the Edmonton Police did recently, is dangerous and irresponsible, especially when that image implicates a Black person and an immigrant,” Jennifer Lynch, the Surveillance Litigation Director of the Electronic Frontier Foundation told Motherboard. “People of color are already disproportionately targeted for criminal investigations, and this will not only exacerbate that problem, it could result in public vigilantism and real harm to misidentified individuals.”AdvertisementThe criminal justice and policing system is laden with racial biases. A Black person is five times more likely to be stopped by police without cause than a white person, and Black, Latinx, and people of color are more likely to be stopped, searched, and suspected of a crime even when no crime has occurred.Seeing the composite image with no context or knowledge of DNA phenotyping, can mislead people into believing that the suspect looks exactly like the DNA profile. “Many members of the public that see this generated image will be unaware that it's a digital approximation, that age, weight, hairstyle, and face shape may be very different, and that accuracy of skin/hair/eye color is approximate,” Schroeder said.In response to the criticism after the release of the image and the use of DNA phenotyping, the Edmonton Police Department shared a press release Thursday morning, in which it announced it removed the composite image from its website and social media.“While the tension I felt over this was very real, I prioritized the investigation – which in this case involved the pursuit of justice for the victim, herself a member of a racialized community, over the potential harm to the Black community. This was not an acceptable trade-off and I apologize for this,” wrote Enyinnah Okere, the chief operating officer of EPS.AdvertisementParabon NanoLabs sent Motherboard a number of case studies where DNA phenotyping alone helped solve murder and assault cases. However, the case studies do not address the larger concerns, which are a lot harder to measure—such as how many innocent people were questioned before the final suspect was arrested, and how the suspect image may have affected the public’s racial biases.According to Parabon, it has worked on hundreds of law enforcement investigations. On its site are a number of case studies, with many showing the comparison between the DNA profile and actual photo of the suspect. There are some similarities between the two photos, in that they both reflect the same race, gender, eye and hair color. Often, however, the resemblance between the generated image and the suspect ends there.“We're making predictions just from the DNA, so we have only so much information. And so when we make those predictions, it's a description and these are standing in. If the police had a witness, then they wouldn't need us,” Dr. Ellen Greytak, the director of bioinformatics and technical lead for the Snapshot division at Parabon NanoLabs, told Motherboard. “We’re providing facts, like a genetic witness, providing this information that the detectives can't get otherwise.”Advertisement“It's just the same as if the police had gotten a description from someone who, maybe you know, didn't see them up close enough to see if they had tattoos or scars, but described the person. What we find is that this can be extremely useful especially for narrowing down who it could be and eliminating people who really don't match that prediction,” Greytak said. “In these cases, by definition, they always have DNA and so we don't have to worry about the wrong person being picked up because they would always just match the DNA.”According to Greytak, the technology creates the composite image by running the suspect’s DNA through machine learning models that are built on thousands of people’s DNAs and their corresponding appearances.“The data that we have on the people with known appearances are from a variety of sources, some of them are publicly available, you can request access for them. Some of them are from studies that we've run, where we've collected that information,” Greytak said.The DNA dataset being used to create these composites raises more red flags regarding the privacy questions of DNA profiling. The “variety of sources,” include GEDmatch and FamilyTree DNA, which are open-source, free genealogy websites that give you access to millions of DNA profiles.Advertisement“People should know that if they send their DNA to a consumer-facing company, their genetic information may fall into the hands of law enforcement to be used in criminal investigations against them or their genetic relatives. None of this data is covered by federal health privacy rules in the United States,” Lynch said. ""While 23 and Me and Ancestry generally require warrants and limit the disclosure of their users’ data to law enforcement, other consumer genetic genealogy companies like GEDmatch and FamilyTree DNA provide near-wholesale law enforcement access to their databases.”Parabon NanoLabs claims that the images they generate aren’t based on race, but on their genetic ancestry. “When we talk about a person's genetic ancestry, or biogeographic ancestry, [which] is the term that we use for that, that is a continuous measure versus race, which is categorical,” Greytak said.However, researchers argue that taking familial origin into consideration while DNA profiling, as Parabon NanoLabs does, is not an objective measurement because it results in general populations being seen as more criminal than others.“Whereas the conventional use of DNA profiling was primarily aimed at the individual suspect, more recently a shift of interest in forensic genetics has taken place, in which the population and the family to whom an unknown suspect allegedly belongs, has moved center stage,” researchers led by anthropologist Amade M’charek wrote in a study titled “The Trouble With Race in Forensic Identification.” “Making inferences about the phenotype or the family relations of this unknown suspect produces suspect populations and families.”AdvertisementAfter a 2019 Buzzfeed investigation revealed that GEDmatch allowed police to upload a DNA profile to investigate an aggravated assault, the site changed its policies so that users had to opt in to law enforcement searches. Still, investigators are able to use a number of similar databases to upload suspect’s DNA and map out the suspect’s family tree until they can pinpoint the suspect’s true identity.A notorious case in which this tactic proved successful was in finding the Golden State Killer, a serial killer named Joseph James DeAngelo. After uploading his DNA to GEDmatch, investigators were able to find one of his family members who was already in the system, and trace DeAngelo down decades after he committed the crime.Many police departments have been collecting DNA from innocent people and people who commit minor crimes, such as Orange County, which has a database of more than 182,000 DNA profiles, almost all from people who faced misdemeanor charges, which include petty theft or driving with a suspended license. Several attorneys filed a lawsuit against the county, who claim that the database is against California law. The lawsuit says that handing over DNA is a “coercive bargain,” because those who hand over a DNA sample will receive lighter punishments or even a dismissed case.A similar lawsuit was filed in New York City by the Legal Aid Society, which accuses the city of operating a DNA database that violates state law and constitutional protections against unreasonable searches. These DNA databases again perpetuate the pervasive racial biases of the criminal justice system. Because people of color, especially Black and Latino people, make up 75 percent of people arrested in the past decade in NYC, the DNA database further inscribes criminality onto marginalized demographics.While race isn’t necessarily measured by DNA phenotyping, race is produced semiotically by the visual nature of DNA composite profiles and in the already biased DNA datasets, which these profiles are derived from. The usage of DNA phenotyping may have broken open a few cold cases, but we have to ask: at what cost.",Police Are Using DNA to Generate 3D Images of Suspects They've Never Seen
434,2,2_meta_facebook_users_said,https://www.nytimes.com/2022/10/26/technology/meta-facebook-q3-earnings.html,"A year ago, Mark Zuckerberg changed Facebook’s name to Meta and said he was going all in on the immersive digital world of the so-called metaverse.Since then, Meta has plowed billions of dollars into, and restructured itself around, the emerging technology — just as the global economy has slowed, inflation has soared and investors have begun paying more attention to costs.The combination has been nothing short of disastrous. This year, Meta’s earnings have been hit hard by its spending on the metaverse and its slowing growth in social networking and digital advertising. In July, the Silicon Valley company posted its first sales decline as a public company. Its stock has plunged more than 60 percent this year.On Wednesday, Meta continued that trajectory and indicated that the decline would not end anytime soon. It said it would be “making significant changes across the board to operate more efficiently,” including by shrinking some teams and by hiring only in its areas of highest priority.",Metaâs Profit Slides by More Than 50 Percent as Challenges Mount
405,2,2_meta_facebook_users_said,https://www.msn.com/en-us/news/us/facebook-workers-are-reportedly-under-duress-as-meta-stock-craters/ar-AA13uDlp,"© Olivier Douliery, AFP Via Getty Images In this file photo illustration taken on March 24, 2020, a Facebook logo is displayed on a smartphone in Arlington, Virginia.Workers at the Bay Area-based tech behemoth Meta, the owners of Facebook, Instagram and the struggling Meta Quest VR devices, are reportedly feeling the pinch as the company's stock falters dramatically.According to a Business Insider report from Wednesday, Facebook workers are feeling heightened pressure to overperform or get sacked.The publication, speaking with multiple anonymous Facebook employees, alleges that a new mandate has been implemented: Workers must “put in 200% effort” within three months — or quit.The report added that even high achievers in the company are not immune to layoffs.This detail dovetails with a recent open letter by prominent shareholder Brad Gerstner, who called for a cutback in Meta funding and a 20% cutback in staff costs.“Like many other companies in a zero rate world — Meta has drifted into the land of excess — too many people, too many ideas, too little urgency,” Gerstner wrote in a Medium post Monday.A Meta spokesperson disputed the specifics of the Business Insider report in a statement to SFGATE, but referenced Zuckerberg’s last earnings call in which he discussed plans to “steadily reduce headcount growth over the next year.”“This is a period that demands more intensity, and I expect us to get more done with fewer resources,” Zuckerberg said at the time. “We're currently going through the process of increasing the goals for many of our efforts. Previously challenging periods have been transformational for our company and helped us develop our next generation of leaders. I expect this period to be no different.”Meta stock cratered by nearly 25% at the end of the day Thursday, the lowest price it’s been since 2016, CNBC reported. (The share cost has risen marginally as of Friday morning, but remains below the $100 mark.) The decline comes after Facebook reported a decline in profit and revenue year over year.Hear of anything going on at Meta or at another Bay Area tech company? Contact Joshua Bote securely on Signal at 707-742-3756.",Facebook workers are reportedly under duress as Meta stock craters
55,2,2_meta_facebook_users_said,https://globalnews.ca/news/9237190/netflix-ad-breaks/,"Send this page to someone via emailCanadian Netflix users will see a new membership option starting Tuesday that costs less but comes with a catch: commercial breaks inserted into their favourite shows.After years of uninterrupted binge-watches, the world’s largest streaming service is making way for a word from its sponsors. And as inflation continues to pinch consumers, the proposal of a cheaper Netflix plan may sound enticing to some.Netflix isn’t alone in believing that commercial television is back in a big way.Several free ad-supported streaming services will launch in Canada over the coming weeks, all of them built on a business model that taps into the country’s multi-billion advertising industry to finance and acquire programming.Story continues below advertisementAnalysts say together the platforms could reshape how we watch and pay for television. More viewers are complaining that streaming costs have soared near the level of their old cable bills, which has pressured each service to reconsider its business model.“Consumers are faced with more choice, more platforms and are making more deliberate decisions as to which streaming services they keep and which ones to cancel,” said Justin Krieger, senior technology and media analyst at consultancy firm RSM Canada.4:43 Disastrous week for Netflix creates concern for future of streamingOf the newcomers, Pluto TV debuts on Dec. 1 with more than 100 channels of free TV series, movies and sports that stream “live” online on a platform that mimics the experience of channel surfing, complete with the commercials.Around the same time, CBC will introduce a revamped free streaming news channel that will be available on CBC Gem and multiple other streaming platforms. A flagship program hosted by Andrew Chang of “The National” will be the main attraction, with advertisements interspersed throughout the day.Story continues below advertisementSouth of the border, Disney Plus rolls out an ad-supported option later this year with some industry observers predicting it will apply the same model in Canada soon after. The ad tier will be introduced at the price of Disney’s existing commercial-free service. Subscribers who want to eliminate the ads will have to pay a premium.Each service has its own reasons for getting into the ad business.For Netflix and Disney, one of the main drivers is growing revenues as programming costs soar and competitors lure away subscribers.Meanwhile, the free streaming services use ad revenues to fund a slate of original and licensed programming, which puts incredible pressure on Netflix to maintain its leading position with attractive new films and shows.Netflix's pitchEarlier this year, after repeatedly swearing off the possibility of ever getting into advertising, Netflix changed its tune by announcing it would launch an ad tier for subscribers in key international markets.Story continues below advertisementIn Canada, the “basic with ads” plan costs $5.99 per month _ less than the plans without ads, which start at $9.99 and peak at $20.99 a month.As a trade-off for the savings, Netflix says subscribers will be presented with an average of four to five minutes of ads per hour played before and during their TV shows and films.Video quality on the Netflix ad plan tops out at 720p, leaving full high-definition streaming at 1080p and 4K for premium subscribers. Viewers also won’t be able to download titles on their devices and not everything in the service’s library will be available.Those restrictions will sour the appeal to many Netflix devotees, suggested Carmi Levy, a technology analyst based in London, Ont.He said Canadians were sold the idea of a commercial-free Netflix a decade ago which led other entrants in the market to mimic their approach with similar models.That’s different than the United States where Peacock, Paramount Plus and HBO Max all offer less expensive ad tiers as a subscription option, while Crackle and Amazon’s Freevee are among the major players in free, ad-supported platforms.“Canadians don’t have that legacy of experience and as a result may be more resistant to the way Netflix is introducing that service,” he said.Story continues below advertisement“It’ll take time for Netflix and others to educate Canadians on the advantages of paying less for a streaming service and getting ads served up in return.”Do Canadians want ads?Kaan Yigit, a technology analyst at Solutions Research Group, said a survey conducted by his firm earlier this year found U.S. viewers have already adopted ad-supported subscription options.About 40 per cent of HBO Max subscribers signed up for its lower-priced ad tier, he said, while an average of 58 per cent of subscribers used the cheaper versions of Paramount Plus and Peacock.He estimates a modest 20 per cent of Canadian Netflix subscribers will join the ad tier over the next 12 to 18 months.However, Netflix’s initial sign-up numbers won’t be the best indicator of long-term success for the ad model, suggested Levy.Story continues below advertisementSubscribers who joined for a deal could be turned off if the ad breaks become as long as they are on network TV stations, which typically air 20 minutes of commercials per hour.“The devil is always in the details whenever a streaming provider introduces an ad-based tier,” Levy said.“What matters most is how intrusive that presentation of ads is to the overall viewing experience. And if it is intrusive in the way that consumers have long complained about traditional broadcast television ads, then this could very well be a non-starter for Netflix.”The ad agenciesUntil those intricacies play out, advertising agencies say their clients are salivating over the prospects of new placement options in the Canadian market.“What we’re seeing is a lot of initial excitement and questions around Netflix, in particular,” said Marissa Cristiano, an account director at Cossette who says she’s “exploring” ad buys on the service with some clients.Story continues below advertisement“They’ve done a really good job of creating … the type of content that brands really do want to ally with.”Cherie Hill, senior vice president of media at marketing firm Society, Etc., said she anticipates Netflix ads will be angled toward “budget-conscious” shoppers, with a strong focus on consumer staples, household items and car companies.She doesn’t anticipate much blowback from viewers, mainly because Netflix is making it an opt-in proposition.“If you’re choosing to have the commercials, it’s not going to leave a negative experience,” she said.“They’re providing an option and they’re managing expectations.”","Goodbye binge-watching: Netflix, others, bringing back ad breaks in coming weeks"
54,2,2_meta_facebook_users_said,https://globalnews.ca/news/9233468/meta-facebook-loses-650-billion-market-value/,"It has been exactly one year to the day since Facebook’s parent company rebranded as Meta, and in that time the company has entered financial free-fall, analysts say.Comparing the market value of Meta from when it was first announced on Oct 28, 2021, with today, the company has shrunk by an astonishing US$650 billion.Shares in Meta crashed 24 per cent on Thursday, to $97.94, sinking its stock to the lowest it’s been in nearly four years. The event cost Meta about US$67 billion, adding to the more than half a trillion dollars in value lost in 2022 alone. Meta’s market cap now stands at $263 billion. It is valued lower than Home Depot and has been forced out of the ranks of the top 20 companies.Mark Zuckerberg, the CEO of Facebook, saw his own personal fortune drop US$11 billion after the stock plummet, according to Forbes, which downgraded him from the 25th richest person in the world to 29th richest at the market close on Thursday.Story continues below advertisementThe incident echoes a previous crash in February which saw about $200 billion in Meta’s market value erased — the biggest loss in market value ever recorded for a U.S. company.Since changing its name to Meta and investing heavily to create the “metaverse,” a virtual reality world, Facebook’s parent company has been plagued with woes. From the start of 2022 to now, the company has shed 70 per cent of its value.At this time last year, on the day that Facebook rebranded as Meta, the company’s market cap was just over $900 billion. Just weeks before it had reached its peak value of over $1 trillion, joining an exclusive club consisting of Apple, Microsoft, Alphabet and Amazon. The company was riding high.Now, one year on, Meta is hopping from crisis to crisis.In February, when Meta’s stock dropped 23 per cent, Facebook’s daily active users declined for the first time, sending shockwaves through the tech giant.Later in July, Meta reported its first ever revenue decline, brought down by a drop in ad spending as the economy began to drag. Competition for ad revenue has also been heating up with Meta’s largest competitor in the social media space, TikTok.Story continues below advertisementAnother problem: recent privacy changes by Apple are making it harder for companies like Meta to track people for advertising purposes. Forbes reported that Apple’s changes cost Meta around $10 billion in ad revenue.All of these factors have been working together to slow Meta’s earnings and they also lead us to this week, when Meta’s stock came crashing down again.Similar to the massive one-day loss seen on Feb. 2, Meta’s stock crash on Thursday was triggered by a weak revenue report, predicting a 50 per cent decline in profits, with plans to maintain their high spending on building out the metaverse — to the sum of around $87 billion, that will increase to around $100 billion next year.There seems to be conflict between Meta and its investors, who see the metaverse as a money pit and are calling for the company to cut costs as the economy slows, while Zuckerberg bets it all on his virtual reality tech.Meta’s virtual and augmented reality division, Reality Labs, has already spent $9.4 billion this year (up to Sept. 30) creating the metaverse, but it seems that consumers aren’t that interested in it.Story continues below advertisementMeta set a goal of having 500,000 monthly active users for Horizon Worlds, its virtual reality platform, but a leaked report found that the company achieved less than half that, as reported by the Wall Street Journal. The report also found that most metaverse users didn’t return to the program again after the first month.“An empty world is a sad world,” one person noted, according to the report.It seems that if Meta wants to become a leader in the next generation of tech, they’re going to have to work to convince both investors and consumers.","Since becoming Meta, Facebookâs parent company has lost US$650 billion in market value"
538,2,2_meta_facebook_users_said,https://www.theverge.com/2022/9/29/23378713/google-stadia-shutting-down-game-streaming-january-2023,"Google is shutting down Stadia, its cloud gaming service. The service will remain live for players until January 18th, 2023. Google will be refunding all Stadia hardware purchased through the Google Store as well as all the games and add-on content purchased from the Stadia store. Google expects those refunds will be completed in mid-January.“A few years ago, we also launched a consumer gaming service, Stadia,” Stadia vice president and GM Phil Harrison said in a blog post. “And while Stadia’s approach to streaming games for consumers was built on a strong technology foundation, it hasn’t gained the traction with users that we expected so we’ve made the difficult decision to begin winding down our Stadia streaming service.” Employees on the Stadia team will be distributed to other parts of the company.Stadia, from our initial review in 2019. Photo by Amelia Holowaty Krales / The VergeHarrison says Google sees opportunities to apply Stadia’s technology to other parts of Google, like YouTube, Google Play, and its AR efforts, and the company also plans to “make it available to our industry partners, which aligns with where we see the future of gaming headed,” he wrote.Google detailed some of the finer points of the shutdown in an FAQ. Refunds will automatically be made through the Google and Stadia stores, and you won’t have to return any hardware. Stadia Pro subscriptions will not be eligible for a refund, but you will not be charged during the shutdown period and can access games you might have redeemed as a Pro user until everything is wound down. Google has closed the Stadia store, so you can’t buy games or in-game transactions.Stadia has been facing rumors of its demise practically from the startThe writing has been on the wall for Stadia for a while now, most recently when Logitech announced its new cloud gaming handheld last week and Stadia was one of the few cloud gaming services not mentioned. But Stadia has been facing rumors of its demise practically from the start. Google has a habit of killing projects only a few years after they launch, and Stadia, a cloud gaming service from a company with few ties in the gaming industry, seemed like a prime candidate for an early demise.Last year, rumors abounded it would shut down after the number of games released to the platform slowed and the company shuttered its in-house game development studios. When those rumors popped up again this year, Google insisted that Stadia was not shutting down. “Rest assured we’re always working bringing more great games to the platform and Stadia Pro,” the company said in a tweet. Which was true... until today.",Google is shutting down Stadia
293,2,2_meta_facebook_users_said,https://www.alternet.org/2022/10/meta-tiktok-stop-the-steal/,"As Brazilians prepare to vote in Sunday's decisive presidential runoff, a report published Saturday revealed that social media giants Meta—Facebook's parent company—and TikTok are driving traffic to content promoting a military coup to overthrow Brazil's democracy.The report—entitled Stop the Steal 2.0: How Meta and TikTok Are Promoting a Coup—was published by the San Francisco-based activist group SumOfUs and asserts that ""on the eve of the second vote in Brazil's most important election in decades, Meta and TikTok continue to put the integrity of the election on the line through their disastrous recommendation systems.""The publication comes ahead of Sunday's second-round contest between far-right incumbent Jair Bolsonaro—who has said he may not accept the outcome of the election if he loses–and former leftist President Luiz Inácio Lula da Silva. Aggregate polling showed the two candidates in a statistical dead heat on Friday.According to the new report:Meta claims that Brazil is a priority region and that the company is committed to enforcing policies and practices that uphold the integrity of the vote. But not only does SumOfUs' previous research show that the platforms are awash with conspiracy theories about the election, claims of electoral fraud, and calls for a military coup, this research report sets out how Facebook's recommender systems are actively pushing users towards this content.Far-right extremists, who are openly agitating for a military coup, are operating freely on Meta's platforms, and Meta is not only allowing them to spread their message and recruit new members, but the platform's algorithms are prioritizing anti-democratic groups, accounts, and posts. The report also looked at the role TikTok is playing in tackling the growing problem of election disinformation on its platform, and found its moderation lacking...The findings confirm civil society organizations' worst fears, that platforms like Facebook and Instagram are enabling bad actors to organize and recruit new members, just as it did in the U.S. 2020 elections, which ended in violent insurrectionists storming the U.S. Capitol on January 6th.""At this point, it is safe to say that Meta has become Bolsonaro's official disinformation machine,"" SumOfUs campaign director Flora Rebello Arduini said in a statement. ""This is not Meta's first time wreaking havoc on democracy and Brazilians deserve better from this multi-billion dollar company.""""As this report shows,"" she added, ""TikTok needs to up its game and not follow Meta's lead in fueling the disinformation crisis in Brazil.""On Saturday evening, SumOfUs activists projected an image of Meta co-founder and CEO Mark Zuckerberg setting the Brazilian flag alight with the message ""Meta is destroying Brazilian democracy"" at Kings Cross tube station in London, just around the corner from Meta's U.K. headquarters.The new report comes amid warnings and acts of right-wing political violence. While no motive has yet been announced, on Friday local São Paulo-area politician Reginaldo Camilo dos Santos, a prominent supporter of da Silva and the left-wing Workers' Party running for Congress, was assassinated in a drive-by shooting near his home in Jandira.Agência Pública, an independent Brazilian investigative journalism outlet, reported earlier this month that from August 16 and the end of the first round on October 2, there were at least 148 cases of electoral violence across the country.A separate report published last week by the anti-corruption and human rights organization Global Witness revealed that YouTube approved 100% of Brazilian election misinformation ads submitted for approval, while Facebook accepted around half of such submissions.",Meta and TikTok busted for 'actively pushing' 'Stop the Steal 2.0' on eve of Brazil's presidential election
563,2,2_meta_facebook_users_said,https://www.wired.com/story/europe-dma-prepares-to-rewrite-the-rules-of-the-internet/,"Next week, a law takes effect that will change the internet forever—and make it much more difficult to be a tech giant. On November 1, the European Union’s Digital Markets Act comes into force, starting the clock on a process expected to force Amazon, Google, and Meta to make their platforms more open and interoperable in 2023. That could bring major changes to what people can do with their devices and apps, in a new reminder that Europe has regulated tech companies much more actively than the US.Content This content can also be viewed on the site it originates from.“We expect the consequences to be significant,” says Gerard de Graaf, a veteran EU official who helped pass the DMA early this year. Last month, he became director of a new EU office in San Francisco, established in part to explain the law’s consequences to Big Tech companies. De Graaf says they will be forced to break open their walled gardens.“If you have an iPhone, you should be able to download apps not just from the App Store but from other app stores or from the internet,” de Graaf says, in a conference room with emerald green accents at the Irish consulate in San Francisco, where the EU’s office is initially located. The DMA requires dominant platforms to let in smaller competitors, and could also compel Meta’s WhatsApp to receive messages from competing apps like Signal or Telegram, or prevent Amazon, Apple, and Google from preferencing their own apps and services.Although the DMA takes force next week, tech platforms don’t have to comply immediately. The EU first must decide which companies are large and entrenched enough to be classified as “gatekeepers” subject to the toughest rules. De Graaf expects that about a dozen companies will be in that group, to be announced in the spring. Those gatekeepers will then have six months to come into compliance.De Graaf has predicted a wave of lawsuits challenging Europe’s new rules for Big Tech, but says he is in California to help make clear to Silicon Valley giants that the rules have changed. The EU has previously levied big fines against Google, Apple, and others through antitrust investigations, a mechanism that put the burden of proof on bureaucrats, he says. Under DMA, the onus is on the business to fall in line. “The key message is that negotiations are over, we’re in a compliance situation,” de Graaf says. “You may not like it, but that’s the way it is.”Like the EU’s digital privacy law, GDPR, the DMA is expected to lead to changes in how tech platforms serve people beyond the EU’s 400 million internet users, because some details of compliance will be more easily implemented globally.",Europe Prepares to Rewrite the Rules of the Internet
298,2,2_meta_facebook_users_said,https://www.businessinsider.com/alphabet-google-thousands-of-workers-added-cut-costs-hiring-freeze-2022-10,"Alphabet is attempting to tighten its belt after going on a hiring spree.CEO Sundar Pichai said the company would review all projects ""pretty granularly"" and make ""course corrections.""Alphabet is facing slowing growth as advertisers pull back spending, and the company has reportedly cut some employee perks.Sign up for our newsletter for the latest tech news and scoops — delivered daily to your inbox. Loading Something is loading. Thanks for signing up! Access your favorite topics in a personalized feed while you're on the go. download the app Email address By clicking ‘Sign up’, you agree to receive marketing emails from Insider as well as other partner offers and accept our Terms of Service and Privacy PolicyAlphabet is attempting to tighten its belt after going on a hiring spree.In the last 12 months alone, the company added nearly 37,000 new workers. Now, Google's parent company faces slowing growth amid a decelerating economy and a pullback in advertising spending.""We're constantly working to make sure everyone we've brought in is working on the most important things as a company... that's a lot of what sharpening our focus has been about,"" CEO Sundar Pichai told investors and reporters on a call on Tuesday.""We are reviewing projects at all scales pretty granularly to make sure we have the right plans there, and, based on that, the right resourcing, and making course corrections. This will be an ongoing thing,"" he added.Alphabet has attempted to cut employee expenses amid a slowdown - the company reported disappointing financial results for the third quarter on Tuesday. Alphabet has said it will reduce the pace of hiring, and it has reportedly cut back on perks like employee travel and team offsites.Pichai's grievances with his company's expanded headcount have been well-documented. Earlier this year, Pichai reportedly told employees at an all-hands meeting that there are ""real concerns that our productivity as a whole is not where it needs to be for the headcount we have.""However, the number of employees at Alphabet may not be the only thing weighing on the company's bottom line. Since the start of this year, Alphabet has reported nearly $4.5 billion in losses from ""Other Bets,"" which the company calls its division that manages smaller early-stage projects.",Alphabet is ramping up scrutiny of all its projects and cutting hiring in half as it tries to curb costs
271,2,2_meta_facebook_users_said,https://techcrunch.com/2022/10/25/google-hit-with-113-million-fine-in-india-for-anti-competitive-practices-with-play-store-policies/,"India’s antitrust watchdog has hit Google with a $113 million fine for abusing the dominant position of its Google Play Store and ordered the firm to allow app developers to use third-party payments processing services for in-app purchases or for purchasing apps, the second such penalty on the Android-maker in just as many weeks in its largest market by users.The Competition Commission of India, which opened the probe into Google in late 2020, said mandating developers to use Google’s own billing system for paid apps and in-app purchases through Play Store “constitutes an imposition of unfair condition” and thus violates provisions of the nation’s Section 4(2)(a)(i) of the Act.The regulator — which interviewed several industry players, including Paytm, Zomato, Info Edge, Samsung, Vivo, Xiaomi, Microsoft and Realme as part of the investigation — said that Google not using its billing system for its own apps such as YouTube amounts to “imposition of discriminatory conditions.”The investigation also concluded that:Mandatory imposition of GPBS [Google Play Billing System] disturbs innovation incentives and the ability of both the payment processors as well as app developers to undertake technical development and innovate and thus, tantamount to limiting technical development in the market for in-app payment processing services. in violation of the provisions of the Act. Thus, Google is found to be in violation of the provisions of Section 4(2)(b)(ii) of the Act. Mandatory imposition of GPBS by Google, also results in denial of market access for payment aggregators as well as app developers, in violation of the provisions of Section 4(2)(c) of the Act. The practices followed by Google results in leveraging its dominance in market for licensable mobile OS and app stores for Android OS, to protect its position in the downstream markets, in violation of the provisions of Section 4(2)(e) of the Act. Different methodologies used by Google to integrate, its own UPI app vis-à-vis other rival UPI apps, with the Play Store results in violation of Sections 4(2)(a)(ii), 4(2)(c) and 4(2)(e) of the Act.India is Google’s largest market by users. The company has poured billions of dollars in the South Asian market over the past decade as it aggressively searched to find major untapped regions worldwide to supercharge its growth.The company reaches nearly all of India’s 600 million internet users. Android commands 97% of the local smartphone market. Its payments app, Google Pay, is the second largest payments on the UPI network, an infrastructure built by a coalition of banks that has become the most popular way Indians transact online.The antitrust watchdog has directed Google to introduce a series of changes to its Play Store policies, which as with allowing developers to use third-party billing system, requires compliance within three months:Google shall not impose any anti-steering provisions on app developers and shall not restrict them from communicating with their users to promote their apps and offerings, in any manner. Google shall not restrict end users, in any manner, to access and use within apps, the features and services offered by app developers. Google shall set out a clear and transparent policy on data that is collected on its platform, use of such data by the platform and also the potential and actual sharing of such data with app developers or other entities, including related entities. The competitively relevant transaction/ consumer data of apps generated and acquired through GPBS, shall not be leveraged by Google to further its competitive advantage. Google shall also provide access to the app developer of the data that has been generated through the concerned app, subject to adequate safeguards, as highlighted in this order. Google shall not impose any condition (including price related condition) on app developers, which is unfair, unreasonable, discriminatory or disproportionate to the services provided to the app developers. Google shall ensure complete transparency in communicating to app developers, services provided, and corresponding fee charged. Google shall also publish in an unambiguous manner the payment policy and criteria for applicability of the fee(s). Google shall not discriminate against other apps facilitating payment through UPI in India vis-à-vis its own UPI app, in any manner.“The Commission hereby directs Google to cease and desist from indulging in anti-competitive practices,” CCI said in a statement Tuesday.Google and Apple have faced heat from developers globally in recent years for requiring them to use their own billing systems and hence accruing considerable commission. In response, Google has started to explore offering developers in some markets including India the ability to use third-party payments system for purchases on Play Store.Last week, the competition regulator fined Google $161.9 million for anti-competitive practices related to Android mobile devices and made a series of stringent redressal measures.The watchdog was investigating whether Google had assumed dominant position in five different markets: licensable OS for smartphones, app store, web search services, non-OS specific mobile web browsers and online video hosting platform in India. Google was dominant in all of those relevant markets, the regulator concluded.The antitrust watchdog said that device manufacturers should not be forced to install Google’s bouquet of apps and the search giant should not deny access to its Play Services APIs and monetary and other incentives to vendors. Amazon told the regulator that over half a dozen hardware vendors had indicated that they could not enter into a TV manufacturing relationship with the e-commerce group over fear of retaliation from Google.In response to the last week’s order, Google said CCI’s decision was a “major setback for consumers and businesses,” opened them to “serious security risks” and will raise the “cost of mobile devices for Indians.”Google said Tuesday its legal team is evaluating the order and had no immediate comment. It expressed concerns about the evidence the regulator relied on to reach its conclusion, CCI said in the order.In a statement Wednesday, a Google spokesperson said: “Indian developers have benefited from the technology, security, consumer protections, and unrivaled choice and flexibility that Android and Google Play provide. And, by keeping costs low, our model has powered India’s digital transformation and expanded access for hundreds of millions of Indians. We remain committed to our users and developers and are reviewing the decision to evaluate the next steps.”","India fines Google yet again, orders to allow third-party payments"
479,2,2_meta_facebook_users_said,https://www.reuters.com/technology/twitter-will-not-reinstate-banned-users-without-clear-process-musk-says-2022-11-02/,"Nov 2 (Reuters) - Banned accounts will not be allowed back onto Twitter until the social media platform has ""a clear process for doing so,"" Elon Musk tweeted in the early hours on Wednesday, giving more clarity about the potential return of Twitter's most famous banned user, former U.S. President Donald Trump.Creating such a process would take at least a few more weeks, Musk tweeted. The new timeline implies Trump will not return in time for the midterm elections on Nov. 8.Twitter users, advertisers and its own employees have been watching closely for signs of what Musk will do in his first week as Twitter's owner. The Tesla chief executive has previously said Twitter should not permanently ban users and that he would reverse the ban on Trump, who was suspended for risk of further incitement of violence after the U.S. Capitol riot last year.Musk's tweets came after he held a call with several civil rights organizations including Color of Change, the Anti-Defamation League and the NAACP on Tuesday.During the call, Musk committed to uphold Twitter's content moderation policies and enforcement around election integrity, said Rashad Robinson, president of Color of Change, in an interview.Musk also reiterated in his tweet on Wednesday that Twitter will create a content moderation council composed of representatives with ""widely divergent views.""The billionaire expressed during the call that he would like the civil society groups to join the council, Robinson said, adding the discussions were still at an early stage.""Actions will speak louder than words,"" he said. ""The issues that were addressed in this meeting were just the tip of the iceberg.""Reporting by Sheila Dang in Dallas Editing by Nick ZieminskiOur Standards: The Thomson Reuters Trust Principles.","Twitter will not reinstate banned users without 'clear process,' Musk says"
302,2,2_meta_facebook_users_said,https://www.businessinsider.com/mark-zuckerberg-adds-legs-metaverse-avatar-after-graphics-criticism-2022-10,"Meta CEO Mark Zuckerberg debuted the new metaverse avatars at the company's Connect conference.Zuckerberg's avatar featured legs, which he said was ""probably the most requested feature.""The company also announced a new $1,499 Quest Pro virtual reality headset, and new partnerships.Sign up for our newsletter for the latest tech news and scoops — delivered daily to your inbox. Loading Something is loading. Thanks for signing up! Access your favorite topics in a personalized feed while you're on the go. download the app Email address By clicking ‘Sign up’, you agree to receive marketing emails from Insider as well as other partner offers and accept our Terms of Service and Privacy PolicyMark Zuckerberg debuted his new avatar in real-time during Meta's Connect event on Tuesday.It was the first time the Facebook founder has held a public conference in Meta's Horizon World metaverse. Zuckerberg took the opportunity to announce the updated avatars, which he said will be rolled out later this year across phones and virtual reality headsets. He called the new avatars ""more expressive and detailed than anything else today.""The new avatars will also have legs, something Zuckerberg said was ""probably the most requested feature on our roadmap.""The metaverse avatars didn't have legs before because it was harder in virtual reality to accurately position where body parts like legs are, the Facebook founder said.""But seriously, legs are hard, which is why other virtual reality systems don't have them either,"" Zuckerberg said.Zuckerberg's announcement came just a few months after the billionaire was slammed on social media for an avatar he posted on Instagram. Social-media users were quick to diss the image which showed Zuckerberg's avatar standing in front of an Eiffel Tower, with some likening the graphics to 1990s video games like Zelda and Quake.Zuckerberg shared an updated metaverse avatar image Friday after being widely mocked for a previous version. Mark ZuckerbergAt the time, Zuckerberg responded with an updated avatar and admitted the previous image was ""basic.""""The graphics in Horizon are capable of much more — even on headsets — and Horizon is improving very quickly,"" Zuckerberg said in an Instagram post in August.Grimes also took the opportunity to slam Zuckerberg and his avatar, calling him ""under-qualified"" to run the metaverse and saying the billionaire's virtual reality plans are ""dead"" before they've truly begun.Earlier this week, The New York Times reported that Zuckerberg had fast-tracked a new avatar following the criticism, with one Meta graphic artist claiming in a since-deleted LinkedIn post that he and his team had designed roughly 40 versions of Mr. Zuckerberg's face over a four-week period before a final version was approved.""We started with simple graphics and we're doing a ton of work to meaningfully improve how Horizon will look and feel over the next year,"" Zuckerberg said at the Meta Connect event. ""The Metaverse needs to feel inspired.""But, Meta has faced headwinds in recent months, as the company's stock has been hammered amid Zuckerberg's new plans for the company. Shares of Meta neared a four-year low on Tuesday.Internal memos obtained by The Verge last week showed Meta's VP of the Metaverse, Vishal Shah, telling Metaverse employees working on the Horizon Worlds app they are on ""quality lockdown"" for the rest of the year to fix issues in the app before its released to more users.In one memo, Shah said the app's onboarding experience is ""confusing and frustrating."" Shah said employees working on the app weren't spending a lot of time on it.""Why don't we love the product we've built so much that we use it all the time,"" Shah wrote. ""The simple truth is, if we don't love it, how can we expect our users to love it?""The Facebook founder announced at the event that the new avatars will eventually be available via a partnership with Zoom, as well as on Android and Apple phones.During the event, Meta unveiled its new $1499 Quest Pro virtual reality headset. The company also announced several new partnerships with NBCUniversal and Microsoft, including a metaverse experience based on ""The Office"" and the opportunity for people to use Microsoft tools like Word and Excel in the metaverse.","Mark Zuckerberg showed his full avatar, which now has legs, in real-time for the first time after Meta was slammed for poor graphics"
300,2,2_meta_facebook_users_said,https://www.businessinsider.com/facebook-has-hidden-tool-to-delete-your-phone-number-email-2022-10,"Facebook almost certainly has your phone number and email address, even if you never handed them over.That's because any friend who shared their address book with Facebook also shared your details.Now the firm has a new tool to let people check if Facebook has that data, and delete it.Sign up for our newsletter for the latest tech news and scoops — delivered daily to your inbox. Loading Something is loading. Thanks for signing up! Access your favorite topics in a personalized feed while you're on the go. download the app Email address By clicking ‘Sign up’, you agree to receive marketing emails from Insider as well as other partner offers and accept our Terms of Service and Privacy PolicyFacebook's parent firm Meta has quietly rolled out a new service that lets people check whether the firm holds their contact information, such as their phone number or email address, and delete and block it.The tool has been available since May 2022, Insider understands, although Meta does not seem to have said anything publicly about it.A tipster pointed us to the tool, which is well-hidden and apparently only available via a link that is embedded 780 words into a fairly obscure page in Facebook's help section for non-users. The linked text gives no indication that it's sending you to a privacy tool, and simply reads: ""Click here if you have a question about the rights you may have.""Meta has buried its new contact-blocking tool 780 words into an obscure help page. Meta/Shona GhoshHere's how the contact-blocking tool worksOnce you have actually found the contact-blocking tool, it's pretty straightforward.Meta will scan its databases for your contact information. Meta/Shona GhoshThe company explains that even though you may not have signed up to use any core Meta service — such as the Facebook app, Messenger, or Instagram — it may still have your contact information.For many years, the firm asked users signing up for any of its apps to share their phone contacts, with the stated goal of helping them find friends. A side effect is that Meta, whose combined apps boast almost 3 billion daily users, has amassed an unknown but likely vast amount of personal contact information for people who have never signed up for an account, nor opted to share their information.The tool, in theory, allows a non-user to mitigate some of this damage. And although the tool is targeted at people who have never signed up for Meta's apps, it's likely also useful to anyone who is a user but never wanted to share this information.Enter any number or email you want to checkThe service asks whether you want to scan for a phone number, landline number, or email address that may have been uploaded by a friend who uses Meta's core apps: Facebook, Messenger, or Instagram.Meta will send a confirmation code to any phone number or email you want to check. Meta/Shona Ghosh""You can ask us to confirm whether we have your phone number or email address,"" the firm states. ""If we do, you can request that we delete it from our address book database. To prevent it from being uploaded to this database again through someone's address book, we need to keep a copy in our block list.""Meta declined to answer questions from Insider about how its block list works.You can enter any contact details you think may have been uploaded to Meta's services.It takes a few seconds to scan for that data and, if it shows up, Meta will ask if you want that contact information blocked.Meta's contact-deletion tool Meta/Shona GhoshMeta's contact-deletion tool Meta/Shona GhoshAnd here's what it looks like if Meta doesn't hold your data on its servers.Meta's contact-deletion tool Meta/Shona GhoshThe bad news: It's a drop in the ocean of what Meta has on youFiguring out the benefits of doing any of this requires a bit of intellectual reverse-engineering. A primary benefit is privacy and the knowledge your data won't contribute to the opaque power of Facebook algorithms, such as its infamous ""people you may know"" feature.It also gives more control to people whose data has been directly affected. Up until now, Meta only allowed users who had uploaded their contacts' data to delete that information.But in reality, privacy experts told Insider, deleting and blocking this small amount of data is one drop in the ocean compared to what else Meta has on you, regardless of whether or not you use its apps.For example, Meta harvests information on what people do outside its apps through Pixel, a piece of code that tracks what they do on different websites. That the firm has both browsing data and phone numbers of people who don't even use its services has given rise to the concept of ""shadow profiles.""""We first heard about shadow profiles very early in Facebook's dominance,"" Heather Burns, author of the book ""Understanding Privacy"", told Insider. ""Facebook was keeping a sort of profile on you, even if you didn't have a Facebook account, composed of data gathered through things like Facebook Pixel.""Parallel to that was this notion of uploading the contact book, which at various times in Facebook's history was enabled by default when you started an account,"" she added. ""Even if you were a privacy-conscious person, if you hit that button, you had uploaded your friend's data. It feeds into the shadow profiles of people whether or not they use the service at all.""Meta CEO Mark Zuckerberg appeared ignorant of the term ""shadow profiles"" during Congressional hearings in 2018, but admitted the firm collects data on non-users.Burns says she has never signed up for a Meta-owned service but found, on trying Meta's new privacy tool, that her email addresses and phone number had been picked up by the company. ""It's all in there, even though I've never had an account,"" she said. ""I don't believe anybody uploaded my data to Facebook in a malicious manner, I'm just in someone's address book.""But, she says, while it's positive that Meta now lets you block this information, ""that's just two strings of data.""""I still have to use a browser with multiple defense plugins to protect myself from Meta on every page I view,"" she added. ""They are still tracking people, by default, even if they don't use an account. The notion that there's a tool to remove two data strings, to me, it's both beneficial and laughable.""Meta declined to answer questions from Insider about why it has rolled out this new tool, and why this year.Meta may use this tool as a regulatory defenseBoth Burns and Eerke Boiten, professor of cybersecurity at De Montfort University, speculated that the motivation could be political or legal as regulatory scrutiny of the major tech firms increases.""There are two things — one is that installing the Messenger app leaks all of your contacts, that's an undeniable invasion of privacy,"" said professor Boiten. ""But now if anybody brings that up, Meta can always say: 'It doesn't have to be like that, you can undo the damage.'""The second reason may be WhatsApp, which was bought by Meta in 2014 and whose core messaging functionality relies on people uploading their contact data to the app. An ongoing battle between Meta and regulators is whether it can connect the data it sucks up through WhatsApp to its other apps.""Having the possibility to delist your number from Facebook in a way that it can't be used by Facebook to make friend suggestions would be an argument that 'OK, we can do this safely,'"" said Boiten. ""It wouldn't be very transparent ... the friend-suggestions algorithm is one of the least transparent of all, so it would be difficult to establish in any way that delisting a phone number would make an impact.""It's possible that Meta isn't really technically deleting your phone number and emails at all, said Boiten.""The way Facebook has organically grown, it's probably true that the way they store information on people is on all sorts of disparate siloes,"" he said.""It may well be that they only implement something like this on the outside — so at a point the phone number threatens to go outside the system, it says, 'This phone number is one we've been told we can't use,' rather than really removing information.""Although both Burns and Boiten welcomed Meta taking tentative pro-privacy steps, Burns described it as ""a classic American approach"" that involves scooping up data without asking, versus the more permission-based approach favored by European privacy regulators.""To me, the Facebook tool is privacy labor,"" she added. ""They collected data they should not have collected in the first place, and are now shifting responsibility on you to remove it.""","Facebook probably has your phone number, even if you never shared it. Now it has a secret tool to let you delete it."
304,2,2_meta_facebook_users_said,https://www.businessinsider.com/meta-accuses-apple-undercutting-businesses-app-store-policy-change-2022-10,"Apple's updated App Store rules give it 30% of in-app purchases for social media post ""boosts.""The rule essentially allows Apple to tax some advertising in apps like Facebook and Instagram.A Meta spokesperson told Insider that Apple's move undercuts other businesses and helps itself.Sign up for our newsletter for the latest tech news and scoops — delivered daily to your inbox. Loading Something is loading. Thanks for signing up! Access your favorite topics in a personalized feed while you're on the go. download the app Email address By clicking ‘Sign up’, you agree to receive marketing emails from Insider as well as other partner offers and accept our Terms of Service and Privacy PolicyMeta is not happy with Apple's latest update to its App Store guidelines around in-app purchases.The updated rule requires iOS app developers to use Apple's system of in-app purchases for post ""boosts"" — advertisements that show up in the same app they're purchased on — in social media apps. That means Apple gets 30% of the in-app purchases in Meta-owned apps like Facebook or Instagram when people use the app to pay to boost their posts and profiles to a wider audience.""Apple continues to evolve its policies to grow their own business while undercutting others in the digital economy,"" a Meta spokesperson told Insider in a statement. ""Apple previously said it didn't take a share of developer advertising revenue, and now apparently changed its mind. We remain committed to offering small businesses simple ways to run ads and grow their businesses on our apps.""During the Epic v. Apple trial last May, Phil Schiller, who is responsible for leading the App Store, testified that Apple never took cuts of iOS developer's advertising revenue.An Apple spokesperson told Insider that, ""for many years,"" the guidelines for the App Store ""have been clear that the sale of digital goods and services within an app must use In-App Purchase.""Boosting is a digital service, the spokesperson said, so it requires in-app purchase.""This has always been the case and there are many examples of apps that do it successfully,"" the spokesperson said.Twitter and TikTok are other apps who use in-app purchase for boosts.The Verge's Alex Heath reported earlier this week that, based on his conversations with employees at Meta, the App Store update isn't anticipated to have too big of an impact on Meta's revenue, but ""there is concern about the precedent set and that Apple will eventually require the same rule for Meta's standalone ads manager app.""Eric Seufert, an ad industry analyst, told Heath that people who buy boosts in their social media apps are going to be more affected by the updated App Store policies because they will have to pay more for the same reach they had before.In Meta's earning call on Wednesday, Meta CEO Mark Zuckerberg acknowledged some of Apple's recent changes, including its latest App Store policy language around boosted posts, saying they are ""obviously big risks"" that Meta ""see as issues.""Zuckerberg also talked about competition and ""ads challenges,"" that he said are ""especially coming from Apple.""Meta experienced a 4% revenue decline in the third quarter of this year, continuing a string of disappointing quarterly earnings. On an analyst call, it told investors the company plans to continue spending even more next year, despite losing money while Zuckerberg continues his quest to build the metaverse.","Meta is mad about Apple's latest policy change, accusing the company of trying to 'grow their own business while undercutting others'"
7,2,2_meta_facebook_users_said,https://abcnews.go.com/Politics/justice-department-charges-chinese-intel-officers-obstruct-probe/story?id=92000006,"The Department of Justice on Monday unsealed charges against two Chinese intelligence officers who allegedly worked to obstruct the DOJ's investigation of the telecommunications giant Huawei.This case is one of three that the DOJ has unsealed against 13 individuals associated with the Chinese government, 11 of which are intelligence officers, FBI Director Christopher Wray said Monday.The indictment in the obstruction case related to Huawei accuses Chinese intelligence agents Guochun He and Zheng Wang of ""attempt[ing] to direct a person they believed they recruited as an asset"" inside a U.S. government law enforcement agency ""to obtain confidential information regarding witnesses, trial evidence and potential new charges to be brought against [Huawei] for the purpose of obstructing justice.""But the ""asset"" within the U.S. government that the two agents allegedly worked with was actually acting as a double agent under the FBI's supervision, according to the DOJ.The DOJ previously unsealed an indictment in 2019 accusing Huawei in a racketeering and corruption conspiracy stemming from the company's alleged practices ""using fraud and deception to misappropriate sophisticated technology from U.S. counterparts.""At the time, the department noted that its investigation of Huawei remained ongoing. The company accused the DOJ of ""political persecution.""The new indictment lays out in great detail He and Wang's alleged attempts to interfere and obstruct the U.S. government's probe of Huawei.The two men, whom the DOJ says remain at large, allegedly paid the double agent working on behalf of the FBI some $61,000 in Bitcoin for what they believed was insider information about the investigation.In this Feb. 2, 2022, file photo, the logo of the telecommunication provider Huawei is shown. Photothek via Getty Images, FILEAt one point in October 2021, the indictment alleges, the undercover agent passed a single-page document to one of the Chinese intelligence officers that appeared to be classified as ""SECRET"" and that detailed U.S. plans to arrest two principals from Huawei living in China.He and Wang paid the undercover $41,000 just for that single page, the indictment claims.Attorney General Merrick Garland and top law enforcement officials addressed that case and others in a Monday news conference focused on China's so-called ""malign influence efforts"" and use of intelligence officers targeting the U.S.""This was an egregious attempt by [People's Republic of China] intelligence officers to shield a PRC-based company from accountability and undermine the integrity of our judicial system,"" Garland told reporters.In another case, the DOJ has charged seven individuals with allegedly forcing someone to return to China through scare tactics and harassment.""Those activities were part of the PRCs global, extra-legal effort known as 'Operation Foxhunt,'"" Garland said. ""Its purpose is to locate and bring back to China alleged fugitives who have fled to foreign countries, including the United States. The PRC has a history of targeting political dissidents and critics of the government who have sought relief and refuge in other countries.""Attorney General Merrick Garland speaks during an event at DEA headquarters, in Arlington, Va., Sept. 27, 2022. Gemunu Amarasinghe/AP, FILEIn a third case, the Justice Department alleges that for almost 10 years, four Chinese nationals attempted to target and recruit individuals to act on China's behalf through a fake think tank. They targeted a former government official and a professor at an American university.""As these cases demonstrate, the government of China sought to interfere with the rights and freedoms of individuals in the United States and to undermine our judicial system that protects those rights,"" the attorney general said. ""They did not succeed. The Justice Department will not tolerate attempts by any foreign power to undermine the rule of law upon which our democracy is based. We will continue to fiercely protect the rights guaranteed to everyone in our country. And we will defend the integrity of our institutions.""Two people accused in the harassment case have been taken into custody; the others remain at large.The DOJ officials stressed to reporters on Monday that the unsealed cases reflect accusations against China's government -- and are not a reflection of views of China as a whole or of Chinese Americans.FBI Director Wray said that the cases were not related to recent political developments in China, where President Xi Jinping's time in office was just extended.Wray said that when cases are ready to be brought, the DOJ presents them to a grand jury.",Justice Department charges 2 Chinese intel officers with trying to obstruct probe into Huawei
303,2,2_meta_facebook_users_said,https://www.businessinsider.com/mark-zuckerberg-should-dial-down-metaverse-grow-facebook-instagram-whatsapp-2022-10,"Mark Zuckerberg is going all in on the metaverse, but he should re-focus on other things.The Meta CEO should prioritize growing engagement and revenue on the company's core apps.Meta reports Q3 earnings next week and analysts have called it a ""make-or-break quarter.""Sign up for our newsletter for the latest tech news and scoops — delivered daily to your inbox. Loading Something is loading. Thanks for signing up! Access your favorite topics in a personalized feed while you're on the go. download the app Email address By clicking ‘Sign up’, you agree to receive marketing emails from Insider as well as other partner offers and accept our Terms of Service and Privacy PolicyMeta, the company formerly known as Facebook, should start focusing on making Facebook Facebook again.Over the past year, CEO Mark Zuckerberg has zeroed in on his passion project: the metaverse. It's a squishy concept that can describe a number of things, but in the broadest sense it's the idea that people connect with each other through virtual worlds rather than on a traditional social network.But as I wrote recently, Meta's big pivot into the metaverse has been a disaster, with little to show for it other than a mediocre experience, increasingly expensive headsets, and its stock plunging over 60% this year.Zuckerberg should instead dial that down and prioritize bolstering his company's core apps, Facebook, Instagram, and WhatsApp, which have felt largely neglected while Meta poured $15 billion into its metaverse project.Staring down the barrel of a potential recession, Meta should be growing the engagement and revenue of those apps, which have billions of users worldwide. Meanwhile, Horizon Worlds, Meta's main metaverse app, has just 200,000 monthly active users, The Wall Street Journal recently reported.Particularly, even as Instagram has faced headwinds recently, it's still Meta's crown jewel. Keeping users happy on the app and charting a plan for it in the years to come should be the company's No. 1 priority. Meta said in its Q2 earnings that Reels was growing and accounted for 20% of the time people spend on Instagram.Instead of angering users by trying to make Instagram more of a TikTok clone, Meta should be spending its time and energy on threading the needle to monetize that usage as much as possible without turning people off.It should also be looking to do the same with WhatsApp, the most popular communications app in the world. The platform doesn't include ads, in an effort to maintain its identity as a user-friendly service first and foremost. But Meta has promised to capitalize on its popularity in other ways to drive revenue, including with paid features.Yet, instead of focusing on its proven apps, Meta is investing billions of dollars on an idea that will maybe see payoffs five or ten years down the line.If left unchecked, a bet of this magnitude risks alienating investors — and staff — while facing choppy economic waters.Zuckerberg will show us his report card later this weekMark Zuckerberg as an avatar during Connect 2022 MetaMeta reports its Q3 earnings next week, and Wall Street has already been spooked. Analyst Neil Campling called a recent metaverse presentation by Zuckerberg ""desperate"" and said ""no wonder investors are in despair.""The investment firm Bernstein called it a ""make or break quarter"" in a recent note, and said that engagement numbers will be ""critical"" for the company this quarter.""We believe that if Meta does not provide incremental information on the call suggesting that aggregate engagement across its family of apps are stable, the bear case will only get louder,"" Bernstein analysts wrote.The analysts think a turnaround is possible by year's end and into 2023 if Meta, among other things, increases the ad load on its TikTok-like video product, Reels.Meta's revenue dropped in Q2, the first time it had done so in the company's decade of being publicly traded. Zuckerberg blamed it on an ""economic downturn"" that was impacting the digital-ad business.Apple was a key part of the problem. Last year, the tech giant introduced an iOS privacy change that asked users if they wanted to opt out of being tracked across other companies' apps. Meta responded at the time saying that advertisers ""may see an overall decrease in ad performance and personalization and an increase in cost per action.""Seeking to escape a future scenario where Apple is a dominant force that can hamstring his business with a single software blow, Zuckerberg is trying to invent the next future platform.But it's rarely the incumbents who create the next big platform, which is why Zuckerberg's metaverse vision sounds like a better fit for a VC-backed startup than a companywide rallying cry.Apple has also been exploring future platforms, too, but far more quietly than Meta (its own VR headset is reportedly coming soon). But the company hasn't been punished for it by Wall Street, because it's still laser-focused on growing its core business sectors — unlike Meta.That hasn't stopped Zuckerberg from making his metaverse push a Meta vs. Apple contest, so it's clearly taking up headspace.""This is a competition of philosophies and ideas, where they believe that by doing everything themselves and tightly integrating that they build a better consumer experience,"" Zuckerberg said of Apple's strategy in an all-hands meeting this year with employees, according to The Verge.But maybe Zuckerberg should take a page out of Apple's book, prioritize the proven cash cows to keep investors happy, and relegate the metaverse stuff to the garage where moonshot projects belong.",Mark Zuckerberg should dial down the metaverse crap and make Facebook 'Facebook' again
309,2,2_meta_facebook_users_said,https://www.businessinsider.com/tiktok-moderator-paid-10-dollars-day-keep-webcam-all-night-2022-10?r=US&IR=T,"A former TikTok moderator in Colombia was required to keep her webcam on all night while working.Moderators in Colombia are paid just $10 a day, The Bureau of Investigative Journalism reported.The moderator said the situation was ""terrible"" because she lived with her family.Sign up for our newsletter for the latest tech news and scoops — delivered daily to your inbox. Loading Something is loading. Thanks for signing up! Access your favorite topics in a personalized feed while you're on the go. download the app Email address By clicking ‘Sign up’, you agree to receive marketing emails from Insider as well as other partner offers and accept our Terms of Service and Privacy PolicyA Colombian ex-moderator for TikTok said she was required to keep her webcam on all night, according to a report by The Bureau of Investigative Journalism.TBIJ spoke to nine moderators who shared their experience but requested that their identity remained secret for fear they might lose their jobs, or risk future employment prospects. All names have been changed, according to the outlet.Carolina, a former TikTok moderator who worked remotely for Teleperformance, a Paris-based company offering moderation services and earned $10 a day, said she had to keep her camera continuously on during her night shift, TBIJ reported. The company also told her that no one should be in view of the camera and was only allowed a drink in a transparent cup on her desk.Carolina said the situation was ""terrible"" because she lived with her family. ""So I felt very guilty telling them, 'please don't pass behind the camera because I could be fired.' Teleperformance is especially paranoid with people seeing what we do.""A TikTok spokesperson told Insider in a statement: ""We strive to promote a caring working environment for our employees and contractors. Our Trust and Safety team partners with third-party firms on the critical work of helping to protect the TikTok platform and community, and we continue to expand on a range of wellness services so that moderators feel supported mentally and emotionally.""Neither TikTok nor Teleperformance responded to The Bureau of Investigative Journalism's detailed list of allegations. Teleperformance, which has more than 42,000 workers in Colombia, did not respond to Insider's request for comment.Current moderators shared with TBIJ that they had to clock in and out and log any breaks on an app called Timekeeper – but they didn't confirm whether they had to work with their cameras on.Another former TikTok moderator, Carlos, told TBIJ it was a video of child sexual abuse that had traumatized him. The video showed a girl of five or six years old: ""She was dancing, like pointing her back to the camera, it was so close.""Luis, 28, worked night shifts moderating videos for TikTok. He listed to the outlet the kind of content he sees regularly: ""Murder, suicide, pedophilia, pornographic content, accidents, cannibalism.""He recalled seeing one video taken at a party, with two people holding what initially looked like pieces of meat. When they turned around, it appeared they were holding skin and gristle flayed off human faces. ""The worst thing was that the friends were playing games and started using the human faces as masks,"" Luis said.Read the full story at The Bureau of Investigative Journalism.","An ex-TikTok moderator, who was paid $10 a day and had to scroll through child abuse and gun violence, was required to keep her webcam on all night, report says"
308,2,2_meta_facebook_users_said,https://www.businessinsider.com/snapchat-disappearing-messages-helped-teenagers-obtain-fentanyl-lawsuit-2022-10,"Snapchat's disappearing messages helped teenagers to obtain the drug fentanyl, a lawsuit stated.Lawyers say the feature was defective as it helped facilitate illicit sales of the drug to minors.A Snap spokesperson said several allegations in the complaint appeared to be ""wholly inaccurate.""Sign up for our newsletter for the latest tech news and scoops — delivered daily to your inbox. Loading Something is loading. Thanks for signing up! Access your favorite topics in a personalized feed while you're on the go. download the app Email address By clicking ‘Sign up’, you agree to receive marketing emails from Insider as well as other partner offers and accept our Terms of Service and Privacy PolicySnapchat's disappearing message feature helped enable the sale of fentanyl to teenagers who went on to die of overdoses, a lawsuit claimed.According to a filing in a Los Angeles court seen by Insider, parents of teens who died from Fentanyl overdoses are pursuing Snap for strict product liability over what they claim is a design defect in the social media app Snapchat.The lawsuit stated that Snapchat is marketed to minors, and that the erasing messages function encourages drug dealers to use the social media app.""The product design of Snap, most notably its disappearing message feature which is engineered to evade parental supervision and law enforcement's detection and acquisition of criminal evidence, was the direct and proximate cause of the untimely and tragic deaths and injuries at issue in this complaint,"" the filing said.A Snap spokesperson told Insider ""several allegations in the complaint appear to be wholly inaccurate,"" but did not give further details.The spokesperson said Snap used ""cutting-edge technology to proactively find and shut down drug dealers' accounts, and we block search results for drug-related terms, instead redirecting Snapchatters to resources from experts about the dangers of fentanyl.""Alexander Neville died of a Fentanyl overdose in June 2020 at the age of 14. His parents, Amy and Aaron, are part of the lawsuit.The couple said their son communicated on Snapchat with a dealer called Aj Smokxy who supplied him with fentanyl, according to subpoenaed documents, the lawsuit said. They said Alexander had become increasingly anxious during COVID-19 as his use of the app increased.""Amy went to her son's room to wake him up for an orthodontist appointment. She opened the bedroom door and found Alexander's body laying lifeless on his bedroom floor,"" the lawsuit stated.""Amy and Aaron administered CPR to their son as they waited for paramedics to arrive, but it was too late.""The lawyers argued that Alexander was only able to obtain fentanyl from the dealer through the app.""AJ Smokxy had no known connection to Alexander. They did not know each other in real life. The two would never have connected but for Snapchat,"" the lawsuit said.One of the parents' lawyers, Laura Marquez-Garrett of the Social Media Victims Law Center, previously led a lawsuit against Meta alleging Instagram caused eating disorders among children.","Snapchat's disappearing message function helped teenagers obtain fentanyl with deadly consequences, lawsuit argues"
307,2,2_meta_facebook_users_said,https://www.businessinsider.com/reports-tiktok-big-brother-type-surveillance-prompt-calls-for-investigation-2022-10?international=true&r=US&IR=T,"Forbes reported TikTok's parent company, ByteDance, planned to use the app to surveil Americans.Top editors give you the stories you want — delivered right to your inbox each weekday. Loading Something is loading. Thanks for signing up! Access your favorite topics in a personalized feed while you're on the go. download the app Email address By clicking ‘Sign up’, you agree to receive marketing emails from Insider as well as other partner offers and accept our Terms of Service and Privacy PolicyPublic Citizen on Thursday urged lawmakers and the FTC to investigate TikTok and ByteDance.The consumer rights group joins a growing chorus of bipartisan calls to investigate the Beijing-based company.Public Citizen in a letter on Thursday urged lawmakers and the Federal Trade Commission to investigate the TikTok app and its parent company, ByteDance, joining a growing number of bipartisan calls to investigate reports that the Beijing-based company planned to use the video-based social media app to surveil specific Americans.After reviewing internal materials, Forbes reported last week that ByteDance's Internal Audit team was planning to use location information gathered from US users of the TikTok app for surveillance of two American citizens who were not employees of the app. While accessing user location data is allowed for purposes like targeting ads or preventing fraudlent activity, the company's surveillance plans were not related to these user-approved purposes, according to the outlet.Representatives for ByteDance and TikTok did not immediately respond to Insider's requests for comment about the Public Citizen letter, but in a statement previously provided to Insider's Travis Clark a TikTok spokesperson denied the app has been used to ""target"" members of the US government, activists, public figures or journalists and said it ""does not collect precise GPS location information from US users.""In its Thursday letter, the nonprofit consumer rights group Public Citizen urged the FTC to investigate and ""take immediate action against ByteDance and Tiktok"" if the reports of surveillance are substantiated.""This threatens a Big Brother-type surveillance that is antithetical to democratic values, may threaten individuals' personal security and violates the most basic societal expectations about personal privacy,"" the letter written by Robert Weissman, president of Public Citizen, read. ""That there are a few disturbing precedents for platforms tracking individuals exacerbates rather than diminishes the concern about the Forbes report... Such a practice could put our privacy, physical safety, and national security at risk.""Lawmakers and privacy advocates alike expressed similar concern following the Forbes report. Sen. Marsha Blackburn, a Republican from Tennessee who has previously been critical of TikTok's ties to China, tweeted it was ""not surprising"" the company planned to surveil US users and advocated for a national privacy law. Alan Butler, executive director and president of the Electronic Privacy Information Center (EPIC), echoed the concerns about user privacy and called for stronger location data protections.Earlier this summer, Sens. Mark Warner, a Virginia Democrat, and Marco Rubio, a Florida Republican, called for a separate FTC investigation after BuzzFeed News reported that sensitive data from US users was repeatedly accessed by ByteDance staff in China, despite the company's assurances that US user data was stored in the states and was inaccessible abroad.""We write in response to public reports that individuals in the People's Republic of China (PRC) have been accessing data on U.S. users, in contravention of several public representations, including sworn testimony in October 2021,"" the senators wrote in a letter addressed to FTC Chair Lina Khan. ""In light of this new report, we ask that your agency immediately initiate a Section 5 investigation on the basis of apparent deception by TikTok, and coordinate this work with any national security or counter-intelligence investigation that may be initiated by the U.S. Department of Justice.""The app's links to the Chinese government have long spurred concerns over propaganda, fake news, and data privacy — with the Trump administration in 2020 even proposing a total ban of TikTok. In 2021, the Biden administration promised a security review of foreign-owned apps, but has yet to publish its results.""It is critical that you act with urgency to uncover the full extent of individually targeted surveillance practices, if any, and hold these companies accountable for any misconduct,"" Public Citizen's letter read.",Pressure mounts for regulators to investigate TikTok over potential 'Big Brother-type surveillance' after reports of plans to track Americans' locations
531,2,2_meta_facebook_users_said,https://www.thesun.co.uk/tech/20277321/instagram-down-outage-users-suspended/,"INSTAGRAM has been hit by a weird outage suspending droves of users from their accounts.Reports from confused users surged around 1.30pm UK time.1 Loads of users report getting the same suspension messagePeople flooded Twitter trying to figure out what's going on.Screenshots show masses being suspended all at once.""We suspended your account on 31 October 2022,"" the alert shows.It's not clear how widespread the problem is.But Instagram has revealed it is a problem on their end.The topic has been trending on Twitter, which usually means quite a lot of people are experiencing the same thing.""Mark Zuckerberg what hell is this my account in suspended,"" one annoyed user wrote.""Earlier it was crashing every 10-20 seconds and now they have suspended my account for no reason,"" another said.""I cannot even login to my account.""A third added: ""What’s going on with Instagram? Did they get hacked or something?""Mine says my account is suspended. I guess my pic of the hummus plate I made sent them over the edge.""A surge in reports was noticed by the Down Detector site at around 1.30pm GMT.Instagram owner Meta has acknowledged the problem, saying: ""We're aware that some of you are having issues accessing your Instagram account.""We're looking into it and apologize for the inconvenience.""We pay for your stories! Do you have a story for The Sun Online Tech & Science team? Email us at tech@the-sun.co.uk",Instagram âdownâ as users report mysterious outage suspending accounts for no apparent reason
306,2,2_meta_facebook_users_said,https://www.businessinsider.com/onlyfans-ceo-says-it-is-the-safest-platform-2022-10,"""BBC Newsnight"" reported that child-abuse images found on other websites originated on OnlyFans.OnlyFans said the BBC had prevented it from investigating because it didn't hand over evidence.The platform said it had ""invested significantly"" in improving age- and identity-verification tools.Sign up for our newsletter for the latest tech news and scoops — delivered daily to your inbox. Loading Something is loading. Thanks for signing up! Access your favorite topics in a personalized feed while you're on the go. download the app Email address By clicking ‘Sign up’, you agree to receive marketing emails from Insider as well as other partner offers and accept our Terms of Service and Privacy PolicyThe CEO of OnlyFans said the social-media platform was the ""safest"" and ""most inclusive"" one after BBC's ""Newsnight"" reported that illegal content found online had originated on the site.A US investigator interviewed by the BBC said they found 10 child-abuse images, including one of a 5-year-old child and others of kids about 12 years old, on other sites, all of which they said had originally been created on OnlyFans. The images were found in about an hour, the investigator, who was not identified by the BBC, said.Amrapali Gan, the CEO of OnlyFans, told the BBC: ""We actively work with law enforcement. If anyone makes the mistake thinking they can upload illegal content, we will report them. We're truly the safest and most inclusive social-media platform.""OnlyFans told Insider in a statement: ""When the BBC raised this anonymous claim, we asked them for evidence to enable us to investigate, determine if it was true, and to take appropriate action to protect people online. The BBC refused to provide any details or evidence preventing OnlyFans from investigating this claim.""The BBC did not immediately respond to a request for comment.The investigator ""Newsnight"" cited said they believed the images were created in the past six months and had OnlyFans watermarks. They added that there were ""still cracks"" in the platform's moderation of illegal content that meant it was ""still slipping through.""Four in five OnlyFans employees work on content moderation, Gan said, according to the report. The chief strategy and operations officer of OnlyFans, Keily Blair, told the BBC the company had ""invested significantly"" in improving its tools for verifying age and identity and that it did more than other social-media sites to prevent underage use.""One of the issues that was raised was being able to pass off the account that you've opened to somebody else — that's not now possible,"" Blair said.The director of strategic engagement at the Child Rescue Coalition, Simon Bailey, told the BBC its investigation last year found children had appeared in and sold videos on OnlyFans. But he added that a new leadership team was now in place and had taken action.""They are monitoring everything through AI and human moderation, making sure abuses are not taking place,"" Bailey said.Ofcom, the UK's communications watchdog, said in a report this month it was ""encouraged"" to see measures the OnlyFans had put in place to tackle child-abuse content.""Ofcom is pleased to see a VSP taking steps to implement age verification processes that appear more robust than the traditional and much more prevalent self-declaration of date of birth,"" it said.Tim Stokely stepped down as CEO of OnlyFans in December and was replaced by Gan. The British businessperson founded the platform in 2016 before selling it to the Ukrainian American entrepreneur Leonid Radvinsky two years later.Radvinsky, Bloomberg reported, has since received dividends of more than $500 million from OnlyFans in less than two years as users flock to the platform during the pandemic.",OnlyFans CEO calls the site 'truly the safest and most inclusive social-media platform' after claims that child-abuse images originated on it
305,2,2_meta_facebook_users_said,https://www.businessinsider.com/most-metaverse-users-dont-even-make-it-a-month-wsj-2022-10,"Internal documents show Meta's Horizon Worlds platform is struggling to retain users, WSJ reported.Those who can afford the $400 pricetag report the metaverse has glitchy features and empty worlds.Most Metaverse users don't return to the Horizon Worlds platform after the first month, WSJ reported.Sign up for our newsletter for the latest tech news and scoops — delivered daily to your inbox. Loading Something is loading. Thanks for signing up! Access your favorite topics in a personalized feed while you're on the go. download the app Email address By clicking ‘Sign up’, you agree to receive marketing emails from Insider as well as other partner offers and accept our Terms of Service and Privacy PolicyInside the Horizon Worlds platform, Meta is struggling to keep users engaged with glitchy features and empty worlds.With a $400 pricetag to access the platform via the Quest 2 headset, the metaverse isn't yet accessible to the casual user. Those who do get a chance to try the tech experience baffling branded content, persistent bugs, and empty worlds with no user interactions. Even employees appear not to enjoy the platform, saying there is a ""quality"" problem.Last month, in response to user complaints, Meta put the Horizon platform into ""lockdown"" — pausing the rollout of new features while it works to improve the user experience of existing elements in the virtual reality world, The Wall Street Journal reported.Users have criticized the avatars' curious lack of legs as well as having few other users to interact with. Internal statistics, WSJ reported, indicate that only 9% of worlds built by users are ever visited by at least 50 people, while most never receive any visits at all.Some female users also report sexual harassment and digital groping, prompting Meta to institute a safety feature that creates a virtual 4-foot buffer around avatars on the platform. WSJ reported men outnumber women by two to one on the platform and that — while reporting — a WSJ staff member was asked to expose herself by a user she interacted with online.Internal documents show Meta has fallen far short of its goals for regular monthly users, WSJ Journal reported. The company initially had set a goal of 500,00 monthly users by the end of 2022, but has changed that figure to 280,000. The documents show the platform has less than 200,000 current users. Most users generally don't return to the app after the first month, while more than half of Quest 2 headsets are out of use within six months WSJ reported.""An empty world is a sad world,"" WSJ reported one document said in its summary of the company's efforts to attract users to worlds where they would find others.In light of the low user retention, feature glitches, and high cost of access, Insider previously reported some investors have raised questions about Meta's $15 billion investment in Reality Labs, the business segment responsible for the metaverse.""Currently it's unclear where the metaverse fits in the investment framework,"" WSJ reported Dare Obasanjo, lead product manager for Horizon and the metaverse platform, wrote in a memo. ""We are overdue for a reassessment of how we invest and allocate resources.""Representatives for Meta did not immediately respond to Insider's request for comment.","Most Metaverse users don't even make it a month, WSJ reports"
89,3,3_cells_brain_blood_heart,https://news.mit.edu/2022/sensors-face-masks-fit-1020,"The study is a collaboration between Dagdeviren’s lab; Siqi Zheng, the STL Champion Professor of Urban and Real Estate Sustainability in the Department of Urban Studies and Planning; and Tolga Durak, managing director of MIT’s Environment, Health, and Safety programs. Jin-Hoon Kim, an MIT postdoc, is the lead author of the paper, which appears today in Nature Electronics.The researchers hope that their sensor will help people to find masks that fit them better, and that designers could use it to create masks that fit a wider variety of face shapes and sizes. The sensor can also be used to monitor vital signs such as breathing rate and temperature, as well as environmental conditions such as humidity.“What we realized by analyzing our collected data from the individuals in the study was that the masks that we use in daily life are not very suitable for female participants,” says Canan Dagdeviren, the LG Career Development Professor of Media Arts and Sciences at MIT and the corresponding author of the study.Using this sensor, the researchers analyzed the fit of surgical masks on male and female subjects, and found that overall, the masks fit women’s faces much less closely than they fit men’s faces.Currently, there are no simple ways to measure the fit of a mask, but a new sensor developed at MIT could make it much easier to ensure a good fit. The sensor, which measures physical contact between the mask and the wearer’s face, can be applied to any kind of mask.Wearing a mask can help prevent the spread of viruses such as SARS-CoV-2, but a mask’s effectiveness depends on how well it fits.Fit qualityThe researchers began working on this project before mask-wearing became common during the Covid-19 pandemic. Their original intention was to use sensors embedded in masks to measure the effectiveness of mask-wearing in areas with high levels of air pollution. However, once the pandemic started, they realized that such a sensor could have more widespread applications.With so many different kinds of masks available during the pandemic, the researchers thought this kind of sensor could be useful to help individuals find the best-fitting mask for them. Currently, the only way to measure mask fit is to use a machine called a mask fit tester, which evaluates the mask fit by comparing air particle concentrations inside and outside of the face mask. However, this type of machine is only available in specialized facilities such as hospitals, which use them to evaluate mask fit for health care workers.The MIT team wanted to create a more user-friendly, portable device to measure mask fit. Dagdeviren’s lab, the Conformable Decoders group, specializes in developing flexible, stretchable electronics that can be worn on the skin or incorporated into textiles to detect signals from the body.“In this project, we wanted to monitor both biological and environmental conditions simultaneously, such as breathing pattern, skin temperature, human activities, temperature and humidity inside the face mask, and the position of the mask, including whether people are wearing it properly or not,” Kim says. “We also wanted to check the fit quality.”To integrate their sensors into face masks, the researchers created a device that they call a conformable multimodal sensor face mask (cMaSK). Sensors that measure a variety of parameters are embedded in a flexible polymer frame that can be reversibly attached to the inside of any mask, around the edges.To measure fit, the cMaSK has 17 sensors around the edge of the mask that measure capacitance, which can be used to determine whether the mask is touching the skin at each of those locations.The cMaSK interface also has sensors that measure temperature, humidity, and air pressure, which can detect activities such as speaking and coughing. An accelerometer within the device can reveal if the wearer is moving around. All of the sensors are embedded into a biocompatible polymer called polyimide, which is used in medical implants such as stents.The conformable electronics are laminated and delaminated on a surgical mask.The researchers tested the cMaSK interface in a group of five men and five women. All of the subjects wore surgical masks, and the researchers monitored the readings from the sensors as the participants performed a variety of activities, such as speaking, walking, and running. They also tested the sensors in a variety of temperature conditions.Using data obtained by the capacitance sensors, the researchers created a machine-learning algorithm to calculate mask fit quality for each subject in the study. These measurements revealed that mask fit was significantly worse for women than men, due to differences in face shape and size. However, the fit for women could be improved slightly by wearing smaller surgical masks. The researchers also found that mask fit quality was low for one of the male subjects who had a beard, which created gaps between the mask and the skin.To verify their results, the researchers also collaborated with MIT’s Environment, Health, and Safety Office on the design and evaluation of the fit, and found that the fit results for each study participant were very similar to what they found using the cMaSK.Customized fitThe researchers hope that their findings will encourage mask manufacturers to design masks that fit a variety of face shapes and sizes, especially women’s faces. Dagdeviren’s lab is planning to work on mass production and large-scale deployment of the cMaSK interface.“We hope to think about ways to design masks and come up with the best fit for individuals,” Dagdeviren says. “We have different sizes for shoes, and you can even customize your shoes. So why can’t you customize and design your mask, for your own health and for societal benefit?”The researchers also hope to return to their original idea of studying the effects of air pollution on people who work outside.“Our technology can really help to quantify the social costs of these environmental hazards, and also to measure the benefits of any kind of policy intervention,” Zheng says.The research was funded by the MIT Media Lab Consortium, the 3M Non-Tenured Faculty Award, and the MIT International Science and Technology Initiative (MISTI) Global Fund.",MIT engineers develop sensors for face masks that help gauge fit
90,3,3_cells_brain_blood_heart,https://news.mit.edu/2022/ultrasound-stickers-0728,"The study also includes lead authors Chonghe Wang and Xiaoyu Chen, and co-authors Liu Wang, Mitsutoshi Makihata, and Tao Zhao at MIT, along with Hsiao-Chuan Liu of the Mayo Clinic in Rochester, Minnesota.“We envision a few patches adhered to different locations on the body, and the patches would communicate with your cellphone, where AI algorithms would analyze the images on demand,” says the study’s senior author, Xuanhe Zhao, professor of mechanical engineering and civil and environmental engineering at MIT. “We believe we’ve opened a new era of wearable imaging: With a few patches on your body, you could see your internal organs.”If the devices can be made to operate wirelessly — a goal the team is currently working toward — the ultrasound stickers could be made into wearable imaging products that patients could take home from a doctor’s office or even buy at a pharmacy.The current design requires connecting the stickers to instruments that translate the reflected sound waves into images. The researchers point out that even in their current form, the stickers could have immediate applications: For instance, the devices could be applied to patients in the hospital, similar to heart-monitoring EKG stickers, and could continuously image internal organs without requiring a technician to hold a probe in place for long periods of time.The researchers applied the stickers to volunteers and showed the devices produced live, high-resolution images of major blood vessels and deeper organs such as the heart, lungs, and stomach. The stickers maintained a strong adhesion and captured changes in underlying organs as volunteers performed various activities, including sitting, standing, jogging, and biking.In a paper appearing today in Science, the engineers present the design for a new ultrasound sticker — a stamp-sized device that sticks to skin and can provide continuous ultrasound imaging of internal organs for 48 hours.Currently, ultrasound imaging requires bulky and specialized equipment available only in hospitals and doctor’s offices. But a new design by MIT engineers might make the technology as wearable and accessible as buying Band-Aids at the pharmacy.Ultrasound imaging is a safe and noninvasive window into the body’s workings, providing clinicians with live images of a patient’s internal organs. To capture these images, trained technicians manipulate ultrasound wands and probes to direct sound waves into the body. These waves reflect back out to produce high-resolution images of a patient’s heart, lungs, and other deep organs.A sticky issueTo image with ultrasound, a technician first applies a liquid gel to a patient’s skin, which acts to transmit ultrasound waves. A probe, or transducer, is then pressed against the gel, sending sound waves into the body that echo off internal structures and back to the probe, where the echoed signals are translated into visual images.For patients who require long periods of imaging, some hospitals offer probes affixed to robotic arms that can hold a transducer in place without tiring, but the liquid ultrasound gel flows away and dries out over time, interrupting long-term imaging.In recent years, researchers have explored designs for stretchable ultrasound probes that would provide portable, low-profile imaging of internal organs. These designs gave a flexible array of tiny ultrasound transducers, the idea being that such a device would stretch and conform with a patient’s body.But these experimental designs have produced low-resolution images, in part due to their stretch: In moving with the body, transducers shift location relative to each other, distorting the resulting image.“Wearable ultrasound imaging tool would have huge potential in the future of clinical diagnosis. However, the resolution and imaging duration of existing ultrasound patches is relatively low, and they cannot image deep organs,” says Chonghe Wang, who is an MIT graduate student.An inside lookThe MIT team’s new ultrasound sticker produces higher resolution images over a longer duration by pairing a stretchy adhesive layer with a rigid array of transducers. “This combination enables the device to conform to the skin while maintaining the relative location of transducers to generate clearer and more precise images.” Wang says.The device’s adhesive layer is made from two thin layers of elastomer that encapsulate a middle layer of solid hydrogel, a mostly water-based material that easily transmits sound waves. Unlike traditional ultrasound gels, the MIT team’s hydrogel is elastic and stretchy.“The elastomer prevents dehydration of hydrogel,” says Chen, an MIT postdoc. “Only when hydrogel is highly hydrated can acoustic waves penetrate effectively and give high-resolution imaging of internal organs.”The bottom elastomer layer is designed to stick to skin, while the top layer adheres to a rigid array of transducers that the team also designed and fabricated. The entire ultrasound sticker measures about 2 square centimeters across, and 3 millimeters thick — about the area of a postage stamp.The researchers ran the ultrasound sticker through a battery of tests with healthy volunteers, who wore the stickers on various parts of their bodies, including the neck, chest, abdomen, and arms. The stickers stayed attached to their skin, and produced clear images of underlying structures for up to 48 hours. During this time, volunteers performed a variety of activities in the lab, from sitting and standing, to jogging, biking, and lifting weights.From the stickers’ images, the team was able to observe the changing diameter of major blood vessels when seated versus standing. The stickers also captured details of deeper organs, such as how the heart changes shape as it exerts during exercise. The researchers were also able to watch the stomach distend, then shrink back as volunteers drank then later passed juice out of their system. And as some volunteers lifted weights, the team could detect bright patterns in underlying muscles, signaling temporary microdamage.“With imaging, we might be able to capture the moment in a workout before overuse, and stop before muscles become sore,” says Chen. “We do not know when that moment might be yet, but now we can provide imaging data that experts can interpret.”The team is working to make the stickers function wirelessly. They are also developing software algorithms based on artificial intelligence that can better interpret and diagnose the stickers’ images. Then, Zhao envisions ultrasound stickers could be packaged and purchased by patients and consumers, and used not only to monitor various internal organs, but also the progression of tumors, as well as the development of fetuses in the womb.“We imagine we could have a box of stickers, each designed to image a different location of the body,” Zhao says. “We believe this represents a breakthrough in wearable devices and medical imaging.”This research was funded, in part, by MIT, the Defense Advanced Research Projects Agency, the National Science Foundation, the National Institutes of Health, and the U.S. Army Research Office through the Institute for Soldier Nanotechnologies at MIT.",MIT engineers develop stickers that can see inside the body
2,3,3_cells_brain_blood_heart,https://9to5mac.com/2022/10/25/apple-watch-blood-oxygen-study/,"A new validation study published this month puts the blood oxygen feature of the Apple Watch to the test. According to the results of the study, the Apple Watch Series 6 is able to “reliably detect states of reduced blood oxygen saturation” in comparison to medical-grade pulse oximeters. Here’s how the study came to determine this…How reliable is the Apple Watch’s blood oxygen sensor?As spotted by MyHealthyApple, the study was published this month in the Digital Health open access journal. The objective of the study was to investigate “how a commercially available smartwatch that measures peripheral blood oxygen saturation (SpO 2 ) can detect hypoxemia compared to a medical-grade pulse oximeter.”For the study, researchers recruited 24 healthy participants. Each person wore an Apple Watch Series 6 on their left wrist and a pulse oximeter sensor on their left middle finger (the Masimo Radical-7).The participants breathed via a breathing circuit with a three-way non-rebreathing valve in three phases. First, in the 2-minute initial stabilization phase, the participants inhaled the ambient air. Then in the 5-minute desaturation phase, the participants breathed the oxygen-reduced gas mixture (12% O 2 ), which temporarily reduced their blood oxygen saturation. In the final stabilization phase, the participants inhaled the ambient air again until SpO 2 returned to normal values. Measurements of SpO 2 were taken from the smartwatch and the pulse oximeter simultaneously in 30-s intervals.The study resulted in 642 individual pairs of blood oxygen measurements:The differences in individual measurements between the smartwatch and oximeter within 6% SpO 2 can be expected for SpO 2 readings 90%-100% and up to 8% for SpO 2 readings less than 90%.As such, the researchers conclude:The bias in SpO 2 between the smartwatch and the oximeter was 0.0% for all the data points. The bias for SpO 2 less than 90% was 1.2%. The differences in individual measurements between the smartwatch and oximeter within 6% SpO 2 can be expected for SpO 2 readings 90%–100% and up to 8% for SpO 2 readings less than 90%.Apple first added blood oxygen measurement support to the Apple Watch Series 6. It’s also a feature on the newest Apple Watch Series 7, Apple Watch Series 8, and Apple Watch Ultra. Apple has been hesitant to promote the feature with any actual medical claims, and the company hasn’t advertised any significant improvements to the technology since it first debuted on the Series 6 in 2020.You can read the complete study right here on the SAGE Journals website.FTC: We use income earning auto affiliate links. More.Check out 9to5Mac on YouTube for more Apple news:",Study finds Apple Watch blood oxygen sensor is as reliable as âmedical-grade deviceâ
569,3,3_cells_brain_blood_heart,https://newsroom.unsw.edu.au/news/science-tech/engineers-light-way-toward-bionics-future?utm_source=reddit&utm_medium=social,"Biomedical and electrical engineers at UNSW Sydney have developed a new way to measure neural activity using light – rather than electricity – which could lead to a complete reimagining of medical technologies like nerve-operated prosthetics and brain-machine interfaces.Biomedical and electrical engineers at UNSW Sydney have developed a new way to measure neural activity using light – rather than electricity – which could lead to a complete reimagining of medical technologies like nerve-operated prosthetics and brain-machine interfaces.Professor François LadouceurProfessor François LadouceurNot only do these optrodes perform just as well as conventional electrodes – that use electricity to detect a nerve impulse – but they also address “very thorny issues that competing technologies cannot address”, says Prof. Ladouceur.Not only do these optrodes perform just as well as conventional electrodes – that use electricity to detect a nerve impulse – but they also address “very thorny issues that competing technologies cannot address”, says Prof. Ladouceur.“Firstly, it’s very difficult to shrink the size of the interface using conventional electrodes so that thousands of them can connect to thousands of nerves within a very small area.“Firstly, it’s very difficult to shrink the size of the interface using conventional electrodes so that thousands of them can connect to thousands of nerves within a very small area.“One of the problems as you shrink thousands of electrodes and put them ever closer together to connect to the biological tissues is that their individual resistance increases, which degrades the signal-to-noise ratio so we have a problem reading the signal. We call this ‘impedance mismatch’.“One of the problems as you shrink thousands of electrodes and put them ever closer together to connect to the biological tissues is that their individual resistance increases, which degrades the signal-to-noise ratio so we have a problem reading the signal. We call this ‘impedance mismatch’.Read moreRead more“Another problem is what we call ‘crosstalk’ – when you shrink these electrodes and bring them closer together, they start to talk to, or affect each other because of their proximity.”“Another problem is what we call ‘crosstalk’ – when you shrink these electrodes and bring them closer together, they start to talk to, or affect each other because of their proximity.”“The real advantage of our approach is that we can make this connection very dense in the optical domain and we don’t pay the price that you have to pay in the electrical domain,” Prof. Ladouceur says.“The real advantage of our approach is that we can make this connection very dense in the optical domain and we don’t pay the price that you have to pay in the electrical domain,” Prof. Ladouceur says.In research published recently in theIn research published recently in theScientia Professor Nigel Lovell, who heads theScientia Professor Nigel Lovell, who heads theHe says the team connected an optrode to the sciatic nerve of an anaesthetised animal. The nerve was then stimulated with a small current and the neural signals were recorded with the optrode. Then they did the same using a conventional electrode and a bioamplifier.He says the team connected an optrode to the sciatic nerve of an anaesthetised animal. The nerve was then stimulated with a small current and the neural signals were recorded with the optrode. Then they did the same using a conventional electrode and a bioamplifier.“We demonstrated that the nerve responses were essentially the same,” says Prof. Lovell. “There’s still more noise in the optical one, but that’s not surprising given this is brand new technology, and we can work on that. But ultimately, we could identify the same characteristics by measuring electrically or optically.”“We demonstrated that the nerve responses were essentially the same,” says Prof. Lovell. “There’s still more noise in the optical one, but that’s not surprising given this is brand new technology, and we can work on that. But ultimately, we could identify the same characteristics by measuring electrically or optically.”New dawn for prostheticsNew dawn for prostheticsSo far the team has been able to show that nerve impulses – which are relatively weak and measured in microvolts – can be registered by optrode technology. The next step will be to scale up the number of optrodes to be able to handle complex networks of nervous and excitable tissue.So far the team has been able to show that nerve impulses – which are relatively weak and measured in microvolts – can be registered by optrode technology. The next step will be to scale up the number of optrodes to be able to handle complex networks of nervous and excitable tissue.Prof. Ladouceur says at the beginning of the project, his colleagues asked themselves, how many neural connections does a man or woman need to operate a hand with a degree of finesse?Prof. Ladouceur says at the beginning of the project, his colleagues asked themselves, how many neural connections does a man or woman need to operate a hand with a degree of finesse?“That you can pick up an object, that you can judge the friction, you can apply just the right pressure to hold it, you can move from A to B with precision, you can go fast and slow – all these things that we don’t even think about when we perform these actions. The answer is not so obvious, we had to search quite a bit in the literature, but we believe it’s about 5000 to 10,000 connections.”“That you can pick up an object, that you can judge the friction, you can apply just the right pressure to hold it, you can move from A to B with precision, you can go fast and slow – all these things that we don’t even think about when we perform these actions. The answer is not so obvious, we had to search quite a bit in the literature, but we believe it’s about 5000 to 10,000 connections.”In other words, between your brain and your hand there is a bundle of nerves that travels down from your cortex and eventually divides into those 5000 to 10,000 nerves that control the delicate operations of your hand.In other words, between your brain and your hand there is a bundle of nerves that travels down from your cortex and eventually divides into those 5000 to 10,000 nerves that control the delicate operations of your hand.If a chip with thousands of optical connections could connect to your brain, or some place in the arm before the nerve bundle separates, a prosthetic hand could potentially be able to function with much the same ability as a biological one.If a chip with thousands of optical connections could connect to your brain, or some place in the arm before the nerve bundle separates, a prosthetic hand could potentially be able to function with much the same ability as a biological one.That’s the dream, anyway, and Prof. Ladouceur says there are likely decades of further research before it’s a reality. This would include developing the ability for optrodes to be bidirectional. Not only would they receive and interpret signals from the brain on the way to the body, they could receive feedback in the form of neural impulses going back to the brain.That’s the dream, anyway, and Prof. Ladouceur says there are likely decades of further research before it’s a reality. This would include developing the ability for optrodes to be bidirectional. Not only would they receive and interpret signals from the brain on the way to the body, they could receive feedback in the form of neural impulses going back to the brain.The long game: brain-machine interfaceThe long game: brain-machine interfaceNeural prosthetics isn’t the only space that optrode technology has the potential to redefine. Humans have long fantasised about integrating technology and machinery into the human body to either repair or enhance it.Neural prosthetics isn’t the only space that optrode technology has the potential to redefine. Humans have long fantasised about integrating technology and machinery into the human body to either repair or enhance it.Some of this is now a reality, such as Cochlear implants, pacemakers and cardiac defibrillators, not to mention smart watches and other tracking devices giving continual biofeedback.Some of this is now a reality, such as Cochlear implants, pacemakers and cardiac defibrillators, not to mention smart watches and other tracking devices giving continual biofeedback.But one of the more ambitious goals in biomedical engineering and neuroscience is the brain-machine interface that aims to connect the brain to not only the rest of the body, but potentially the world.But one of the more ambitious goals in biomedical engineering and neuroscience is the brain-machine interface that aims to connect the brain to not only the rest of the body, but potentially the world.“The area of neural interfacing is an incredibly exciting field and will be the subject of intense research and development over the next decade,” says Prof. Lovell.“The area of neural interfacing is an incredibly exciting field and will be the subject of intense research and development over the next decade,” says Prof. Lovell.While this is more fiction than fact right now, there are many biotech companies taking this very seriously. Entrepreneur Elon Musk was one of the co-founders ofWhile this is more fiction than fact right now, there are many biotech companies taking this very seriously. Entrepreneur Elon Musk was one of the co-founders ofThe Neuralink approach uses conventional wire electrodes in its devices so it must overcome impedance mismatch and crosstalk – among many other challenges – if they are to develop devices that host thousands, if not millions, of connections between the brain and the implanted device. RecentlyThe Neuralink approach uses conventional wire electrodes in its devices so it must overcome impedance mismatch and crosstalk – among many other challenges – if they are to develop devices that host thousands, if not millions, of connections between the brain and the implanted device. RecentlyRead moreRead moreProf. Ladouceur says time will tell whether Neuralink and its competitors succeed in removing these obstacles. However, given that implantable, in vivo devices that capture neural activity are currently constrained to about 100 or so electrodes, there is still a long way to go.Prof. Ladouceur says time will tell whether Neuralink and its competitors succeed in removing these obstacles. However, given that implantable, in vivo devices that capture neural activity are currently constrained to about 100 or so electrodes, there is still a long way to go.“I'm not saying that it's impossible, but it becomes really problematic if you were to stick to standard electrodes,” Prof. Ladouceur says.“I'm not saying that it's impossible, but it becomes really problematic if you were to stick to standard electrodes,” Prof. Ladouceur says.“We don't have these problems in the optical domain. In our devices, if there is neural activity, its presence influences the orientation of the liquid crystal which we can detect and quantify by shining light on it. It means we don't extract current from the biological tissues as the wire electrodes do. And so the biosensing can be done much more efficiently.”“We don't have these problems in the optical domain. In our devices, if there is neural activity, its presence influences the orientation of the liquid crystal which we can detect and quantify by shining light on it. It means we don't extract current from the biological tissues as the wire electrodes do. And so the biosensing can be done much more efficiently.”Now that the researchers have shown that the optrode method works in vivo, they will shortly publish research that shows the optrode technology is bidirectional – that it can not only read neural signals, but can write them too.Now that the researchers have shown that the optrode method works in vivo, they will shortly publish research that shows the optrode technology is bidirectional – that it can not only read neural signals, but can write them too.",Engineers light the way to bionics of the future
420,3,3_cells_brain_blood_heart,https://www.newscientist.com/article/2338447-face-recognition-technology-for-pigs-could-improve-welfare-on-farms/,"Machine learning software can identify individual pigs based on their facial features with high accuracy, which could help farmers give animals individualised food and veterinary carePigs have unique facial features that can be identified by machine learning software SRUCPigs could be issued with biometric passports based on facial recognition technology, giving farmers a more practical and welfare-friendly way of identifying individuals than ear notches or tags, the current industry standards.Identifying pigs based on their unique facial features could enable them to receive individualised food and veterinary care, and be traced as they go through meat processing. With advanced algorithms and machine learning, it is possible to distinguish between the faces of even the most similar-looking …",Face recognition technology for pigs could improve welfare on farms
424,3,3_cells_brain_blood_heart,https://www.newscientist.com/article/2344551-first-3d-quantum-accelerometer-could-let-ships-navigate-without-gps/,"A device that measures acceleration very precisely using quantum effects could be used for navigation when GPS is unavailableA quantum accelerometer could help ships navigate d13/ShutterstockA quantum device that can determine its position in three dimensions is more accurate than non-quantum versions. Vehicles could use it to navigate even if GPS stopped working.One way to keep track of something’s position is with an accelerometer, which is a small device that is found in everything from phones to drones. Accelerometers work by detecting changes in movement and therefore position.It has been known for decades that quantum effects could be used make more accurate accelerometers, but most …",First 3D quantum accelerometer could let ships navigate without GPS
494,3,3_cells_brain_blood_heart,https://www.smithsonianmag.com/smart-news/this-wearable-ultrasound-sticker-can-continuously-image-organs-for-48-hours-180980504/,"Ultrasound is a convenient, noninvasive tool for doctors to look inside the human body and check out a person’s liver, heart and other internal structures, as well as the developing fetus of a pregnant patient. But today’s ultrasound imaging technology is large and technical, so it’s only available in healthcare facilities and must be operated by highly trained technicians. Plus, patients, who take time out of their schedules to go to an appointment, have to be covered in a sticky gel.Now, researchers say they’ve developed an innovative solution to some of these challenges. Engineers at the Massachusetts Institute of Technology have unveiled a new adhesive ultrasound patch that’s about the size of a postage stamp and can provide continuous imaging of the body’s inner workings for up to 48 hours. The scientists shared their new technology in a paper published last week in the journal Science.“We believe we’ve opened a new era of wearable imaging,” says Xuanhe Zhao, a mechanical engineer at MIT and one of the study’s authors, in a statement. “With a few patches on your body, you could see your internal organs.”In the past, engineers developing wearable ultrasound technologies have run into issues with image quality and flexibility, but the new MIT stickers seem to have struck the right balance. To create the small devices, which are about three millimeters thick and two square centimeters in size, engineers combined rigid transducers with a stretchy, sticky layer that encapsulates a layer of water-based hydrogel.The researchers put the stickers to the test by placing them on various body parts of healthy volunteers, including their neck, abdomen, arms and chest. Then, they asked the volunteers to move their bodies in various ways, including drinking juice, jogging, lifting weights, biking, sitting and standing.Not only did the stickers adhere to the volunteers’ skin, but they also produced clear images of their internal organs and major blood vessels for up to 48 hours. The patches allowed the researchers to watch volunteers’ stomachs expand, then shrink, when they drank juice. They could observe major blood vessels changing sizes when a volunteer was sitting versus standing, and witness the heart change shape while pumping blood during exercise.For now, the stickers must be connected to instruments to make sense of the data they’ve gathered. But in the near future, the scientists hope to refine their design so that the stickers can operate wirelessly, communicating with a patient’s smart phone for real-time, on-demand imaging.“We are working hard on the wireless version,” Zhao tells the Guardian’s Ian Sample. “Because there are already wireless point-of-care handhold ultrasounds, we are confident that we will be able to achieve the wireless version in a few years.”Already, researchers are dreaming up all the ways the stickers could be useful. Patients could wear them at home to help monitor an array of medical conditions, including cardiovascular disease and cancer. In hospitals, they could free up medical technicians for other tasks or make it possible to obtain ultrasound images in healthcare facilities with staffing issues.Recommended Videos“You don’t need a trained sonographer and you don’t need a huge ultrasound machine,” says Philip Tan, an electrical engineer at the University of Texas at Austin, to New Scientist ’s Jeremy Hsu. “You could deploy it to very low-resource communities.”",This Wearable Ultrasound Sticker Can Continuously Image Organs for 48 Hours
500,3,3_cells_brain_blood_heart,https://www.tcd.ie/news_events/top-stories/featured/our-brains-use-quantum-computation/,"Our brains use quantum computationPosted on: 19 October 2022Scientists from Trinity believe our brains could use quantum computation after adapting an idea developed to prove the existence of quantum gravity to explore the human brain and its workings. The discovery may shed light on consciousness, the workings of which remain scientifically difficult to understand and explain. Quantum brain processes could also explain why we can still outperform supercomputers when it comes to unforeseen circumstances, decision making, or learning something newScientists from Trinity believe our brains could use quantum computation after adapting an idea developed to prove the existence of quantum gravity to explore the human brain and its workings.The brain functions measured were also correlated to short-term memory performance and conscious awareness, suggesting quantum processes are also part of cognitive and conscious brain functions.If the team’s results can be confirmed – likely requiring advanced multidisciplinary approaches –they would enhance our general understanding of how the brain works and potentially how it can be maintained or even healed. They may also help find innovative technologies and build even more advanced quantum computers.Dr Christian Kerskens, lead physicist at the Trinity College Institute of Neuroscience (TCIN), is the co-author of the research article that has just been published in the Journal of Physics Communications. He said:“We adapted an idea, developed for experiments to prove the existence of quantum gravity, whereby you take known quantum systems, which interact with an unknown system. If the known systems entangle, then the unknown must be a quantum system, too. It circumvents the difficulties to find measuring devices for something we know nothing about.“For our experiments we used proton spins of ‘brain water’ as the known system. ‘Brain water’ builds up naturally as fluid in our brains and the proton spins can be measured using MRI (Magnetic Resonance Imaging). Then, by using a specific MRI design to seek entangled spins, we found MRI signals that resemble heartbeat evoked potentials, a form of EEG signals. EEGs measure electrical brain currents, which some people may recognise from personal experience or simply from watching hospital dramas on TV.”Electrophysiological potentials like the heartbeat evoked potentials are normally not detectable with MRI and the scientists believe they could only observe them because the nuclear proton spins in the brain were entangled.Dr Kerskens added:“If entanglement is the only possible explanation here then that would mean that brain processes must have interacted with the nuclear spins, mediating the entanglement between the nuclear spins. As a result, we can deduce that those brain functions must be quantum.“Because these brain functions were also correlated to short-term memory performance and conscious awareness, it is likely that those quantum processes are an important part of our cognitive and conscious brain functions.“Quantum brain processes could explain why we can still outperform supercomputers when it comes to unforeseen circumstances, decision making, or learning something new. Our experiments performed only 50 metres away from the lecture theatre, where Schrödinger presented his famous thoughts about life, may shed light on the mysteries of biology, and on consciousness which scientifically is even harder to grasp.”This research was supported by Science Foundation Ireland and TCIN. The open access journal article can be read on the publisher's website.",Our brains use quantum computation
29,3,3_cells_brain_blood_heart,https://aws.amazon.com/blogs/quantum-computing/amazon-braket-launches-aquila-the-first-neutral-atom-quantum-processor-from-quera-computing/,"IntroductionQuantum researchers require access to different types of quantum hardware from digital, also known as gate-based, quantum processing units (QPUs) to analog devices that are capable of addressing specific problems that are hard to solve using classical computers. Today, Amazon Braket, the quantum computing service from AWS, continues to deliver on its commitment to provide that choice by launching Aquila, pictured in Figure 1 below, a new neutral-atom QPU from QuEra Computing with up to 256 qubits. As a special purpose device designed for solving optimization problems and simulating quantum phenomena in nature, it enables researchers to explore a new analog paradigm of quantum computing.Analog Hamiltonian SimulationThe QuEra QPU is the first device available on Amazon Braket capable of a paradigm of quantum computing known as Analog Hamiltonian Simulation (AHS). AHS refers to the ability to encode a problem of interest into a mathematical object known as the Hamiltonian. The Hamiltonian represents the energy levels of a quantum system such as interacting spins on a lattice. The computer is then tuned such that it directly simulates the continuous time evolution of the quantum system under that Hamiltonian.In traditional gate-based quantum computers, users can program gates acting on the qubits directly. Quantum processors such as the Oxford Quantum Circuits, or Rigetti devices on Amazon Braket function that way, with qubits consisting of the ground and excited states of an anharmonic oscillator. The QuEra QPU operates by trapping atoms with lasers, arranging them in programmable one or two-dimensional layouts, and inducing interatomic interactions via van der Waals forces. The qubit consists of the ground state of the atom and a highly excited state, known as a Rydberg state. By exciting atoms from the ground to the excited state, the QuEra QPU is able to realize a phenomenon known as Rydberg blockade, whereby the quantum states of neighboring qubits are fixed by the state of a control qubit. Furthermore, customers can dynamically tune driving field parameters, thus controlling qubit states and their interactions.What can I do with the QuEra QPU?The QuEra QPU is a special purpose device, trading-off the ability to perform universal or gate-based computation for the ability to efficiently solve specific tasks. The flexible atom arrangements and tunability of optical controls enable Aquila to realize a rich class of Hamiltonians. Customers can explore the static and dynamic properties of quantum states under these Hamiltonians by adiabatic or diabatic quantum evolution. To date, the Hamiltonian realized by the QuEra QPU has already been used to study several scientific questions of interest in condensed matter and quantum many-body physics. One such example is observing the emergence of a spin-liquid phase, a state of matter with non-local, topological order. These phases are hard to study numerically owing to the system sizes needed to demonstrate non-local order. The QuEra QPU allows customers to program complex lattice geometries such as the Kagome lattice with up to 256 qubits, a system size large enough to explore these novel states.“Special purpose analog quantum devices are likely to outperform classical computation for direct simulation of other quantum systems before we realize a fault-tolerant, universal quantum computer” said Ignacio Cirac, Director and Head of Theory Division, Max Planck Institute for Quantum Optics (MPQ). “In the Theory Division of MPQ we are excited by the launch of the QuEra device on Amazon Braket which enables our team of researchers to experiment and pursue new ideas in the field of analog quantum simulation.”Aquila’s users are not constrained to specific lattice geometries, or limited to arranging qubits in any regular pattern at all, as shown in Fig 2 above. In addition to strongly correlated many-particle systems, scientists were able to show that neutral-atom processors such as Aquila are suited to arranging atoms in graph patterns, and solving certain combinatorial optimization problems. Namely, these machines can encode the maximum independent set (MIS) problem, which has broad applications in optimization such as resource allocation, network design and others. The MIS problem can be cast as a variational problem which can be computed using iterative optimization cycles that combine a hybrid of quantum and classical operations. With the launch of the QuEra QPU on Amazon Braket, researchers can leverage Amazon Braket Hybrid Jobs to study hybrid algorithms using Aquila.Getting StartedOver forty years ago, Richard Feynman proposed harnessing quantum computers to simulate nature at the quantum scale. Aquila, the neutral-atom QPU available on Amazon Braket does just that: it uses an inherently quantum system, atoms on a tunable lattice, to solve specific problems that are of interest to a wide community of researchers. Aquila is now available in the Northern Virginia (us-east-1) region, and can be accessed using the same Amazon Braket SDK and APIs that you use to access other QPUs. Create your AHS program locally or through our managed Jupyter notebooks. Looking for inspiration? Check out our Getting Started documentation and our example notebooks. More information about pricing can be found on our webpage.","Amazon Braket launches Aquila, the first neutral-atom quantum processor from QuEra Computing"
535,3,3_cells_brain_blood_heart,https://www.theverge.com/2022/10/7/23392375/ai-scan-retina-predict-heart-disease-stroke-risk-machine-learning,"Software developed using machine learning can be used to predict someone’s risk of heart disease in less than a minute by analyzing the veins and arteries in their eye.The new research, published in the British Journal of Ophthalmology, paves the way for the development of quick and cheap cardiovascular screenings, if the findings are validated in future clinical trials. These screenings would let individuals know their risk of stroke and heart attack without the need for blood tests or even blood pressure measurements.“This AI tool could let someone know in 60 seconds or less their level of risk,” the lead author of the study, Alicja Rudnicka, told The Guardian. The study found that the predictions were as accurate as those produced by current tests.“The eye can be used as a window to the rest of the body.”The software works by analyzing the web of blood vessels contained within the retina of the eye. It measures the total area covered by these arteries and veins, as well as their width and “tortuosity” (how bendy they are). All these factors are affected by an individual’s heart health, allowing the software to make predictions about a subject’s risk from heart disease just by looking at a non-invasive snapshot of their eye.“The study adds to a growing body of knowledge that the eye can be used as a window to the rest of the body,” Pearse Keane, a researcher in ophthalmology and AI analysis not connected to the study, told The Verge. “Doctors have known for more than a hundred years that you could look in the eye and see signs of diabetes and high blood pressure. But the problem was manual assessment: the manual delineation of the vessels by human experts.” The use of machine learning, says Keane, can overcome this challenge.Using AI to diagnose disease from eye scans has proven to be one of the fastest-developing fields of machine learning medicine. The first ever AI diagnostic device approved by the FDA was used to screen for eye disease, and research suggests AI can detect a range of ailments in this way, from diabetic retinopathy to Alzheimer’s (Keane’s own area of research). Tools applying these findings are in various stages of development, but questions do remain about the reliability and universality of their diagnoses.The researcher named their software QUARTZ. Image: Rudnicka et alThis recent study, carried out by a team from St George’s, University of London, was only tested on the eye scans of white patients, for example. The team sourced their test data from the UK Biobank, a database that happens to be 94.6 percent white (reflecting the UK’s own demographics in age range of patients included in the BioBank). Such biases would have to be balanced in the future to ensure any diagnostic tool is equally accurate for different ethnicities.The researchers compared the results from their software, named QUARTZ (an inventive acronym derived from the phrase “QUantitative Analysis of Retinal vessels Topology and siZe”) with 10-year risk predictions produced by the standard Framingham Risk Score test (FRS). They found the two methods had “comparable performance.”",AI tool can scan your retina and predict your risk of heart disease âin 60 seconds or lessâ
79,3,3_cells_brain_blood_heart,https://medicine.yale.edu/news-article/study-reveals-how-genetics-impacts-susceptibility-to-developing-ptsd-following-trauma-exposure/,"Stem cell-derived neurons from combat veterans with and without post-traumatic stress disorder (PTSD) provide insights into how genetics can make someone more susceptible to developing PTSD following trauma exposure, according to a study conducted by scientists from several research institutions, including Yale School of Medicine.Post-traumatic stress disorder can develop following severe trauma and is an enormous public health problem for both veterans and civilians. However, the extent to which genetic and environmental factors contribute to individual clinical outcomes remains unknown.To bridge this information gap, the research team studied a cohort of 39 combat veterans with and without PTSD who were recruited from the James J. Peters Bronx Veterans Affairs Hospital. Veterans underwent skin biopsies and their skin cells were reprogrammed into induced pluripotent stem cells.“Reprogramming cells into induced pluripotent stem cells is like virtually taking cells back in time to when they were embryonic and had the ability to generate all the cells of the body,” said Rachel Yehuda, PhD, professor of psychiatry and neuroscience at Icahn Mount Sinai, director of mental health for the Department of Veterans Affairs and senior author of the paper. “These cells can then be differentiated into neurons with the same properties as that person’s brain cells had before trauma occurred to change the way they function. The gene expression networks from these neurons reflect early gene activity resulting from genetic and very early developmental contributions, so they are a reflection of the ‘pre-combat’ or ‘pre-trauma’ gene expression state.”“Two people can experience the same trauma, but they won't necessarily both develop PTSD,” said Kristen Brennand, PhD, Elizabeth Mears and House Jameson Professor of Psychiatry at Yale School of Medicine who co-led the study. “This type of modeling in brain cells from people with and without PTSD helps explain how genetics can make someone more susceptible to PTSD.”Published October 20 in Nature Neuroscience, this is the first study to use induced pluripotent stem cell models to study PTSD.The study involved researchers from Yale School of Medicine, Icahn School of Medicine at Mount Sinai, the Office of Veterans Affairs, and The New York Stem Cell Foundation Research Institute (NYSCF).To mimic the stress response that triggers PTSD, the scientists exposed the induced pluripotent stem cell-derived neurons to the stress hormone hydrocortisone, a synthetic version of the body’s own cortisol that is used as part of the ‘fight-or-flight’ response.“The addition of stress hormones to these cells simulates biological effects of combat, which allows us to determine how different gene networks mobilize in response to stress exposure in brain cells,” Yehuda said.Using gene expression profiling and imaging, the scientists found that neurons from individuals with PTSD were hypersensitive to this pharmacological trigger. The scientists also were able to identify the specific gene networks that responded differentially following exposure to the stress hormones.Most similar studies of PTSD to date have used blood samples from patients, but since PTSD is rooted in the brain, scientists need a way to capture how the neurons of prone individuals are affected by stress. Therefore, the team opted to use stem cells, as they are uniquely equipped to provide a patient-specific, non-invasive window into the brain.You can’t easily reach into a living person’s brain and pull out cells, so stem cells are our best way to examine how neurons are behaving in a patient. Kristen Brennand, PhD, Elizabeth Mears and House Jameson Professor of Psychiatry at Yale School of Medicine“You can’t easily reach into a living person’s brain and pull out cells, so stem cells are our best way to examine how neurons are behaving in a patient,” Brennand said.NYSCF scientists used their scalable, automated, robotic system – The NYSCF Global Stem Cell Array® – to create stem cells and then glutamatergic neurons from patients with PTSD. Glutamatergic neurons help the brain send excitatory signals and have previously been implicated in PTSD.“As this was the first study using stem cell models of PTSD, it was important to study a large number of individuals,” said Daniel Paull, PhD, NYSCF Senior Vice President, Discovery & Platform Development, and co-leader of the study. “At the scale of this study, automation is essential. With the Array, we can make standardized cells that allow for meaningful comparisons between numerous individuals, pointing to key differences that could be critical for discovering new treatments.”The team’s gene expression analysis revealed a set of genes that were particularly active in PTSD-prone neurons following their exposure to stress hormones.Moreover, the distinctions between how PTSD and non-PTSD cells responded to stress could be informative in predicting which individuals are at higher risk for PTSD.The researchers plan to continue leveraging their induced pluripotent stem cell models to further investigate the genetic risk factors pinpointed by this study and to study how PTSD affects other types of brain cells, helping to broaden opportunities for therapeutic discovery.“What’s special about this study is that it could have only been done by this group,” Brennand said. “It involved some of the best clinicians in this space, incredible stem cell biologists, and amazing psychiatric geneticists. Each group has unique expertise, and none of this could have been accomplished by any one team alone.”“This study is a true testament to the power of team science,” Paull said. “When researchers combine forces, we are able to ask bigger questions, make bigger discoveries, and hopefully, make a bigger difference for patients.”This work was supported by a grant to the Icahn School of Medicine at Mount Sinai from the U.S Army Medical Research Department of Defense.",Study Reveals How Genetics Impacts Susceptibility to Developing PTSD Following Trauma Exposure
507,3,3_cells_brain_blood_heart,https://www.theatlantic.com/magazine/archive/2018/11/the-pentagon-wants-to-weaponize-the-brain-what-could-go-wrong/570841/?utm_source=copy-link&utm_medium=social&utm_campaign=share,"I. Who Could Object?“Tonight I would like to share with you an idea that I am extremely passionate about,” the young man said. His long black hair was swept back like a rock star’s, or a gangster’s. “Think about this,” he continued. “Throughout all human history, the way that we have expressed our intent, the way we have expressed our goals, the way we have expressed our desires, has been limited by our bodies.” When he inhaled, his rib cage expanded and filled out the fabric of his shirt. Gesturing toward his body, he said, “We are born into this world with this. Whatever nature or luck has given us.”His speech then took a turn: “Now, we’ve had a lot of interesting tools over the years, but fundamentally the way that we work with those tools is through our bodies.” Then a further turn: “Here’s a situation that I know all of you know very well—your frustration with your smartphones, right? This is another tool, right? And we are still communicating with these tools through our bodies.”And then it made a leap: “I would claim to you that these tools are not so smart. And maybe one of the reasons why they’re not so smart is because they’re not connected to our brains. Maybe if we could hook those devices into our brains, they could have some idea of what our goals are, what our intent is, and what our frustration is.”So began “Beyond Bionics,” a talk by Justin C. Sanchez, then an associate professor of biomedical engineering and neuroscience at the University of Miami, and a faculty member of the Miami Project to Cure Paralysis. He was speaking at a tedx conference in Florida in 2012. What lies beyond bionics? Sanchez described his work as trying to “understand the neural code,” which would involve putting “very fine microwire electrodes”—the diameter of a human hair—“into the brain.” When we do that, he said, we would be able to “listen in to the music of the brain” and “listen in to what somebody’s motor intent might be” and get a glimpse of “your goals and your rewards” and then “start to understand how the brain encodes behavior.”He explained, “With all of this knowledge, what we’re trying to do is build new medical devices, new implantable chips for the body that can be encoded or programmed with all of these different aspects. Now, you may be wondering, what are we going to do with those chips? Well, the first recipients of these kinds of technologies will be the paralyzed. It would make me so happy by the end of my career if I could help get somebody out of their wheelchair.”Sanchez went on, “The people that we are trying to help should never be imprisoned by their bodies. And today we can design technologies that can help liberate them from that. I’m truly inspired by that. It drives me every day when I wake up and get out of bed. Thank you so much.” He blew a kiss to the audience.The mission is to make human beings something other than what we are, with powers beyond the ones we’re born with.A year later, Justin Sanchez went to work for the Defense Advanced Research Projects Agency, the Pentagon’s R&D department. At DARPA, he now oversees all research on the healing and enhancement of the human mind and body. And his ambition involves more than helping get disabled people out of their wheelchair—much more.DARPA has dreamed for decades of merging human beings and machines. Some years ago, when the prospect of mind-controlled weapons became a public-relations liability for the agency, officials resorted to characteristic ingenuity. They recast the stated purpose of their neurotechnology research to focus ostensibly on the narrow goal of healing injury and curing illness. The work wasn’t about weaponry or warfare, agency officials claimed. It was about therapy and health care. Who could object? But even if this claim were true, such changes would have extensive ethical, social, and metaphysical implications. Within decades, neurotechnology could cause social disruption on a scale that would make smartphones and the internet look like gentle ripples on the pond of history.Most unsettling, neurotechnology confounds age-old answers to this question: What is a human being?II. High Risk, High RewardIn his 1958 State of the Union address, President Dwight Eisenhower declared that the United States of America “must be forward-looking in our research and development to anticipate the unimagined weapons of the future.” A few weeks later, his administration created the Advanced Research Projects Agency, a bureaucratically independent body that reported to the secretary of defense. This move had been prompted by the Soviet launch of the Sputnik satellite. The agency’s original remit was to hasten America’s entry into space.During the next few years, arpa’s mission grew to encompass research into “man-computer symbiosis” and a classified program of experiments in mind control that was code-named Project Pandora. There were bizarre efforts that involved trying to move objects at a distance by means of thought alone. In 1972, with an increment of candor, the word Defense was added to the name, and the agency became DARPA. Pursuing its mission, DARPA funded researchers who helped invent technologies that changed the nature of battle (stealth aircraft, drones) and shaped daily life for billions (voice-recognition technology, GPS devices). Its best-known creation is the internet.The agency’s penchant for what it calls “high-risk, high-reward” research ensured that it would also fund a cavalcade of folly. Project Seesaw, a quintessential Cold War boondoggle, envisioned a “particle-beam weapon” that could be deployed in the event of a Soviet attack. The idea was to set off a series of nuclear explosions beneath the Great Lakes, creating a giant underground chamber. Then the lakes would be drained, in a period of 15 minutes, to generate the electricity needed to set off a particle beam. The beam would accelerate through tunnels hundreds of miles long (also carved out by underground nuclear explosions) in order to muster enough force to shoot up into the atmosphere and knock incoming Soviet missiles out of the sky. During the Vietnam War, DARPA tried to build a Cybernetic Anthropomorphous Machine, a jungle vehicle that officials called a “mechanical elephant.”One aspiration: the ability, via computer, to transfer knowledge and thoughts from one person’s mind to another’s.The diverse and sometimes even opposing goals of DARPA scientists and their Defense Department overlords merged into a murky, symbiotic research culture—“unencumbered by the typical bureaucratic oversight and uninhibited by the restraints of scientific peer review,” Sharon Weinberger wrote in a recent book, The Imagineers of War. In Weinberger’s account, DARPA’s institutional history involves many episodes of introducing a new technology in the context of one appealing application, while hiding other genuine but more troubling motives. At DARPA, the left hand knows, and doesn’t know, what the right hand is doing.The agency is deceptively compact. A mere 220 employees, supported by about 1,000 contractors, report for work each day at DARPA’s headquarters, a nondescript glass-and-steel building in Arlington, Virginia, across the street from the practice rink for the Washington Capitals. About 100 of these employees are program managers—scientists and engineers, part of whose job is to oversee about 2,000 outsourcing arrangements with corporations, universities, and government labs. The effective workforce of DARPA actually runs into the range of tens of thousands. The budget is officially said to be about $3 billion, and has stood at roughly that level for an implausibly long time—the past 14 years.The Biological Technologies Office, created in 2014, is the newest of DARPA’s six main divisions. This is the office headed by Justin Sanchez. One purpose of the office is to “restore and maintain warfighter abilities” by various means, including many that emphasize neurotechnology—applying engineering principles to the biology of the nervous system. For instance, the Restoring Active Memory program develops neuroprosthetics—tiny electronic components implanted in brain tissue—that aim to alter memory formation so as to counteract traumatic brain injury. Does DARPA also run secret biological programs? In the past, the Department of Defense has done such things. It has conducted tests on human subjects that were questionable, unethical, or, many have argued, illegal. The Big Boy protocol, for example, compared radiation exposure of sailors who worked above and below deck on a battleship, never informing the sailors that they were part of an experiment.Eddie GuyLast year I asked Sanchez directly whether any of DARPA’s neurotechnology work, specifically, was classified. He broke eye contact and said, “I can’t—We’ll have to get off that topic, because I can’t answer one way or another.” When I framed the question personally—“Are you involved with any classified neuroscience project?”—he looked me in the eye and said, “I’m not doing any classified work on the neurotechnology end.”If his speech is careful, it is not spare. Sanchez has appeared at public events with some frequency (videos are posted on DARPA’s YouTube channel), to articulate joyful streams of good news about DARPA’s proven applications—for instance, brain-controlled prosthetic arms for soldiers who have lost limbs. Occasionally he also mentions some of his more distant aspirations. One of them is the ability, via computer, to transfer knowledge and thoughts from one person’s mind to another’s.III. “We Try to Find Ways to Say Yes”Medicine and biology were of minor interest to DARPA until the 1990s, when biological weapons became a threat to U.S. national security. The agency made a significant investment in biology in 1997, when DARPA created the Controlled Biological Systems program. The zoologist Alan S. Rudolph managed this sprawling effort to integrate the built world with the natural world. As he explained it to me, the aim was “to increase, if you will, the baud rate, or the cross-communication, between living and nonliving systems.” He spent his days working through questions such as “Could we unlock the signals in the brain associated with movement in order to allow you to control something outside your body, like a prosthetic leg or an arm, a robot, a smart home—or to send the signal to somebody else and have them receive it?”Human enhancement became an agency priority. “Soldiers having no physical, physiological, or cognitive limitation will be key to survival and operational dominance in the future,” predicted Michael Goldblatt, who had been the science and technology officer at McDonald’s before joining DARPA in 1999. To enlarge humanity’s capacity to “control evolution,” he assembled a portfolio of programs with names that sounded like they’d been taken from video games or sci-fi movies: Metabolic Dominance, Persistence in Combat, Continuous Assisted Performance, Augmented Cognition, Peak Soldier Performance, Brain-Machine Interface.The programs of this era, as described by Annie Jacobsen in her 2015 book, The Pentagon’s Brain, often shaded into mad-scientist territory. The Continuous Assisted Performance project attempted to create a “24/7 soldier” who could go without sleep for up to a week. (“My measure of success,” one DARPA official said of these programs, “is that the International Olympic Committee bans everything we do.”)Dick Cheney relished this kind of research. In the summer of 2001, an array of “super-soldier” programs was presented to the vice president. His enthusiasm contributed to the latitude that President George W. Bush’s administration gave DARPA—at a time when the agency’s foundation was shifting. Academic science gave way to tech-industry “innovation.” Tony Tether, who had spent his career working alternately for Big Tech, defense contractors, and the Pentagon, became DARPA’s director. After the 9/11 attacks, the agency announced plans for a surveillance program called Total Information Awareness, whose logo included an all-seeing eye emitting rays of light that scanned the globe. The pushback was intense, and Congress took DARPA to task for Orwellian overreach. The head of the program—Admiral John Poindexter, who had been tainted by scandal back in the Reagan years—later resigned, in 2003. The controversy also drew unwanted attention to DARPA’s research on super-soldiers and the melding of mind and machine. That research made people nervous, and Alan Rudolph, too, found himself on the way out.In this time of crisis, DARPA invited Geoff Ling, a neurology‑ICU physician and, at the time, an active-duty Army officer, to join the Defense Sciences Office. (Ling went on to work in the Biological Technologies Office when it spun out from Defense Sciences, in 2014.) When Ling was interviewed for his first job at DARPA, in 2002, he was preparing for deployment to Afghanistan and thinking about very specific combat needs. One was a “pharmacy on demand” that would eliminate the bulk of powdery fillers from drugs in pill or capsule form and instead would formulate active ingredients for ingestion via a lighter, more compact, dissolving substance—like Listerine breath strips. This eventually became a DARPA program. The agency’s brazen sense of possibility buoyed Ling, who recalls with pleasure how colleagues told him, “We try to find ways to say yes, not ways to say no.” With Rudolph gone, Ling picked up the torch.Ling talks fast. He has a tough-guy voice. The faster he talks, the tougher he sounds, and when I met him, his voice hit top speed as he described a first principle of Defense Sciences. He said he had learned this “particularly” from Alan Rudolph: “Your brain tells your hands what to do. Your hands basically are its tools, okay? And that was a revelation to me.” He continued, “We are tool users—that’s what humans are. A human wants to fly, he builds an airplane and flies. A human wants to have recorded history, and he creates a pen. Everything we do is because we use tools, right? And the ultimate tools are our hands and feet. Our hands allow us to work with the environment to do stuff, and our feet take us where our brain wants to go. The brain is the most important thing.”Ling connected this idea of the brain’s primacy with his own clinical experience of the battlefield. He asked himself, “How can I liberate mankind from the limitations of the body?” The program for which Ling became best known is called Revolutionizing Prosthetics. Since the Civil War, as Ling has said, the prosthetic arm given to most amputees has been barely more sophisticated than “a hook,” and not without risks: “Try taking care of your morning ablutions with that bad boy, and you’re going to need a proctologist every goddamn day.” With help from DARPA colleagues and academic and corporate researchers, Ling and his team built something that was once all but unimaginable: a brain-controlled prosthetic arm.No invention since the internet has been such a reliable source of good publicity for DARPA. Milestones in its development were hailed with wonder. In 2012, 60 Minutes showed a paralyzed woman named Jan Scheuermann feeding herself a bar of chocolate using a robotic arm that she manipulated by means of a brain implant.​Eddie GuyYet DARPA’s work to repair damaged bodies was merely a marker on a road to somewhere else. The agency has always had a larger mission, and in a 2015 presentation, one program manager—a Silicon Valley recruit—described that mission: to “free the mind from the limitations of even healthy bodies.” What the agency learns from healing makes way for enhancement. The mission is to make human beings something other than what we are, with powers beyond the ones we’re born with and beyond the ones we can organically attain.The internal workings of DARPA are complicated. The goals and values of its research shift and evolve in the manner of a strange, half-conscious shell game. The line between healing and enhancement blurs. And no one should lose sight of the fact that D is the first letter in DARPA’s name. A year and a half after the video of Jan Scheuermann feeding herself chocolate was shown on television, DARPA made another video of her, in which her brain-computer interface was connected to an F-35 flight simulator, and she was flying the airplane. DARPA later disclosed this at a conference called Future of War.Geoff Ling’s efforts have been carried on by Justin Sanchez. In 2016, Sanchez appeared at DARPA’s “Demo Day” with a man named Johnny Matheny, whom agency officials describe as the first “osseointegrated” upper-limb amputee—the first man with a prosthetic arm attached directly to bone. Matheny demonstrated what was, at the time, DARPA’s most advanced prosthetic arm. He told the attendees, “I can sit here and curl a 45-pound dumbbell all day long, till the battery runs dead.” The next day, Gizmodo ran this headline above its report from the event: “DARPA’s Mind-Controlled Arm Will Make You Wish You Were a Cyborg.”Since then, DARPA’s work in neurotechnology has avowedly widened in scope, to embrace “the broader aspects of life,” Sanchez told me, “beyond the person in the hospital who is using it to heal.” The logical progression of all this research is the creation of human beings who are ever more perfect, by certain technological standards. New and improved soldiers are necessary and desirable for DARPA, but they are just the window-display version of the life that lies ahead.IV. “Over the Horizon”Consider memory, Sanchez told me: “Everybody thinks about what it would be like to give memory a boost by 20, 30, 40 percent—pick your favorite number—and how that would be transformative.” He spoke of memory enhancement through neural interface as an alternative form of education. “School in its most fundamental form is a technology that we have developed as a society to help our brains to do more,” he said. “In a different way, neurotechnology uses other tools and techniques to help our brains be the best that they can be.” One technique was described in a 2013 paper, a study involving researchers at Wake Forest University, the University of Southern California, and the University of Kentucky. Researchers performed surgery on 11 rats. Into each rat’s brain, an electronic array—featuring 16 stainless-steel wires—was implanted. After the rats recovered from surgery, they were separated into two groups, and they spent a period of weeks getting educated, though one group was educated more than the other.The less educated group learned a simple task, involving how to procure a droplet of water. The more educated group learned a complex version of that same task—to procure the water, these rats had to persistently poke levers with their nose despite confounding delays in the delivery of the water droplet. When the more educated group of rats attained mastery of this task, the researchers exported the neural-firing patterns recorded in the rats’ brains—the memory of how to perform the complex task—to a computer.“What we did then was we took those signals and we gave it to an animal that was stupid,” Geoff Ling said at a DARPA event in 2015—meaning that researchers took the neural-firing patterns encoding the memory of how to perform the more complex task, recorded from the brains of the more educated rats, and transferred those patterns into the brains of the less educated rats—“and that stupid animal got it. They were able to execute that full thing.” Ling summarized: “For this rat, we reduced the learning period from eight weeks down to seconds.”“They could inject memory using the precise neural codes for certain skills,” Sanchez told me. He believes that the Wake Forest experiment amounts to a foundational step toward “memory prosthesis.” This is the stuff of The Matrix. Though many researchers question the findings—cautioning that, really, it can’t be this simple—Sanchez is confident: “If I know the neural codes in one individual, could I give that neural code to another person? I think you could.” Under Sanchez, DARPA has funded human experiments at Wake Forest, the University of Southern California, and the University of Pennsylvania, using similar mechanisms in analogous parts of the brain. These experiments did not transfer memory from one person to another, but instead gave individuals a memory “boost.” Implanted electrodes recorded neuronal activity associated with recognizing patterns (at Wake Forest and USC) and memorizing word lists (at Penn) in certain brain circuits. Then electrodes fed back those recordings of neuronal activity into the same circuits as a form of reinforcement. The result, in both cases, was significantly improved memory recall.Doug Weber, a neural engineer at the University of Pittsburgh who recently finished a four-year term as a DARPA program manager, working with Sanchez, is a memory-transfer skeptic. Born in Wisconsin, he has the demeanor of a sitcom dad: not too polished, not too rumpled. “I don’t believe in the infinite limits of technology evolution,” he told me. “I do believe there are going to be some technical challenges which are impossible to achieve.” For instance, when scientists put electrodes in the brain, those devices eventually fail—after a few months or a few years. The most intractable problem is blood leakage. When foreign material is put into the brain, Weber said, “you undergo this process of wounding, bleeding, healing, wounding, bleeding, healing, and whenever blood leaks into the brain compartment, the activity in the cells goes way down, so they become sick, essentially.” More effectively than any fortress, the brain rejects invasion.Even if the interface problems that limit us now didn’t exist, Weber went on to say, he still would not believe that neuroscientists could enable the memory-prosthesis scenario. Some people like to think about the brain as if it were a computer, Weber explained, “where information goes from A to B to C, like everything is very modular. And certainly there is clear modular organization in the brain. But it’s not nearly as sharp as it is in a computer. All information is everywhere all the time, right? It’s so widely distributed that achieving that level of integration with the brain is far out of reach right now.”Peripheral nerves, by contrast, conduct signals in a more modular fashion. The biggest, longest peripheral nerve is the vagus. It connects the brain with the heart, the lungs, the digestive tract, and more. Neuroscientists understand the brain’s relationship with the vagus nerve more clearly than they understand the intricacies of memory formation and recall among neurons within the brain. Weber believes that it may be possible to stimulate the vagus nerve in ways that enhance the process of learning—not by transferring experiential memories, but by sharpening the facility for certain skills.Will an enhanced human being—a human being possessing a neural interface with a computer—still be a human being?To test this hypothesis, Weber directed the creation of a new program in the Biological Technologies Office, called Targeted Neuroplasticity Training (TNT). Teams of researchers at seven universities are investigating whether vagal-nerve stimulation can enhance learning in three areas: marksmanship, surveillance and reconnaissance, and language. The team at Arizona State has an ethicist on staff whose job, according to Weber, “is to be looking over the horizon to anticipate potential challenges and conflicts that may arise” regarding the ethical dimensions of the program’s technology, “before we let the genie out of the bottle.” At a TNT kickoff meeting, the research teams spent 90 minutes discussing the ethical questions involved in their work—the start of a fraught conversation that will broaden to include many others, and last for a very long time.DARPA officials refer to the potential consequences of neurotechnology by invoking the acronym elsi, a term of art devised for the Human Genome Project. It stands for “ethical, legal, social implications.” The man who led the discussion on ethics among the research teams was Steven Hyman, a neuroscientist and neuroethicist at MIT and Harvard’s Broad Institute. Hyman is also a former head of the National Institute of Mental Health. When I spoke with him about his work on DARPA programs, he noted that one issue needing attention is “cross talk.” A man-machine interface that does not just “read” someone’s brain but also “writes into” someone’s brain would almost certainly create “cross talk between those circuits which we are targeting and the circuits which are engaged in what we might call social and moral emotions,” he said. It is impossible to predict the effects of such cross talk on “the conduct of war” (the example he gave), much less, of course, on ordinary life.Weber and a DARPA spokesperson related some of the questions the researchers asked in their ethics discussion: Who will decide how this technology gets used? Would a superior be able to force subordinates to use it? Will genetic tests be able to determine how responsive someone would be to targeted neuroplasticity training? Would such tests be voluntary or mandatory? Could the results of such tests lead to discrimination in school admissions or employment? What if the technology affects moral or emotional cognition—our ability to tell right from wrong or to control our own behavior?Recalling the ethics discussion, Weber told me, “The main thing I remember is that we ran out of time.”V. “You Can Weaponize Anything”In The Pentagon’s Brain, Annie Jacobsen suggested that DARPA’s neurotechnology research, including upper-limb prosthetics and the brain-machine interface, is not what it seems: “It is likely that DARPA’s primary goal in advancing prosthetics is to give robots, not men, better arms and hands.” Geoff Ling rejected the gist of her conclusion when I summarized it for him (he hadn’t read the book). He told me, “When we talk about stuff like this, and people are looking for nefarious things, I always say to them, ‘Do you honestly believe that the military that your grandfather served in, your uncle served in, has changed into being Nazis or the Russian army?’ Everything we did in the Revolutionizing Prosthetics program—everything we did—is published. If we were really building an autonomous-weapons system, why would we publish it in the open literature for our adversaries to read? We hid nothing. We hid not a thing. And you know what? That meant that we didn’t just do it for America. We did it for the world.”I started to say that publishing this research would not prevent its being misused. But the terms use and misuse overlook a bigger issue at the core of any meaningful neurotechnology-ethics discussion. Will an enhanced human being—a human being possessing a neural interface with a computer—still be human, as people have experienced humanity through all of time? Or will such a person be a different sort of creature?​Eddie GuyThe U.S. government has put limits on DARPA’s power to experiment with enhancing human capabilities. Ling says colleagues told him of a “directive”: “Congress was very specific,” he said. “They don’t want us to build a superperson.” This can’t be the announced goal, Congress seems to be saying, but if we get there by accident—well, that’s another story. Ling’s imagination remains at large. He told me, “If I gave you a third eye, and the eye can see in the ultraviolet, that would be incorporated into everything that you do. If I gave you a third ear that could hear at a very high frequency, like a bat or like a snake, then you would incorporate all those senses into your experience and you would use that to your advantage. If you can see at night, you’re better than the person who can’t see at night.”Enhancing the senses to gain superior advantage—this language suggests weaponry. Such capacities could certainly have military applications, Ling acknowledged—“You can weaponize anything, right?”—before he dismissed the idea and returned to the party line: “No, actually, this has to do with increasing a human’s capability” in a way that he compared to military training and civilian education, and justified in economic terms.“Let’s say I gave you a third arm,” and then a fourth arm—so, two additional hands, he said. “You would be more capable; you would do more things, right?” And if you could control four hands as seamlessly as you’re controlling your current two hands, he continued, “you would actually be doing double the amount of work that you would normally do. It’s as simple as that. You’re increasing your productivity to do whatever you want to do.” I started to picture his vision—working with four arms, four hands—and asked, “Where does it end?”“It won’t ever end,” Ling said. “I mean, it will constantly get better and better—” His cellphone rang. He took the call, then resumed where he had left off: “What DARPA does is we provide a fundamental tool so that other people can take those tools and do great things with them that we’re not even thinking about.”Judging by what he said next, however, the number of things that DARPA is thinking about far exceeds what it typically talks about in public. “If a brain can control a robot that looks like a hand,” Ling said, “why can’t it control a robot that looks like a snake? Why can’t that brain control a robot that looks like a big mass of Jell-O, able to get around corners and up and down and through things? I mean, somebody will find an application for that. They couldn’t do it now, because they can’t become that glob, right? But in my world, with their brain now having a direct interface with that glob, that glob is the embodiment of them. So now they’re basically the glob, and they can go do everything a glob can do.”VI. Gold RushDARPA’s developing capabilities still hover at or near a proof-of-concept stage. But that’s close enough to have drawn investment from some of the world’s richest corporations. In 1990, during the administration of President George H. W. Bush, DARPA Director Craig I. Fields lost his job because, according to contemporary news accounts, he intentionally fostered business development with some Silicon Valley companies, and White House officials deemed that inappropriate. Since the administration of the second President Bush, however, such sensitivities have faded.Over time, DARPA has become something of a farm team for Silicon Valley. Regina Dugan, who was appointed DARPA director by President Barack Obama, went on to head Google’s Advanced Technology and Projects group, and other former DARPA officials went to work for her there. She then led R&D for the analogous group at Facebook, called Building 8. (She has since left Facebook.)DARPA’s neurotechnology research has been affected in recent years by corporate poaching. Doug Weber told me that some DARPA researchers have been “scooped up” by companies including Verily, the life-sciences division of Alphabet (the parent company of Google), which, in partnership with the British pharmaceutical conglomerate GlaxoSmithKline, created a company called Galvani Bioelectronics, to bring neuro-modulation devices to market. Galvani calls its business “bioelectric medicine,” which conveys an aura of warmth and trustworthiness. Ted Berger, a University of Southern California biomedical engineer who collaborated with the Wake Forest researchers on their studies of memory transfer in rats, worked as the chief science officer at the neurotechnology company Kernel, which plans to build “advanced neural interfaces to treat disease and dysfunction, illuminate the mechanisms of intelligence, and extend cognition.” Elon Musk has courted DARPA researchers to join his company Neuralink, which is said to be developing an interface known as “neural lace.” Facebook’s Building 8 is working on a neural interface too. In 2017, Regina Dugan said that 60 engineers were at work on a system with the goal of allowing users to type 100 words a minute “directly from your brain.” Geoff Ling is on Building 8’s advisory board.Talking with Justin Sanchez, I speculated that if he realizes his ambitions, he could change daily life in even more fundamental and lasting ways than Facebook’s Mark Zuckerberg and Twitter’s Jack Dorsey have. Sanchez blushes easily, and he breaks eye contact when he is uncomfortable, but he did not look away when he heard his name mentioned in such company. Remembering a remark that he had once made about his hope for neurotechnology’s wide adoption, but with “appropriate checks to make sure that it’s done in the right way,” I asked him to talk about what the right way might look like. Did any member of Congress strike him as having good ideas about legal or regulatory structures that might shape an emerging neural-interface industry? He demurred (“DARPA’s mission isn’t to define or even direct those things”) and suggested that, in reality, market forces would do more to shape the evolution of neurotechnology than laws or regulations or deliberate policy choices. What will happen, he said, is that scientists at universities will sell their discoveries or create start-ups. The marketplace will take it from there: “As they develop their companies, and as they develop their products, they’re going to be subject to convincing people that whatever they’re developing makes sense, that it helps people to be a better version of themselves. And that process—that day-to-day development—will ultimately guide where these technologies go. I mean, I think that’s the frank reality of how it ultimately will unfold.”He seemed entirely untroubled by what may be the most troubling aspect of DARPA’s work: not that it discovers what it discovers, but that the world has, so far, always been ready to buy it.",The Pentagonâs Push to Program Soldiersâ Brains
373,3,3_cells_brain_blood_heart,https://www.euronews.com/next/2022/10/14/inside-the-us-facility-where-199-legally-dead-humans-and-almost-100-pets-await-being-reviv,"On the wall full of patients' portraits is Matheryn Naovaratpong from Thailand who lost her life at an early age.""[She is] by far our youngest patient, not quite three years old... who had brain cancer. Both her parents were doctors and she had multiple brain surgeries and nothing worked, unfortunately,"" said Max More, President emeritus of the cryonics facility Alcor Life Extension Foundation in Arizona, the United States.According to More, the facility currently accommodates 199 people and almost 100 pets inside tanks filled with liquid nitrogen in hopes to revive them when technology has been advanced enough to treat them.He says the patients are only ""legally dead"" but not biologically and believes cryonics could be their saviour.The process is a lot more complicated than just freezing and defrosting.After the patient has been declared legally dead, the body will be soaked in an ice bath.During this procedure, a mechanical CPR device is used to ensure blood circulation and medications in order to protect the cells against damage.At the end of the day, I think this notion of freezing ourselves into the future is pretty science fiction and it's naive Dr Arthur Caplan Professor of Bioethics, New York University Grossman School of MedicineMore says this prevents the patient from returning to consciousness and blood clots. He also adds maintaining blood pressure is crucial for viability, much as in the organ donation process.He says the bodies are not technically frozen, but vitrified.""We don't want to freeze the patient. We want to vitrify them... And the reason is that once you cool to very cold below freezing, the solution, instead of crystallising, will just get thicker and thicker and it's like a glassy block holding all the cells in place without any internal structure and so does no damage,"" said More.""And once we reach that point, around minus 110 degrees, the body becomes truly solid and absolutely nothing is happening in the body. There's no biochemical activity whatsoever, certainly no neurological activity. So at that point, it doesn't matter whether you wait a day or 100 years, you're going to be just the same as when you started"".'Pretty science fiction and naive'More’s wife, futurist and author Natasha Vita-More believes those who opted in for cryopreservation won’t be lonely once revived.""(They) will most likely have family members and or friends who have also signed up for cryonics… A person who had cancer or ALS or some other type of injury or disease is revived. The disease or injury cured or fixed, and the person is, has a new body cloned or a whole body prosthetic or their body reanimated and meet up with their friends again,"" she said.For Dr Arthur Caplan, Director of the Division of Medical Ethics and a professor of Bioethics at the New York University Grossman School of Medicine, the idea of cryonics is no more than “a college dormitory discussion”.""So, at the end of the day, I think this notion of freezing ourselves into the future is pretty science fiction and it's naive,"" he said.""It's almost like what you'd be thinking about in a college dormitory discussion, 'if I could just freeze myself and then defrost myself kind of like a bag of peas and wind up way in the future, wouldn't that be cool?' Sounds okay, but then you realise how much we are products of our own time,"" Caplan added.For more on this story, watch the video in the media player above.",Inside the US facility where 199 'legally dead' humans and almost 100 pets await being revived
509,3,3_cells_brain_blood_heart,https://www.thedailybeast.com/scientists-found-a-way-to-predict-your-death-by-how-you-walk?via=mobile&source=Reddit,"We’re all going to die eventually—but what if you knew when you’d be at risk for dropping dead, based solely on the way you walk? A new study shows that measurements taken with wrist-worn motion sensors can be used to predict one’s mortality risk up to five years later. As one of the largest validations of wearable technology to date, the research raises the possibility of one day using the motion detection system in smartphones to survey patient health without the need for in-person visits to the doctor’s office.The study, published Thursday in the journal PLOS Digital Health, was run using data from over 100,000 Britons from the massive UK Biobank project, which began collecting health and biometric information from participants in 2006 and will follow them for another 14 years. From a week of wrist sensor data, researchers at the University of Illinois at Urbana-Champaign designed a model that pares down a person’s acceleration and the distance they traveled into six-minute chunks. According to study author Bruce Schatz, a University of Illinois computer science researcher, the scientists chose this duration to mimic the six-minute walk test: a measurement of heart and lung function commonly taken during a medical appointment that tasks participants with walking at a normal pace for six minutes and compares their total distance traveled to benchmarks according to their age.The test is “a very good external measure of what's going on internally,” and could easily be replicated using the accelerometer present in a wrist sensor or a cheap phone, Schatz told The Daily Beast. “I know for a fact that these kinds of models will work with cheap phones.”Predictions of future death made by the researchers’ model were correct 76 percent of the time after one year, and 73 percent after five years—a similar rate of accuracy found in a study published last year that analyzed the same data set but used hours, rather than minutes, of data. This new study, argued Schatz, is a more promising demonstration of passive monitoring technology like phone and wrist sensors as his team’s model requires less data and affords a great degree of privacy to the user.“If you record all of the data, it's true that people have characteristic walks and you can tell who the individual is. But it's totally possible to take part of the signal, which is good enough to do the vitals but completely disguises who the person is,” he said.“ I know for a fact that these kinds of models will work with cheap phones. ” — Bruce Schatz, University of IllinoisEven so, using everyday technology to passively monitor patients could raise issues if users cannot give continuous informed consent, situations which could be complicated by degenerative illnesses or a lack of technological literacy. These ethical issues, said Schatz, are still speculative, but deserve coordinated thought from scientists as the research moves forward.While the sensors used in the study were near-identical to the ones in both simple cell phones and smartphones, future work should validate this model in a large sample when users carry phones in their pockets, rather than wear sensors on their wrists. Downloading an app that can measure your health as you go about your day-to-day could be a convenient and painless way to keep people healthier, longer.“If you want to raise the general health of the entire population, this kind of project is really important,” Schatz said.",Scientists Found a Way to Predict Your Death by How You Walk
504,3,3_cells_brain_blood_heart,https://www.technologyreview.com/2022/10/12/1061204/human-brain-cells-transplanted-baby-rats-brains/,"“It’s an important step forward in progress into [understanding and treating] brain diseases,” says Julian Savulescu, a bioethicist at the National University of Singapore, who was not involved in the study. But the development also raises ethical questions, he says, particularly surrounding what it means to “humanize” animals.Sergiu Pașca at the University of Stanford has been working for more than a decade with neural organoids—small clumps of neurons, grown in a dish, that resemble specific brain regions. These organoids are often created from human skin cells, which are first made into stem cells. The stem cells can then be encouraged to form neurons in the lab, under the right conditions. The resulting organoids can be used to study how brain cells fire and communicate—and how they malfunction in some disorders.But there’s only so much a clump of cells in the lab can tell you. When it comes down to it, these cells don’t really replicate what is happening in our brains—which is why Pașca and many others in the field avoid the commonly used term “mini-brains”. The organoid cells can’t form the same complex connections. They don’t fire in the same way, either. And they aren’t as big as the cells in our brains. “Even when we kept human neurons for hundreds of days … we noticed that human neurons don’t grow to the size to which a human neuron in a human brain would grow,” says Pașca.It is also impossible to tell how changes to neurons in the lab might lead to symptoms of a neuropsychiatric disorder. If cells in a dish show a change in their shape, the way they fire, or the proteins they make, what does that mean for a person’s memory or behavior, for example?To get around these issues, Pașca and his colleagues transplanted organoids into the brains of living rats—specifically, newborn rats. The brains of very young animals undergo extensive growth and rewiring as they develop. Neurons transplanted at such an early stage should have the best chance of being integrated with the rats’ own brain circuits, Pașca reasoned.Building brain organoidsThe team used organoids made from skin cells. These cells were made into stem cells in the lab before being encouraged to form layers of cells that resemble those in the human cortex, the folded outer part of the brain that contains regions responsible for thought, vision, hearing, memory, and sensing the environment, among other things. This process took around two months in the lab.The resulting three-dimensional organoids were then injected into the brains of days-old rats through an incision in the skull. The organoids were transplanted into the sensory cortex, a region that plays a role in helping animals sense their environment.Within four months, brain scans showed that the organoids had grown to around nine times their original volume—and made up around a third of one brain hemisphere. The cells appeared to have formed connections with rat brain cells and been incorporated into brain circuits.",Human brain cells transplanted into baby ratsâ brains grow and form connections
295,3,3_cells_brain_blood_heart,https://www.bbc.co.uk/news/science-environment-63195653,"""When people look at tissues in a dish, at the moment they are seeing if there is activity or no activity. But the purpose of brain cells is to process information in real time,"" he says. ""Tapping into their true function unlocks so many more research areas that can be explored in a comprehensive way.""",Lab-grown brain cells play video game Pong
208,3,3_cells_brain_blood_heart,https://techcrunch.com/2022/01/07/abbott-lingo/,"U.S. medical device maker Abbott is moving into making general-purpose consumer biosensing wearables.The company has been making continuous glucose monitor (CGM) hardware for diabetes management for years (since 2014) — but in a healthtech keynote at CES yesterday, Abbott’s chairman and CEO, Robert B. Ford, announced it’s developing a new line of consumer biowearables — called Lingo — intended for more general fitness and wellness purposes.“Technology gives us the power to digitize, decentralize and democratize healthcare, create a shared language between you and your doctor — and put more control of your health in your hands,” he said during the keynote. “We’re creating a future that will bring you and your loved ones care that’s more personal and precise. It’s happening right now. And its potential is no less than incredible.”Ford said the Lingo sensing technology is being designed to track “key signals” in the body — such as glucose, ketones and lactate — adding that it could also be used to track alcohol levels in the future.Last year the company launched a biosensor designed for athletes, called the Libre Sense Glucose Sport Biowearable iii — which was made available in Europe, and has been used by the likes of marathon world-record holder Eliud Kipchoge to support their training needs.Abbott said its goal with Lingo is to expand glucose monitoring to people looking to manage their weight, sleep better, improve energy and think clearer.To support this expanded utility it said it’s developing the biosensor to measure other biomarkers than glucose.“A ketone biowearable is being developed to track ketones continuously, see how fast you are getting into ketosis, and understand exactly what keeps you there by providing insights on dieting and weight loss,” the company noted in a press release. “A lactate biowearable is in development to track continuous lactate build-up during exercise, which can be used as an indicator of athletic performance.”In recent years a number of startups in the U.S., Europe and Asia have been seeking to productize CGM hardware — including existing sensors made by Abbott — for a variety of non-medical purposes, launching real-time blood glucose tracking services targeted at fitness enthusiasts, peopler wanting to lose weight or generally health conscious consumers.Abbott jumping into the space itself so quickly suggests it sees significant potential for biosensing consumer wearables to go mainstream.For a deep dive on what it’s like living with a CGM biosensor attached to your arm — and the constantly updating window into biological process that it provides — check out TechCrunch’s review of Ultrahuman’s Cyborg service, an Indian-based startup that’s repurposing current-gen sensing hardware made by Abbott.Update: An Abbott spokeswoman confirmed the first biowearable to launch will be Lingo Keto — starting in Europe “later this year”.",Abbott tells CES itâs getting into consumer biowearables
238,3,3_cells_brain_blood_heart,https://techcrunch.com/2022/08/10/mawi-patch/,"If you’ve ever had the misfortune of needing continuous EKG monitoring, you’ve probably used a Holter monitor. It’s like carrying a 1980s walkman made of metal with a bunch of wires going from it to your chest. If that sounds uncomfortable, and as if you won’t sleep or enjoy showers much for the two weeks you need to carry it around, you’ve neatly stumbled across the use case for the Mawi Heart Patch. The company just released its product, a two-lead cardiac monitor that can be read in real time.There are consumer-grade products that can do EKG readings, including the Withings ScanWatch (and its fancier-looking sibling, the ScanWatch Horizon), and there are other patches on the market, such as the Zio patch, but Mawi claims to have done something unique, and suggests that its Heart Patch is the first ever single-use, two-lead cardiac monitor to reach the market.The company describes it as “a stick-and-go, wireless solution” and further suggests that the disposable nature of the device is a benefit; it means that cardiologists can run tests on as many patients as they need to without having to wait for reusable Holter monitors to come back from other patients and get sanitized and maintained between uses.“Holter monitors aren’t great,” Andrew Klymenko, the CEO of Mawi, says drily in an interview we did last week, and explains that the existing solutions are prone to dislodging, peeling and causing allergic reactions, thus restricting monitoring time. As a result, Mawi claims that more than 50% of arrhythmias go undetected. Equally bad: patients have to wait up to a month to receive the results.Mawi Heart Patch, the company claims, can be applied in under a minute, and you can live like normal as you wear it.“Patients can shower, sleep, work out,” says Klymenko, and highlights that it’s possible to wear the patch and live all aspects of life as normal. “Sex is a big and important part of life, and patients can have sex as normal when wearing the Mawi patch.”“Cardiovascular diseases pose the greatest risk to our long-term health and are the leading cause of death globally. With a lack of or ineffective monitoring often proving critical, many of these deaths can be avoided with the right preventative measures,” Klymenko said. “Too often, sufferers do not realize the severity of their symptoms before it’s too late. Many patients that use the Mawi Heart Patch look healthy, exercise daily and show no signs of disease, yet have a potentially lethal heart condition. We’re on a mission to prevent the heart’s ‘silent killers,’ and we’re already working with like-minded clinics that are seeing amazing results.”The patch connects to a smartphone device in the doctor’s office. That device pipes the data through to the cloud, where an AI analyzes the results and highlights anything unusual for the cardiologists to take a closer look at. The process is very quick indeed, meaning that patients can have feedback and next steps for their treatment.“In less than 24 hours [the doctors] have a very detailed, precise and actionable report,” says Klymenko, suggesting that doctors can focus on treatment, rather than having to spend a lot of time analyzing data. “It takes just two seconds to manage.”The company currently has around 30 staff, primarily concentrated in Europe. Klymenko himself is from Ukraine, and his team is spread across the world, including teams in Thailand and in the U.S. To date, the company has been bootstrapped.Mawi won’t share exactly how many devices it has in the field, but Klymenko admits they are shipping “thousands of devices” every month, to customers across the U.S., the EU, and the Middle East.The devices need to be prescribed by a doctor, and pricing is heavily dependent on whatever medical insurance and what medical care system you are operating on, but Klymenko says that the devices typically cost “under $250 per study.”",Mawi launches a patch to track your heart health faster and in real time
209,3,3_cells_brain_blood_heart,https://techcrunch.com/2022/02/09/scopio-aims-to-turn-hematology-into-remote-work-with-50m-c-round/,"Today, if a hematologist wanted to dive into the exact organization and structure of your blood cells, they’d probably need a microscope in a lab. Scopio, an Israel-based startup that just closed a $50 million Series C round, argues that soon, a lot of that work could be done with nothing more than a laptop.Founded in 2015, Scopio is an imaging company looking to re-imagine a common blood test called a peripheral blood smear. In essence, that’s a test where a doctor, usually looking to understand an anomaly in a blood cell count, literally takes a look at your blood cells. That process involves placing a smear of blood onto a slide, and examining the shape, size and structure of certain cells using a well-trained eye.Scopio has developed a scanner, called Scopio100x, capable of imaging that whole blood sample, while maintaining the ability to achieve 100x magnification. The result is a zoomable, digital image that CEO and founder Itai Hayut argues will allow peripheral blood smears to be done remotely, and bring down the costs of these procedures in the first place. Once samples are scanned in the lab, they could be reviewed by hematologists working from anywhere.You can zoom around in one of the images here.“We’ve seen lines of people in hematology labs leaning over microscopes, in some cases, using a manual clicker to count cells,” Hayut told TechCrunch. “We thought this is just a perfect example of how computer vision tools can assist the experts, and get better results much quicker.”Peripheral smear blood tests are part of a battery of different assessments that can be used to identify blood diseases. But these days, they’re not the first-line option, in most cases.If your doctor is concerned you might have a blood-based disease they might first order a complete blood count. Those tests are done almost entirely autonomously: an analyzer will count out different levels of blood cells types in your body, and give the doctor a rough idea of how much of each type of cell is present.If those tests present anomalies, a doctor might want to see the samples for themselves. In that case, they’ll perform a peripheral blood test to examine cell size, structure and look for indicators for a specific disease.There has been evidence that the peripheral blood smear landscape has some big pain points. For instance, some papers argue, the manual review of samples doesn’t often add much to doctors’ diagnostic dataset. One paper published in 2020 in Diagnostic Pathology, for example, found that just 23% of 515 peripheral blood smears ordered across three medical centers added clinical value.That paper doesn’t necessarily mean the technique itself is extraneous; the authors do add that efforts to make the process more efficient are probably warranted.Plenty of other papers confirm that the peripheral blood smear isn’t likely to fall by the wayside. Some 15% of analyzer tests are still referred to hematologists for a blood smear to confirm findings. And, on a less scientific note, some researchers see the ability to divine diagnoses from blood cells as something of an art.In short, it seems like we’re still going to have to manually look at blood cells to confirm diagnoses for the time being. And other companies have already looked to develop imaging tools that can make that manual review faster, and more automated. For example, two major companies in this space already are Cellavision and Sysmex, systems which are responsible for most of the peer-reviewed studies in this space, per a 2019 review paper.In some ways, the fact that this tech already exists works in Scopio’s favor, because research already suggests that cell-imaging systems have benefits to offer, and scientists are already familiar with them. Namely, these systems allow for remote review (though about 10-20% of samples still need in-person confirmation), reduce eyestrain, can lower labor costs, make it easy to archive and retrieve blood films and make good teaching tools.Hayut argues that Scopio can represent the next generation of this technology because the company appears to have found a niche within that world: The existing imaging tech doesn’t capture the entire slide.To attain greater magnification, the breadth of the image is compromised. Think about what it’s like to zoom in on something using a camera lens: the field of view gets smaller. In this case, that means only portions of the slide are scanned and displayed, said Hayut. Independently published papers have confirmed this idea.“Scopio is the first company in the world that managed to break the trade-off between field of view and resolution,” Hayut said. In short, they’ve managed to capture the whole slide and still achieve up to 100x magnification, in line with what’s needed to perform a peripheral blood smear.The ability to scan the entire slide leads to the second half of Scopio’s pitch: that “two orders of magnitude” more visualized and digitized cells means that more applications can be developed around them. In essence, Hayut said this will allow new algorithms to learn more from the peripheral blood smear in the first place.Scopio, like several of the other major players in this space, has been investing in getting certain clinical decision-making algorithms FDA approved. (Think of this like support software that helps a hematologist distinguish between cell types.)In October 2020, the FDA approved a clinical support system from Scopio that classifies cells, and allows a hematologist to review those automatic classifications, through the 510(k) new device pathway. (It was granted approval through this pathway because the classification process was substantially similar to an equivalent already on the market.)The company has also recently completed a clinical study on an application intended to assist technicians with review of bone marrow aspirate — samples usually used to scan for conditions like leukemia, multiple myeloma or anemia. The data on that study has yet to be published, but the company plans to file for FDA approval of another clinical decision-support system targeted at bone marrow aspirate analysis in March.As for the Series C funds, the company has a singular focus: commercial delivery. The company plans to expand the commercial team in Europe and the U.S., and attempt to make inroads into clinical labs (Scopio’s core client).This Series C round brings Scopio’s total funding to $85 million. The round was led by OurCrowd, and an unnamed strategic investor. It includes new investors Mizrahi-Tefahot Bank Invest, Ilex Medical and existing investors Olive Tree Ventures and Aurum Ventures.",Scopio aims to turn hematology into remote work with $50M C round
270,3,3_cells_brain_blood_heart,https://techcrunch.com/2022/10/19/labby-product-launch/,"For most dairy farmers, milk flowing from their cows is tested by a traveling technician once per month. But in a world where bovine mastitis can appear from one day to the next, it is udderly ridiculous to test milk flowing from cows once per month. Today at TechCrunch Startup Battlefield, Labby offered a different solution, with an inline optical sensor that can test cows every time they are milked. For now, the product detects potential issues early, but over time, the company believes it can start predicting issues before they occur.The company’s product is called MilKey and comes in two variants: a hand-held product that can be used anywhere, or an inline product that can be hooked into the milking machines, which enables daily farmers to test continuously.The main difference between the two products is also their strengths. The handheld device can be used by any technician out in the (literal) field; you select the cow you’re testing on a smartphone app, and the test results show up with the right animal. That’s great when a cow is wandering about or if you have suspicions about a particular animal having an illness. The inline device is fully automatic and works over Wi-Fi. For this device, the results need to be assigned to the right cow manually, but it makes it feasible to test every cow, every milking.Labby tells TechCrunch that the device takes spectral measurements of milk samples and uploads them to the cloud. From there, the company uses machine-learning models to take spectral readings as inputs. It can estimate the content of the milk, broken down into fats, proteins and somatic cell counts. Once the measurements are taken and assigned to an animal, the farmers can use an app or any web browser to see the full testing history of any animal, to ensure they are going a-bovine and beyond in terms of milk production.“Animal health records are like human records; they give critical indications about animal health and feed efficiency. It turns out that milk is the best biomarker for everything. Currently, the industry only tests once a month for each animal. We think this is a systemic failure for the farmers and for the animals,” says Julia Somerdin, CEO and founder of Labby, in an interview with TechCrunch. “One complication for animal health is mastitis. It one of the most common yet expensive diseases, and it can change from day to day. So when they do 30-day testing, the test will tell you everything is fine, but the next day the animal could develop a case, which can be subclinical with no symptoms. So for farmers, between testing days, they have no idea how the animal is doing.”You may be wondering “who cares,” but dairy farming is a hell of an industry. There are 9 million cows across 40,000 farms in the U.S. Worldwide, there are 250 million cows across 115 million farms; it all adds up.“With our solution, we can provide on-farm real-time testing to help provide the farmer with daily, weekly and monthly health records,” says Somerdin. “Animal health is the critical indicator that’s missing from today’s industry practices.”From the numbers and the impact, you’ll be unsurprised that there are big sums of money involved. The best milk gets farmers the best price, which means that milk quality is directly linked to revenue, the Labby team tells me. The benefit is two-fold: Healthier cows need less veterinary attention, and higher-quality milk nets the milk producers more money per gallon of milk delivered.“We can insert Labby in the value chain. Dairy is a very input-intensive industry so we have all kinds of suppliers that help farmers produce more and better milk, and then the dairy farmers sell their milk to dairy processors. With our service, the big battle, besides the money-saving aspect, is we create all this real-time data,” says Somerdin. “Animal genetics companies can use that data, helping them refine their algorithms. We can also bridge the gap between dairy producers and veterinarians, enabling telehealth for cows.”Apart from the fact that when I hear “telehealth for cows,” I giggle at the thought of a cow staring into a Zoom screen and talking about its feelings and its four upset stomachs, it’s easy to understand how Labby adds significant value and the ability to be an early warning system for animal health.“The most important thing is that you don’t need a technician to sample the milk anymore. The cleaning can also be integrated with the current system,” says Somerdin, explaining how the company has designed a set-it-and-forget-it approach to continuous testing.Labby was part of Techstars, and raised a total of $1.3 million from them and a number of other investors, including MIT Media lab’s E14 fund.The company officially started selling its products in early October, and has only just started shipping its products to customers. In the short term, it’s a hardware+SaaS business, but after that, it’s time to start milking the data itself for wisdom.“Our business model has three revenue streams. For the dairy farmers, they pay once for the hardware equipment, then monthly for us to provide the testing in the cloud. The farmer pays per cow per day,” says Somerdin. “In addition, we’re looking at data. We believe we are generating significant value for the industry, such as for genetic companies. We will have a data licensing fee, but we will wait to offer that until we have half a million cows on the platform.”Over time, the company hopes to be able to use big data to catch a glimpse of the future, too.“The data will help us develop a reliable benchmark for each animal,” says Somerdin, and suggests that deviations from the benchmark could tell you something about what’s going on for the cows, health-wise. “Based on that, we can look at pattern recognition for disease onset among the herd. We could also predict patterns for milk production, which currently relies only on historical data, which limits their accuracy.”All in all, the company seems eager to (milk)shake up the industry, and bring all the farmers to the yard. And they’re like, it’s better than yours. They will teach you, but they’ll have to charge.",Labby wants to make milk healthier and cows happier with better sensors
207,3,3_cells_brain_blood_heart,https://techcrunch.com/2022/01/06/scanbo-non-invasive-blood-glucose/,"If you have diabetes, or ever suspected that you might, you will have done the poke-your-finger-and-drop-blood-on-a-stick thing until your finger goes numb. Finger-prick blood glucose monitoring is the de facto standard, but AI company Scanbo wants to put an end to all that, replacing the droplet with some off-the-shelf diagnostic tools and a big helping of data analysis.The company has developed a prototype device that combines a three-lead ECG measurement and a Photoplethysmogram (PPG). The 60-second measurement is fed to a set of algorithms that can give you some very promising measurements. For starters, the device is doing non-invasive blood glucose monitoring, but the company’s founder claims that it can also do blood pressure measurements.I spoke to the company’s founder and CEO, Ashissh Raichura, as part of TechCrunch’s virtual CES coverage, where he told me a bit more about the technology. He also gave me a demonstration, first testing his own blood with a conventional, off-the-shelf fingerprick blood glucose monitor, and then with the company’s own prototype. The measurements were 6.2 and 6.3 mmol/L, respectively, which puts the two devices within a couple of percent accuracy of each other.“We use the three electrodes for ECG data, and an additional measurement for PPG. We measure for 60 seconds, and then take the raw data and analyze that using the machine learning convolutional neural network, and the deep neural network. We combine all of that data, take the three machine learning algorithms, see what comes as a result, and then analyze the glucose,” Raichura told me, as he was preparing to demo the device for me. “We want to commercialize our product, and will be looking looking for US FDA in Health Canada approval.”I was surprised to learn that it is possible to do non-invasive blood glucose measuring — most of the so-called non-invasive methods do use an implant or a filament sensor wire to get readings, but the method that Scanbo uses has been studied and covered in medical journals. It doesn’t look like the FDA has approved any products that take this approach yet, so the company is certainly facing a lengthy medical approval process in order to bring its products to market.The company also claims it is able to do blood pressure measurements, the kind that you might do at home or in the doctor’s office with a blood pressure cuff.“When we take your EKG data, we convert that into what is called a short wave transmission length,” Raichura summarizes in how the company is able to pull off blood pressure metrics. “Based on that, we calculate your non-invasive cuff-less blood pressure too. That is another piece of algorithm that we have patent pending.”With all of this tech in its back pocket, the company has an interesting choice to make; is it going to start manufacturing its own hardware devices, or is there an opportunity to license the algorithms and technology to other manufacturers that already have devices with PPG and ECG capabilities on the market.“We have two patents pending, purely on the hardware, how we designed it, how we amalgamated electrodes and all the sensors in a way where we can take all the parameters in one time,” explains Raichura, hinting at how the company is trying to measure all the things at once. “If you look at the traditionally available all the devices, you measure one thing at a time, not everything at once. You do blood pressure one time, your EKG at another time. One after another, all in sequential processes. In our case, we are asking you to put four fingers on the device, so we can capture all the data, and use algorithms to report different aspects of your health.”Scanbo sees its technology as an at-home alternative to some of the currently existing clinical techniques and technology.“As a company, we are a combination of AI and med-tech,” Raichura says, and mentions that the market is starting to take notice. “With this product, we are just getting started now. Medtronic, Samsung, LG, and others are already talking to us and see can we collaborate with them, and we are exploring a couple of strategic partnerships that could help us in taking us to different markets globally. We see the needs from 400 million people in the world with type two diabetes, and most of the populations can’t afford glucometers — never mind continuous glucose meters. We are very, very cost-effective, and we can bring the cost down to as low as $20 a month. There is no biowaste, you don’t need any disposables, you don’t need strips or anything, just purely the machine learning algorithm and a device with a chargeable battery.”The company is about to take its prototype and clinical results as leverage to go raise a seed round and kick off the process toward regulatory approval and, eventually, make a product available to the public.",How sweet is your blood? Scanbo gives an answer without poking holes in you
113,3,3_cells_brain_blood_heart,https://qbi.uq.edu.au/article/2022/10/new-window-brain-computational-function,"The function of the human brain is exceptional, driving all aspects of our thoughts and creativity. Yet the part of the human brain – the neocortex – responsible for such cognitive functions has a similar overall structure to other mammals.Through close collaboration between The University of Queensland (UQ), The Mater Hospital and the Royal Brisbane and Women’s Hospital, researchers have discovered the human brain’s enhanced processing power may stem from differences in the structure and function of our neurons.The results of this study have been published in Cell Reports as “High-fidelity dendritic sodium spike generation in human layer 2/3 neocortical pyramidal neurons.”Professor Stephen Williams at UQ’s Queensland Brain Institute (QBI) explained that his research team had studied the electrical properties of human neocortical pyramidal neurons embedded in their neuronal networks.Professor Stephen Williams“To study human neurons, we prepared live tissue slices from small blocks of the human neocortex collected from patients who were undergoing neurosurgery for the alleviation of refractory epilepsy or the removal of brain tumours at the two hospitals,” Professor Williams said.“We compared the electrical properties of human and rodent neocortical pyramidal neurons by making intricate simultaneous electrical recordings from their cell bodies and thin dendrites.“Our research revealed that human and rodent neocortical pyramidal neurons share fundamental biophysical properties.“For example, we showed that both the dendrites of human and rodent neocortical pyramidal neurons generate dendritic sodium spikes, suggesting a conservation of the machinery for integrating the many thousands of input signals that a neuron receives.“However, we discovered the computational function of human neocortical pyramidal neurons is dramatically enhanced.”QBI postdoctoral fellow Dr Helen Gooch and co-author on the paper, said the team found that the architecture of the dendritic trees of human neocortical pyramidal neurons – the branch-like extensions that conduct electrical signals – were larger and more complex than other mammals like rodents.“This elaboration of the dendritic tree in humans was accompanied by the generation of dendritic spikes at multiple sites, which actively spread through the neuron to drive the output signals of each neuron,” Dr Gooch said.“We suggest that this enhancement of distributed dendritic information processing may therefore be one factor that increases our brain’s overall processing power.”The translation of such discoveries paves the way for a better understanding of how the electrical activity of the human brain is disturbed in disease.Mater Hospital Neurologist and co-author, Dr Lisa Gillinder said “As clinician researchers, we are not only excited to learn more about the normal function of human brain cells, but through future research in this field we also aim to better understand the functional changes that occur in conditions like epilepsy with the hopes of improving treatments.”The research was first published in Cell Reports (DOI: https://doi.org/10.1016/j.celrep.2022.111500)Media: QBI Communications, communications@qbi.uq.edu.au, Elaine Pye +61 (0)415 222 606, Merrett Pye +61 (0)422 096 049.",New window into brainâs computational function
334,3,3_cells_brain_blood_heart,https://www.cnn.com/2022/06/01/health/ghost-heart-life-itself-wellness/index.html,"CNN —The first time molecular biologist Doris Taylor saw heart stem cells beat in unison in a petri dish, she was spellbound.“It actually changed my life,” said Taylor, who directed regenerative medicine research at Texas Heart Institute in Houston until 2020. “I said to myself, ‘Oh my gosh, that’s life.’ I wanted to figure out the how and why, and re-create that to save lives.”That goal has become reality. On Wednesday at the Life Itself conference, a health and wellness event presented in partnership with CNN, Taylor showed the audience the scaffolding of a pig’s heart infused with human stem cells – creating a viable, beating human heart the body will not reject. Why? Because it’s made from that person’s own tissues.“Now we can truly imagine building a personalized human heart, taking heart transplants from an emergency procedure where you’re so sick, to a planned procedure,” Taylor told the audience.“That reduces your risk by eliminating the need for (antirejection) drugs, by using your own cells to build that heart it reduces the cost … and you aren’t in the hospital as often so it improves your quality of life,” she said.Debuting on stage with her was BAB, a robot Taylor painstakingly taught to inject stem cells into the chambers of ghost hearts inside a sterile environment. As the audience at Life Itself watched BAB functioning in a sterile environment, Taylor showed videos of the pearly white mass called a “ghost heart” begin to pinken.Video Ad Feedback Can we grow a personalized human heart? 24:16 - Source: CNN“It’s the first shot at truly curing the number one killer of men, women and children worldwide – heart disease. And then I want to make it available to everyone,” said Taylor to audience applause.“She never gave up,” said Michael Golway, lead inventor of BAB and president and CEO of Advanced Solutions, which designs and creates platforms for building human tissues.“At any point, Dr. Taylor could have easily said ‘I’m done, this just isn’t going to work. But she persisted for years, fighting setbacks to find the right type of cells in the right quantities and right conditions to enable those cells to be happy and grow.”Giving birth to a heartTaylor’s fascination with growing hearts began in 1998, when she was part of a team at Duke University that injected cells into a rabbit’s failed heart, creating new heart muscle. As trials began in humans, however, the process was hit or miss.“We were putting cells into damaged or scarred regions of the heart and hoping that would overcome the existing damage,” she told CNN. “I started thinking: What if we could get rid of that bad environment and rebuild the house?”Taylor’s first success came in 2008 when she and a team at the University of Minnesota washed the cells out of a rat’s heart and began to work with the translucent skeleton left behind.Soon, she graduated to using pig’s hearts, due to their anatomical similarity to human hearts.“We took a pig’s heart, and we washed out all the cells with a gentle baby shampoo,” she said. “What was left was an extracellular matrix, a transparent framework we called the ‘ghost heart.’“Then we infused blood vessel cells and let them grow on the matrix for a couple of weeks,” Taylor said. “That built a way to feed the cells we were going to add because we’d reestablished the blood vessels to the heart.”The next step was to begin injecting the immature stem cells into the different regions of the scaffold, “and then we had to teach the cells how to grow up.”“We must electrically stimulate them, like a pacemaker, but very gently at first, until they get stronger and stronger. First, cells in one spot will twitch, then cells in another spot twitch, but they aren’t together,” Taylor said. “Over time they start connecting to each other in the matrix and by about a month, they start beating together as a heart. And let me tell you, it’s a ‘wow’ moment!”This ""ghost heart,"" created by using the scaffolding of a pig's heart and injected it with human stem cells, may soon be ready for human clinical trials. Advanced Solutions Life SciencesBut that’s not the end of the “mothering” Taylor and her team had to do. Now she must nurture the emerging heart by giving it a blood pressure and teaching it to pump.“We fill the heart chambers with artificial blood and let the heart cells squeeze against it. But we must help them with electrical pumps, or they will die,” she explained.The cells are also fed oxygen from artificial lungs. In the early days all of these steps had to be monitored and coordinated by hand 24 hours a day, 7 days a week, Taylor said.“The heart has to eat every day, and until we built the pieces that made it possible to electronically monitor the hearts someone had to do it person – and it didn’t matter if it was Christmas or New Year’s Day or your birthday,” she said. “It’s taken extraordinary groups of people who have worked with me over the years to make this happen.”But once Taylor and her team saw the results of their parenting, any sacrifices they made became insignificant, “because then the beauty happens, the magic,” she said.“We’ve injected the same type of cells everywhere in the heart, so they all started off alike,” Taylor said. “But now when we look in the left ventricle, we find left ventricle heart cells. If we look in the atrium, they look like atrial heart cells, and if we look in the right ventricle, they are right ventricle heart cells,” she said.“So over time they’ve developed based on where they find themselves and grown up to work together and become a heart. Nature is amazing, isn’t she?”Billions and billions of stem cellsAs her creation came to life, Taylor began to dream about a day when her prototypical hearts could be mass produced for the thousands of people on transplant lists, many of whom die while waiting. But how do you scale a heart?“I realized that for every gram of heart tissue we built, we needed a billion heart cells,” Taylor said. “That meant for an adult-sized human heart we would need up to 400 billion individual cells. Now, most labs work with a million or so cells, and heart cells don’t divide, which left us with the dilemma: Where will these cells come from?”The answer arrived when Japanese biomedical researcher Dr. Shinya Yamanaka discovered human adult skin cells could be reprogrammed to behave like embryonic or “pluripotent” stem cells, capable of developing into any cell in the body. The 2007 discovery won the scientist a Nobel Prize, and his “induced pluripotent stem cells (iPS),” soon became known as “Yamanaka factors.”“Now for the first time we could take blood, bone marrow or skin from a person and grow cells from that individual that could turn into heart cells,” Taylor said. “But the scale was still huge: We needed tens of billions of cells. It took us another 10 years to develop the techniques to do that.”The solution? A bee-like honeycomb of fiber, with thousands of microscopic holes where the cells could attach and be nourished.“The fiber soaks up the nutrients just like a coffee filter, the cells have access to food all around them and that lets them grow in much larger numbers. We can go from about 50 million cells to a billion cells in a week,” Taylor said. “But we need 40 billion or 50 billion or 100 billion, so part of our science over the last few years has been scaling up the number of cells we can grow.”Another issue: Each heart needed a pristine environment free of contaminants for each step of the process. Every time an intervention had to be done, she and her team ran the risk of opening the heart up to infection – and death.“Do you know how long it takes to inject 350 billion cells by hand?” Taylor asked the Life Itself audience. “What if you touch something? You just contaminated the whole heart.”Once her lab suffered an electrical malfunction and all of the hearts died. Taylor and her team were nearly inconsolable.“When something happens to one of these hearts, it’s devastating to all of us,” Taylor said. “And this is going to sound weird coming from a scientist, but I had to learn to bolster my own heart emotionally, mentally, spiritually and physically to get through this process.”Dr. Doris Taylor (left) is teaching BAB the robot how to properly inject stem cells into a ghost heart. Advanced Solutions Life SciencesEnter BAB, short for BioAssemblyBot, and an “uber-sterile” cradle created by Advance Solutions that could hold the heart and transport it between each step of the process while preserving a germ-free environment. Taylor has now taught BAB the specific process of injecting the cells she has painstakingly developed over the last decade.“When Dr. Taylor is injecting cells, it has taken her years to figure out where to inject, how much pressure to put on the syringe, and the best speed and pace to add the cells,” said BAB’s creator Golway.“A robot can do that quickly and precisely. And as we know, no two hearts are the same, so BAB can use ultrasound to see inside the vascular pathway of that specific heart, where Dr. Taylor is working blind, so to speak,” Golway added. “It’s exhilarating to watch – there are times where the hair on the back of my neck literally stands up.”Taylor left academia in 2020 and is currently working with private investors to bring her creation to the masses. If transplants into humans in upcoming clinical trials are successful, Taylor’s personalized hybrid hearts could be used to save thousands of lives around the world.In the US alone, some 3,500 people were on the heart transplant waiting list in 2021.“That’s not counting the people who never make it on the list, due to their age or heath,” Taylor said. “If you’re a small woman, if you’re an underrepresented minority, if you’re a child, the chances of getting an organ that matches your body are low.If you do get a heart, many people get sick or otherwise lose their new heart within a decade. We can reduce cost, we can increase access, and we can decrease side effects. It’s a win-win-win.”Taylor can even envision a day when people bank their own stem cells at a young age, taking them out of storage when needed to grow a heart – and one day even a lung, liver or kidney.“Say they have heart disease in their family,” she said. “We can plan ahead: Grow their cells to the numbers we need and freeze them, then when they are diagnosed with heart failure pull a scaffold off the shelf and build the heart within two months.“I’m just humbled and privileged to do this work, and proud of where we are,” she added. “The technology is ready. I hope everyone is going to be along with us for the ride because this is game-changing.”","âGhost heartâ: Built from the scaffolding of a pig and the patientâs cells, this cardiac breakthrough may soon be ready for transplant into humans"
296,3,3_cells_brain_blood_heart,https://www.bbc.com/news/science-environment-63195653,"""When people look at tissues in a dish, at the moment they are seeing if there is activity or no activity. But the purpose of brain cells is to process information in real time,"" he says. ""Tapping into their true function unlocks so many more research areas that can be explored in a comprehensive way.""",Lab-grown brain cells play video game Pong
106,3,3_cells_brain_blood_heart,https://phys.org/news/2022-10-brains-quantum.html,"Credit: CC0 Public DomainScientists from Trinity College Dublin believe our brains could use quantum computation. Their discovery comes after they adapted an idea developed to prove the existence of quantum gravity to explore the human brain and its workings.The brain functions measured were also correlated to short-term memory performance and conscious awareness, suggesting quantum processes are also part of cognitive and conscious brain functions.If the team's results can be confirmed—likely requiring advanced multidisciplinary approaches—they would enhance our general understanding of how the brain works and potentially how it can be maintained or even healed. They may also help find innovative technologies and build even more advanced quantum computers.Dr. Christian Kerskens, lead physicist at the Trinity College Institute of Neuroscience (TCIN), is the co-author of the research article that has just been published in the Journal of Physics Communications. He said, ""We adapted an idea, developed for experiments to prove the existence of quantum gravity, whereby you take known quantum systems, which interact with an unknown system. If the known systems entangle, then the unknown must be a quantum system, too. It circumvents the difficulties to find measuring devices for something we know nothing about.""For our experiments we used proton spins of 'brain water' as the known system. 'Brain water' builds up naturally as fluid in our brains and the proton spins can be measured using MRI (Magnetic Resonance Imaging). Then, by using a specific MRI design to seek entangled spins, we found MRI signals that resemble heartbeat evoked potentials, a form of EEG signals. EEGs measure electrical brain currents, which some people may recognize from personal experience or simply from watching hospital dramas on TV.""Electrophysiological potentials like the heartbeat evoked potentials are normally not detectable with MRI and the scientists believe they could only observe them because the nuclear proton spins in the brain were entangled.Dr. Kerskens added, ""If entanglement is the only possible explanation here then that would mean that brain processes must have interacted with the nuclear spins, mediating the entanglement between the nuclear spins. As a result, we can deduce that those brain functions must be quantum.""Because these brain functions were also correlated to short-term memory performance and conscious awareness, it is likely that those quantum processes are an important part of our cognitive and conscious brain functions.""Quantum brain processes could explain why we can still outperform supercomputers when it comes to unforeseen circumstances, decision making, or learning something new. Our experiments, performed only 50 meters away from the lecture theater where Schrödinger presented his famous thoughts about life, may shed light on the mysteries of biology, and on consciousness which scientifically is even harder to grasp.""More information: Christian Matthias Kerskens et al, Experimental indications of non-classical brain functions, Journal of Physics Communications (2022). DOI: 10.1088/2399-6528/ac94be",New research suggests our brains use quantum computation
341,3,3_cells_brain_blood_heart,https://www.digitaltrends.com/mobile/non-invasive-blood-glucose-measurement-wearables-breakthrough/,"Blood glucose monitoring is touted to be the next big breakthrough for wearable devices like the Apple Watch. However, the hardware is yet to be seen on a commercially available, mass-market device. That might change soon.A team from Georgia’s Kennesaw State University claims to have developed a noninvasive system of blood glucose level measurement, thanks to a device called GlucoCheck. It follows the same fundamental approach as the oxygen-level analysis sensor on smartwatches like the Apple Watch Series 8 and Samsung Galaxy Watch 5.Team lead Maria Valero, an assistant professor at the institution’s College of Computing and Software Engineering (CCSE), notes that the device delivers 90% accuracy in analyzing glucose concentration in blood samples. The biosensor works in tandem with a phone application, but the team is already at work on integrating Amazon’s Alexa virtual assistant.GlucoCheck shines light across the human skin, and then a camera captures the view from the other side. The goal is to study the varying level of light absorption by blood flowing in the vessels to determine the glucose concentration.The team has already filed a patent for the tech and now aims to test it on more body types to diversify the test data. This step is of critical importance, because commercially available wearables like those made by Fitbit and even Apple are known to be inaccurate at reading data from people with dark or tattoed skin types.The latest development is remarkable because it achieves the holy grail of glucose-level monitoring, which is to develop a noninvasive method that can be miniaturized and connected to devices such as phones. Currently, people with diabetes need to prick their fingers to obtain a blood sample to analyze their sugar levels.Commercial players are also at itThis is not the first research of its kind. In July 2020, Samsung showcased a noninvasive method for blood glucose monitoring in partnership with experts from the Massachusetts Institute of Technology (MIT). The same year, Movano revealed a wearable device that can measure blood sugar levels using a light diffusion method. But it didn’t do much else.A year later, a Japanese company named Quantum Operation showcased a wearable-mounted sensor at CES 2021 that was capable of noninvasive blood glucose analysis. According to multiple reports that have surfaced over the past couple of years, both Apple and Samsung are interested in the promising tech for their smartwatches.Quantum Operation Non-Invasive Blood GlucometerU.K.-based Rockley Photonics is also working toward the same objective, but instead of LEDs, the company is focused more on laser-based analysis. Regarding the in-house tech, CEO Dr. Andrew Rickman told Digital Trends that it “collects incredibly rich data that we extract to measure, amongst other things, hydration, lactate, and blood pressure.”Notably, Apple is said to be one of the biggest clients of Rockley Photonics and is rumored to include the noninvasive blood glucose monitoring tech on the Apple Watch portfolio in the near future.Editors' Recommendations",Sensor breakthrough brings us closer to blood glucose monitoring on wearables
142,3,3_cells_brain_blood_heart,https://techcrunch.com/2016/11/30/sensor-studded-suit-helps-track-recovery-of-stroke-patients/,"Millions of people suffer from strokes every year worldwide, and severe ones can require long-term care — but as with other rehabilitation regimens, it can be hard for doctors to track the everyday activities of their patients. A high-tech suit created by a Dutch grad student may be a powerful new way to help keep better tabs on those in recovery.Bart Klaassen, from the Netherlands’ University of Twente, and the rest of his team pursued it as part of a project resulting from Europe’s FP7 research initiative. Klaassen’s doctoral thesis is based on the work, as well.The idea is that while patients are closely monitored while at the rehabilitation clinic or in regular checkups, real-world tasks such as getting out of bed, navigating the house and cooking can only be reported secondhand.“There has long been a great need for systems like this, but the technology simply was not ready,” explained Klaassen in a news release from the school. But as other projects along these lines have proved, that’s not the case any more: sensors can be made small enough, fabrics smart enough and the resulting data coherent enough that a high-tech suit like this is not only a possibility, but potentially a major advance.The suit, which is worn underneath the clothes for several months, contains 41 sensors that track strength, flexibility, gait and other relevant metrics. They’re collected wirelessly and processed by (at present) university servers.“We have succeeded in modelling all of the relevant movements, and in cleaning up the data that is relevant for the therapist by filtering out the rest,” said Klaassen. “Our project has delivered new techniques and methods that can be used to monitor patients at home for extended periods of time, and to identify any differences with structured clinical measurements.”Presumably the sensors and data could also be used for tracking progress in cases other than those suffering from strokes, but Klaassen’s research is focused on the one domain for now. If it proves useful it could be adapted for use in other forms of physical therapy.The suit isn’t in production or anything right now and there are plenty of questions, from whether it can be made cheaply to whether it can be washed at all — I’ve emailed the university with some questions and will update the post if I hear back.",Sensor-studded suit helps track recovery of stroke patients
145,3,3_cells_brain_blood_heart,https://techcrunch.com/2016/12/07/this-artificial-iris-is-like-a-pair-of-programmable-shades-in-contact-lens-form/,"Smart contact lenses have been the stuff of science fiction for a long time, but as with jetpacks and faster-than-light travel, we’re still waiting on them. Research is ongoing, though, and a project at the University of Ghent shows promise not just in advancing the technology but providing some therapeutic value, as well.Herbert De Smet’s group has been working for some time with EU grant money on initial applications and executions of smart lenses, and some early results were presented at IEEE’s International Electron Devices Meeting this week. Their device embeds a tiny monochrome LCD in the lens that can be set to varying opacities and patterns.Now, these patterns would be far too close to the eye for you to make them out, except perhaps as smudges or dark areas in your vision. They’re not intended to form images, however, but rather to darken the entire field of view for people who can’t do it themselves.Some people suffer from conditions that limit the ability of their eye’s iris — that’s the colored circle — to contract and dilate the pupil and control the amount of light admitted to reach the retina. If the pupil is stuck in the open state, bright situations — normally handled by reducing the pupil to a pinhole — will overwhelm the iris and cause pain or even serious damage.A contact lens that automatically changes its shade from totally transparent to as dark as a pair of sunglasses, as required by the ambient light, would fill this role nicely. That’s exactly what De Smet’s team has created; head over to IEEE Spectrum for a video of the LCD in action.The parts are in place: the LCD-infused lens and the chip that controls it are solid, and the power system, a set of tiny photovoltaic cells, captures enough energy — but the two have yet to be integrated. Once they are, the lenses will still, of course, need to be tested for safety.You can keep up with De Smet’s work at the Centre for Microsystems Technology’s webpage.",This âartificial irisâ is like a pair of programmable shades in contact lens form
143,3,3_cells_brain_blood_heart,https://techcrunch.com/2016/12/02/augmented-reality-treatment-reduces-phantom-pain-in-missing-limbs/,"Phantom limb pain is a mysterious ailment: people with amputations experience aches and acute pains in an arm or leg that isn’t there — making the problem notoriously difficult to treat. But a new type of therapy using augmented reality is surprisingly effective at reducing even the most intractable phantom pain.The AR therapy method, first proposed in 2014 by Max Ortiz Catalan of the Chalmers University of Technology, just completed its first, highly promising clinical trial. The team selected 14 amputees whose phantom limb pain was chronic and unresponsive to other therapy methods.The patients were equipped with myoelectric sensors that detect the signals in muscles that once controlled the missing limb. These signals are tracked and analyzed, and linked to movements in a virtual environment — opening the hand or twisting the wrist of an on-screen limb.Once this calibration is complete, the virtual limb is superimposed on a live webcam image of the patient, starting just where the real limb stops. The user thinks of movements, and the virtual limb executes them. Over 12 semimonthly sessions, patients were asked to put the virtual limb into various positions, use the sensors to control a car in a racing game, and so on.https://youtu.be/ek7JHGC-T4EAmazingly, by the end of the 12 sessions, reported pain was reduced by about half, and interruptions of daily activity or sleep from it were similarly cut down. The four patients on pain medication reduced their dose, two of them by 81 percent. Six months later, the improvements were still present, implying a lasting therapeutic benefit.“The results are very encouraging, especially considering that these patients had tried up to four different treatment methods in the past with no satisfactory results,” said Catalan in a news release. “We also saw that the pain continuously decreased all the way through to the last treatment. The fact that the pain reduction did not plateau suggests that further improvement could be achieved with more sessions.”If the idea that moving a virtual limb around in AR could relieve pain strikes you as strange, don’t worry — it is. But phantom pain is a poorly understood phenomenon and sometimes the effectiveness of treatments is matched only by their strangeness.Phantom itches are also a problem, for example: imagine how maddening it must be to have an itch you can’t scratch because the limb it’s on isn’t there. The solution, some have been lucky enough to find, is to arrange mirrors so that a limb that’s present appears to be in the place of the missing one. Someone scratches it, and the phantom itch disappears. Believe it or not, this and other forms of mirror therapy are established practice — though not always effective.This AR-based method is sort of like mirror therapy taken to the next logical level, and it may prove a valuable tool in the treatment of this mysterious but very real condition.Next up: more tests. 30 patients, this time with leg amputees as well. The paper describing this initial clinical trial was published in The Lancet.",Augmented reality treatment reduces phantom pain in missing limbs
146,3,3_cells_brain_blood_heart,https://techcrunch.com/2017/01/06/touch-surgery-ar/,"Training surgeons on specific surgical procedures is expensive and hard. London-based Touch Surgery has created more than 200 training programs for surgical procedures to be completed on a mobile phone or tablet. At CES, it announces support for a new type of deeply immersive surgery training — and potentially assistance in an operating room — on DAQRI and HoloLens.“We believe in working with surgeons to optimize and scale best in class surgical procedures to enable training and delivery of safer surgery for patients globally,” Touch Surgery’s CEO Dr Jean Nehme told me. “To date our output has been through mobile devices, primarily for training. For 2017 we have enabled our pipeline to deliver to augmented reality platforms.”The company is teasing its augmented reality content for the first time at the Consumer Electronics Show (CES) in Las Vegas this week.The real opportunity is to move Touch Surgery’s training content from the relatively sterile scenario of being able to tap and swipe your way through a surgery simulation on a phone, to bringing it into the operating room. It’s easy to imagine how a surgeon about to pick up a scalpel and a dozen other sharp instruments might want to have a live overlay of the surgery they are about to commence.It all sounds a little bit futuristic, but I remain to be convinced about a surgeon wearing a hololens while doing a surgery. On the one hand, perhaps it reduces the number of mistakes made in complex procedures, but on the other, perhaps it is a little unsettling to think that there’s a chance that your surgeon might not quite know what they’re doing. Also, given the GIF looping to the right of this article, I’m starting to have some serious doubts about my use of the “on the one hand” idiom.It’s still early days for augmented reality for use in operating theaters, but if it makes surgeries safer, quicker, or with fewer complications, it’s hard to argue against its potential usefulness. Bring on the HoloLenses, I say.",Touch Surgery brings surgery training to augmented reality
173,3,3_cells_brain_blood_heart,https://techcrunch.com/2019/07/16/how-to-watch-elon-musks-neuralink-brain-control-interface-startup-presentation-live/,"https://youtu.be/r-vbh3t7WVIOne of Elon Musk’s stealthier endeavors is set to become a lot less stealthy tonight, with a presentation set for 8 PM PT (11 PM ET) streaming live directly via the embedded YouTube video above, in which we’ll learn a lot more about Neuralink, the company Musk founded in 2017 to work on brain control interfaces (BCIs) and essentially part of his larger strategy to help mitigate the risks of AI and enhance its potential benefits.Here’s what we do know about Neuralink already: Its initial goal, at least as of two years ago, was to figure out how brain interfaces could be helpful in alleviating the symptoms of chronic medical conditions, including epilepsy. This goal will involve the development of “ultra high bandwidth brain-machine interfaces to connect humans and computers,” which is the only formal description Neuralink provides of its overall mission on its own website.In a post on Wait Buy Why back when the company first broke cover, we got a lot more in-depth background about what problem Musk wants to solve and why. Summarized, Neuralink’s mission is very much on trend with Musk’s other ventures, in that it hopes to help humans avoid something he perceives as an existential threat in order that we may survive, thrive and I guess come up with other potential existential threats for him to also then solve.Ultimately, Neuralink seems to be aiming well beyond its initial exploration of medical technology, which was really just a way to potentially get testing faster with a practical application that’s easier to work with in terms of rules and regulators. Musk’s goal, per the Wait But Why explainer, is actually to eliminate the “compression” that happens when we translate our thoughts into language, and then into input via keyboard, mouse, etc. before actually transmitting it to a computer. Taking away the need to compress and then decompress the signal, in other words, will make communication between people and computers much faster, lossless and very high bandwidth.This has an existential angle because this is a key step, Musk believes, in ensuring that humanity can keep up with the increasingly advanced AI it’s developing. So to avoid a doomsday scenario where the robots take over, basically Musk proposes more or less mind-melding with the robots instead.That was a lot to digest two years ago — it’s wild to think about what Neuralink may have done in the interim to work toward or modify this goal. Luckily, we won’t have to wait much longer. That stream kicks off at 8 PM PT (11 PM ET) and will be carried live directly at the top of this post and on Neuralink.com.",Watch Elon Muskâs Neuralink brain control interface startup presentation live
350,3,3_cells_brain_blood_heart,https://www.engadget.com/the-cutting-edge-cellular-therapies-aiming-to-ease-america-organ-shortage-140025730.html,"Despite being the wealthiest nation on the face of the planet, the United States chronically runs short of transplantable organs . Kidneys are far and away the most sought-after organ for transplantation , followed by livers. While the liver is the only human organ known capable of regenerating itself, if you damage yours badly enough for long enough — as some 30 million Americans have — then the only treatment is a transplant. Assuming you can even acquire one for doctors to stick in you. Every year demand for replacement livers outstrips supply by a scope of tens of thousands.“Only one-third of those on the liver transplant waiting list will be transplanted, and the demand for livers is projected to increase 23 percent in the next 20 years,” a multidisciplinary team of researchers observed in 2016’s Liver-Regenerative Transplantation: Regrow and Reset . “Exacerbating the organ shortage problem, the donor pool is expected to shrink further because of the obesity epidemic. Liver steatosis [aka fatty liver disease ] is increasingly common in donors and is a significant risk factor in liver transplantation.”To address this critical shortage, the study authors note that doctors have explored a variety of cutting-edge regimens, from cell repopulation and tissue engineering, nanoparticles to genomics, mechanical aids to porcine-derived xenotransplantation, all with varying degrees of success. Cellular repopulation has been used for years, a process that injects healthy liver cells into the patient’s damaged organ through a portal vein where they adhere themselves to the existing cellular scaffolding and grow into new, functional liver tissue.Turn on browser notifications to receive breaking news alerts from Engadget You can disable notifications at any time in your settings menu. Not now Turned on Turn onFabian Bimmer / reuters“Creating an immediately available and inexhaustible supply of functioning liver cells from autologous tissue would allow early intervention in patients with hepatic failure and would allow liver cells to be infused over a longer period of time,” the 2016 study’s authors note. “Combined with recent advances in genome-editing technology, such liver cells could be used widely to treat devastating liver-based inborn errors of metabolism and to eliminate the need for a life-long regimen of immunosuppressive drugs and their complications.” The downside to this technique is the pace at which the donor cells proliferate, making it a poor tool against acute liver failure.Extracellular Vesicle-based therapies, on the other hand, leverage the body’s intracellular communications pathways to deliver drugs with, “high bioavailability, exceptional biocompatibility, and low immunogenicity,” according to 2020’s Extracellular Vesicle-Based Therapeutics: Preclinical and Clinical Investigations . “They provide a means for intercellular communication and the transmission of bioactive compounds to targeted tissues, cells, and organs” including “fibroblasts, neuronal cells, macrophages, and even cancer cells.”EVs are the postal letters that cells send one another. They come in a variety of sizes from 30 to 1000 nm and have exterior membranes studded with multiple adhesive proteins that grant them entry into any number of different types of cells. Exploiting the biological equivalent to a janitor’s key ring, researchers have begun tucking therapeutic nanoparticles into EVs and using them to discreetly inject treatments into the targeted cells. However, these treatments are still in the experimental stages and are most effective against acute liver failure and inborn metabolic diseases rather than end-stage liver failure.Mayo ClinicMechanical aids, the hepatocytic equivalent to a dialysis machine, like the Mayo Spheroid Reservoir Bioartificial Liver (SRBAL, above) are ideal for treating cases of acute liver failure, able to take over the entirety of the patient’s liver function externally and immediately. However, such procedures are both expensive and temporary. The SRBAL can only support a patient for up to two weeks, making it more suitable for keeping someone alive until a donor can be located rather than as a permanent, pacemaker-like solution.The bioprinting and implantation of replacement livers has also shown promise, though they too are still in early development and largely not near ready for widespread adoption. Interspecies transplantation using genetically-engineered pig organs are a bit closer to clinical use , with surgeons successfully transplanting a porcine heart into a human patient for the first time this past January (though he died of complications two months later). Pig kidneys and livers have similarly been transplanted into human recipients, often with less drastic side effects than death.No matter where the transplanted organ comes from, getting it into the patient is invariably going to involve a significant surgical procedure. However, the Lygenesis company recently unveiled its non-invasive solution: tricking the patient’s body into growing a series of miniature, ectopic liver “organoids” in its own lymphatic system like a crop of blood-scrubbing potatoes.For those of you who dozed through high school bio, a quick recap of terms. The lymphatic system is a part of the immune system that serves to circulate some 20 liters of lymph throughout your body, absorb excess interstitial fluids back into the bloodstream, and incubate critical lymphocytes like T-cells . Organoids, on the other hand, are biological masses artificially grown from stem cells that perform the same functions as natural organs , but do so ectopically, in that they function in a different part of the body as a regular liver. Blood-scrubbing potatoes are self-explanatory.“Fundamentally, Lygenesis uses the lymph node, your body's natural bio reactors typically used for T-cells,” company CEO and co-founder Michael Hufford, told Engadget. “We hijacked that same biology, we engraft our therapies into the lymph nodes to grow functioning ectopic organs.”“We use an outpatient endoscopic ultrasound procedure where we're going down through the mouth of the patient using standard endoscopic equipment,” Hufford continued. “We engraft ourselves there in minutes under light sedation, so it's very low medical risk and also is really quite inexpensive.” He notes that the average cost for a proper, in-hospital liver transplant will set you back around a million dollars. Lygenesis’ outpatient procedure “is billed at a couple of thousand or so,” he said.More importantly, the Lygenesis technique doesn’t require a full donated liver, or even a large fraction of one. In fact, each donated organ can be split among several dozen recipients. “Using our technology a single donated liver can reach 75 or more patients,” Hofford said. The process of converting a single donated liver into all those engraftable samples takes a team of three technicians more than six hours and 70 steps to complete. The process does not involve any gene manipulation, such as CRISPR editing.This process is quite necessary as patients cannot donate culturable liver cells to themselves. “Once you have end-stage liver disease, you typically have a very fibrotic liver ,” Hofford noted. “It will bleed at the slightest sort of intervention.” Even the simple act of collecting cellular samples can quickly turn deadly if the wrong bit of organ is bisected.And it’s not only the transplant recipients themselves who are unable to donate. Hofford estimates between 30 and 40 percent of donated livers are too worn to be successfully transplanted. “One of the benefits of our technology is we're using organs that have been donated but will otherwise be discarded,” he said.Once engrafted into a lymph node, the liver organoid will grow and vascularize over the course of two to three months, until it is large enough to begin supporting the existing liver. Hufford points out that even with end-stage disease, a liver can retain up to 30 percent of its original functionality, so these organoids are designed to augment and support the existing organ rather than replace it outright.Lygenesis is currently in Phase 2A of the FDA approval process , meaning that a small group of four patients have each received a single engraftment in a lymph node located in their central body cavity near the liver itself (the body has more than 500 lymph nodes and apparently this treatment can technically target any of them). Should this initial test prove successful subsequent study groups will receive increasing numbers of engraftment, up to a half dozen, to help the company and federal regulators figure out the optimal number of organoids to treat the disease.",The cutting-edge cellular therapies aiming to ease America's organ shortage
167,3,3_cells_brain_blood_heart,https://techcrunch.com/2019/04/17/consumers-get-another-digital-home-health-offering-as-tyto-care-and-best-buy-launch-tytohome/,"Best Buy is partnering with the Israeli technology company Tyto Care to become the official retailer for the company’s all-in-one digital diagnostics kit through its physical stores in California, the Dakotas, Ohio and Minnesota, and through its online store.Tyto previously sold its technology through healthcare plans, making its handheld examination device (with attachments that act as a thermometer, a stethoscope, an otoscope and a tongue depressor) available to families with insurance that wanted to reduce the cost of checkups through remote monitoring. The company’s handheld device comes with an exam camera so it can prompt users where to position the device to get the most accurate readings.Now, through Best Buy, consumers can buy the company’s kit for $299.99. Through a partnership with American Well, users of the TytoHome kit have access to the company’s LiveHealth Online consultation service (if they live outside of Minnesota or the Dakotas). Which means patients can use the device to perform a medical exam and send the information to a physician for a diagnosis any time of the day or night.As part of the deal, Tyto Care is partnering with additional regional healthcare systems to provide medical care to consumers throughout the country. The first is Sanford Health, a Minnesota-based not-for-profit health system operating in Minnesota, North Dakota and South Dakota.For Best Buy, the move builds on the company’s attempts to move quickly into providing digital healthcare services just like it provides technical support through its Geek Squad.Last year the company bought GreatCall, which sells connected health and emergency response services to the AARP crowd.“We’re excited to partner with Best Buy, LiveHealth Online, American Well and regional health systems to extend our on-demand telehealth platform across the U.S., enhancing primary care delivery,” said Dedi Gilad, the chief executive and co-founder of Tyto Care, in a statement.The company, based in Herzliya, Israel, has raised $56.7 million to date from investors, including Sanford Health, the Japanese Itochu Corp., Shenzhen Capital Group, Ping An, LionBird, Fosun Group, Orbimed and Walgreens.The company said at the time that it would use the cash to expand in the U.S. and to other international markets in Asia and Europe.“These strategic partnerships will enable us to gain further momentum and accelerate our growth, deepening our foothold in the U.S. and other new strategic markets,” said GiladTyto Care said in a statement at the time.",Consumers get another digital home health offering as Tyto Care and Best Buy launch TytoHome
128,3,3_cells_brain_blood_heart,https://t.uga.edu/8sX,"Closeup of a wireless pacemaker (left) and a traditional pacemaker (right) held in the hands of Dr. Kent Nilsson. (Photo by Andrew Davis Tucker/UGA)New type of pacemaker is a safer, more advanced option than traditional surgeryThis March, Dr. Kent Nilsson successfully implanted one of the first wireless, dual chamber pacemakers in the world into a patient. This accomplishment makes Piedmont Hospital the first center in the Southeast and fourth in the U.S. to implant this new device. It was only the 32nd implant in a human in the world.A cardiologist at Piedmont Athens Regional Hospital and professor of medicine at the Augusta University/University of Georgia Medical Partnership, Nilsson has become an expert in implanting single chamber, wireless pacemakers. He was called upon again to test the dual chamber version as part of the clinical trial for the Abbott Aveir DR Leadless (wireless) Pacemaker System.The dual chamber wireless pacemaker is honestly one of the most transformative technologies in cardiology. Ever.” —Dr. Kent NilssonWith this procedure, the hospital joined the ranks of leading medical centers and universities in the country. The three locations ahead of Piedmont for the implantation were the University of Arizona, Cleveland Clinic and Cornell University.“The dual chamber wireless pacemaker is honestly one of the most transformative technologies in cardiology. Ever,” said Nilsson. “When the dual chamber launches, it will completely change the field.”How does a pacemaker work?A pacemaker is a device implanted into the chest to stabilize an irregular or slowly beating heart. A traditional pacemaker is inserted through an incision in the chest and is then connected to wires (leads) that have been inserted into the heart through the subclavian vein. The device sits beneath the skin and on top of the chest muscle, and the wires deliver electrical pulses to correct the heartbeat.In 2014, Piedmont joined a clinical trial to test single chamber leadless – or wireless – pacemakers, which are put directly into the lower right ventricle of the heart via the femoral vein and do not require the chest to be opened for implantation. Nilsson gladly came on board for the trial.Piedmont was the first center in Georgia and 15th in the country to implant a wireless pacemaker. By the time the 2014 trial for the single wireless pacemaker was complete, Nilsson had implanted over 30 devices.Although traditional pacemakers have come a long way (the first implantation in 1958 failed within three hours and the patient went on to have 26 pacemakers in his lifetime), Nilsson said this wireless technology is the biggest advancement in the field since the invention.Benefits of wireless pacemakersThe wireless pacemaker is about 90% smaller than the average pacemaker and the surgery lasts around 30 minutes. The battery life is also comparable with a traditional pacemaker’s 10-12 years and could be longer in some cases.Patients also will not have an incision scar on the chest or a bump protruding from the skin.“The psychological aspect of not seeing an incision or having something protruding out of your skin is beneficial,” said Nilsson. “Just being able to not broadcast to the world that there is something wrong.”Another advantage is the elimination of several complications associated with traditional pacemakers. One in 10 patients see complications with pacemaker leads, or wires. Some also experience pocket infections, hematoma, lead dislodgment and lead fracture.Wireless pacemaker patients will also see a shorter, less complicated recovery time. There are no restrictions with taking showers or getting the area wet, and there are also no mobility issues. Traditional pacemaker patients are not able to move their arm above their shoulder for six weeks after surgery.“It’s same day discharge and people are up and moving around and doing everything they need to do,” said Nilsson.The single chamber wireless pacemaker was approved by the FDA in 2016, and Nilsson is now teaching other physicians how to implant the device.“I’m one of 10 proctors teaching close to 5,000 physicians how to implant,” said Nilsson. “The reception nationwide has been overwhelming. Every day my Twitter is blowing up with pictures of people doing their first single chamber implant.”Single chamber pacemakers connect to one chamber of the heart while the dual chamber version connects to both chambers on the right side of the heart. Dual chamber pacemakers are the standard of care for pacemaker patients in the United States, so this new technology will affect more patients than the single chamber version.“Dual chambers are 95% of what we do,” said Nilsson.First implant of the new deviceNilsson kicked off the first dual implantation of the dual chamber, wireless pacemaker in March with a meaningful milestone. “Almost eight years to the day after we implanted the single chamber, we put in the dual chamber,” he said.Sixty-four-year-old Anna McKuhen was Nilsson’s first patient. She was the first in the Southeast to have the dual chamber pacemaker implanted.“I am all about new technology, so when they asked if I’d like to be a part of this trial, I said sure. It sounded exciting,” said McKuhen. “The technology amazes me.”McKuhen said she was interested in receiving the wireless pacemaker, in part, because of the greatly reduced recovery time.“My recovery was fantastic,” said McKuhen. “I did not have any problems whatsoever. I was only confined to a week of not lifting heavy objects. I’ve had zero problems.”Nilsson implanted four dual pacemakers by the time the trial concluded in September.Nilsson hopes the dual chamber will be FDA approved in 2023 and widely available soon after.“I would say this is the future of pacemakers 100 percent,” said Nilsson. “Everyone is trying to catch up at this point. This is really transformative technology.”","Local doctor implants one of the first wireless, dual chamber pacemakers"
166,3,3_cells_brain_blood_heart,https://techcrunch.com/2019/03/22/medtronic-defibrillators-critical-flaws/,"Homeland Security has issued a warning for a set of critical-rated vulnerabilities in Medtronic defibrillators that put the devices at risk of manipulation.These small cardio-defibrillators are implanted in a patient’s chest to deliver small electrical shocks to prevent irregular or dangerously fast heartbeats, which can prove fatal. Most modern devices come with wireless or radio-based technology to allow patients to monitor their conditions and their doctors to adjust settings without having to carry out an invasive surgery.But the government-issued alert warned that Medtronic’s proprietary radio communications protocol, known as Conexus, wasn’t encrypted and did not require authentication, allowing a nearby attacker with radio-intercepting hardware to modify data on an affected defibrillator.Homeland Security gave the alert a 9.3 out of 10 rating, describing it as requiring “low skill level” to exploit.It doesn’t mean that anyone with an affected defibrillator is suddenly a walking target for hackers. These devices aren’t always broadcasting a radio frequency as it would be too battery intensive. Medtronic said patients would be most at risk when patients are getting their implant checked while they’re at their doctor’s office. At all other times, the defibrillator will occasionally wake up and listen for a nearby monitoring device if it’s in range, narrowing the scope of an attack.More than 20 different Medtronic defibrillators and models are affected, the alert said, including the CareLink programmer used in doctor’s offices and the MyCareLink monitor used in patient homes.Peter Morgan, founder and principal at Clever Security, found and privately reported the bug to Medtronic in January. In an email, Morgan told TechCrunch that the bugs weren’t easy to discover, but warned of a potential risk to patients.“It is possible with this attack to cause harm to a patient, either by erasing the firmware that is giving necessary therapy to the patient’s heart, or by directly invoking shock related commands on the defibrillator,” he said. “Since this protocol is unauthenticated, the ICD cannot discern if communications its receiving are coming from a trusted Medtronic device, or an attacker.”A successful attacker could erase or reprogram the defibrillator’s firmware, and run any command on the device.Medtronic said in its own advisory that it’s not aware of any patient whose devices have been attacked, but that the company was “developing updates” to fix the vulnerabilities, but did not say when fixes would be rolled out.The Food and Drug Administration (FDA), which regulates medical devices, provided a list of the affected devices.It’s the latest example of smart medical devices taking a turn for the worst, even as spending in healthcare cybersecurity is set to become a $65 billion industry by 2021.The FDA rolled out non-binding recommendations in 2016 to advise medical device makers into practicing better cybersecurity to prevent these kinds of flaws from occurring in the first place, advising companies to “build in cybersecurity controls when they design and develop the device to assure proper device performance in the face of cyber threats.”Yet, this latest government alert marks the second time in two years Medtronic was forced to respond to security flaws in its medical devices. In October, the company finally shuttered an internet-based software update system that put its pacemaker-monitoring devices at risk.",Homeland Security warns of critical flaws in Medtronic defibrillators
125,3,3_cells_brain_blood_heart,https://spectrum.ieee.org/brain-computer-interface-speech,"Edward Chang, the chair of neurological surgery at University of California, San Francisco, is developing brain-computer interface technology for people who have lost the ability to speak. His lab works on decoding brain signals associated with commands to the vocal tract, a project that requires not only today’s best neurotechnology hardware, but also powerful machine learning models.",This Implant Turns Brain Waves Into Words
473,4,4_cancer_licl_gsk3_mice,https://www.reuters.com/business/healthcare-pharmaceuticals/covid-raises-risk-long-term-brain-injury-large-us-study-finds-2022-09-22/,"CHICAGO, Sept 22 (Reuters) - People who had COVID-19 are at higher risk for a host of brain injuries a year later compared with people who were never infected by the coronavirus, a finding that could affect millions of Americans, U.S. researchers reported on Thursday.The year-long study, published in Nature Medicine, assessed brain health across 44 different disorders using medical records without patient identifiers from millions of U.S. veterans.Brain and other neurological disorders occurred in 7% more of those who had been infected with COVID compared with a similar group of veterans who had never been infected. That translates into roughly 6.6 million Americans who had brain impairments linked with their COVID infections, the team said.""The results show the devastating long-term effects of COVID-19,"" senior author Dr. Ziyad Al-Aly of Washington University School of Medicine said in a statement.Al-Aly and colleagues at Washington University School of Medicine and the Veterans Affairs St. Louis Health Care System studied medical records from 154,000 U.S. veterans who had tested positive for COVID from March 1, 2020 to Jan. 15, 2021.They compared these with records from 5.6 million patients who did not have COVID during the same time frame, and another group of 5.8 million people from the period just before the coronavirus arrived in the United States.Al-Aly said prior studies looked at a narrower group of disorders, and were focused largely on hospitalized patients, whereas his study included both hospitalized and non-hospitalized patients.Memory impairments, commonly referred to as brain fog, were the most common symptom. Compared with the control groups, people infected with COVID had a 77% higher risk of developing memory problems.People infected with the virus also were 50% more likely to have an ischemic stroke, which is caused by blood clots, compared with the never infected group.Those who had COVID were 80% more likely to have seizures, 43% more likely to have mental health issues, such as anxiety or depression, 35% more likely to have headaches and 42% more likely to suffer movement disorders, such as tremors, compared with the control groups.Researchers said governments and health systems must devise plans for a post-COVID world.“Given the colossal scale of the pandemic, meeting these challenges requires urgent and coordinated - but, so far, absent - global, national and regional response strategies,” Al-Aly said.Reporting by Julie Steenhuysen Editing by Bill BerkrotOur Standards: The Thomson Reuters Trust Principles.","COVID raises risk of long-term brain injury, large U.S. study finds"
437,4,4_cancer_licl_gsk3_mice,https://www.ox.ac.uk/news/2022-10-13-new-study-finds-monkeypox-virus-can-spread-widely-within-specialist-hospital,"The UK Health Security Agency (UKHSA) recommends that patients with monkeypox who have severe disease requiring hospital admission are cared for in isolation rooms, with infection prevention and control (IPC) precautions that aim to contain potentially infectious virus within the room and protect staff who enter. However, to date it has been unclear whether these measures are proportionate to the potential virus exposure risks.To investigate this, researchers from the Liverpool School of Tropical Medicine, the University of Oxford’s Nuffield Department of Medicine and the UKHSA conducted a study which collected samples from the rooms of patients hospitalised with monkeypox. The findings have been published in The Lancet Microbe.The research team assessed the extent of virus shedding onto surfaces in specialist isolation rooms containing patients admitted to hospital for the management of severe monkeypox. They also investigated whether the virus was detectable in air samples from the rooms.The researchers found that viral DNA shed by the patients could be found on multiple surfaces throughout the isolation rooms (56 (93%) positive by PCR out of 60 samples). Monkeypox virus DNA was also found on personal protective equipment (PPE) worn by healthcare workers caring for these patients, and in the anterooms where they remove their PPE. Monkeypox virus DNA was also detected in five out of twenty air samples taken within these isolation rooms.Changing bed linen was an activity particularly associated with detection of monkeypox virus DNA in air samples in the room. This suggests that viral particles, probably in shed skin particles, can become suspended in the air when bed sheets are changed. Monkeypox virus capable of replicating in cells under laboratory conditions (an indicator that the virus could infect other people) was identified in two of four PCR-positive samples selected for virus isolation. This includes air samples collected during the bed linen change.Lead author, Dr Susan Gould, from the Liverpool School of Tropical Medicine, said: ‘Our results found that changing a patient’s bedding appears to be particularly associated with an increased ability to detect monkeypox virus in air samples. In 2018, a UK healthcare worker was thought to have developed monkeypox after being exposed to the virus while changing a patient’s bedding, before monkeypox had been considered and diagnosed. Our results suggest that changing bed linen used by hospitalised patients with monkeypox does indeed increase the risk of exposure to virus, by disturbing virus on bed linen and allowing it to be suspended in the air.’In addition to detecting virus DNA, the researchers were able to isolate replication-competent virus in some surface and air samples. The results show, for the first time, that monkeypox virus in some air samples taken around patients hospitalised with monkeypox is capable of replicating in cells and is not just ‘dead’ virus. Dr Gould said ‘These results suggest that monkeypox virus shed into a hospitalised patient’s environment poses an infection risk that needs to be managed.’Senior author on the paper, Dr Jake Dunning, of the University of Oxford’s Nuffield Department of Medicine and the Royal Free London NHS Foundation Trust, said: ‘It is important to note that detection of virus, even when demonstrated to be infectious, does not necessarily mean that exposure to the virus in real life would result in infection of the exposed person. However, it does reveal a potential transmission risk and one that is reasonable to control in hospital settings. Our results confirm that the strict IPC measures we follow in specialist infectious diseases centres are necessary and appropriate.’This investigation specifically evaluated exposure risks when caring for patients admitted to specialist facilities in hospitals. The results and recommendations may therefore not apply to other settings, such as outpatient clinics where patients attend for a short time, interactions differ, and the virus is unlikely to accumulate to such an extent. There is no suggestion that transmission of monkeypox virus via aerosols is a common way for the infection to spread from one person to another.Dr Gould added: ‘In the context of ward-based care, our results support infection prevention and control measures designed to protect against exposure to infectious virus on surfaces and in the air, such as appropriate PPE, as well as applying measures designed to contain shed virus within hospitalised patients’ isolation rooms, including the use of negative pressure rooms and doffing areas.’The study was led by scientists from the Liverpool School of Tropical Medicine and the University of Oxford, as part of the National Institute for Health and Care Research Health Protection Unit in Emerging and Zoonotic Infections, which includes the University of Liverpool as a partner. The HPRU in Emerging and Zoonotic Infections and UKHSA identified a need for environmental sampling in hospitals managing inpatients with monkeypox and rapidly began investigations. Prior to publication the team shared its results with those caring for patients with monkeypox admitted to hospitals in the UK, and with international partners, infectious diseases specialist networks, and public health organisations.Dr Dunning said: ‘This work demonstrates the ability of HPRUs to conduct rapid, reactive studies to obtain data from novel outbreaks, focussing on research that informs public health guidance and policies, as well as improving patient care.’The study ‘Air and surface sampling for monkeypox virus in a UK hospital: an observational study’ was published in The Lancet Microbe.",New study finds that monkeypox virus can spread widely within specialist hospital isolation rooms
421,4,4_cancer_licl_gsk3_mice,https://www.newscientist.com/article/2342535-personal-lubricant-made-from-cow-mucus-may-protect-against-hiv/,"In a laboratory study, human epithelial cells were treated with the lubricant before being exposed to HIV or a herpes virus, with subsequent infection rates being as low as 20 per centThe mucus in cows’ salivary glands may have anti-viral properties Getty ImagesA lubricant derived from the mucus of cow salivary glands has shown promise at preventing the human immunodeficiency virus (HIV) and a herpes virus from infecting healthy human cells.Mucus is a protective gel that lubricates the epithelial tissues that cover our organs and line our body cavities, as well as acting as a first line of defence against microorganisms. The main component of mucus, a protein called mucin, may have antiviral properties.Hongji Yan at the KTH Royal …",Personal lubricant made from cow mucus may protect against HIV
61,4,4_cancer_licl_gsk3_mice,https://interestingengineering.com/health/breakthrough-cancer-research-dark-matter,"Professor Trevor Graham, Director of the Centre for Evolution and Cancer at the Institute of Cancer Research (ICR), said in a statement: ""We've unveiled an extra level of control for how cancers behave – something we liken to cancer’s 'dark matter.' For years, our understanding of cancer has focused on genetic mutations which permanently change the DNA code. But our research has shown that the way the DNA folds up can change which genes are read without altering the DNA code and this can be very important in determining how cancers behave.""The research was led by scientists at The Institute of Cancer Research, London, Human Technopole in Milan, and the Queen Mary University of London.Epigenetic changes are involved in cancer's evolutionIn the first paper, the researchers collected 1,373 samples from 30 bowel cancers and looked at epigenetic changes as cancers grew. Their observations showed that epigenetic changes are common in cancerous cells, are heritable, and were present in cancer cells that had ""survival advantages.""The second paper intended to study why cancer cells within the same tumor can be different from one another. So, the researchers looked at the DNA sequence in diverse samples taken from different parts of the same tumor. The study revealed that less than two percent of ""changes in the DNA code in independent areas of a tumor were associated with changes in gene activity and variation in cancer cell characteristics throughout tumors is often governed by factors other than DNA mutations"".With these studies, scientists were able to track the influence of epigenetic control on the growth and evolution of bowel cancers. They also noted that epigenetic changes are heavily involved in cancer's evolution.""I hope our work will change the way we think about cancer and its treatment – and should ultimately affect the way patients are treated. Genetic testing for cancer mutations only gives us part of the picture about a person’s cancer – and is blind to ‘epigenetic’ changes to how genes are read. By testing for both genetic and epigenetic changes, we could, potentially, much more accurately predict which treatments will work best for a particular person’s cancer,"" said Graham.",Major breakthrough in cancer research: Papers reveal 'dark matter' that contributes to disease's growth
57,4,4_cancer_licl_gsk3_mice,https://health.ucdavis.edu/news/headlines/worlds-first-stem-cell-treatment-for-spina-bifida-delivered-during-fetal-surgery--/2022/10,"Download the media kit, which contains still photos, interviews and B-roll video, at https://ucdavis.health/mediakitThree babies have been born after receiving the world’s first spina bifida treatment combining surgery with stem cells. This was made possible by a landmark clinical trial at UC Davis Health. The one-of-a-kind treatment, delivered while a fetus is still developing in the mother’s womb, could improve outcomes for children with this birth defect. Launched in the spring of 2021, the clinical trial is known formally as the “CuRe Trial: Cellular Therapy for In Utero Repair of Myelomeningocele.” Thirty-five patients will be treated in total. The three babies from the trial that have been born so far will be monitored by the research team until 30 months of age to fully assess the procedure’s safety and effectiveness. The first phase of the trial is funded by a $9 million state grant from the state’s stem cell agency, the California Institute for Regenerative Medicine (CIRM). Emily with baby Robbie, 15 days after her birth. “This clinical trial could enhance the quality of life for so many patients to come,” said Emily, the first clinical trial participant who traveled from Austin, Tex. to participate. Her daughter Robbie was born last October. “We didn’t know about spina bifida until the diagnosis. We are so thankful that we got to be a part of this. We are giving our daughter the very best chance at a bright future.” Spina bifida, also known as myelomeningocele, occurs when spinal tissue fails to fuse properly during the early stages of pregnancy. The birth defect can lead to a range of lifelong cognitive, mobility, urinary and bowel disabilities. It affects 1,500 to 2,000 children in the U.S. every year. It is often diagnosed through ultrasound. While surgery performed after birth can help reduce some of the effects, surgery before birth can prevent or lessen the severity of the fetus’s spinal damage, which worsens over the course of pregnancy. “I’ve been working toward this day for almost 25 years now,” said Diana Farmer, the world’s first woman fetal surgeon, professor and chair of surgery at UC Davis Health and principal investigator on the study.I’ve been working toward this day for almost 25 years now.” — Diana FarmerThe path to a future cureAs a leader of the Management of Myelomeningocele Study (MOMS) clinical trial in the early 2000s, Farmer had previously helped to prove that fetal surgery reduced neurological deficits from spina bifida. Many children in that study showed improvement but still required wheelchairs or leg braces.Farmer recruited bioengineer Aijun Wang specifically to help take that work to the next level. Together, they launched the UC Davis Health Surgical Bioengineering Laboratory to find ways to use stem cells and bioengineering to advance surgical effectiveness and improve outcomes. Farmer also launched the UC Davis Fetal Care and Treatment Center with fetal surgeon Shinjiro Hirose and the UC Davis Children’s Surgery Center several years ago.Farmer, Wang and their research team have been working on their novel approach using stem cells in fetal surgery for more than 10 years. Over that time, animal modeling has shown it is capable of preventing the paralysis associated with spina bifida.It’s believed that the stem cells work to repair and restore damaged spinal tissue, beyond what surgery can accomplish alone.Preliminary work by Farmer and Wang proved that prenatal surgery combined with human placenta-derived mesenchymal stromal cells, held in place with a biomaterial scaffold to form a “patch,” helped lambs with spina bifida walk without noticeable disability.“When the baby sheep who received stem cells were born, they were able to stand at birth and they were able to run around almost normally. It was amazing,” Wang said.When the team refined their surgery and stem cells technique for canines, the treatment also improved the mobility of dogs with naturally occurring spina bifida.A pair of English bulldogs named Darla and Spanky were the world’s first dogs to be successfully treated with surgery and stem cells. Spina bifida, a common birth defect in this breed, frequently leaves them with little function in their hindquarters.By their post-surgery re-check at 4 months old, Darla and Spanky were able to walk, run and play.The world’s first human trialWhen Emily and her husband Harry learned that they would be first-time parents, they never expected any pregnancy complications. But the day that Emily learned that her developing child had spina bifida was also the day she first heard about the CuRe trial.For Emily, it was a lifeline that they couldn’t refuse.Emily with Diana Farmer and baby Robbie.Participating in the trial would mean that she would need to temporarily move to Sacramento for the fetal surgery and then for weekly follow-up visits during her pregnancy.After screenings, MRI scans and interviews, Emily received the life-changing news that she was accepted into the trial. Her fetal surgery was scheduled for July 12, 2021, at 25 weeks and five days gestation.Farmer and Wang’s team manufactures clinical grade stem cells – mesenchymal stem cells – from placental tissue in the UC Davis Health’s CIRM-funded Institute for Regenerative Cures. The cells are known to be among the most promising type of cells in regenerative medicine.The lab is a Good Manufacturing Practice (GMP) Laboratory for safe use in humans. It is here that they made the stem cell patch for Emily’s fetal surgery.“It’s a four-day process to make the stem cell patch,” said Priya Kumar, the scientist at the Center for Surgical Bioengineering in the Department of Surgery, who leads the team that creates the stem cell patches and delivers them to the operating room. “The time we pull out the cells, the time we seed on the scaffold, and the time we deliver, is all critical.”A first in medical historyDuring Emily’s historic procedure, a 40-person operating and cell preparation team did the careful dance that they had been long preparing for.After Emily was placed under general anesthetic, a small opening was made in her uterus and they floated the fetus up to that incision point so they could expose its spine and the spina bifida defect. The surgeons used a microscope to carefully begin the repair.Then the moment of truth: The stem cell patch was placed directly over the exposed spinal cord of the fetus. The fetal surgeons then closed the incision to allow the tissue to regenerate.“The placement of the stem cell patch went off without a hitch. Mother and fetus did great!” Farmer said.The team declared the first-of-its-kind surgery a success.Delivery dayOn Sept. 20, 2021, at 35 weeks and five days gestation, Robbie was born at 5 pounds, 10 ounces, 19 inches long via C-section.“One of my first fears was that I wouldn’t be able to see her, but they brought her over to me. I got to see her toes wiggle for the first time. It was so reassuring and a little bit out of this world,” Emily said.This experience has been larger than life and has exceeded every expectation. I hope this trial will enhance the quality of life for so many patients to come.” — EmilyFor Farmer, this day is what she had long hoped for, and it came with surprises. If Robbie had remained untreated, she was expected to be born with leg paralysis.“It was very clear the minute she was born that she was kicking her legs and I remember very clearly saying, ‘Oh my God, I think she’s wiggling her toes!’” said Farmer, who noted that the observation was not an official confirmation, but it was promising. “It was amazing. We kept saying, ‘Am I seeing that? Is that real?’”Both mom and baby are at home and in good health. Robbie just celebrated her first birthday.The CuRe team is cautious about drawing conclusions and says a lot is still to be learned during this safety phase of the trial. The team will continue to monitor Robbie and the other babies in the trial until they are 6 years old, with a key checkup happening at 30 months to see if they are walking and potty training.“This experience has been larger than life and has exceeded every expectation. I hope this trial will enhance the quality of life for so many patients to come,” Emily said. “We are honored to be part of history in the making.”Related links",Worldâs first stem cell treatment for spina bifida delivered during fetal surgery
418,4,4_cancer_licl_gsk3_mice,https://www.nejm.org/doi/full/10.1056/NEJMoa2208375,"BackgroundAlthough colonoscopy is widely used as a screening test to detect colorectal cancer, its effect on the risks of colorectal cancer and related death is unclear.MethodsDownload a PDF of the Research Summary.We performed a pragmatic, randomized trial involving presumptively healthy men and women 55 to 64 years of age drawn from population registries in Poland, Norway, Sweden, and the Netherlands between 2009 and 2014. The participants were randomly assigned in a 1:2 ratio either to receive an invitation to undergo a single screening colonoscopy (the invited group) or to receive no invitation or screening (the usual-care group). The primary end points were the risks of colorectal cancer and related death, and the secondary end point was death from any cause.ResultsFollow-up data were available for 84,585 participants in Poland, Norway, and Sweden — 28,220 in the invited group, 11,843 of whom (42.0%) underwent screening, and 56,365 in the usual-care group. A total of 15 participants had major bleeding after polyp removal. No perforations or screening-related deaths occurred within 30 days after colonoscopy. During a median follow-up of 10 years, 259 cases of colorectal cancer were diagnosed in the invited group as compared with 622 cases in the usual-care group. In intention-to-screen analyses, the risk of colorectal cancer at 10 years was 0.98% in the invited group and 1.20% in the usual-care group, a risk reduction of 18% (risk ratio, 0.82; 95% confidence interval [CI], 0.70 to 0.93). The risk of death from colorectal cancer was 0.28% in the invited group and 0.31% in the usual-care group (risk ratio, 0.90; 95% CI, 0.64 to 1.16). The number needed to invite to undergo screening to prevent one case of colorectal cancer was 455 (95% CI, 270 to 1429). The risk of death from any cause was 11.03% in the invited group and 11.04% in the usual-care group (risk ratio, 0.99; 95% CI, 0.96 to 1.04).ConclusionsIn this randomized trial, the risk of colorectal cancer at 10 years was lower among participants who were invited to undergo screening colonoscopy than among those who were assigned to no screening. (Funded by the Research Council of Norway and others; NordICC ClinicalTrials.gov number, NCT00883792.)",Effect of Colonoscopy Screening on Risks of Colorectal Cancer and Related Death
62,4,4_cancer_licl_gsk3_mice,https://interestingengineering.com/health/moderna-merck-personalized-skin-cancer-vaccine,"Now, Merck has exercised its option by agreeing to pay a sum of $250 million to Moderna, which will pave the way for further development and commercialization of the vaccine, a Merck press release said.How will the vaccine work?Cancerous cells can have mutations — changes in their DNA sequence that make conventional treatment ineffective. By using a personalized cancer vaccine, the patient's immune system can be prepared to generate a mutation-specific anti-tumor response.Moderna's mRNA-4517/V940 is designed to generate Tcell responses based on the mutational signature of the tumor in the patient. T cells are a type of white blood cell that are tasked with defending the body in the event of an infection.Merck's KEYTRUDA is an anti-programmed death receptor-1 (PD-1) therapy that targets the PD-1 protein that cells can use to evade detection by T cells. Since both healthy as well as cancerous cells use this mechanism to evade T cell attack, using KEYTRUDA even puts healthy cells in the firing line.In combination with Moderna's vaccine, the treatment can aim to specifically target tumor cells.KEYNOTE-942 trialMerck's KEYTRUDA is currently being used in over 1,600 trials across a wide range of cancers. Through these trials, Merck is trying to understand the various factors that might affect the effectiveness of the treatment in patients.KEYNOTE 942 is one such trial studying the effectiveness of KEYTRUDA in high-risk melanoma patients. Currently, in Phase 2, the trial has enrolled 157 patients who have been randomized into two groups following the surgical removal of their tumors.",Moderna teams up with Merck for personalized vaccine against skin cancer
50,4,4_cancer_licl_gsk3_mice,https://gizmodo.com/herpes-virus-cancer-treatment-rp2-1849574410,"Scientists may be able to turn a long-time germ foe into a cancer-fighting ally, new research this week suggests. In preliminary data from a Phase I trial, a genetically modified version of the herpes virus has shown promise in treating difficult-to-eradicate tumors, with one patient having experienced a complete remission for 15 months so far. Much more research will be needed to confirm the treatment’s early success, however.The viral treatment is known as RP2 and is a genetically engineered strain of herpes simplex 1, the virus responsible for most cases of oral herpes in humans, as well as some cases of genital herpes. Developed by the company Replimune, RP2 is designed to work on two fronts. Injected directly into the tumor, the virus is supposed to selectively infect and kill certain cancer cells. But it also blocks the expression of a protein known as CTLA-4 produced by these cells, and it hijacks their machinery to produce another molecule called GM-CSF. The net result of these cellular changes is to weaken the cancer’s ability to hide from and fend off the immune system.In a Phase I trial conducted by scientists at The Institute of Cancer Research and The Royal Marsden NHS Foundation Trust in the UK, RP2 was given as the only treatment to nine patients with advanced cancers that failed to respond to other therapies; it was also given in combination with another immunotherapy drug to 30 patients. Three patients on RP2 alone appeared to respond to the treatment, meaning their cancers shrank or stopped growing, and seven patients on the combination therapy responded as well. One patient in particular, with a form of carcinoma along his salivary gland, has shown no signs of cancer for at least 15 months after treatment with RP2 alone. There were no life-threatening adverse events reported in the trial, with the most common symptoms post-treatment being fever, chills, and other flu-like illness.AdvertisementThe findings, presented this week at the 2022 European Society for Medical Oncology Congress (ESMO), are preliminary, since they’ve yet to be vetted through the formal peer review process. They’re also based on a very small sample size, meaning that any results should be taken with caution. But Phase I trials aren’t intended to show that a treatment is effective, only that it’s safe enough for humans to take. So the fact that some people with seemingly incurable cancers already appear to be responding to RP2, the team argues, is a very good sign that it can live up to its potential.“Our study shows that a genetically engineered, cancer-killing virus can deliver a one-two punch against tumors—directly destroying cancer cells from within while also calling in the immune system against them,” said lead author Kevin Harrington, professor of Biological Cancer Therapies at The Institute of Cancer Research, in a statement from the organization.Scientists have been hopeful about cancer-fighting viruses for a long time. But it’s only recently that this hope has finally been starting to pay off. In 2015, the first viral therapy was approved in the U.S. for certain advanced cases of melanoma. This May, scientists in California launched a Phase I clinical trial of their anticancer virus, called Vaxinia. Other companies are developing their own candidates, either alone or in combination with other treatments. And Replimune is developing two other candidates based on their modified herpes virus.While many experimental therapies ultimately fail to cross the finish line and reach the public, it’s possible at least some of these viruses could one day become a new standard cancer treatment.",A Cancer-Fighting Version of Herpes Shows Promise in Early Human Trial
358,4,4_cancer_licl_gsk3_mice,https://www.eurekalert.org/news-releases/967916,"A study at Karolinska Institutet in Sweden shows that the coronavirus variant BA.2.75.2, an Omicron sublineage, largely evades neutralizing antibodies in the blood and is resistant to several monoclonal antibody antiviral treatments. The findings, published in the journal The Lancet Infectious Diseases, suggest a risk of increased SARS-CoV-2 infections this winter, unless the new updated bivalent vaccines help to boost immunity in the population.“While antibody immunity is not completely gone, BA.2.75.2 exhibited far more dramatic resistance than variants we’ve previously studied, largely driven by two mutations in the receptor binding domain of the spike protein,” says the study’s corresponding author Ben Murrell, assistant professor at the Department of Microbiology, Tumor and Cell Biology, Karolinska Institutet.The study shows that antibodies in random serum samples from 75 blood donors in Stockholm were approximately only one-sixth as effective at neutralizing BA.2.75.2 compared with the now-dominant variant BA.5. The serum samples were collected at three time points: In November last year before the emergence of Omicron, in April after a large wave of infections in the country, and at the end of August to early September after the BA.5 variant became dominant.Only one of the clinically available monoclonal antibody treatments that were tested, bebtelovimab, was able to potently neutralize the new variant, according to the study. Monoclonal antibodies are used as antiviral treatments for people at high risk of developing severe COVID-19.BA.2.75.2 is a mutated version of another Omicron variant, BA.2.75. Since it was first discovered earlier this fall, it has spread to several countries but so far represents only a minority of registered cases.“We now know that this is just one of a constellation of emerging variants with similar mutations that will likely come to dominate in the near future,” Ben Murrell says, adding “we should expect infections to increase this winter.”Some questions remain. It is unclear whether these new variants will drive an increase in hospitalization rates. Also, while current vaccines have, in general, had a protective effect against severe disease for Omicron infections, there is not yet data showing the degree to which the updated COVID vaccines provide protection from these new variants. “We expect them to be beneficial, but we don’t yet know by how much,” Ben Murrell says.The study was conducted in collaboration with researchers at ETH Zürich in Switzerland and Imperial College London in the U.K.Funding was provided by SciLifeLab, the Erling-Persson Foundation, the European Union’s Horizon 2020 research and innovation programme. Daniel J. Sheward, Gunilla B. Karlsson Hedestam and Ben Murrell have intellectual property rights associated with antibodies that neutralize Omicron variants.Publication: “Omicron sublineage BA.2.75.2 exhibits extensive escape from neutralising antibodies.” Daniel J. Sheward, Changil Kim, Julian Fischbach, Kenta Sato, Sandra Muschiol, Roy A. Ehling, Niklas K. Björkström, Gunilla B. Karlsson Hedestam, Sai T. Reddy, Jan Albert, Thomas P. Peacock, Ben Murrell, The Lancet Infectious Diseases, correspondence, online Oct. 13, 2022, doi: 10.1016/S1473-3099(22)00663-6",New Omicron subvariant largely evades neutralizing antibodies
42,4,4_cancer_licl_gsk3_mice,https://english.elpais.com/science-tech/2022-10-31/gut-dwelling-bacterium-singled-out-as-the-possible-cause-of-colorectal-cancer.html,"A microbe that is common in the human intestine is suspected of playing a major role in the development of colorectal cancer, the second deadliest and third most common kind in the world, with two million diagnosed cases and one million deaths a year.A team of scientists from Yale University recently discovered in a group of volunteers that some strains of the bacterium Morganella morganii produce molecules called indolimines that are toxic for human DNA. In the laboratory, the researchers proved that these substances cause tumors in mice. The finding was published in the journal Science on October 28.A human being has more bacterial cells (38 trillion) than human cells (30 trillion). However, defecation can reverse the proportion in favor of human cells. Through this act, in which a third of the microbes in the colon is expelled, the person ceases to be numerically bacterial and becomes fully human. Most of these microorganisms are harmless, or even beneficial, but some can cause illness, explains Noah Palm, lead author of the study, and it is possible that the indolimines have an effect on colorectal cancer. However, much more work will be needed to prove that they are indeed the cause.More information Scientists transplant human neurons into rats and modify their behaviorThe lifetime risk of colorectal cancer is 1 in 23 in men and 1 in 25 in women, according to data from European tumor registries. The usual risk factors are age, smoking, alcohol consumption, obesity and a diet low in fruit and high in processed meats. Having an inflammatory bowel disease, such as ulcerative colitis or Crohn’s disease, increases the risk as well.Palm’s team developed a new technique that allows for the simultaneous study of a hundred types of microbes and their products. The researchers detected the (previously unknown) indolimines in strains of Morganella morganii in people with inflammatory diseases. However, even though there is a larger amount of this bacterium in patients with inflammatory bowel disease or colorectal cancer, it is also present in seemingly healthy people. Even the epithelial cells of the intestines of healthy individuals show some mutations that can be caused by toxins from these communities of microorganisms, such as indolimines, explains Palm.Morganella morganii, which measures one thousandth of a millimeter, is commonly found in water, soil and the intestines of mammals. It is usually a benign microbe, but it is also associated with urinary tract infections.Spanish biotechnologist Cayetano Pleguezuelos and his colleagues at the Hubrecht Institute in the Netherlands were the first to show the direct connection between the bacteria that live in the human digestive system and the genetic alterations that cause the development of cancer. The researchers saw that a specific strain of Escherichia coli produces a toxic molecule called colibactin which damages the DNA of human cells. This was confirmed in miniature versions of intestinal tissue generated in the laboratory. Their discovery was published in the journal Nature on February 27, 2020, while the human race had its attention on another microorganism: a coronavirus that was spreading around the world from China.Pleguezuelos applauds the new work, but suggests prudence. “Our intestinal microbiota is very complex, with many different species of bacteria, and among them there are mutualistic relationships, symbiosis, negative competition… and there are many other parameters. Bacteria can produce these toxic compounds in humans but, for some reason, they may not be able to reach the epithelial cells of the intestine and damage the DNA. These factors are not seen in experiments with mice,” he warns.The Spanish researcher believes that the new Yale University technique “opens the door to evaluating a large number of bacteria and their ability to damage DNA.” A person that weighs 70 kilos has about 46 kilos of human cells, according to a study carried out by a team from the Weizmann Institute of Science in Rehovot, Israel. The heaviest ones are muscle and fat cells. The 38 trillion bacteria only weigh about 200 grams, but they make up an extremely complex universe. “Their ability to carry out different enzymatic reactions is immense. And we don’t know most things,” says Pleguezuelos.The Spanish biotechnologist explains that every agent that damages human DNA causes a specific pattern of mutations called a mutational signature. Pleguezuelos and his colleagues identified the mutational signature of the harmful strains of the Escherichia coli and found this characteristic trace in more than 5% of the colorectal cancer patients that were analyzed, compared to 0.1% detected in other types of tumors. Of course, that figure must be taken with a grain of salt – pending further studies in other populations – but it gives an idea of the magnitude of the problem: 5% of the two million annual cases means 100,000 colorectal cancer patients with the mutational signature of these harmful strains of Escherichia coli.Doctor Palm points out that most cases of colorectal cancer occur in people who have no family history. Therefore, environmental factors, including the microbiome, play a key role in most colorectal cancer cases. However, he explains, it is still impossible to calculate the relative importance of the microbiome against other environmental factors.Although currently there are no specific treatments to prevent DNA damage induced by the microbiome, treatments could be developed to neutralize, or even eliminate, these toxin-producing microbes.",Gut-dwelling bacterium singled out as the possible cause of colorectal cancer
41,4,4_cancer_licl_gsk3_mice,https://elifesciences.org/articles/80315,"We then set out to understand what signaling cascade governs cell-in-cell structures. Initially, we sought to test if the observed results were cell fusion. Thus, we incubated tumor cells whose cytosols were labeled with either Wasabi or tdTomato with tumor-reactive T cells. Confocal analysis indicated that each cell type in the cell-in-cell formation maintained its cytoplasm, and no mix between colors was detected (Figure 6—figure supplement 1). Furthermore, long-term follow-up of cell dissemination from this structure indicated that each cell maintains its initial single labelling color (Figure 6—video 1 and Figure 6—figure supplement 1). To corroborate this, we also incubated tumor cells whose membrane, nucleus, and F-actin were labeled with different fluorophores. Similarly, each cell in this formation was separated and maintained the integrity of its original cell components (Figure 6A). Given the similarity of this formation to entosis, we used ROCK inhibitor, which is the key regulator of this process. Indeed, blocking ROCK almost completely prevented T cell-mediated cell-in-cell formation (Figure 6B, C). We then tested whether the molecular machinery reported to mediate tumor spontaneous entosis applies to govern the current cell-in-cell structure. However, we observed no increase or changes in the cellular localization of phosphorylated β catenin, E-cadherin, and phosphorylated integrin β1 (26) (Figure 6—figure supplement 1) suggesting other mediators promote this entosis. Furthermore, there was no reduction in cell-in-cell formation upon blocking of E- and N- cadherins, or inhibition of Wnt signaling (Figure 6D–E). In sharp contrast, disruption of actin filaments, blocking of mRNA synthesis or protein production completely abrogated tumor cells’ capacity to form a cell-in-cell formation, suggesting that the structure requires de novo synthesis of genes (Figure 6D–E).Figure 6 with 2 supplements with 2 supplements see all Download asset Open asset STAT3 and EGR1 signaling govern T cell-mediated cell-in-cell tumor formation. (A) Representative images of B16F10, co-expressing Lifeact-GFP and H2b-tdTomato or MyrPalm-tdTomato and H2b-GFP, following incubation with gp100-reactive T cells. (B) Mean percentage of cell-in-cell tumor formations in B16F10 following overnight incubation with gp100-reactive CD8+ T cells with or without ROCK inhibitor (n=3). (C) Representative images of cell-in-cell tumor formations in B16F10 following overnight incubation with gp100-reactive CD8+ T cells with or without ROCK inhibitor. (D) Mean percentage of cell-in-cell tumor formations in B16F10 cells following overnight incubation with specific inhibitors and reactive CD8+ T cells (n=4). (E) Representative images of B16F10 cells treated with inhibitors and incubated overnight with gp100-reactive CD8+ T cells. (F) Significantly increased genes in B16F10 cells incubated with T cell-derived granules (Lyso) or isolated directly from relapsed tumors (Tumor), compared to B16F10 control cells (WT) (Bottom) and relative expression of the top 25 genes (Top) (n=3). (G) STAT3 and EGR1 expression levels in B16F10 cells isolated directly from relapsed tumors (Tumor) and after incubation with T-cell-derived granules (Lyso) compared to B16F10 control cells (WT) (n=3) (H) Mean percentage and representative images of cell-in-cell tumor formations in B16F10 48 hours after transfection with STAT3-T2A-iRFP670, EGR1-T2A-GFP or both (n=3). (I) Mean percentage of apoptotic B16F10, transfected with STAT3-T2A-iRFP670, EGR1-T2A-GFP or both, following incubation with tumor reactive T cells (n=3). (J) Mean percentage of cell-in-cell tumor formations in B16F10, transfected with siRNA, following incubation with tumor reactive T cells or T cells secreted granules. (K–L) B16F10 tumor size in mice treated with gp100-reactive T cells (ACT) (K) or Dc adjuvant and anti-TRP1 antibodies (L) with or without Stattic (n=4). Orange arrowheads indicate Stattic treatments and black arrowheads indicate injection of immunotherapies. All experiments were repeated independently at least three times. Statistical significance was calculated using ANOVA with Tukey’s correction for multiple comparisons (**denotes p<0.01, *** denotes p<0.001, **** denotes p<0.0001). Error bars represent standard error. Scale bars = 20 μm. Figure 6—source data 1 Significantly elevated genes in both B16F10 cells incubated with T cell secreted granules and tumor cells sorted from treated animals, related to Figure 6. https://cdn.elifesciences.org/articles/80315/elife-80315-fig6-data1-v1.xlsx Download elife-80315-fig6-data1-v1.xlsx Figure 6—source data 2 STAT3 and EGR1 pathways regulate cell-in-cell tumor formation, related to Figure 6. https://cdn.elifesciences.org/articles/80315/elife-80315-fig6-data2-v1.xlsx Download elife-80315-fig6-data2-v1.xlsx Figure 6—source data 3 Log2 expression of genes related to cancer pathways in tumor cells sorted from treated animals compared to B16F10 WT, reated to Figure 6. https://cdn.elifesciences.org/articles/80315/elife-80315-fig6-data3-v1.xlsx Download elife-80315-fig6-data3-v1.xlsxTo assess what genes govern this formation, we compared the gene signature of untreated B16F10 cells, to cell-in cell formation induced following incubation with T-cell-derived secreted granules and to that of tumor cells sorted from in vivo five days after immunotherapy. Over 400 genes were increased in cell-in-cell formation induced by T cell secreted granules compared to untreated B16F10 cells, 215 of which were also upregulated by the tumor cells sorted from treated animals (Figure 6F, ). KEGG analysis further indicated that multiple signaling cascades, including the JAK/STAT3 axis and FGF-receptors downstream pathways, are enriched in cell-in-cell formation generated in vivo, with approximately 80 significantly elevated genes relating to these pathways (Figure 6—figure supplement 1, ). Indeed, EGR1 and STAT3 expression was significantly elevated in cell-in-cell tumors (Figure 6G). Both mice and human tumors incubated with reactive T cells had elevated their p-STAT3 levels, in comparison to untreated tumor cells (Figure 6—figure supplement 1), suggesting this mechanism is conserved across species. Since these results may also reflect adaptation to the tumor microenvironments, we next corroborate the necessity of these factors to cell-in-cell formation ex vivo. Overexpression of either STAT3 or EGR1 was sufficient to induce cell-in-cell formation without additional stimulation (Figure 6H). Furthermore, these cells were significantly more resistant to killing by CD8+ T cell, compared to sham transfected cells (Figure 6I). We also tested if inhibiting these genes would reduce cell-in-cell tumor formation. Inhibition of STAT3 and EGR1, but not MAPK3 and EGR2, significantly abrogated the capacity of tumor cells to form cell-in-cell structures upon incubation with IFNγ-stimulated T cells and with T cell secreted granules (Figure 6J and Figure 6—figure supplement 1). In order to integrate cell-in-cell inhibition to an in vivo therapy, we first tested the effect of small molecule inhibitors on tumor cell formation. We found that inhibition of STAT3, or EGR pathway, completely prevented cell-in-cell formation upon incubation with reactive T cells or T cell-secreted granules (Figure 6—figure supplement 1). Since blocking EGR1 also reduced T cell viability, we then set out to establish a treatment protocol that combines STAT3 inhibition (which also inhibits T cell activity) and immunotherapy. To this end, mice bearing palpable tumors were injected with Stattic or with PBS for two consecutive days. On the second day, we sub-lethally irradiated the mice and injected them with 5 × 106 gp100-reactive T cells and IL-2. In another model, we treated mice for two days with Stattic, followed by treatments with anti-CD40, TNFα, and anti-TRP1. Recurrent tumors were treated with the same regimen. In both models, injection of Stattic alone had no effect on tumor growth. We found, however, that injection of Stattic prior to administration of immunotherapy partially restored the responsiveness of tumors that re-occur following immunotherapies (Figure 6K–L). While tumor cell sensitization may be a result from multiple mechanisms, these results stress the benefit of combining immunotherapy with STAT3 inhibition.",Transient cell-in-cell formation underlies tumor relapse and resistance to immunotherapy
501,4,4_cancer_licl_gsk3_mice,https://www.technologynetworks.com/cancer-research/news/new-report-shows-decline-in-us-cancer-deaths-continues-367025,"The latest Annual Report to the Nation on the Status of Cancer has provided an update on the most recent statistics and trends in cancer cases and deaths in the United States. This year’s report, published in Cancer, also places focus on pancreatic cancer.Knowledge in the fight against cancerCancer is one of the leading causes of death worldwide. It is expected that the number of new cancer cases globally will grow to 27.5 million, with 16.3 million cancer deaths, by 2040. Breast, lung, colon and prostate cancers are the four most common cancers worldwide, accounting for approximately 40% of all new cases collectively.In the US, annual reports on the statistical trends in cancer cases have been published since 1998 as part of a collaboration between the Centers for Disease Control and Prevention (CDC), the American Cancer Society, the National Cancer Institute (NCI) and the North American Association of Central Cancer Registries (NAACCR).These reports examine a plethora of data, analyzing trends in cancer incidence, mortality and survival by cancer type, sex, age and racial/ethnic groups.The current report combined information about race and ethnicity to create five mutually exclusive groups: non‐Hispanic White (White), non‐Hispanic Black (Black), non‐Hispanic American Indian/Alaska Native (AI/AN), non‐Hispanic Asian/Pacific Islander (API) and Hispanic (of any race).Additionally, data for the various groups analyzed in the current report were collected at time points ranging from 2001–2019 – therefore, it is important to note that all data were collected before the COVID-19 pandemic.Cancer incidence ratesOverall incidence rates per 100,000 individuals were 497 for males and 431 for females from 2014–2018.API and Black males had the lowest and highest incidence rates among men, respectively, and API and AI/AN females also had the lowest and highest rates respectively for women.However, incidence varied across 18 of the most common cancer types included in this report:For males – incidence rates for three of these cancers increased (including pancreas and kidney), seven were stable (including prostate) and eight decreased (including lung and larynx).For females – incidence rates of seven cancers increased (including melanoma, liver and breast), four cancers were stable (including uterine) and seven decreased (including thyroid and ovary).Additionally, for breast cancer – the most common cancer among adolescents and young adults (15–39 years of age) – the incidence rate increased by 1.0% per year on average between 2010 and 2018.Mortality ratesCombining data from males and females showed that the decline in overall death rates from cancer steepened from 2001 to 2019, decreasing by 2.1% per year. Similar trends in overall death rates were also observed in each of the racial/ethnic groups analyzed in the report.Data from adolescents and young adults showed a decrease in death rates by 3% per year between 2001 and 2005, however, this rate of decline slowed to around 0.9% per year thereafter.Racial and ethnic disparitiesThe report also highlighted racial and ethnic disparities, in both the incidence rates (2014–2018) and death rates (2015–2019) across many cancer types, as summarized in the table below.Type of cancer Incidence and death rates Bladder cancer In White, Black, ApI and Hispanic males, incidence of bladder cancer decreased. Incidence increased among AI/ AN males. Uterine cancer Incidence rates remained stable in White females but increased in every other racial/ethnic group. Lung, breast and colon cancer Incidence rates decreased for females in all racial/ethnic groups. For AI/ AN females, death rates for breast cancer increased but remained stable for colon cancer. Breast cancer death rates remained stable among API females. Prostate cancer Death rates decreased for API, AI/AN and Hispanic males. Rates remained stable for Black and White males.Lisa C. Richardson MD, director of the CDC’s Division of Cancer Prevention and Control, elaborated on these findings in a press release: “Factors such as race, ethnicity and socioeconomic status should not play a role in people’s ability to be healthy or determine how long they live. [The] CDC works with its public health partners – within and outside the government – to address these disparities and advance health equity through a range of key initiatives, including programs, research and policy initiatives. We know that we can meet this challenge together and create an America where people are free of cancer.”A special focus on pancreatic cancerThe pancreas is an organ located behind the stomach which produces enzymes that help digest food, as well as hormones like insulin and glucagon that regulate our blood sugar levels.Cancer of the pancreas is commonly diagnosed once it is already in its advanced stages – early pancreatic cancers often have vague symptoms or even none at all. For these reasons, the combined five-year survival rate for pancreatic cancer is just 5–10%.In the current report, the researchers explain that diagnoses of pancreatic cancer account for around 3% of new cancer cases – however, it also accounts for 8% of cancer deaths.Nevertheless, the report also highlights significant steps forward for the survival rates of certain types of pancreatic cancer. Between 2001 and 2017, one-year relative survival for patients with neuroendocrine tumors increased from 65.9% to 84.2%, and there was also an increase from 24% to 36.7% for patients with adenocarcinomas.“Pancreatic cancer incidence and survival reflect both the underlying risk of disease as well as the difficulty of diagnosing pancreatic cancer at a treatable stage,” said Betsy A. Kohler, M.P.H, NAACCR executive director. “As advancements in screening technology and effective treatments for early-stage disease become available, we are hopeful for greater improvements in pancreatic cancer survival, which historically has been a particularly lethal cancer type.”",New Report Shows Decline in US Cancer Deaths Continues
503,4,4_cancer_licl_gsk3_mice,https://www.technologyreview.com/2022/09/12/1059266/us-trial-cancer-blood-tests-early-detection/,"Questions remain about how to interpret MCED test results. Only some blood tests can pinpoint which organ the cancer is actually in. Lab tests must be run on potentially cancerous tissue to confirm a diagnosis, but you can’t biopsy someone’s entire body. False positives remain an issue for the entire field of cancer screening, which, by design, involves sifting through mounds of healthy tests to find cancer. Galleri—the MCED furthest along the path to widespread use—incorrectly flagged 57 healthy blood samples as cancerous in the aforementioned study.There’s also a risk of jumping the gun—some cancers never become invasive or life-threatening, but early detection could prompt harsh treatment like chemotherapy. Some data suggests that less worrisome cancers actually show up in the bloodstream less, which could minimize that problem.The NIC trial will help determine how blood test results for cancer should be interpreted, and it should provide a standard approach to launching cancer screening studies as companies flood the field with new tests.“I don't think most companies tend to want to compare their tests head to head,” says Timothy Rebbeck, a professor of cancer prevention at Harvard. “It's expensive and difficult. So somebody else, a neutral party like the NCI, needs to.”Rebbeck thinks the blood tests the new trial will vet will prove most helpful in the cases of pancreatic, liver, and ovarian cancer, which kill often and have no other form of screening. Still, longer trials are needed to confirm whether the time bought by these blood tests saves lives.But Rebbeck is optimistic about the Cancer Moonshot’s ultimate goal: “It seems very realistic to me to think that we could reduce death by half,” he says.",The US is launching a trial for blood tests that promise to catch cancers earlier
518,4,4_cancer_licl_gsk3_mice,https://www.theguardian.com/society/2022/oct/16/vaccines-to-treat-cancer-possible-by-2030-say-biontech-founders?CMP=twt_gu#Echobox=1665943949,"Vaccines that target cancer could be available before the end of the decade, according to the husband and wife team behind one of the most successful Covid vaccines of the pandemic.Uğur Şahin and Özlem Türeci, who co-founded BioNTech, the German firm that partnered with Pfizer to manufacture a revolutionary mRNA Covid vaccine, said they had made breakthroughs that fuelled their optimism for cancer vaccines in the coming years.Speaking on the BBC’s Sunday with Laura Kuenssberg, Prof Türeci described how the mRNA technology at the heart of BioNTech’s Covid vaccine could be repurposed so that it primed the immune system to attack cancer cells instead of invading coronaviruses.Asked when cancer vaccines based on mRNA might be ready to use in patients, Prof Sahin said they could be available “before 2030”.An mRNA Covid vaccine works by ferrying the genetic instructions for essentially harmless spike proteins on the Covid virus into the body. The instructions are taken up by cells which churn out the spike protein. These proteins, or antigens, are then used as “wanted posters” – telling the immune system’s antibodies and other defences what to search for and attack.The same approach can be taken to prime the immune system to seek out and destroy cancer cells, said Türeci, BioNTech’s chief medical officer. Rather than carrying code that identifies viruses, the vaccine contains genetic instructions for cancer antigens – proteins that stud the surfaces of tumour cells.BioNTech was working on mRNA cancer vaccines before the pandemic struck but the firm pivoted to produce Covid vaccines in the face of the global emergency. The firm now has several cancer vaccines in clinical trials. Türeci said the development and success of the Pfizer/BioNTech vaccine, which is similar to the Moderna Covid shot, “gives back to our cancer work”.The German firm hopes to develop treatments for bowel cancer, melanoma and other cancer types, but substantial hurdles lie ahead. The cancer cells that make up tumours can be studded with a wide variety of different proteins, making it extremely difficult to make a vaccine that targets all of the cancer cells and no healthy tissues.Türeci told Kuenssberg that BioNTech had learned how to manufacture mRNA vaccines faster during the pandemic, and had a better understanding of how people’s immune systems responded to mRNA. The intense development and rapid rollout of the Covid shot had also helped medicines regulators work out how to approve the vaccines. “This will definitely accelerate also our cancer vaccine,” she added.But Türeci remained cautious about the work. “As scientists we are always hesitant to say we will have a cure for cancer,” she said. “We have a number of breakthroughs and we will continue to work on them.”In August, Moderna said it was suing BioNTech and its partner, the US pharmaceutical giant Pfizer, for patent infringement over the company’s Covid-19 vaccine.Asked about that, Sahin said: “Our innovations are original. We have spent 20 years of research in developing this type of treatment and of course we will fight for our intellectual property.”","Vaccines to treat cancer possible by 2030, say BioNTech founders"
141,4,4_cancer_licl_gsk3_mice,https://techcrunch.com/2016/11/18/syounika-online/,"As a parent, if your child breaks out with a rash, what do you do? You panic, you search the internet for what could possibly be wrong and the next thing you know is that you’re convinced your child is dying. Syounika Online is a company that’s trying to reduce this problem in the Japanese market — and they’re the winners of the TechCrunch Tokyo 2016 pitch competition.“A trip to a clinic takes a lot of time and can cost ¥6,000 ($60), but there are other challenges than time and money, too,” says Naoya Hashimoto, CEO and founder of Syounika Online. “More than 90 percent of clinic visits are not necessary, and that’s the main problem we are solving. At the clinic, there are per definition a lot of sick children, so if you bring your child to a clinic, they are subjected to infectious diseases.”The company solves the problem by charging parents (more accurately, parents’ insurance companies) ¥3,000 for a tele-consultation with a pediatric doctor. From the comfort of their own house, using a Skype call or the popular LINE app, parents can send pictures and describe the symptoms. If the issue needs to be escalated, they can bee-line it to the nearest emergency room.“Most of the time, there is nothing to worry about, and the parents just need to be reassured that everything is OK,” says Hashimoto.The company is using a B2B strategy for growth, encouraging large corporates to offer its services as a perk, or selling to insurance companies as a cost-saving measure.“Pediatrics is particularly well-suited to telemedicine,” Hashimoto explains. “Parents in Japan are sometimes shy about explaining what the exact problem is face-to-face. By using photos and text chat, the problem is reduced.”A big congratulations to Syounika for picking up the TechCrunch Tokyo Excellence award and the ¥1 million ($10,000) that goes with it — and best of luck with future growth and development!",Telemedicine company Syounika Online snags top prize at TechCrunch Tokyo
23,4,4_cancer_licl_gsk3_mice,https://arstechnica.com/science/2022/10/a-bold-effort-to-cure-hiv-using-crispr/,"In July, an HIV-positive man became the first volunteer in a clinical trial aimed at using Crispr gene editing to snip the AIDS-causing virus out of his cells. For an hour, he was hooked up to an IV bag that pumped the experimental treatment directly into his bloodstream. The one-time infusion is designed to carry the gene-editing tools to the man’s infected cells to clear the virus.Later this month, the volunteer will stop taking the antiretroviral drugs he’s been on to keep the virus at undetectable levels. Then, investigators will wait 12 weeks to see if the virus rebounds. If not, they’ll consider the experiment a success. “What we’re trying to do is return the cell to a near-normal state,” says Daniel Dornbusch, CEO of Excision BioTherapeutics, the San Francisco-based biotech company that’s running the trial.The HIV virus attacks immune cells in the body called CD4 cells and hijacks their machinery to make copies of itself. But some HIV-infected cells can go dormant—sometimes for years—and not actively produce new virus copies. These so-called reservoirs are a major barrier to curing HIV.Advertisement“HIV is a tough foe to fight because it’s able to insert itself into our own DNA, and it’s also able to become silent and reactivate at different points in a person’s life,” says Jonathan Li, a physician at Brigham and Women’s Hospital and HIV researcher at Harvard University who’s not involved with the Crispr trial. Figuring out how to target these reservoirs—and doing it without harming vital CD4 cells—has proven challenging, Li says.While antiretroviral drugs can halt viral replication and clear the virus from the blood, they can’t reach these reservoirs, so people have to take medication every day for the rest of their lives. But Excision BioTherapeutics is hoping that Crispr will remove HIV for good.Crispr is being used in several other studies to treat a handful of conditions that arise from genetic mutations. In those cases, scientists are using Crispr to edit peoples’ own cells. But for the HIV trial, Excision researchers are turning the gene-editing tool against the virus. The Crispr infusion contains gene-editing molecules that target two regions in the HIV genome important for viral replication. The virus can only reproduce if it’s fully intact, so Crispr disrupts that process by cutting out chunks of the genome.In 2019, researchers at Temple University and the University of Nebraska found that using Crispr to delete those regions eliminated HIV from the genomes of rats and mice. A year later, the Temple group also showed that the approach safely removed viral DNA from macaques with SIV, the monkey version of HIV.",A bold effort to cure HIVâusing Crispr
169,4,4_cancer_licl_gsk3_mice,https://techcrunch.com/2019/05/30/pitching-accuracy-rates-of-over-99-for-multiple-cancer-screens-thrive-launches-with-110-million/,"For more than 25 years the founders of Thrive Earlier Detection have been researching ways to improve the accuracy of liquid biopsy tests.The fruits of that labor from Dr. Bert Vogelstein, Dr. Kenneth Kinzler and Dr. Nickolas Papadopoulos — all professors and researchers at Johns Hopkins University — is CancerSEEK, a liquid biopsy test that has demonstrated specificity of over 99% in a retrospective study published by Science.By minimizing false positives in cancer screening tools and providing a test with proven accuracy, doctors can take treatment actions earlier, which can lead to better survival rates for cancer patients.Now, with FDA approval for its tests for pancreatic and ovarian cancer and a new study underway with a large healthcare provider, CancerSEEK is being rolled out to market through Thrive Earlier Detection with the help of a new $110 million round of funding.Thrive works by analyzing highly targeted sets of DNA and proteins in the blood to detect cancer.“Over the past 30 years we have made great strides in understanding cancer. Combining this knowledge with the latest in molecular testing technologies, our founders have developed a simple and affordable blood test for the detection of many cancers at relatively early stages,” said Christoph Lengauer, PhD, partner at Third Rock Ventures, and co-founder and chief innovation officer of Thrive, in a statement. “We envision a future where routine preventative care includes a blood test for cancer, just as patients are now routinely tested for early stages of heart disease. We know that if cancer is caught early enough, it can often be cured.”As part of its rollout, the company’s screening tool is being evaluated in DETECT, a study of 10,000 currently healthy individuals that’s being conducted in conjunction with the healthcare organization Geisinger. So far, 10,000 women between the ages of 65 and 75 without a history of cancer have been enrolled in the trial.“To be truly useful to patients, new medical technology must be developed with rigorous evidence and designed to be affordable and readily integrated into routine medical care,” Steven J. Kafka, PhD, partner at Third Rock Ventures and chief executive officer of Thrive, said in a statement. “With the help of experts and strategic partners, Thrive is launching today to advance a novel test for the earlier detection of multiple cancers, which we aim to augment with an integrated service that helps patients maneuver the often confusing path that follows a cancer diagnosis.”Third Rock Ventures actually led the Series A financing for Thrive, and comprise the bulk of the company’s executive team, while Kinzler and Papadopoulos — the researchers from Johns Hopkins who developed the technology — will have seats on the company’s board.Other investors in the round include Bill Maris’ Section 32 investment firm, Casdin Capital, Biomatics Capital, BlueCross BlueShield Venture Partners, Invus, Exact Sciences, Cowin Venture, Camden Partners, Gamma 3 LLC and others.According to Thrive, ovarian, pancreatic and liver cancers are difficult to detect because they can develop in pathways that aren’t always well understood.Using CancerSEEK, Thrive hopes to develop a blood-based test that can be used in routine medical care, with the goal of identifying multiple cancer types at earlier stages.The technology works by following genomic mutations in circulating tumor DNA (ctDNA) and cancer-associated protein markers in plasma to identify abnormalities that are common across multiple cancers. In a retrospective study published by Science in 2018, CancerSEEK was shown to perform with greater than 99% specificity and with sensitivities ranging from 69% to 98% for the detection of five cancer types — ovarian, liver, stomach, pancreas and esophageal, which the company says are cancers for which there are no screening tests available for average-risk individuals.Thrive’s research has attracted an all-star executive team in addition to Lengauer and Kafka from Third Rock. Former Goldman Sachs lead medical technology analyst Isaac Ro is joining the company as chief financial officer, and the company’s head of research is Isaac Kinde, a co-inventor of the CancerSEEK technology.It’s hard to overstate how transformative the Thrive test could prove to be. Having a blood-based diagnostic test for cancer prevalence and the ability to initiate treatment earlier radically improves the chances for surviving a cancer diagnosis.","Pitching accuracy rates of over 99% for multiple cancer screens, Thrive launches with $110 million"
561,4,4_cancer_licl_gsk3_mice,https://www.welcometothejungle.com/en/articles/dealing-with-grief-and-loss-at-work,"Experts warn that unresolved grief in the aftermath of the pandemic could have a severe impact on people’s physical and mental health and fester into a public health emergency.Employers are desperate to corral employees, many of them having suffered the loss of loved ones, back to work. But grief remains a taboo subject at work, and productivity-obsessed workplaces are often hostile toward the grieving.<Trigger warning: mentions suicide.>Suman* lost her mother to Covid at the peak of the pandemic’s second wave in India last year. Her father, who had several comorbidities, was also fighting the infection. Suman too developed post-Covid complications, including pain in her legs that made standing for long periods difficult.“My mother was everything to me,” she says. “When she fell ill, we had to turn our house into a mini-ICU since there was no bed available in any hospital. For days, I couldn’t sleep more than three hours.”Even as she struggled with grief and exhaustion, Suman returned to her workplace — a nonprofit working in human rights and social development.“It had been barely two weeks since my mother’s passing, but I found zero empathy from the organization,” she says. “Instead, I had to entertain demands of work travel. All attempts at explaining my situation as a grieving person, as someone with post-Covid symptoms, and as a carer to an elderly parent, were ignored.”The stress triggered the worst anxiety attack of her life. “I thought I was getting a heart attack. It was physically and mentally impossible for me to continue, and even though I was worried about the repercussions, I wanted to resign.”Suman found strong support in her colleagues, many of whom were her close friends. They were upset at how she had been treated. The organization tried to do damage control, and after talking to her friends, she eventually stayed on. But she had made up her mind that she would leave at the first opportunity.Earlier this year, she did just that. Her reporting boss, who had fought the management to get her some time off after her mother’s death — only to be told that “No one needs so much time to grieve” — also quit.Sadly, Suman’s experience is far from unique. Denial of grief and hostility towards the grieving is rampant in our workplaces. And yet we seldom hear about it: the stigma and the fear of adverse career consequences are too great (which explains why several people shared their stories with me anonymously.)It isn’t difficult to see why grief makes workplaces queasy: Grief is a reminder of loss. Work exists for profit. Grief courses through us in loopy, messy, unwieldy waves; it rises unpredictably and unleashes chaos. Work requires us to participate in a disciplined march towards ever greater productivity; it cannot tolerate chaos.With the worst of the pandemic seemingly behind us (or at least we are pretending it is), employers are desperate to corral employees back to business as usual. But what we aren’t talking about enough is what this return to work means for employees struggling with grief.For this piece, I spoke to workers across (white-collar) sectors about their experiences at work during periods of personal grief. Most of them repeated the same themes: how workplaces lack formal policies and safe spaces to talk about grief; how productivity-obsessed employers gaslight and retraumatize grieving employees; and how a few compassionate individuals are left to bear the burden of compensating for non-existing organizational support.These stories also contain powerful insights into what a work culture sensitive to grief could look like. (Hint: It’s not about grand gestures but doing the little things right.)A global crisisIn the pre-pandemic world, grief could be brushed aside as an individual’s problem. This pretense is now unviable. Each of the millions of Covid deaths is estimated to have left an average of nine close family members in grief, according to a 2020 study by the University of Southern California. To boot, pandemic grief is aggravated by a constellation of uniquely painful factors — such as the social isolation of the mourner, the unexpectedness of the death, and lack of closure for family members who couldn’t bid a dignified farewell to loved ones.It doesn’t bear reminding that the pandemic is just one source of grief in the world circa 2022. There is also climate grief; the grief caused by an increasingly fraught social and political climate that is especially harsh on marginalized populations; and the grief triggered by conflict and war. Indeed, scrolling through our newsfeeds, we flit from one grief to the next. Thanks to social media, we are hyper-connected to each other’s pain. We no longer have the time to honor each separate grief, and even less so to observe the five stages of grieving — denial, anger, bargaining, depression, and acceptance — that grief experts say can help us get closure.This festering grief could metastasize into a public health crisis. In the New York Times, Allison Gilbert, who has written extensively on the topic, lists a litany of grief-induced health problems, including poor sleep, higher blood pressure, depression, and anxiety.People who lose a spouse die earlier than their married peers, research has shown. Gilbert also points out that children who lose a parent may suffer lasting consequences, including lower grades and failing in school, as well as increased experimentation with drugs and alcohol.Neuroscientists claim there is such a thing as the ‘grief brain,’ characterized by an overload of loneliness, sadness, confusion, and other complex feelings. Grief makes it difficult for the brain to concentrate, recall memories, and process the perspective of another person.Our ability to cope with grief is also connected to privilege. For example, there’s a well-established relationship between poverty and depression.Gilbert also writes that “Although grief is a universal experience, it can contribute to lifelong racial inequality, as Black Americans experience the loss of loved ones far more frequently and earlier in life than white Americans, contributing to differences in mental and physical health outcomes.”While the health risks of profound grief are well established, questions remain as to whether certain forms of grief are in themselves an illness. According to psychiatry, grief that exceeds a reasonable timeline and is accompanied by ‘symptoms’ such as yearning for the deceased, emotional detachment, and avoidance of reminders of the reality of the death is not normal but a disorder. This is problematic, especially in the context of work.For one, grief doesn’t care about clocks and calendars. By labeling all prolonged grief a disorder, the mental healthcare system plays into the hands of employers who measure a person’s worth by their ‘resilience’ — their ability to quickly bounce back from crisis.“It’s going to be two years that I am grieving,” says medical oncologist Mansi Khanderia in a twitter thread about “prolonged grief disorder.” “It’s not even 1% less painful, only worse. It’s love with no place to go. The Indian society, urban or rural, [whether] in the personal or the professional zone, is [already] apathetic to the bereaved. How [much] worse could calling it a disorder make it?”What happens when unresolved grief collides with work?According to the World Economic Forum (WEF), up to 4.5% of the American workforce could be grieving the loss of a loved one. For the $20-trillion US economy, this means that up to $942 billion of productivity and business growth has been directly affected by grief. While it’s hard to find such data for lower-income countries, you can make an educated guess about the scale, considering people in these countries have been disproportionately impacted by the pandemic.For some people living through profound grief, work can bring welcome relief. When Akanksha Kapoor, a former colleague of mine, lost her father, she was granted only a two-week leave. “It was surprisingly helpful to get back [to work] though because I was in a different city and a different home, and had distraction through the day,” she says.However, there’s a fine line between using work as a distraction and burying yourself in work as an avoidance technique. Berlin-based writer Zahra Salha Uddin saw grief turn her father into a workaholic, the stress of which then caused a lot of illnesses such as heart disease and pancreatitis. After some recent losses, she found herself dealing with her own unprocessed grief from her childhood. “The pain made it too hard to use work as a coping mechanism,” she says.This is where an employer can play a critical role. A workplace attentive to grief will help reinforce healthy boundaries and make sure people get the time and space they need to heal. At the other end, a workplace where grief isn’t legitimized will add to the grievers’ trauma.In a paper on the relationship between grief and the workplace in the aftermath of the 2006-08 financial crisis, Mary Ann Hazen, professor in the College of Business Administration at the University of Detroit Mercy, writes that the behaviors expected of someone in the role of griever, such as expressions of sadness, anger, and exhaustion, are usually unacceptable in the role of employee. “Such expressions by someone in grief can be misunderstood if managers and other employees are uninformed about what to expect from someone grieving and how they might respond.”Long before the pandemic, Vivek Sharma paid a high price for this failure by his employer. Sharma is a graduate of the Indian Institute of Management, India’s premier business school. For 12 years, he worked in senior roles in the healthcare industry and lived the high-flying corporate life.Then, eight years ago, Sharma lost his only child, Amogh, to an undiagnosed illness.“My whole life was about my job,” he says. “I was proud of what I had achieved in my career, and I liked flaunting it. But that day, when I came back home after performing my son’s last rites, I wanted to burn it all down. None of it mattered anymore.”Still, Sharma joined work in two weeks. “I was conditioned to think that I couldn’t take too much time to grieve even my child’s death. But when I went back to the office, I got no support from my employer.”At a meeting with the top bosses of the company, Sharma felt lost. “A senior leader told me, ‘Vivek, why are you so low? You are a young man. You can simply produce another child.’ That comment broke me.”Sharma’s work friends tried to cheer him up, but their gesture came out all wrong. “All they could tell me was, ‘let’s go for a smoke,’ or ‘let’s go for tea.’ They thought I was stressed. I don’t blame them. Most people who have not experienced what I have can’t tell the difference between stress and intense grief.”Sharma and his wife Sweta attempted suicide twice, but somehow they survived. (Today they run a number of social entrepreneurship projects, including a matrimonial platform for people living with chronic illness, and a healthcare nonprofit. Sharma also has a podcast dedicated to suicide prevention.)“Employers don’t like to think about death,” Sharma says. “They simply don’t care about people in grief. They just want you to dust yourself off and get on with it.”For writer and independent consultant Dhruti Shah, the reckoning came after the death of a close relative.“Not one person in management asked me how I was,” Shah says. “It was very alienating. And there have been several occasions when I worked in newsrooms that I had to beg [for] time to go to funerals. When death is an important part of your culture and community, that is hard.”Who in an organization is to blame for such lapses? Typically, middle managers are the ones in closest contact with team members, who expect them to be their voice before the higher-ups. Except managers are themselves highly vulnerable to depression and burnout — sometimes even more so than the people they lead.And even the most well-meaning managers don’t necessarily have the power to effect meaningful cultural change. Often they fight a lonely battle without any backing from the top.Take the case of Aditi*, a journalist from Delhi, who found herself bedridden with depression during the second wave of the pandemic. At one point, she grew paranoid about her poor performance at work and was set to resign. She credits her recovery to a compassionate boss.“Not only did my boss grant me leave without any questions, he also took an interest in my treatment and encouraged me to do whatever I needed to feel better. It was such a huge relief.”Aditi says her boss extended the same courtesy to at least one other teammate — but believes it was a purely personal initiative. “Even in the worst days of the pandemic, when journalists put their own lives at risk every day, we didn’t get a single email from the management,” she says.What could a healing workplace look like?What would it take for our workplaces to forge a different relationship with grief? Can work, in fact, help us heal by offering a community of care when we most need it? What does a healing workplace look like?For starters, these are workplaces where the wellbeing of employees is prioritized by the top management, backed by clear policies, and not left to ad hoc decisions by a few thoughtful managers. They recognize the unique path that grief charts in every individual. Therefore, these workplaces don’t restrict themselves to ‘one size fits all’ solutions, such as the ‘mental health’ and ‘mindfulness’ apps that have become a staple of employee wellbeing programs. Instead, they throw their might behind deep structural reforms, such as creating safe spaces where employees can be vulnerable and share authentic stories, without a business goal attached to it.Angi Yoder-Maina leads Kenya-based Green String Network, which uses peacebuilding techniques to help communities affected by shared trauma. One of her current projects is with a large international humanitarian organization in Nigeria. “They usually only prioritized output,” Yoder-Maina says. “But now their top boss is prioritizing his team of over 100 and investing in setting up intentional peer support structures and systems. They believe that by providing social support, especially for teams working in a high-stress environment, they will become a better [organization],” she says.Yoder-Maina is currently training a group of employees — designated “Circle Keepers” — on peer support, who will in turn handhold other employees. In a recent session, she witnessed the team rally around each other and validate each other’s loss and sadness.“It gave me so much hope,” she says. “I wish other organizations would breathe into this.”The Indian IT giant Wipro started a peer support programme called Mitr (friend in Hindi) way back in 2002. As part of the program, employees volunteer to offer confidential and anonymous telephone-based counseling to colleagues in emotional distress. Becoming a Mitr volunteer requires rigorous training and is a highly competitive process. Volunteers cherish the responsibility and prestige that comes with it, reflected in the low attrition rates from the program, which have delivered a 40% reduction in costs that would have otherwise gone to external counselors.That last point dispels a common myth in the workplace — that investing in a compassionate culture is expensive. Sure, if you are a company with the deep pockets of Google, you can pay out 50% of a deceased employee’s salary to their significant other for a decade, plus a monthly subsidy for each school-age child, regardless of the employee’s role or tenure.But as I repeatedly heard over the course of reporting on this story, what employees living through acute personal crises value the most from their employers is simple humanity.Norah was 24 and less than three months into her job as a content writer in an Indian IT firm when her father passed away from a sudden heart attack.“The HR team quickly booked a cab for me to get to the hospital,” she says. “Two senior managers from the company came home that evening to show support. The next few weeks are blurry, but I remember they encouraged me to take as much time as I needed. There was no pressure to get back to work.”Norah’s eventual return to work was rough. Sometimes she would have a fit of panic and rush out of the office. “When I was not working, I spent a lot of time writing about what I was going through and attempting to process it,” she says. “None of this was singled out or remarked upon. It was a low-key environment, and while I struggled, it could have been much worse elsewhere.”During the pandemic, Vivek Sharma’s assistant lost her mother. He called all his team members and spoke to them about how they could best support her and ease her workload, and even what kind of language they should use with her so that she doesn’t get triggered.“Our priority was to make her feel safe and comforted,” he says. “For instance, for a few weeks she wanted to spend more time in the office because it was painful for her to be at home without her mother. We don’t work Saturdays but we started keeping the office open so she could use it if she wanted to.”“I don’t feel we have done anything extraordinary,” adds Sharma. “I know how horrible it is to not get the most basic support from your employer when you are at your most vulnerable.”*Some names have been changed to protect the speakers’ privacy.Photo: Welcome to the JungleFollow Welcome to the Jungle on Facebook, LinkedIn, and Instagram, and subscribe to our newsletter to get our latest articles every day!","Forget the Great Resignation, the real crisis in the workplace is grief"
8,4,4_cancer_licl_gsk3_mice,https://ajph.aphapublications.org/doi/10.2105/AJPH.2022.307050,"Objectives. To characterize COVID-19 vaccine uptake and hesitancy among US nurses.Methods. We surveyed nurses in 3 national cohorts during spring 2021. Participants who indicated that they did not plan to receive or were unsure whether they planned to receive the vaccine were considered vaccine hesitant.Results. Among 32 426 female current and former nurses, 93% had been or planned to be vaccinated. After adjustment for age, race/ethnicity, and occupational variables, vaccine hesitancy was associated with lower education, living in the South, and working in a group care or home health setting. Those who experienced COVID-19 deaths and those reporting personal or household vulnerability to COVID-19 were less likely to be hesitant. Having contracted COVID-19 doubled the risk of vaccine hesitancy (95% confidence interval [CI] = 1.85, 2.53). Reasons for hesitancy that were common among nurses who did not plan to receive the vaccine were religion/ethics, belief that the vaccine was ineffective, and lack of concern about COVID-19; those who were unsure often cited concerns regarding side effects or medical reasons or reported that they had had COVID-19.Conclusions. Vaccine hesitancy was unusual and stemmed from specific concerns.Public Health Implications. Targeted messaging and outreach might reduce vaccine hesitancy. (Am J Public Health. 2022;112(11):1620–1629. https://doi.org/10.2105/AJPH.2022.307050)","COVID-19 Vaccine Uptake and Factors Affecting Hesitancy Among US Nurses, MarchâJune 2021"
6,4,4_cancer_licl_gsk3_mice,https://abcnews.go.com/Health/half-covid-survivors-fully-recovered-months-study/story?id=91444148,"The most common symptoms were tiredness, headache and muscle aches.Nearly half of COVID-19 survivors may have symptoms of long COVID months after they were first infected, a new study suggests.Researchers from across Scotland looked at more than 33,000 patients over the age of 16 with a confirmed PCR test for COVID-19 in the past and tracked their symptoms.Results, published in the journal Nature Communications Wednesday, found that six months later, of the more than 31,000 patients who had had symptomatic COVID, 6% reported not having recovered at all. An additional 42% felt they were only partially recovered.Patients who reported no recovery were more likely to be women, to have been hospitalized when they had COVID, and to have multiple underlying conditions.Long COVID symptoms among symptomatic vs. asymptomatic patients Nature CommunicationsWhen the team looked at symptoms, they found the most common was tiredness, followed by headache, muscles aches, joint pain and difficulty breathing, respectively.Patients with an asymptomatic infection were not at increased risk of experiencing symptoms months later.What's more, having received at least one dose of a COVID-19 vaccine prior to infection reduced the risk of some symptoms including change in taste and/or smell, poor appetite, confusion and difficulty concentrating.""Our study is important because it adds to our understanding of long-COVID in the general population, not just in those people who need to be admitted to hospital with COVID-19,"" lead author Jill Pell, a professor of public health at the University of Glasgow, said in a statement.""By comparing symptoms with those uninfected, we were able to distinguish between health problems that are due to COVID-19 and health problems that would have happened anyway,"" the statement continued.Long COVID occurs when patients who have cleared the active infection still have symptoms lasting more than four weeks after recovering. In some cases, these symptoms can persist for months or even years.Patients can experience a variety of lingering symptoms including fatigue, difficulty breathing, headaches, brain fog, joint and muscle pain, and continued loss of taste and smell, according to the Centers for Disease Control and Prevention.The authors mentioned some limitations including that most of the participants were white because the study was conducted in Scotland, which has a 96% white population.""Therefore, it is important that ethnic-specific outcomes are reported by other long-COVID studies with more ethnically diverse populations,"" the authors wrote.Additionally, some of the common symptoms were also reported among a control group who had never tested positive for COVID. The symptoms that were most strongly associated with COVID infection were breathlessness, chest pain, palpitations and loss of taste and smell.",Nearly half of COVID survivors haven't fully recovered 6 months later: Study
5,4,4_cancer_licl_gsk3_mice,https://abc7news.com/covid-long-haul-study-aging-faster-after-organs-kidney/12340213/,"""You can start thinking about getting COVID as almost as an accelerant to aging,"" Dr. Ziyad Al-Aly said.Study conducted from millions of Americans who previously had COVID showed that several of their organs aged faster by three to four years.SAN FRANCISCO -- After over two and a half years of COVID research, scientists are seeing the first data points that prove a dramatic change in human organs after a COVID infection.""You can start thinking about getting COVID as almost as an accelerant to aging. The viral infection accelerates the aging process in people,"" said Dr. Ziyad Al-Aly, director of the Clinical Epidemiology Center at Washington University in St. Louis and the chief of research and education service at Veterans Affairs St. Louis Health Care System.Dr. Al-Aly gathered data from millions of people across the country. Their studies on kidney outcomes in long COVID, long COVID in the brain and long COVID in the heart had similar patterns.RELATED: Return to masking? It's possible, if we see COVID surge this fall, Bay Area health officials sayAll pointing to multiple human organs aging faster after COVID. The majority happening among people who were hospitalized but also some with mild COVID symptoms.""Almost by three to four years in the span of just one,"" said Dr. Al-Aly and added, ""What we have seen is that people are losing about three to four percent kidney function in the year that follows that infection. That usually happens with aging. Three to four years of aging.""We took these findings to Dr. Michael Peluso, infectious disease specialist at UCSF. His team was one of the first in the country to begin long COVID research in April of 2020.""Dr. Al-Aly group at the VA in St. Louis has been really important in trying to frame the issues of what people experience after they have COVID. Particularly the effects on the organ system after somebody has COVID,"" said Dr. Peluso and added, ""Now, what we are trying to do is actually figure out what is the biology of what causes those long term effects.""RELATED: New research sheds light on an emerging parallel COVID epidemicDr. Peluso said his team has an idea of why some organs may be experiencing aging or injury after COVID.""Some of the theories for what may causing long COVID symptoms include persistence of the virus, so instead of the virus coming and going - it sticks around, inflammation, auto-immune problems. Changes in the microbiome. The good bacteria that are in our bodies,"" said Peluso.Even though more years of data are necessary, Dr. Al Aly believes this increased aging process will eventually stop.""My hunch from the data and also my hope that this would really eventually flatten out and there are some early indications that this really may be the case that the risk or the kidney function decline really flattens out with time,"" said Dr. Al-Aly.RELATED STORIES & VIDEOS:If you're on the ABC7 News app, click here to watch live","If you had COVID, several of your organs could be aging 3-4 years faster: Study"
484,4,4_cancer_licl_gsk3_mice,https://www.science.org/content/article/evidence-suggests-pandemic-came-nature-not-lab-panel-says,"The acrimonious debate over the origins of the COVID-19 pandemic flared up again this week with a report from an expert panel concluding that SARS-CoV-2 likely spread naturally in a zoonotic jump from an animal to humans—without help from a lab.“Our paper recognizes that there are different possible origins, but the evidence towards zoonosis is overwhelming,” says co-author Danielle Anderson, a virologist at the University of Melbourne. The report, which includes an analysis that found the peer-reviewed literature overwhelmingly supports the zoonotic hypotheses, appeared in the Proceedings of the National Academy of Sciences ( PNAS ) on 10 October.The panel’s own history reflects the intensity of the debate. Originally convened as a task force of the Lancet COVID-19 Commission, a wide-reaching effort to derive lessons from the pandemic, it was disbanded by Columbia University economist Jeffrey Sachs, the commission’s chair. Sachs alleged that several members had conflicts of interest that would bias them against the lab-origin hypothesis.Sachs and other researchers who contend the scientific community has too blithely dismissed the lab-leak possibility aren’t persuaded by the new analysis. The task force’s literature analysis was a good idea, says Jesse Bloom, a virologist at the Fred Hutchinson Cancer Center who has pushed for more investigations of the lab-leak hypothesis. But he says the zoonosis proponents haven’t provided much new data. “What we’ve seen is mostly reanalysis and reinterpretation of existing evidence.”Sachs adds that the task force report does not “systematically address” the possible research-related origins of the pandemic. And he contends there was a “rush to judgment” by the National Institutes of Health and “a small group of virologists” to dismiss the possible research-related origins of the pandemic. In September, The Lancet published a report from his commission that gave equal weight to both hypotheses.When Sachs launched the Lancet origin task force in December 2020, he tapped conservation biologist Peter Daszak to lead it. Daszak heads the nonprofit EcoHealth Alliance, which has funded work on bat coronaviruses at the Wuhan Institute of Virology (WIV). Because the first COVID-19 cases were reported in Wuhan, China, some scientists suspect research conducted at WIV led to the spread of SARS-CoV-2. Sachs came to believe Daszak and other task force members who had links to WIV and the EcoHealth Alliance could not assess that possibility fairly and should step down. After fierce infighting over issues including transparency and access to information, Sachs pulled the plug on the task force in September 2021.But the members continued to meet. “We had a distinguished, diverse group of experts across a whole range of disciplines, and we thought we had something to offer whether or not we were part of the commission,” says Gerald Keusch, an infectious disease specialist at Boston University.In assembling its report, the task force interviewed researchers who have different perspectives on the pandemic’s origin. It also reviewed the history of RNA viruses, like SARS-CoV-2, that naturally have made zoonotic jumps and triggered outbreaks. And it combed through the scientific literature for papers addressing COVID-19’s origins.The final product overlaps with the wider ranging Lancet commission report. Both stress the need to address how forces such as growing deforestation and the illicit trade of wild animals increase the risk of viral spillovers. Both emphasize the risk of lax safety measures in labs, as well as in field studies that hunt for pathogens.But the two reports part ways when it comes to the origin of the pandemic.The PNAS authors say their literature search revealed “considerable scientific peer-reviewed evidence” that SARS-CoV-2 moved from bats to other wildlife, then to people in the wildlife trade, finally causing an outbreak at the Huanan Seafood Market in Wuhan. In contrast, they say, relatively few peer-reviewed studies back the lab-leak idea, and Daszak notes much of the argument has been advanced through opinion pieces. “The most parsimonious hypothesis is that the pandemic emerged through the animal market system,” Daszak says. “And while the evidence could be a lot better, it’s fairly good.”He also agrees, however, that the question of how the pandemic began has yet to be answered conclusively. No one has independently audited how viruses were handled at WIV, for example. And no reports exist of scientists testing mammals at animal farms in China that supplied the Huanan market or the humans who handled them. “Absent those two critical pieces of data, you’re left with what’s available,” Daszak says. “What we concluded is that the weight and quality of the evidence is far higher on the natural origins idea.”The PNAS perspective also stands apart for its recommendations on how to improve warnings that a pandemic is brewing. In a section called “looking forward,” the authors promote “smart surveillance” that would concentrate on transmission hot spots where humans and wild animals frequently come in contact, using cutting-edge technologies to look for novel viruses. Assays now exist that can measure antibodies to an enormous range of viruses, offering evidence of infections that occurred in the past. Wastewater sampling could use new polymerase chain reaction techniques to fish for both known and novel pathogens. And researchers could sample the air on public transport and manure pits on farms.“For nearly 3 years we’ve been running in circles about different lab-leak scenarios, and nothing has really added to this hypothesis,” says co-author Isabella Eckerle, a virologist at the University of Geneva. “We have missed the chance to say … what can we do better the next time?”Co-author Linda Saif, a swine coronavirus researcher at Ohio State University, Wooster, says studies of human and animal viral infections remain too siloed and must be combined. “There’s no source of funding for those at this time.”","Evidence suggests pandemic came from nature, not a lab, panel says"
76,4,4_cancer_licl_gsk3_mice,https://ktla.com/news/money-smart/people-return-to-offices-productivity-plunges/amp/,"One of the most interesting things about the pandemic, at least from an employment perspective, is that productivity didn’t suffer as a result of remote work.In many cases, employees became even more productive while working from home, either because they were happier or because they were making an extra effort to impress far-away bosses.Now comes word from the Bureau of Labor Statistics that productivity plunged during the first half of 2022, down by the sharpest rate since the 1940s.Economists are trying to come up with explanations for the decline, and to understand what this means for post-pandemic changes to the workplace, including hybrid schedules that allow more time out of the office.My hunch is that many workers are letting their employers know that things are different now.First, there’s the trend of “quiet quitting,” by which some employees throttle back on their work tempo to express a recalibration of their work-life balance. That is, they’re reprioritizing the importance of work in their lives.More important, I suspect, productivity is falling because many people simply came away from the pandemic feeling that their employer wasn’t there for them in a meaningful fashion as Mr. Corona pushed us all around.And now that many businesses are once again requiring a physical presence in the office, some employees are consciously or subconsciously expressing their dissatisfaction by taking their foot off the gas pedal, as it were.People who previously went above and beyond at work are now giving little more than the expected output.People who previously gave only the expected output are now just going through the motions.Managers will say this is a problem with their workforce. I see it more as a management issue.If managers can’t motivate their staffs with positive, satisfying workplaces, they’ll see productivity fall by the wayside.Some managers are even responding to the changes by imposing productivity-tracking software on employees — programs that monitor how long you’re at your desk and how busy you are at the keyboard.I may not know how precisely to create a positive, satisfying workplace. But I know that spying on employees isn’t a great start.So, yeah, productivity is down as wary workers are returning to offices.It’s not rocket science.","People return to offices, productivity plunges"
474,4,4_cancer_licl_gsk3_mice,https://www.reuters.com/business/healthcare-pharmaceuticals/india-develops-its-first-cervical-cancer-vaccine-2022-09-01/,"NEW DELHI, Sept 1 (Reuters) - The Serum Institute of India (SII), the world's biggest vaccine maker, has developed the country's first cervical cancer shot that will hit the market soon, the company and the government said on Thursday.Cervical cancer is the fourth most common cancer among women globally, with an estimated 604,000 new cases and 342,000 deaths in 2020, according to the World Health Organization. About 90% of the new cases and deaths worldwide occurred in low- and middle-income countries that year.Two human papillomavirus (HPV) types, 16 and 18, are responsible for at least 70% of cervical cancers, and India's Department of Biotechnology said the Indian vaccine would work on HPV types 16 and 18, as well as 6 and 11.Merck & Co (MRK.N) and GSK Plc (GSK.L) are the main makers of HPV vaccines.""The indigenously developed vaccine will make our country self-sufficient in curbing female mortality caused by cervical cancer,"" SII Chief Executive Adar Poonawalla said in a statement.He told reporters the vaccine would be out for sale in a few months, first for the Indian market and then the world. It may be priced between 200 rupees and 400 rupees ($2.51-$5.03) and the company will aim to produce about 200 million doses in two years, Reuters partner ANI cited Poonawalla as saying.The vaccine will be administered via injection in two doses among 9-14 year olds and in three doses for those between 15 and 26.($1 = 79.5275 Indian rupees)Reporting by Krishna N. Das, Editing by William MacleanOur Standards: The Thomson Reuters Trust Principles.",India develops its first cervical cancer vaccine
356,4,4_cancer_licl_gsk3_mice,https://www.eurekalert.org/news-releases/967801,"HERSHEY, Pa. — The risk of developing myocarditis — or inflammation of the heart muscle — is seven times higher with a COVID-19 infection than with the COVID-19 vaccine, according to a recent study by Penn State College of Medicine scientists. Patients with myocarditis can experience chest pains, shortness of breath or an irregular heartbeat. In severe cases, the inflammation can lead to heart failure and death.“Our findings show that the risk of myocarditis from being infected by COVID-19 is far greater than from getting the vaccine,” said Dr. Navya Voleti, a resident physician in the Department of Medicine at Penn State Health Milton S. Hershey Medical Center. “Moving forward, it will be important to monitor the potential long-term effects in those who develop myocarditis.”Myocarditis is one of the complications of SARS-CoV-2 infection. Although vaccines have been shown to reduce severe COVID-19 symptoms, heart complications have been associated with mRNA COVID-19 vaccination — particularly myocarditis in teenage boys. However, the relative risk of myocarditis due to vaccines and infections had not been well characterized in large studies.The Penn State team conducted the largest study to date on the risk of developing myocarditis as a result of having the coronavirus vs. experiencing inflammation following COVID-19 vaccination. The researchers compared patients with COVID-19 — vaccinated and unvaccinated — to those without the virus. They found the risk of myocarditis was 15 times higher in COVID-19 patients, regardless of vaccination status, compared to individuals who did not contract the virus.Next, the researchers separately compared the rates of myocarditis in those who received the vaccines to those in unvaccinated individuals. According to the findings, the rates of myocarditis in people who were vaccinated against COVID-19 were only twofold higher than in unvaccinated people.Based on all the findings, the researchers concluded that the risk of myocarditis due to COVID-19 was seven times higher than the risk related to the vaccines.Investigators conducted a systematic review and meta-analysis of 22 studies published worldwide from December 2019 through May 2022. The studies included nearly 58 million patients who reported cardiac complications and belonged to one of two groups: the 55.5 million who were vaccinated against COVID-19 compared to those who were not vaccinated (vaccination group), and the 2.5 million who contracted the virus compared to those who did not contract the virus (COVID-19 group).In the vaccination group, the researchers separately compared the risk of myocarditis for various COVID-19 vaccines, including mRNA (Pfizer, Moderna), Novavax, AstraZeneca, and Johnson and Johnson. The median age of the study population was 49 years; 49% were men; and the median follow-up time after infection or COVID-19 vaccination was 28 days.The researchers found that among those diagnosed with myocarditis after receiving the vaccine or having COVID-19, the majority (61%) were men. Of patients diagnosed with myocarditis in both vaccination and COVID-19 groups, 1.07% were hospitalized and 0.015% died.“COVID-19 infection and the related vaccines both pose a risk for myocarditis. However, the relative risk of heart inflammation induced by COVID-19 infection is substantially greater than the risk posed by the vaccines,” said Dr. Paddy Ssentongo, a resident physician in the Department of Medicine at Penn State Health Milton S. Hershey Medical Center and the lead author of the study. “We hope our findings will help mitigate vaccine hesitancy and increase vaccine uptake.”Surya Reddy from Osmania Medical College also contributed to this research.The researchers declare no conflicts of interest or specific funding for this research.Read the full study in Frontiers in Cardiovascular Medicine.",Myocarditis seven times more likely with COVID-19 than vaccines
397,4,4_cancer_licl_gsk3_mice,https://www.jbc.org/article/S0021-9258%2822%2901012-2/fulltext,"Adaptive thermogenesis is the cellular process where, in response to prolonged cold exposure or caloric excess, energy expenditure and heat production are increased resulting in a greater combustion of metabolic substrates (). In mammals, brown adipose tissue (BAT)/beige adipose tissue and skeletal muscle are the primary sites for adaptive thermogenesis. Beige adipose tissue and BAT are characterized by an abundance of mitochondria and high uncoupling protein 1 (UCP1) content, which acts to uncouple the proton gradient from ATP synthesis, ultimately dissipating the stored energy in the form of heat. In skeletal muscle, the sarco(endo)plasmic reticulum Ca-ATPase (SERCA) pump is a major energy (ATP) consumer and is a known mediator of muscle-based thermogenesis (). Specifically, the SERCA pump catalyzes the active transport of Cafrom the cytosol to the sarcoplasmic reticulum (SR), a process that is important for muscle relaxation. Based on its structure, SERCA has two Ca-binding sites and one ATP-binding site (), which suggests that under optimal conditions, SERCA can transport two Caions for every one ATP hydrolyzed (). Although in vivo, the presence of SERCA uncouplers such as sarcolipin (SLN) and the newly identified neuronatin (NNAT), as well as changes in membrane lipid composition or ryanodine receptor (RYR) Caleak, makes the SERCA pump much less efficient, thereby lowering the apparent coupling ratio and increasing energy expenditure and heat release ().Both adipose-based (via UCP1) and muscle-based (via SERCA uncoupling) thermogenesis have shown promise in the fight against obesity—a metabolic disorder caused by a chronic imbalance between excessive caloric intake and a deficit in energy expenditure (). Thus, discovering novel cellular targets that could promote both adipose-based and muscle-based thermogenesis would likely aid in offsetting the onset or progression of obesity. First recognized for its role in the regulation of glycogen synthase (), glycogen synthase kinase 3 (GSK3) has been recently identified as a significant contributor to numerous disease states including obesity and diabetes (). GSK3 is a constitutively active serine/threonine kinase, which exists in two isoforms: GSK3α and GSK3β, though the latter is most dominant in adipose and muscle tissues (). It has been suggested that the overactivation of GSK3 may contribute to the onset of obesity. For example, GSK3β overexpression in mice resulted in increased body mass and adiposity along with impaired glucose tolerance and insulin sensitivity (). Mechanistically, the association between GSK3 activity and obesity may in part be mediated through adaptive thermogenesis. Recently, it has been demonstrated that GSK3 negatively regulates BAT–based thermogenesis, where the expression of thermogenic genes, including UCP1, are suppressed by GSK3 activity (). However, analysis was restricted to the effects of GSK3 on brown adipocytes, and the role of GSK3 on regulating browning of white adipose tissue (WAT) remains unknown. Furthermore, the role of GSK3 in regulating muscle-based thermogenesis and SERCA uncoupling remains unknown. This is important as it was muscle-specific overexpression of GSK3β in mice that resulted in increased adiposity, even under chow-fed conditions ().Lithium is a well-known GSK3 inhibitor that has been commonly used in the treatment of bipolar disorder (). Although higher doses (i.e., ≥1.0 mM serum concentration) taken over a prolonged period have been associated with weight gain and obesity (), low-dose lithium supplementation produces the opposite effect (). We have recently shown that trace levels of lithium found naturally in water negatively correlates with the prevalence of obesity (). In mice, others have shown that low-dose lithium supplementation (provided as lithium chloride [LiCl], 10 mg/kg/day) for 14 weeks attenuated high-fat diet (HFD)–induced obesity and atherosclerosis (). In skeletal muscle specifically, we have shown that male mice fed this same dose for 6 weeks have reduced GSK3 activation, leading to increased calcineurin activation, peroxisome proliferator–activated receptor-gamma coactivator 1-alpha (PGC-1α) content and fatigue resistance (). Calcineurin is a Ca-dependent phosphatase that promotes PGC-1α expression and fatigue resistance (). Furthermore, recent studies have shown that calcineurin stimulates both muscle- and adipose-based thermogenesis by increasing SLN and UCP1 in muscle and adipose tissue, respectively (). In muscle, calcineurin is well known to be counteracted by GSK3 (). Thus, in the present study, we used a cell and rodent model approach to determine whether GSK3 inhibition via LiCl supplementation would enhance muscle-based and adipose-based thermogenic mechanisms: SERCA uncoupling and UCP1 expression.Glucose tolerance test (GTT) and insulin tolerance test (ITT) in control and LiCl-treated control (CON), high-fat diet (HFD), and HFD-lithium (HFD-Li)-fed mice. A and B, GTT curves and corresponding area under the curve (AUC) analysis. C and D, ITT curves and corresponding AUC analysis. For A and C: ∗p < 005, ∗∗p < 0.01, ∗∗∗p < 0.001, and ∗∗∗∗p < 0.0001 for HFD versus chow; and # p < 005, ## p < 0.01, ### p < 0.001, and #### p < 0.0001 for HFD-Li versus chow with a two-way ANOVA and a Tukey’s post hoc test. For B and D: ∗∗∗p < 0.001 and ∗∗∗∗p < 0.0001 with a one-way ANOVA and a Tukey’s post hoc test (n = 22–24 per group). LiCl, lithium chloride.Figure 11 Glucose tolerance test (GTT) and insulin tolerance test (ITT) in control and LiCl-treated control (CON), high-fat diet (HFD), and HFD-lithium (HFD-Li)-fed mice. A and B, GTT curves and corresponding area under the curve (AUC) analysis. C and D, ITT curves and corresponding AUC analysis. For A and C: ∗p < 005, ∗∗p < 0.01, ∗∗∗p < 0.001, and ∗∗∗∗p < 0.0001 for HFD versus chow; and # p < 005, ## p < 0.01, ### p < 0.001, and #### p < 0.0001 for HFD-Li versus chow with a two-way ANOVA and a Tukey’s post hoc test. For B and D: ∗∗∗p < 0.001 and ∗∗∗∗p < 0.0001 with a one-way ANOVA and a Tukey’s post hoc test (n = 22–24 per group). LiCl, lithium chloride.Glucose tolerance test (GTT) and insulin tolerance test (ITT) in control and LiCl-treated chow-fed mice. A and B, GTT curves and corresponding area under the curve (AUC) analysis. C and D, ITT curves and corresponding AUC analysis (n = 12 per group). LiCl, lithium chloride.Figure 10 Glucose tolerance test (GTT) and insulin tolerance test (ITT) in control and LiCl-treated chow-fed mice. A and B, GTT curves and corresponding area under the curve (AUC) analysis. C and D, ITT curves and corresponding AUC analysis (n = 12 per group). LiCl, lithium chloride.Lithium is known to have insulin-sensitizing effects, and Choi et al. () found that LiCl treatment, with the same dose used in this study, lowered fasting glucose in HFD-fed mice. Here, we did not find any effect of LiCl supplementation on glucose or insulin tolerance in either chow-fed ( Fig. 10 ) or HFD-fed mice ( Fig. 11 ).LiCl supplementation did not inhibit GSK3 in iWAT or BAT from mice fed a high-fat diet (HFD). A, inhibitory serine9 phosphorylation of GSK3 in iWAT and BAT from control (CON; standard chow) HFD and HFD-Li mice (n = 8 per group). B–D, representative Western blot images and analyses of mitochondrial proteins PGC-1α, citrate synthase (CS), cytochrome c oxidase subunit IV (COXIV), pyruvate dehydrogenase E1-alpha subunit (PDH), and UCP1 in iWAT (C) and BAT (D). E, H&E stain of iWAT and BAT sections from chow, HFD, and HFD-Li mice (the scale bar represents 200 μm). For B and C, ∗p < 005, ∗∗p < 0.01 with a one-way ANOVA and Tukey’s post hoc test. Western blot data are presented as relative to chow. All values are means ± SEM. BAT, brown adipose tissue; GSK3, glycogen synthase kinase 3; iWAT, inguinal white adipose tissue; LiCl, lithium chloride; PGC-1α, peroxisome proliferator–activated receptor-gamma coactivator 1-alpha; UCP1, uncoupling protein 1.Figure 9 LiCl supplementation did not inhibit GSK3 in iWAT or BAT from mice fed a high-fat diet (HFD). A, inhibitory serine9 phosphorylation of GSK3 in iWAT and BAT from control (CON; standard chow) HFD and HFD-Li mice (n = 8 per group). B–D, representative Western blot images and analyses of mitochondrial proteins PGC-1α, citrate synthase (CS), cytochrome c oxidase subunit IV (COXIV), pyruvate dehydrogenase E1-alpha subunit (PDH), and UCP1 in iWAT (C) and BAT (D). E, H&E stain of iWAT and BAT sections from chow, HFD, and HFD-Li mice (the scale bar represents 200 μm). For B and C, ∗p < 005, ∗∗p < 0.01 with a one-way ANOVA and Tukey’s post hoc test. Western blot data are presented as relative to chow. All values are means ± SEM. BAT, brown adipose tissue; GSK3, glycogen synthase kinase 3; iWAT, inguinal white adipose tissue; LiCl, lithium chloride; PGC-1α, peroxisome proliferator–activated receptor-gamma coactivator 1-alpha; UCP1, uncoupling protein 1.In contrast with chow-fed mice, LiCl supplementation did not lead to any alterations in iWAT or BAT in HFD-fed mice ( Fig. 9 ). There were no differences in inhibitory serine9 phosphorylation of GSK3β between HFD and HFD-Li groups when expressed as a ratio of total GSK3β ( Fig. 9 A). When assessed separately, our results show that the HFD led to a significant ∼44% reduction in serine9 phosphorylated GSK3β in both the HFD (p = 0.01) and HFD-Li (p = 0.01) groups compared with control (F = 5.8, p = 0.009). This occurred without any changes in total GSK3β (F = 0.14, p = 0.86). This suggests that LiCl supplementation could not overcome the overactivation of GSK3 in iWAT observed with high fat feeding. When measuring protein content in iWAT, there were significant reductions found in several mitochondrial-related proteins with no change in UCP1 in both HFD and HFD-Li groups compared with chow-fed mice ( Fig. 9 , B and C). In BAT, there were no differences observed for UCP1 or most mitochondrial proteins, except for citrate synthase, which was upregulated under both HFD conditions ( Fig. 9 , B and D). Finally, histological analyses did not reveal a beige-like phenotype in iWAT from the HFD-Li group ( Fig. 9 E).LiCl supplementation inhibits GSK3 and promotes a beige-like phenotype in iWAT from mice fed a chow diet. A, inhibitory serine9 GSK3 phosphorylation is elevated in iWAT but not BAT after LiCl treatment (n = 11–12 per group). B and C, Western blot analyses of mitochondrial proteins UCP1, cytochrome c (Cyto C), cytochrome c oxidase subunit IV (COXIV), citrate synthase (CS), pyruvate dehydrogenase E1-alpha subunit (PDH), and PGC-1α in iWAT (B) and BAT (C) from control and LiCl-fed mice (n = 6–12 per group, apart from UCP1 where 18 per group were used). D, H&E stain of iWAT and BAT sections from control and LiCl-fed mice (the scale bar represents 100 μm). E, percent of multilocular adipocytes quantified using ImageJ. F, Western blot quantification of SERCA2 and RYR2 in iWAT (n = 4 per group). ∗p < 0.05 using a Student’s t test. All Western blot data are presented as relative to control. All values are means ± SEM. BAT, brown adipose tissue; GSK3, glycogen synthase kinase 3; iWAT, inguinal white adipose tissue; LiCl, lithium chloride; PGC-1α, peroxisome proliferator–activated receptor-gamma coactivator 1-alpha; RYR, ryanodine receptor; SERCA, sarco(endo)plasmic reticulum Ca 2+ -ATPase; UCP1, uncoupling protein 1.Figure 8 LiCl supplementation inhibits GSK3 and promotes a beige-like phenotype in iWAT from mice fed a chow diet. A, inhibitory serine9 GSK3 phosphorylation is elevated in iWAT but not BAT after LiCl treatment (n = 11–12 per group). B and C, Western blot analyses of mitochondrial proteins UCP1, cytochrome c (Cyto C), cytochrome c oxidase subunit IV (COXIV), citrate synthase (CS), pyruvate dehydrogenase E1-alpha subunit (PDH), and PGC-1α in iWAT (B) and BAT (C) from control and LiCl-fed mice (n = 6–12 per group, apart from UCP1 where 18 per group were used). D, H&E stain of iWAT and BAT sections from control and LiCl-fed mice (the scale bar represents 100 μm). E, percent of multilocular adipocytes quantified using ImageJ. F, Western blot quantification of SERCA2 and RYR2 in iWAT (n = 4 per group). ∗p < 0.05 using a Student’s t test. All Western blot data are presented as relative to control. All values are means ± SEM. BAT, brown adipose tissue; GSK3, glycogen synthase kinase 3; iWAT, inguinal white adipose tissue; LiCl, lithium chloride; PGC-1α, peroxisome proliferator–activated receptor-gamma coactivator 1-alpha; RYR, ryanodine receptor; SERCA, sarco(endo)plasmic reticulum Ca 2+ -ATPase; UCP1, uncoupling protein 1.We next examined the effects of LiCl treatment on both iWAT and BAT in chow-fed and HFD-fed mice. In chow-fed mice and as early as 6 weeks of treatment, LiCl significantly increased GSK3 serine9 phosphorylation in iWAT ( Fig. 8 A); however, this was not observed in BAT ( Fig. 8 A). In turn, LiCl treatment resulted in significant elevations of UCP1 and several other mitochondria-related proteins including PGC-1α in iWAT but not BAT ( Fig. 8 , B and C). Histological analysis demonstrated for the first time that GSK3 inhibition associated with LiCl supplementation leads to a significant “beiging” effect on iWAT, taking on a multilocular-like phenotype ( Fig. 8 , D and E). As SERCA-mediated Casignaling has been shown to contribute to energy homeostasis in beige adipocytes (), we next examined whether LiCl supplementation would alter protein levels of SERCA2 and RYR2 in iWAT. However, we did not find any differences in either SERCA2 or RYR2 content ( Fig. 8 F).Muscle-specific partial GSK3 knockdown (GSK3 mKD ) promotes SERCA uncoupling in soleus muscles from chow-fed male C57BL/6J mice. A, GSK3β protein content assessed via Western blotting in soleus muscles from GSK3 floxed (GSK3 floxed , control) and GSK3 mKD mice (n = 4 per group). B, rates of Ca 2+ uptake in soleus muscles from GSK3 floxed and GSK3 mKD mice (n = 4 per group). C, SERCA activity in soleus muscles from GSK3 floxed and GSK3 mKD mice (n = 4 per group). D, apparent coupling ratio (Ca 2+ uptake divided by SERCA activity at matching [Ca 2+ ] free ) (n = 4 per group). E, Western blot analysis of SERCA1, SERCA2, RYR1, SLN, and NNAT (n = 3–4 per group). ∗p < 0.05, ∗∗p < 0.01, using an independent Student’s t test. All values are means ± SEM. GSK3, glycogen synthase kinase 3; NNAT, neuronatin; RYR, ryanodine receptor; SERCA, sarco(endo)plasmic reticulum Ca 2+ -ATPase; SLN, sarcolipin.Figure 7 Muscle-specific partial GSK3 knockdown (GSK3 mKD ) promotes SERCA uncoupling in soleus muscles from chow-fed male C57BL/6J mice. A, GSK3β protein content assessed via Western blotting in soleus muscles from GSK3 floxed (GSK3 floxed , control) and GSK3 mKD mice (n = 4 per group). B, rates of Ca 2+ uptake in soleus muscles from GSK3 floxed and GSK3 mKD mice (n = 4 per group). C, SERCA activity in soleus muscles from GSK3 floxed and GSK3 mKD mice (n = 4 per group). D, apparent coupling ratio (Ca 2+ uptake divided by SERCA activity at matching [Ca 2+ ] free ) (n = 4 per group). E, Western blot analysis of SERCA1, SERCA2, RYR1, SLN, and NNAT (n = 3–4 per group). ∗p < 0.05, ∗∗p < 0.01, using an independent Student’s t test. All values are means ± SEM. GSK3, glycogen synthase kinase 3; NNAT, neuronatin; RYR, ryanodine receptor; SERCA, sarco(endo)plasmic reticulum Ca 2+ -ATPase; SLN, sarcolipin.Acknowledging the fact that Li also has GSK3-independent effects, we generated a partial muscle-specific GSK3 knockdown (GSK3) mouse colony. These mice heterozygously express the flox sequence flanking both GSK3 α and β, whereas also heterozygously expressing human skeletal actin (HSA)-Cre to direct skeletal muscle–specific KD of GSK3. In turn, the GSK3mice display a ∼55% reduction in GSK3β in the soleus compared with GSK3control mice ( Fig. 7 A). Similar to LiCl treatment and under a chow diet, we did not find any differences in rates of Cauptake or SERCA activity when examined separately; however, a significant reduction in the apparent coupling ratio of SERCA, particularly at 500 nM of [Ca, was detected ( Fig. 7 , B–D). Moreover, while we did not observe any differences in RYR1 content, both SLN and NNAT were elevated in muscles from GSK3mice compared with GSK3mice ( Fig. 7 E).LiCl supplementation inhibits GSK3 and promotes SERCA uncoupling in soleus muscles from male C57BL/6J high-fat diet (HFD)–fed mice. A, inhibitory serine9 phosphorylation of GSK3 and PGC-1α protein in soleus muscles from control (CON; standard chow) HFD and HFD-Li mice (n = 8–11 per group). B, rates of Ca 2+ uptake in soleus muscles obtained from HFD and HFD-Li mice (n = 3–4 per group with each n representing pooled soleus from three mice). C, SERCA activity in soleus muscles obtained from HFD and HFD-Li mice (n = 3–4 per group with each n representing pooled soleus from three mice). D, apparent coupling ratio (Ca 2+ uptake divided by SERCA activity at matching [Ca 2+ ] free ). E, Western blot analyses of SERCA uncouplers SLN and NNAT as well as RYR1 (n = 6–8 per group). For A, ∗∗p < 0.01 with a one-way ANOVA and Tukey’s post hoc test. For B–E, ∗p < 0.05, ∗∗∗p < 0.001 with a Student’s t test. All values are means ± SEM. GSK3, glycogen synthase kinase 3; LiCl, lithium chloride; NNAT, neuronatin; PGC-1α, peroxisome proliferator–activated receptor-gamma coactivator 1-alpha; RYR, ryanodine receptor; SERCA, sarco(endo)plasmic reticulum Ca 2+ -ATPase; SLN, sarcolipin.Figure 6 LiCl supplementation inhibits GSK3 and promotes SERCA uncoupling in soleus muscles from male C57BL/6J high-fat diet (HFD)–fed mice. A, inhibitory serine9 phosphorylation of GSK3 and PGC-1α protein in soleus muscles from control (CON; standard chow) HFD and HFD-Li mice (n = 8–11 per group). B, rates of Ca 2+ uptake in soleus muscles obtained from HFD and HFD-Li mice (n = 3–4 per group with each n representing pooled soleus from three mice). C, SERCA activity in soleus muscles obtained from HFD and HFD-Li mice (n = 3–4 per group with each n representing pooled soleus from three mice). D, apparent coupling ratio (Ca 2+ uptake divided by SERCA activity at matching [Ca 2+ ] free ). E, Western blot analyses of SERCA uncouplers SLN and NNAT as well as RYR1 (n = 6–8 per group). For A, ∗∗p < 0.01 with a one-way ANOVA and Tukey’s post hoc test. For B–E, ∗p < 0.05, ∗∗∗p < 0.001 with a Student’s t test. All values are means ± SEM. GSK3, glycogen synthase kinase 3; LiCl, lithium chloride; NNAT, neuronatin; PGC-1α, peroxisome proliferator–activated receptor-gamma coactivator 1-alpha; RYR, ryanodine receptor; SERCA, sarco(endo)plasmic reticulum Ca 2+ -ATPase; SLN, sarcolipin.Western blot analysis revealed that the consumption of an HFD significantly reduced GSK3β serine9 phosphorylation, although this effect was blunted with LiCl treatment ( Fig. 6 A). Furthermore, PGC-1α content appeared to be elevated in HFD-Li supplemented mice in comparison to chow-fed controls; however, this was not significantly different ( Fig. 6 B). With respect to SERCA, we identified a significant reduction in Cauptake in the HFD-Li group when compared with untreated counterparts, whereas, there were no differences between groups in SERCA activity ( Fig. 6 , B and C). We also found a significant reduction in the apparent coupling ratio of SERCA in the soleus of HFD-Li group compared with HFD ( Fig. 6 D). Notably, the rates of Cauptake, activity, and the calculated apparent coupling ratio were obtained at a 1500 nM [Casince Cauptake in soleus muscles from HFD-fed mice (with or without LiCl) did not reach levels below 1000 nM [Ca. To identify the cellular mechanisms leading to the reduction in apparent coupling ratio observed with a HFD, we examined the protein levels of SERCA uncoupling proteins SLN and NNAT as well as RYR1. Our results show a significant increase in SLN and NNAT content in HFD-Li versus HFD soleus; however, there were no differences in RYR1 ( Fig. 6 E).LiCl supplementation inhibits GSK3 and promotes SERCA uncoupling in soleus muscles from male C57BL/6J chow-fed mice. A, inhibitory serine9 phosphorylation of GSK3β in soleus muscles from control and LiCl-treated mice (n = 5 per group). B, rates of Ca 2+ uptake in soleus muscles from control and LiCl-treated mice (n = 5 per group). C, SERCA activity in soleus muscles from control and LiCl-treated mice (n = 5 per group). D, apparent coupling ratio (Ca 2+ uptake divided by SERCA activity at matching [Ca 2+ ] free ) (n = 5 per group). E, Western blot analysis of SERCA1, SERCA2, RYR1, SLN, and NNAT. For SERCA2 and RYR1, these targets were probed for on the same membrane and have therefore the same Ponceau. ∗p < 0.05, ∗∗p < 0.01, using an independent Student’s t test. All values are means ± SEM. GSK3, glycogen synthase kinase 3; LiCl, lithium chloride; NNAT, neuronatin; RYR, ryanodine receptor; SERCA, sarco(endo)plasmic reticulum Ca 2+ -ATPase.Figure 5 LiCl supplementation inhibits GSK3 and promotes SERCA uncoupling in soleus muscles from male C57BL/6J chow-fed mice. A, inhibitory serine9 phosphorylation of GSK3β in soleus muscles from control and LiCl-treated mice (n = 5 per group). B, rates of Ca 2+ uptake in soleus muscles from control and LiCl-treated mice (n = 5 per group). C, SERCA activity in soleus muscles from control and LiCl-treated mice (n = 5 per group). D, apparent coupling ratio (Ca 2+ uptake divided by SERCA activity at matching [Ca 2+ ] free ) (n = 5 per group). E, Western blot analysis of SERCA1, SERCA2, RYR1, SLN, and NNAT. For SERCA2 and RYR1, these targets were probed for on the same membrane and have therefore the same Ponceau. ∗p < 0.05, ∗∗p < 0.01, using an independent Student’s t test. All values are means ± SEM. GSK3, glycogen synthase kinase 3; LiCl, lithium chloride; NNAT, neuronatin; RYR, ryanodine receptor; SERCA, sarco(endo)plasmic reticulum Ca 2+ -ATPase.SERCA coupling ratio was tested in the soleus muscle from LiCl-supplemented and control mice. This muscle was chosen based on its known expression of SERCA uncouplers, SLN and NNAT (). Under a chow diet, western blot analysis showed a significant increase in inhibitory serine9 phosphorylation on GSK3β in soleus muscles from LiCl-treated mice versus control ( Fig. 5 A). When measuring rates of Cauptake and SERCA activity separately, we did not find any significant effect of LiCl treatment; however, LiCl treatment resulted in a significant reduction in the apparent coupling ratio, particularly at 1000 nM [Ca Fig. 5 , B–D). The promotion of SERCA uncoupling was associated with a significant increase in NNAT but not SLN ( Fig. 5 E). We also did not observe any changes in SERCA or RYR1 content ( Fig. 5 E).LiCl supplementation does not alter body mass despite increases in food intake in male C57BL/6J high-fat diet (HFD)–fed mice. A, weekly analysis of body mass throughout the 12-week LiCl treatment period (n = 24 per group). B, cumulative food intake throughout the 12-week LiCl treatment period (n = 24 per group). C, daily food intake in control and LiCl-fed mice (n = 24 per group). For A and B, two-way ANOVA was used to test the main effects of time, diet, and their potential interaction. ∗∗p < 0.01, ∗∗∗p < 0.001, and ∗∗∗∗p < 0.0001 for HFD versus control (CON, chow); # p < 0.05, ## p < 0.01, ### p < 0.001, #### p < 0.0001 for HFD-Li versus control; † p < 0.05 for HFD versus HFD-Li. For C, a one-way ANOVA with a Tukey’s post hoc test was used, ∗∗∗p < 0.001. All values are means ± SEM. LiCl, lithium chloride.Figure 4 LiCl supplementation does not alter body mass despite increases in food intake in male C57BL/6J high-fat diet (HFD)–fed mice. A, weekly analysis of body mass throughout the 12-week LiCl treatment period (n = 24 per group). B, cumulative food intake throughout the 12-week LiCl treatment period (n = 24 per group). C, daily food intake in control and LiCl-fed mice (n = 24 per group). For A and B, two-way ANOVA was used to test the main effects of time, diet, and their potential interaction. ∗∗p < 0.01, ∗∗∗p < 0.001, and ∗∗∗∗p < 0.0001 for HFD versus control (CON, chow); # p < 0.05, ## p < 0.01, ### p < 0.001, #### p < 0.0001 for HFD-Li versus control; † p < 0.05 for HFD versus HFD-Li. For C, a one-way ANOVA with a Tukey’s post hoc test was used, ∗∗∗p < 0.001. All values are means ± SEM. LiCl, lithium chloride.LiCl supplementation does not alter body mass despite increases in food intake because of increases in daily energy expenditure in male C57BL/6J chow-fed mice. A, weekly analysis of body mass throughout the 12-week LiCl treatment period (n = 10–12 per group). B, cumulative food intake throughout the 12-week LiCl treatment period (n = 10–12 per group). C, daily food intake in control and LiCl-fed mice (n = 10–12 per group). D and E, body fat and fat-free mass measured through small animal DXA (n = 10–12 per group). F, absolute energy expenditure (VO 2 ) in control and LiCl-treated mice at 6 and 11 weeks of treatment (n = 10–12 per group). G, relative energy expenditure (per lean mass) in control and LiCl-treated mice at 6 and 11 weeks of treatment (n = 10–12 per group). H, cage activity in control and LiCl-treated mice (n = 10–12 per group). For A, B, D–F, a two-way repeated-measures ANOVA was used to test the main effects of time, lithium, and their potential interaction. For C and G, comparisons between control and LiCl were made using independent Student's t tests, ∗p < 0.05. All values are means ± SEM. DXA, dual-energy X-ray absorbtiometry; LiCl, lithium chloride.Figure 3 LiCl supplementation does not alter body mass despite increases in food intake because of increases in daily energy expenditure in male C57BL/6J chow-fed mice. A, weekly analysis of body mass throughout the 12-week LiCl treatment period (n = 10–12 per group). B, cumulative food intake throughout the 12-week LiCl treatment period (n = 10–12 per group). C, daily food intake in control and LiCl-fed mice (n = 10–12 per group). D and E, body fat and fat-free mass measured through small animal DXA (n = 10–12 per group). F, absolute energy expenditure (VO 2 ) in control and LiCl-treated mice at 6 and 11 weeks of treatment (n = 10–12 per group). G, relative energy expenditure (per lean mass) in control and LiCl-treated mice at 6 and 11 weeks of treatment (n = 10–12 per group). H, cage activity in control and LiCl-treated mice (n = 10–12 per group). For A, B, D–F, a two-way repeated-measures ANOVA was used to test the main effects of time, lithium, and their potential interaction. For C and G, comparisons between control and LiCl were made using independent Student's t tests, ∗p < 0.05. All values are means ± SEM. DXA, dual-energy X-ray absorbtiometry; LiCl, lithium chloride.To determine if LiCl could enhance muscle-based and adipose-based thermogenesis in vivo, we treated chow-fed and HFD-fed mice with a dose we had previously shown to cause GSK3 inhibition in muscle (). Under a chow diet, 12 weeks of LiCl supplementation (10 mg/kg/day via drinking water) did not alter body mass; however, LiCl treatment appeared to increase food consumption with a significant difference in cumulative food intake by week 12 compared with control ( Fig. 3 , A and B). Daily food intake in the LiCl group also tended to be higher compared with control, but this was not statistically significant ( Fig. 3 C). Furthermore, body composition analysis showed no significant differences between LiCl and control ( Fig. 3 , D and E). This apparent increase in food consumption without an increase in body mass or percent of body fat points toward an increase in energy expenditure with LiCl supplementation, which was observed across light, dark, and daily periods ( Fig. 3 , F and G). In both absolute and relative to lean mass, Oconsumption was significantly elevated in the LiCl group at both 6 weeks and 11 weeks of treatment. Importantly, these changes in energy expenditure could not be explained by any differences in cage ambulation ( Fig. 3 H). Similar findings were also observed in mice fed an HFD (60% kcal from fat), a condition that would benefit from adaptive thermogenesis. Like the chow-fed mice, no differences in body mass between HFD and HFD-Li groups were observed after 12 weeks of feeding ( Fig. 4 A) despite the HFD-Li group having the highest levels of cumulative food consumption and daily food intake ( Fig. 4 , B and C). We presume this to be due to elevated daily energy expenditure, similar to what was found in the chow study ( Fig. 3 ). However, at the time of this experiment, we did not have the Promethion Metabolic Cages nor Small Animal DXA scanner in place to conduct measures of Oconsumption or body composition.The acute effects of LiCl treatment were also investigated in differentiated 3T3-L1 adipocytes. As observed previously with C2C12 myocytes, 3 days of LiCl treatment did not alter the protein levels of neither UCP1 nor PGC-1α ( Fig. 2 F). This again indicates that the ability of LiCl to affect UCP1 and PGC-1α levels requires that the treatment occurs during differentiation.LiCl treatment inhibits GSK3 and increases UCP1 and cellular respiration in 3T3-L1 adipocytes. A, increased inhibitory serine9 phosphorylation of GSK3β with 0.5 mM LiCl treatment (n = 5–6 per group). B, resting cellular respiration rates in LiCl-treated and control 3T3-L1 adipocytes (n = 3–4 per group). C, UCP1 and PGC-1α protein content measured with Western blotting (n = 6–8 per group). D and E, mitochondrial footprint and branch length in 3T3-L1 adipocytes treated with LiCl. F, acute (3 days) of 0.5 mM LiCl treatment in 10-day differentiated 3T3-L1 adipocytes did not alter UCP1 or PGC-1α protein content (n = 3 per group). ∗p < 0.05 using a Student’s t test, with each n representing a technical replicate. All Western blot data are presented as relative to control. All values are means ± SEM. GSK3, glycogen synthase kinase 3; LiCl, lithium chloride; PGC-1α, peroxisome proliferator–activated receptor-gamma coactivator 1-alpha; UCP1, uncoupling protein 1.Figure 2 LiCl treatment inhibits GSK3 and increases UCP1 and cellular respiration in 3T3-L1 adipocytes. A, increased inhibitory serine9 phosphorylation of GSK3β with 0.5 mM LiCl treatment (n = 5–6 per group). B, resting cellular respiration rates in LiCl-treated and control 3T3-L1 adipocytes (n = 3–4 per group). C, UCP1 and PGC-1α protein content measured with Western blotting (n = 6–8 per group). D and E, mitochondrial footprint and branch length in 3T3-L1 adipocytes treated with LiCl. F, acute (3 days) of 0.5 mM LiCl treatment in 10-day differentiated 3T3-L1 adipocytes did not alter UCP1 or PGC-1α protein content (n = 3 per group). ∗p < 0.05 using a Student’s t test, with each n representing a technical replicate. All Western blot data are presented as relative to control. All values are means ± SEM. GSK3, glycogen synthase kinase 3; LiCl, lithium chloride; PGC-1α, peroxisome proliferator–activated receptor-gamma coactivator 1-alpha; UCP1, uncoupling protein 1.We next examined the effects of LiCl treatment on UCP1 content in 3T3-L1 adipocytes. As with C2C12 cells, 3T3-L1 cells were treated with 0.5 mM LiCl for 10 days during differentiation. As expected, there was a significant increase in inhibitory serine9 phosphorylation of GSK3β with LiCl treatment compared with nontreated cells ( Fig. 2 A). This inhibition was associated with an increase in cellular respiration ( Fig. 2 B) and increased UCP1 and PGC-1α content ( Fig. 2 C), further supporting the role of GSK3 in negatively regulating UCP1 expression in adipocytes. However, LiCl treatment had no impact on the mitochondrial footprint, indicating no increase in mitochondrial abundance ( Fig. 2 , D and E).We have previously shown that LiCl treatment enhances myoblast differentiation (), which could influence our findings. Therefore, we next examined the acute effects of LiCl in 10-day differentiated myoblasts. Our results show that 3 days of LiCl treatment after C2C12 cells were already differentiated produced no effect on coupling ratio or RYR content ( Fig. 1 , J and K). This suggests that the effect of LiCl on lowering SERCA efficiency, presumably through increased RYR Caleak, only manifests when the treatment occurs throughout differentiation.LiCl treatment inhibits GS K3, increases cellular respiration, and promotes SERCA uncoupling in C2C12 myocytes. A, increased inhibitory serine9 phosphorylation of GSK3β and PGC-1α protein with 0.5 mM LiCl treatment (n = 5–6 per group). B, no changes in mitochondrial proteins: pyruvate dehydrogenase (PDH), cytochrome c, cytochrome c oxidase subunit IV (COXIV), or citrate synthase was found with 0.5 mM LiCl treatment (n = 3 per group). C, resting cellular respiration rates in LiCl-treated and control C2C12 cells (n = 6 per group). Cellular respiration in the presence and absence of 10 mM MgCl 2 (D) to calculate the energetic contribution of SERCA (E) (n = 3 per group). F, Ca 2+ uptake is significantly reduced without altering SERCA activity (G), leading to a significant reduction in the apparent coupling ratio of SERCA (H) (n = 6 per group). I, Western blot analyses of SERCA isoform, SLN, NNAT, and RYR1 in LiCl-treated and control C2C12 cells (n = 5–6 per group). J and K, acute (3 days) of 0.5 mM LiCl treatment in 10-day differentiated C2C12 cells did not alter SERCA coupling ratio or RYR1 content (n = 3 per group). ∗p < 0.05, ∗∗p < 0.01 using a Student’s t test, with each n representing a technical replicate. All Western blot data are presented as relative to control. All values are means ± SEM. GSK3, glycogen synthase kinase 3; LiCl, lithium chloride; NNAT, neuronatin; PGC-1α, peroxisome proliferator–activated receptor-gamma coactivator 1-alpha; RYR1, ryanodine receptor 1; SERCA, sarco(endo)plasmic reticulum Ca 2+ -ATPase; SLN, sarcolipin.Figure 1 LiCl treatment inhibits GS K3, increases cellular respiration, and promotes SERCA uncoupling in C2C12 myocytes. A, increased inhibitory serine9 phosphorylation of GSK3β and PGC-1α protein with 0.5 mM LiCl treatment (n = 5–6 per group). B, no changes in mitochondrial proteins: pyruvate dehydrogenase (PDH), cytochrome c, cytochrome c oxidase subunit IV (COXIV), or citrate synthase was found with 0.5 mM LiCl treatment (n = 3 per group). C, resting cellular respiration rates in LiCl-treated and control C2C12 cells (n = 6 per group). Cellular respiration in the presence and absence of 10 mM MgCl 2 (D) to calculate the energetic contribution of SERCA (E) (n = 3 per group). F, Ca 2+ uptake is significantly reduced without altering SERCA activity (G), leading to a significant reduction in the apparent coupling ratio of SERCA (H) (n = 6 per group). I, Western blot analyses of SERCA isoform, SLN, NNAT, and RYR1 in LiCl-treated and control C2C12 cells (n = 5–6 per group). J and K, acute (3 days) of 0.5 mM LiCl treatment in 10-day differentiated C2C12 cells did not alter SERCA coupling ratio or RYR1 content (n = 3 per group). ∗p < 0.05, ∗∗p < 0.01 using a Student’s t test, with each n representing a technical replicate. All Western blot data are presented as relative to control. All values are means ± SEM. GSK3, glycogen synthase kinase 3; LiCl, lithium chloride; NNAT, neuronatin; PGC-1α, peroxisome proliferator–activated receptor-gamma coactivator 1-alpha; RYR1, ryanodine receptor 1; SERCA, sarco(endo)plasmic reticulum Ca 2+ -ATPase; SLN, sarcolipin.First, we questioned whether 0.5 mM LiCl would alter SERCA coupling ratio in C2C12 myoblasts, which is a dose that we have previously shown to inhibit GSK3 in this cell line (). Cells were treated with or without LiCl for 7 days during differentiation. This resulted in a significant increase in inhibitory serine9 GSK3 phosphorylation ( Fig. 1 A). PGC-1α is also negatively regulated by GSK3 (), and LiCl increased PGC-1α protein content nearly four-fold ( Fig. 1 A); however, this occurred without any significant changes in other mitochondrial markers ( Fig. 1 B). Nonetheless, LiCl-treated C2C12 cells had elevated cellular respiration rates ( Fig. 1 C), and 10 mM MgCllowered respiration rates in control and LiCl-treated cells ( Fig. 1 D). This reduction is presumed to have been caused by an inhibition of RYR Caleak that minimizes SERCA activity (). Calculating the energetic contribution of SERCA as the difference in respiration with and without 10 mM MgClrevealed a near two-fold increase with LiCl treatment ( Fig. 1 E). Corresponding well with these data, we observed a significant reduction in Cauptake with no change in SERCA activity, leading to a significant reduction in the apparent coupling ratio of SERCA with LiCl treatment ( Fig. 1 , F–H). This suggests that the increase in the energy consumption of SERCA is due to a reduction in Catransport efficiency. To investigate the cellular mechanisms leading to this reduction, we examined the protein levels of SERCA1a/2a, SLN, NNAT, and RYR1. No changes were detected in either SERCA isoform, SLN, or NNAT; however, we did observe a significant upregulation in RYR1 ( Fig. 1 I). Thus, the increase in SERCA energy expenditure resulting from a reduction in the apparent coupling ratio of SERCA may be due to enhanced RYR Caleak in LiCl-treated C2C12 cells.DiscussionIn this study, we examined whether GSK3 inhibition, via LiCl supplementation, would promote SERCA uncoupling in C2C12 cells and in skeletal muscles obtained from chow-fed and HFD-fed mice. We also examined whether LiCl supplementation would enhance UCP1 expression in 3T3-L1 adipocytes, and in iWAT and BAT from chow-fed and HFD-fed mice.2 experiments that indirectly inhibit SERCA by lowering SR Ca2+ leak ( 3 Smith I.C.Bombardier E.Vigna C.Tupling A.R. ATP consumption by sarcoplasmic reticulum Ca(2)(+) pumps accounts for 40-50% of resting metabolic rate in mouse fast and slow twitch skeletal muscle. 2+ leak as enhanced RYR Ca2+ leak is known to cause futile Ca2+ cycling with SERCA, thereby increasing energy expenditure and heat release ( 35 Ikeda K.Kang Q.Yoneshiro T.Camporez J.P.Maki H.Homma M.et al. UCP1-independent signaling involving SERCA2b-mediated calcium cycling regulates beige fat thermogenesis and systemic glucose homeostasis. 36 da Costa D.C.Landeira-Fernandez A.M. Thermogenic activity of the Ca2+-ATPase from blue marlin heater organ: regulation by KCl and temperature. 37 Berchtold M.W.Brinkmeier H.Muntener M. Calcium ion in skeletal muscle: its crucial role for muscle function, plasticity, and disease. 38 Maclennan D.H.Zvaritch E. Mechanistic models for muscle diseases and disorders originating in the sarcoplasmic reticulum. 18 Markussen L.K.Winther S.Wicksteed B.Hansen J.B. GSK3 is a negative regulator of the thermogenic program in brown adipocytes. 2+ transport efficiency. Using cellular models first, our results demonstrate that LiCl treatment enhanced respiration in C2C12 cells and 3T3-L1 adipocytes. In C2C12 cells, the increase in respiration was accounted for, at least in part, by an approximately twofold increase in the energetic contribution of SERCA to respiration as estimated with MgClexperiments that indirectly inhibit SERCA by lowering SR Caleak (). This increase in the energy expenditure of SERCA was likely because of enhanced SERCA uncoupling, which was mediated through an increase in RYR rather than any changes in known regulatory proteins that can uncouple the SERCA pump. An increase in RYR can lower the transport efficiency of SERCA via Caleak as enhanced RYR Caleak is known to cause futile Cacycling with SERCA, thereby increasing energy expenditure and heat release (). In 3T3-L1 adipocytes, we attribute the increase in energy expenditure to an increase in PGC-1α and UCP1, which was expected based on previous work showing that GSK3 negatively regulated UCP1 expression in BAT (). However, in both adipocytes and myocytes, the effect of LiCl treatment was abolished when treatment occurred acutely, for 3 days, after cellular differentiation. Together, these results show that in cultured adipocytes and myocytes, LiCl supplementation and GSK3 inhibition throughout cellular differentiation can increase energy expenditure via alterations in UCP1 and SERCA Catransport efficiency.39 Kurgan N.Bott K.N.Helmeczi W.E.Roy B.D.Brindle I.D.Klentrou P.et al. Low dose lithium supplementation activates Wnt/beta-catenin signalling and increases bone OPG/RANKL ratio in mice. 40 Hamstra S.I.Kurgan N.Baranowski R.W.Qiu L.Watson C.J.F.Messner H.N.et al. Low-dose lithium feeding increases the SERCA2a-to-phospholamban ratio, improving SERCA function in murine left ventricles. 41 Hamstra S.I.Whitley K.C.Baranowski R.W.Kurgan N.Braun J.L.Messner H.N.et al. The role of phospholamban and GSK3 in regulating rodent cardiac SERCA function. 25 Whitley K.C.Hamstra S.I.Baranowski R.W.Watson C.J.F.MacPherson R.E.K.MacNeil A.J.et al. GSK3 inhibition with low dose lithium supplementation augments murine muscle fatigue resistance and specific force production. To determine if these effects would translate to the in vivo setting, we treated mice with 10 mg/kg/day of LiCl for 12 weeks under chow-fed and HFD-fed conditions. Though we have previously shown that this dose results in a serum concentration that is much lower than that used in our cell culture experiments (0.02 mM versus 0.5 mM, respectively) (), this dose and duration were effective in inhibiting GSK3 and increasing PGC-1α in murine soleus muscle (). Here, our results show that LiCl supplementation did not alter body mass despite increasing total food intake in both chow-fed and HFD-fed conditions. We attribute this effect to an increase in daily energy expenditure in LiCl-treated mice. However, since this was only measured in chow-fed conditions, we can only assume that this was also the case in HFD-fed mice.3 Smith I.C.Bombardier E.Vigna C.Tupling A.R. ATP consumption by sarcoplasmic reticulum Ca(2)(+) pumps accounts for 40-50% of resting metabolic rate in mouse fast and slow twitch skeletal muscle. 2+ transport efficiency may influence total daily energy expenditure, given the energetic nature of skeletal muscle. In chow-fed mice, the reduction in SERCA Ca2+ transport efficiency was observed at 1000 nM [Ca2+] free ; however, statistical significance was lost at 500 nM [Ca2+] free . Similarly, when fed a HFD, LiCl treatment significantly reduced SERCA coupling ratio, though we could only measure this at 1500 nM [Ca2+] free as Ca2+ uptake in these samples did not reach levels below 1000 nM. While the exact reason for this remains unknown, we believe that this finding is indicative of an impairment in Ca2+ uptake or excessive leak of Ca2+ from the SR with a HFD ( 42 Jain S.S.Paglialunga S.Vigna C.Ludzki A.Herbst E.A.Lally J.S.et al. High-fat diet-induced mitochondrial biogenesis is regulated by mitochondrial-derived reactive oxygen species activation of CaMKII. In skeletal muscle, the SERCA pumps account for ∼50% of resting metabolic rate and were estimated to account for 22 to 25% of total daily energy expenditure (). Thus, the SERCA pumps may be viewed as a metabolic hub in muscle, and alterations in SERCA Catransport efficiency may influence total daily energy expenditure, given the energetic nature of skeletal muscle. In chow-fed mice, the reduction in SERCA Catransport efficiency was observed at 1000 nM [Ca; however, statistical significance was lost at 500 nM [Ca. Similarly, when fed a HFD, LiCl treatment significantly reduced SERCA coupling ratio, though we could only measure this at 1500 nM [Caas Cauptake in these samples did not reach levels below 1000 nM. While the exact reason for this remains unknown, we believe that this finding is indicative of an impairment in Cauptake or excessive leak of Cafrom the SR with a HFD (). Nonetheless, under both chow-fed and HFD-fed conditions, LiCl treatment significantly reduced the apparent coupling ratio of SERCA, which could contribute at least in part to the increase in daily energy expenditure.2+ transport efficiency by increasing the expression of SERCA uncoupling proteins. In chow-fed mice, LiCl treatment significantly increased NNAT without altering SLN content. This sole increase in NNAT content in chow-fed conditions was sufficient in lowering SERCA Ca2+ transport efficiency. Conversely, in HFD-fed mice, both SLN and NNAT were significantly upregulated with LiCl treatment. This is interesting since both SLN ( 43 Gamu D.Juracic E.S.Fajardo V.A.Rietze B.A.Tran K.Bombardier E.et al. Phospholamban deficiency does not alter skeletal muscle SERCA pumping efficiency or predispose mice to diet-induced obesity. 10 Braun J.L.Teng A.C.T.Geromella M.S.Ryan C.R.Fenech R.K.MacPherson R.E.K.et al. Neuronatin promotes SERCA uncoupling and its expression is altered in skeletal muscles of high-fat diet fed mice. mKD mice, the results presented in this study establish a novel regulatory link between GSK3 and the expression of both SLN and NNAT and SERCA Ca2+ transport efficiency in skeletal muscle. Unlike C2C12 cells, the promotion of SERCA uncoupling in vivo did not occur with any changes in RYR content, which highlights an important difference between cell culture and in vivo models. Instead, in chow-fed and HFD-fed mice, LiCl treatment lowered SERCA Catransport efficiency by increasing the expression of SERCA uncoupling proteins. In chow-fed mice, LiCl treatment significantly increased NNAT without altering SLN content. This sole increase in NNAT content in chow-fed conditions was sufficient in lowering SERCA Catransport efficiency. Conversely, in HFD-fed mice, both SLN and NNAT were significantly upregulated with LiCl treatment. This is interesting since both SLN () and NNAT () content were recently shown to be lowered in soleus muscles obtained from mice fed a HFD. It is thus tempting to speculate that the enhancement in GSK3 activation observed here with high fat feeding (and blunted with LiCl treatment) could contribute to the lowered expression of SLN and NNAT content in mice fed a HFD. In further support of this, partial skeletal muscle–specific GSK3 KD increased both SLN and NNAT in soleus muscle, ultimately reducing the apparent coupling ratio of SERCA. While future studies in our laboratory will investigate the effects of high fat feeding in the GSK3mice, the results presented in this study establish a novel regulatory link between GSK3 and the expression of both SLN and NNAT and SERCA Catransport efficiency in skeletal muscle.In adipose tissue, we questioned whether LiCl treatment could increase UCP1 in iWAT and BAT by inhibiting GSK3, ultimately contributing to changes in daily energy expenditure. Under chow-fed conditions, our results show that LiCl treatment significantly inhibited GSK3 in iWAT, which in turn increased UCP1 and PGC-1α content and promoted a multilocular histological phenotype. To our knowledge, ours is the first study establishing a beiging-like effect of LiCl and GSK3 inhibition in iWAT obtained from mice. However, there was no effect of LiCl on UCP1 or PGC-1α in BAT—a result we attributed to an inability to inhibit GSK3 in this specific adipose depot. Interestingly, when mice were fed a HFD, LiCl treatment did alter neither the UCP1 or PGC-1α content nor the histological appearance of iWAT, which we also attribute to an inability to inhibit GSK3. Similar to our findings in skeletal muscle, GSK3 was more active in iWAT from HFD-fed mice with significantly lowered serine9 phosphorylation. However, unlike skeletal muscle, LiCl treatment could not attenuate this effect. Thus, in combination with our in vitro results, these findings demonstrate that when GSK3 is inhibited with LiCl, UCP1 and PGC-1α are increased and may elevate energy expenditure. Future studies from our laboratory will investigate the effects of more potent GSK3 inhibitors and adipose tissue–specific GSK3 KD on iWAT beiging.23 Choi S.E.Jang H.J.Kang Y.Jung J.G.Han S.J.Kim H.J.et al. Atherosclerosis induced by a high-fat diet is alleviated by lithium chloride via reduction of VCAM expression in ApoE-deficient mice. The enhancement of GSK3 activation observed in both adipose and muscle obtained from mice fed a HFD may suggest a role for GSK3 overactivation in diet-induced obesity. In this context, previous work conducted by Choi et al. () found that treating mice with LiCl for 10 mg/kg/day (same dose used in this study) reduced the weight gained after an HFD. Moreover, lithium is known to have insulin-sensitizing effects, and Choi et al. found that LiCl treatment in HFD-fed mice lowered fasting glucose levels. However, in our hands, we did not find any alterations in body mass or glucose handling in chow-fed or HFD-fed conditions. It should be noted that Choi et al. used an HFD that only comprised 20% fat, whereas we provided a 60% fat diet in our study, which strongly induced obesity in mice and may have overpowered any effect of LiCl. Moreover, the increase in food intake found here in chow-fed and HFD-fed mice treated with LiCl could have masked the effect of LiCl on lowering body mass and potentially glucose handling. Thus, our study is limited in that we did not conduct pair-feeding experiments. Our study is also limited in that we only utilized male mice, primarily to avoid potential confounding effects of hormonal shifts in female mice. However, future studies should examine the effects of LiCl treatment and GSK3 inhibition in female mice.44 Davis J.Desmond M.Berk M. Lithium and nephrotoxicity: unravelling the complex pathophysiological threads of the lightest metal. 22 Mangge H.Bengesser S.Dalkner N.Birner A.Fellendorf F.Platzer M.et al. Weight gain during treatment of bipolar disorder (BD)—facts and therapeutic options. 45 Peselow E.D.Dunner D.L.Fieve R.R.Lautin A. Lithium carbonate and weight gain. Nonetheless, our study does provide novel insight into the effect of lithium on appetite and food intake. Lithium is conventionally used in the treatment of bipolar disorder although clinical use of lithium is prescribed at higher doses (serum concentration of 0.5–1.0 mM) in order to overcome the blood–brain barrier and exert beneficial effects on mental health (). At this high dose, it has been suggested that some patients undergoing lithium therapy are at an increased risk of developing obesity (), perhaps because of an increase in appetite (). Here, we observed that low-dose LiCl-supplemented mice also had increased appetite, consuming more food than controls, which is consistent with high-dose lithium therapy. However, at low doses of LiCl, our results show that this increase in appetite and food intake was met with an increase in SERCA uncoupling and/or UCP1 expression, which we believe attenuated any additional weight gain expected with an increase in food intake. Whether the promotion of SERCA uncoupling and UCP1 in skeletal muscle and adipose tissue, respectively, is lost with higher doses of lithium should be investigated with future studies.In conclusion, our study examined the effects of GSK3 inhibition via low-dose LiCl supplementation on SERCA uncoupling in skeletal muscle and UCP1 expression in adipose tissue. Our results provide novel regulatory connections between GSK3 and the expression of NNAT and SLN and SERCA uncoupling in skeletal muscle; and for the first time show that GSK3 inhibition can promote a beiging-like effect in iWAT.",Low dose lithium supplementation promotes adipose tissue browning and sarco(endo)plasmic reticulum Ca
394,4,4_cancer_licl_gsk3_mice,https://www.inverse.com/mind-body/new-mrna-vaccine-universal-flu-shot,"In the spring of 1933 , three scientists at the U.K.’s Medical Research Council received a treasure trove of vials containing human mucus from sick hospital patients. It was gross, yes, but not unwarranted: The culprit behind what had caused the 1918 influenza pandemic was still at large.Months later, the trio of scientists published a paper that found that a virus, not a novel strain of bacteria like some within the scientific community originally thought, was to blame. Over the following decades, other scientists unfurled the gnarly branches of the large influenza family tree, gathering enough information to formulate a vaccine, which (hopefully) most of us get before every flu season.But here’s the catch: Influenza is a master shapeshifter. Every year, strains of the virus that infect humans — influenza type A and B — evolve in ways that evade vaccines and, subsequently, our immune systems. This results in uneven vaccine effectiveness from year to year and also undermines efforts to pack a flu shot with a broad, long-lasting immune punch.But we may have an ace in the hole thanks to mRNA, the same technology used for our Covid-19 vaccines. In a study published Monday in the journal Proceedings of the National Academy of Sciences, researchers at the University of Pennsylvania, Icahn School of Medicine at Mount Sinai, and other institutions have cooked up an mRNA-based influenza vaccine that targets four viral proteins that tend to remain the same across different strains of influenza.“[It’s] a nice study with significant clinical implications that uses the same strategy as for Covid vaccination to create a better flu vaccine with broader capabilities by targeting multiple conserved flu [viral proteins] antigens instead of the way we do it now, which is to grow candidate strains in eggs and then inactivate them,” Anne Davidson, a researcher at the Feinstein Institutes for Medical Research in New York, who was not involved in the study, told Inverse in an email.Artistic rendering of a molecule of hemagglutinin. ShutterstockHere’s the background — Flu vaccines are updated every year based on influenza patterns most recently seen in the southern hemisphere, such as Australia; its flu season runs from April to October.Scientists pay close attention to one sugar-covered protein called hemagglutinin that dots the surface of the influenza virus, Norbert Pardi, an assistant professor of microbiology at the University of Pennsylvania Perelman School of Medicine who co-led the study, tells Inverse.“Hemagglutinin has two major parts: the head domain, which is variable and immunodominant, [and a stalk domain],” explains Pardi. “The current seasonal vaccines that use three or four inactivated [influenza] viruses primarily target the immunodominant head domain… but the problem is that the virus can change that pretty easily and escape from protective immunity.”A better option would be to target viral proteins that don’t switch up and stay pretty much the same regardless of which strain of influenza you’re infected with, says Pardi and Florian Krammer, a virologist at the Icahn School of Medicine at Mount Sinai who also co-led the study.Those proteins include neuraminidase, nucleoprotein, and matrix protein 2, all of which help the virus make copies of itself.Krammer says while there have been efforts over the years to include conserved influenza proteins, such studies have run up against failure and a whole host of hurdles. For example, to measure a potential shot’s effectiveness, scientists look to see whether it triggers an immune response, indicated via the presence of antibodies.“The vaccines that we’re testing these proteins with, they don’t induce those types of antibodies that are used as correlates,” explains Krammer. “So there’s not a lot of confidence during clinical development.”Of the four different types of influenza, A and B primarily infect humans. ShutterstockHow they did it — What has really made the dream of a potent flu shot come alive, says Pardi and Krammer, is messenger RNA (or mRNA) technology, the same used for Moderna and Pfizer-BioNTech’s Covid-19 vaccines. It involves taking a piece of genetic code that isn’t capable of altering anyone’s DNA and instructing it to make a particular type of protein that the immune system can then learn to recognize and target.For their influenza vaccine, the researchers created an mRNA cocktail encoding the four influenza proteins neuraminidase, nucleoprotein, matrix protein 2, and the stalk portion of hemagglutinin (which is conserved compared to its head domain).The vaccine was then injected into a group of twenty or so naive mice who had never experienced influenza before. They either got a quadrivalent jab (meaning all four mRNA segments for each protein was present) or monovalent (the conventional flu vaccine or vaccines containing an individual mRNA for any one of the proteins). Some animals received one shot, while others lucked out with one shot plus a booster four weeks later. The mice were then challenged with an assortment of different influenza strains, both that infect humans and other animals like dogs.What they found — There were two important phenomena the researchers noticed. First, while both the quadrivalent and monovalent jabs encouraged antibody production, only the quadrivalent shot protected the mice against the viral challenge, with exception of the monovalent vaccine containing nucleoprotein, which seemed to protect vaxxed mice from death.While antibodies tend to hog the vaccine limelight, immune cells called T cells, which roam the body and kill infected cells, are also crucial in the fight against viruses. Pardi, Krammer, and their colleagues found that the nucleoprotein-specific jab encouraged a class of cytotoxic T cells that studies have found play an important role in protection against severe influenza infection in humans and animal models.On average around five to 20 percent of Americans will get the flu every year. However, during the Covid-19 pandemic, there has been a sharp drop in the number of flu cases reported. bluecinema/E+/Getty ImagesKrammer says this motley of immune responses to the four different influenza viral proteins suggests it's more powerful to have a colorful vaccine cocktail rather than a drab monotone.“When we mix all of them together, we get the broadest immune response,” he says. “You get the engagement of T cells against the nucleoprotein, you get antibodies, and we get a pretty strong neuraminidase response. That’s kind of the beauty here that you’re flexible in what types of [viral proteins] you use… and you have a lot of possibilities to try [out].”The researchers also expect it wouldn’t need to undergo annual updates as our current ones do. Instead, they might last for a few years.What’s next — While mice studies are the launch point for future studies into other organisms like monkeys and eventually humans, Pardi and Krammer say it’s not clear whether what we see in mice will necessarily translate to us. For one, humans don’t have a naive immune system like the mice used in the study. Our pre-existing immunity against the flu may impact the quality of any potential vaccine’s antibody response, according to some studies.“Clinical trials will be necessary to see how vaccines can overcome the issue with the shortfall from previous immunity,” says Pardi. “Pretty much all adults have antibodies against the flu, and we need to see how our vaccine can overcome this issue.”It may take several years of research and clinical trials before their mRNA influenza vaccine sees the light of day, say Pardi and Krammer. In the meanwhile, there’s no point waiting: Go get your flu shot, dear reader.",Scientists use mRNA technology to create a potent flu vaccine that could last for years
78,4,4_cancer_licl_gsk3_mice,https://link.springer.com/article/10.1007/s12015-022-10465-2,"Recently, an article by Seneff et al. entitled “Innate immunosuppression by SARS-CoV-2 mRNA vaccinations: The role of G-quadruplexes, exosomes, and MicroRNAs” was published in Food and Chemical Toxicology (FCT). Here, we describe why this article, which contains unsubstantiated claims and misunderstandings such as “billions of lives are potentially at risk” with COVID-19 mRNA vaccines, is problematic and should be retracted. We report here our request to the editor of FCT to have our rebuttal published, unfortunately rejected after three rounds of reviewing. Fighting the spread of false information requires enormous effort while receiving little or no credit for this necessary work, which often even ends up being threatened. This need for more scientific integrity is at the heart of our advocacy, and we call for large support, especially from editors and publishers, to fight more effectively against deadly disinformation.In this commentary, we would like to alert the scientific community against the dissemination of pseudoscience in presumed trustful scientific journals, and the dangers that such a spreading are causing to public health [1]. We will explain and detail our recent failure to get a problematic paper retracted and our rebuttal published by the editor to raise awareness among the scientific community of the rising misuse of the scientific publication process. The problem is far from novel, and we have seen during the pandemic an explosion of misinformation, especially in the domain of poorly conducted clinical trials of unproven drugs such as hydroxychloroquine and ivermectin [2, 3]. Predatory journals have taken advantage of the threats on public health to publish hundreds of papers of low or null scientific value [4, 5]; they considered the pandemic as an opportunity to gain access to the mainstream media and to flatter the general public [6]. In contrast, when a true scientific journal published erroneous reports, as it was the case for The Lancet or The New England Journal of Medicine in 2020 [7, 8], the paper was rapidly retracted with the apologies of the journal’s editors. This is the way allowing science to improve, but it is very difficult to combat predatory journals or journals whose editor remains deaf to substantiated alerts and supports the dissemination of fake medicine.The problem is different when seemingly rigorous scientific journals publish false science under pressure from the Editor in order to increase their impact factors points and, they think, notoriety. Such an attitude is also predatory and authors, editors and publishers of such articles should be publicly condemned by the scientific community. This technique of using science to vehiculate nonsense has been named ‘agnotology’ by Robert N. Proctor, which he defines as “the study of deliberate, culturally-induced ignorance or doubt, typically to sell a product or win favor, particularly through the publication of inaccurate or misleading scientific data” [9]. There is some similarity between the connivance of the tobacco industry with some ‘key opinion leaders’ who made the propaganda in favor of tobacco consumption; just to name a few [9]: Clarence Cook Little, renown geneticist, former president of the universities of Maine and of Michigan, who declared in 1969 that “there is no demonstrated causal relationship between smoking and any disease”; Victor Buhler, former president of the College of American Pathologists who declared, also in 1969, that “the cause of lung cancer remained unknown”; more recently, Suzanne Oparil, former president of the American Heart Association, who claimed in 1997 that the epidemiological data relating lung cancer to tobacco consumption were old and that “how accurate they are is really not clear to [her]”.We are presently witnessing the same type of misinformation carried out by scientists and journal, endangering millions of people. We would like to describe the fight that we have engaged, and share with the readers our concerns on public health matters. In April 2022, Food and Chemical Toxicology, an Elsevier journal, published a review article dealing with mRNA anti-SARS-CoV-2 vaccines [10], pretending that these vaccines are at the cause of a series of dreadful diseases for a large number of people (neurodegenerative disease, myocarditis, immune thrombocytopenia, Bell’s palsy, liver disease, impaired adaptive immunity, impaired DNA damage response and tumorigenesis). This 16,071-words review including 231 references was first submitted on February 9th 2022 and accepted on April 8th. It was submitted just one month after a call for papers on potential toxic effects of Covid-19 vaccines in this journal made by its Editor in chief, Prof J.L. Domingo [11]. Alerted by the unusual number of shares on social networks (> 30 k tweets and > 10 k Shares, Likes & Comments on Facebook) [12] and overwhelmingly within the anti-vaccination spheres, we were concerned by the content of the manuscript which contains unsubstantiated claims such as “billions of lives are potentially at risk” with mRNA COVID-19 vaccines. The authors pretend that mRNA COVID-19 vaccines are responsible for the “suppression of type I interferon responses” resulting “in impaired innate immunity” and therefore that they “potentially cause increased risk to infectious diseases and cancer”.Such important statements should be supported by undoubtable facts, especially when they are made in a scientific article (published in a journal with an impact factor of 6), and should not solely rely on the authors’ fallacious inferences. We thus contacted Prof J.L. Domingo to warn him against the highly misleading nature of many of the authors’ assertions and the highly contentious nature of previous authors’ publications. We therefore asked for the retraction of the article to prevent it from being used as a scientific reference for the dissemination of false information on vaccination. Prof Domingo answered on the April 18th 2022: “When this manuscript was submitted to the journal, due to its topic, I already anticipated it could be potentially controversial. Therefore, for the review process, instead of making my decision in the comments / recommendations of 2–3 reviewers, as usual, the decision for that paper was based on the comments of 5 reviewers experts in the field. Based on your e-mail, and of course if you wish it, I invite you to submit a Letter to the Editor on that paper where you can state your concerns.”We therefore wrote a Letter to the Editor [13], in which we demonstrated that this article contained several fallacious scientific assumptions leading to misunderstandings and thus invalidating the conclusions drawn by the authors. We suggested that the article be withdrawn because a careful analysis of the provided bibliography indicates profound misinterpretations of the topics and conclusions about the negative impact that vaccination against SARS-CoV-2 could have on immunity. To illustrate our point, we have detailed a non-exhaustive list of 10 misunderstandings in the literature interpreted by the authors (Table 1 in Ref 13). We were thus able, on the basis of the published literature, to show exactly the opposite of what the authors have asserted on the effect of IFN type I by the vaccine. From the abstract, the authors allege that they will provide “evidence that vaccination induces a profound impairment in Type I interferon (IFN) signaling, which has various adverse consequences to human health”. This claim relies on a still unpublished preprint available on medRxiv since August 2021 [14] but of which it should be noted that the final conclusion established “[….] that despite the lack of dramatic inflammation observed during infection, the vaccine elicits a robust adaptive immune response”. Data shows a differential gene expression profile in peripheral dendritic cells based on vaccinal status, but does not support the authors’ claim that there is Type I IFN suppression due to the vaccine. Published research shows this is simply the reaction expected from any vaccine: a high immune response without a systemic and uncontrolled inflammation [15, 16]. Furthermore, arguing that vaccination would result in loss of the interferon-mediated Type I immune response (and therefore leading to a higher infectious risk or lack of cancer surveillance) contradicts other published data on the immune response after vaccination [17]. To date only a set of SARS-COV-2 viral proteins have been shown to antagonize type I interferon response, not the vaccine [18].Of course, we argued that relying on hypothetical physiological disturbances induced by vaccination to suggest a possible increased risk of various cancers, which had never been published so far was unacceptable, especially for patients for whom COVID-19 vaccination is still strongly recommended such as patients with cancer [19,20,21]. We then concluded that “the important shortcomings and misusage of scientific literature and data have no place in a scientific journal. Therefore, we suggest that this article should be retracted in an effort to prevent further damages to health care policies.”The revised and corrected letter is available as a preprint since it was finally rejected on June 13th 2022 after 3 rounds of reviewing by 4 anonymous referees. One reviewer argued there was no reason for retraction because “there is no evidence of scientific fraud that justifies the demand for retraction of the original submission”. Another one stated that “the original paper need not be somewhat accurate since this is a review, so conjecture is allowed, if disproven it is fine.” And the last one questioned our experience and previous works, going even as far as to check our résumés and arguing that “we might not have the required experience”. Only one out 4 reviewers made more constructive remarks and gave us feedback for the letter to be published.We then contacted the ethical board of the publisher, Elsevier. To date, we received no answer.In 2015, the United Nations established 17 Sustainable Development Goals (SDG), including Target 3.3: Fight communicable diseases. We believe that vaccine disinformation is hindering such efforts. Vaccine hesitancy was also flagged by the World Health Organization as one of the ten major global health threats in 2019. That's why papers criticizing vaccines should only be published when claims are strongly supported, which is not the case in Seneff et al.'s paper.We therefore made a militant choice, refusing to change our position about the demand of retraction of the paper. We do think that this is not a scientific controversy, but a matter of public health; millions of people have been protected from the disease by the vaccines that have been developed and distributed all over the world (although not enough in developing countries) [22], and this article, as well as the publicity that it has received on social media, tends to destroy the unprecedented efforts to save people from disease and premature death. Fighting against scientific disinformation may be risky, too slow and insufficient [23, 24]. The scientists involved in such efforts receive little to no credit for this necessary work and can often end up being threatened [25]. Aside from the recent recommendation by Besançon et al. to improve the error-checking culture of academics and the correction of the scientific literature, we believe that the present case of the Seneff et al. article in Food and Chemical Toxicology, further highlights the need for more transparency in reviewing and editorial processes. Indeed, there is currently no information above the article from Seneff et al. that would highlight to readers the fact that the scientific community heavily disputes the claim of the article. We believe that the valid concerns we have raised, to which the Editor in Chief seemed to initially adhere, should have rapidly been reflected in an editorial note above the article. Further, this case illustrates the need for reviewers’ reports to be made transparently available particularly with “potentially controversial” research articles. If this had been done, scientists and readers could have verified the rigor of the reviewing process [25] and have had access to the potential concerns raised by the reviewers about the article at hand. We also argue that the pervasive use of metrics to assess scientists and their productions is partially responsible for the continued existence of questionable research practices [26]. As long as the sole metric to evaluate scientific journals remains the Impact Factor, editors and publishers will have no incentives to take actions on problematic papers. Eventually, we join Besançon et al. in their suggestions to destigmatize and speed up the scientific correction process. We hope our efforts in rebutting Seneff et al. will successfully counter the misinformation on COVID-19 vaccine and the risk for cancers as well as promote the thankless but essential tasks of fighting against scientific fraud [27].",Scientific Integrity Requires Publishing Rebuttals and Retracting Problematic Papers
82,4,4_cancer_licl_gsk3_mice,https://newatlas.com/medical/cancer-metastasis-breakthrough-rethink/,"Cancer’s ability to spread through the body is one of its most devastating tricks. Scientists at Cambridge have now identified a protein that plays a key role in metastasis, which not only hints at a new potential treatment but reveals for the first time that this process isn’t unique to cancer.No matter where in the body it originates, cancer can eventually begin to colonize other organs and tissues through a process known as metastasis, which makes it much harder to treat. Unfortunately, there’s still much about metastasis that scientists don’t understand, but ongoing research is continually uncovering mechanisms that could lead to new therapy options.In the new study, Cambridge scientists discovered not just a new mechanism for metastasis, but completely recontextualized its role. It’s long been thought that metastasis was an abnormal process that arises in cancer, but the new study found that it’s a process used by healthy cells as well – cancer just hijacks it for its own purposes.The team made the discovery while investigating a cellular structure known as sodium leak channel, non-selective (NALCN). These channels are located on cell membranes and control how salt goes in and out of the cell. In the new study, the researchers found that NALCN also regulates the release of cells from tissues into the bloodstream, where they can be taken up by other organs and tissues.In tests in mice, the scientists blocked the function of the NALCN protein, and found that it triggered metastasis in stomach, intestinal and pancreatic cancers. That suggests this could be a new target for preventing metastasis, potentially improving outcomes for patients with cancer.But the most surprising discovery came when the team tested the technique in mice without cancer. Blocking NALCN also caused healthy cells to migrate away from their original organs to other ones – pancreatic cells, for instance, moved to the kidney and became healthy kidney cells instead.“These findings are among the most important to have come out of my lab for three decades,” said Professor Richard Gilbertson, Group Leader of the study. “Not only have we identified one of the elusive drivers of metastasis, but we have also turned a commonly held understanding of this on its head, showing how cancer hijacks processes in healthy cells for its own gains. If validated through further research, this could have far-reaching implications for how we prevent cancer from spreading and allow us to manipulate this process to repair damaged organs.”The team now plans to investigate ways to take advantage of this discovery to prevent metastasis, including repurposing existing drugs.The research was published in the journal Nature Genetics.Source: University of Cambridge",Cambridge cancer breakthrough may prompt rethink of metastasis
378,4,4_cancer_licl_gsk3_mice,https://www.freethink.com/health/personalized-cancer-vaccine,"Pharma giant Merck has paid Moderna $250 million to co-develop and commercialize a promising personalized cancer vaccine based on its mRNA technology.The background: In 2016, pharma giant Merck gave Moderna $200 million to research and develop personalized cancer vaccines based on mRNA. At the time, it was a big bet — mRNA technology had yet to be translated into any approved vaccine.Part of the original agreement specified that if Moderna could complete proof-of-concept studies of mRNA-based personalized cancer vaccines in humans, Merck had the option to pay an undisclosed amount to co-develop and commercialize the shots with Moderna.Since then, mRNA has proven itself with the development of COVID-19 vaccines, and many other vaccines are in development. And now, based on that success, they’re moving ahead with cancer vaccines, too.Most vaccines are mass produced, but Moderna’s cancer vaccine is personalized for each patient.What’s new? Merck is now exercising its option on mRNA-4157, a personalized cancer vaccine in a phase 2 clinical trial for skin cancer. It’s being studied in combination with Merck’s cancer treatment Keytruda, a humanized monoclonal antibody.“This long-term collaboration combining Merck’s expertise in immuno-oncology with Moderna’s pioneering mRNA technology has yielded a novel tailored vaccine approach,” said Eliav Barr, head of global clinical development and CMO of Merck Research Laboratories.A personalized cancer vaccine: Moderna’s cancer vaccine works differently than most vaccines because it’s designed to treat an existing disease rather than prevent it entirely. It’s also personalized for each patient, while other shots are typically mass produced and distributed to the population.To create each vaccine, Moderna takes a sample of a patient’s tumor. It then uses genetic sequencing technology to identify proteins in the tissue called “neoantigens.” These proteins are found only on the surface of cancer cells, and they are unique to each person’s tumor.Moderna then uses its mRNA technology to create a vaccine that instructs cells to make up to 34 of the cancer’s specific neoantigens, which are shown off to immune cells as potential targets. The idea is that this will trigger an immune response against the neoantigens, helping the immune system identify and attack cancerous cells.Given Merck’s new investment in the vaccine, it seems reasonable to expect positive trial results.Looking ahead: For Moderna’s ongoing phase 2 trial of mRNA-4157, 157 patients with high-risk melanoma had their tumors removed via surgery.Some of the patients were then given nine doses of a personalized cancer vaccine and a dose of Keytruda every three weeks for approximately one year or until their melanoma came back. The rest were given just a dose of Keytruda every three weeks for one year.The trial’s primary endpoint is recurrence-free survival, and given Merck’s new investment in the vaccine, it seems reasonable to expect positive results.“With data expected this quarter on [personalized cancer vaccine], we continue to be excited about the future and the impact mRNA can have as a new treatment paradigm in the management of cancer,” said Moderna President Stephen Hoge.We’d love to hear from you! If you have a comment about this article or if you have a tip for a future Freethink story, please email us at [email protected].",Merck pays Moderna $250m for personalized cancer vaccine
377,4,4_cancer_licl_gsk3_mice,https://www.freethink.com/health/oxfords-malaria-vaccine,"A malaria vaccine developed by researchers at the University of Oxford continues to impress, with a booster dose demonstrating up to 80% efficacy in children in a trial with over a year of follow-up.The challenge: Malaria is a parasitic disease spread by mosquitoes. While it’s already eradicated in many parts of the world, it’s still a major threat in Africa, killing more than 600,000 people every year.About half of all malaria deaths are children under the age of five, and even if a child survives their first malaria infection, they’re often reinfected. This repeated battle can take a toll on their immune systems and leave them vulnerable to other illnesses.The only approved malaria vaccine is a shot made by British drugmaker GSK, and it’s only 40% effective. GSK is also only committed to supplying 15 million doses annually, but 25 million children are born in Africa alone every year, and each needs multiple doses.About half of all malaria deaths are children under the age of five.Oxford’s vaccine: In 2019, 450 children between the ages of 5 months and 17 months participated in a phase 2b trial of Oxford’s malaria vaccine (R21/Matrix-M) in the African nation of Burkina Faso.The children were split into three groups. Two groups received a three-shot regimen of Oxford’s malaria vaccine, combined with either a high or low dose of an immunity-boosting adjuvant. The third group received a rabies vaccine as a control.Oxford reported that the vaccine with the high dose of the adjuvant was up to 77% effective at preventing clinical malaria over 12 months of follow-up — making it the first malaria vaccine to reach the World Health Organization’s goal of 75% efficacy.What’s new? In June 2020, 409 of the participants in Oxford’s trial were given booster shots of the same type of vaccine they received initially (i.e., if their primary vaccination was with the high dose of the adjuvant, so was their booster shot).Oxford has now reported that the booster with the high adjuvant dose was 80% effective over 12 months of follow-up, while the lower dose was 70% effective, meaning the initial three-shot regimen plus a booster results in at least two years of robust protection.“Without this investment, we risk losing the gains that have been made over the last decades.” Azra GhaniLooking ahead: Results from a phase 3 trial involving 4,800 children — ranging in age from 5 months to 36 months and living in four African nations — are expected in another few months. Those will be key to getting Oxford’s malaria vaccine approved by regulators.If the shot is approved, the Serum Institute of India, one of the world’s biggest vaccine makers, is ready to manufacture 200 million doses annually starting in 2023, and study co-author Adrian Hill told BBC News the vaccine should cost just “a few dollars” to produce.The bottom line: Oxford’s malaria vaccine could be huge in the fight against this deadly disease — but only if health officials are able to secure the funding needed to get the shots into arms.“Without this investment, we risk losing the gains that have been made over the last decades and witnessing a rising tide of malaria resurgence,” Azra Ghani, chair in infectious disease epidemiology at Imperial College London, who isn’t involved in Oxford’s vaccine, told BBC News.We’d love to hear from you! If you have a comment about this article or if you have a tip for a future Freethink story, please email us at [email protected].",Breakthrough drug could save hundreds of thousands of childrenâs lives
354,4,4_cancer_licl_gsk3_mice,https://www.eurekalert.org/news-releases/967153,"Four of 10 Americans surveyed report that they were often less than truthful about whether they had COVID-19 and/or didn’t comply with many of the disease’s preventive measures during the height of the pandemic, according to a new nationwide study led in part by University of Utah Health scientists. The most common reasons were wanting to feel normal and exercise personal freedom.The study, which appears in the Oct. 10, 2022, issue of JAMA Network Open, raises concerns about how reluctance to accurately report health status and adherence to masking, social distancing, and other public health measures could potentially lengthen the current COVID-19 pandemic or promote the spread of other infectious diseases in the future, according to Angela Fagerlin, Ph.D., senior author of the study and chair of the Department of Population Health Sciences at U of U Health.“COVID-19 safety measures can certainly be burdensome, but they work,” says Andrea Gurmankin Levy, Ph.D., a professor of social sciences at Middlesex Community College in Connecticut. As co-lead author of the study, she worked in collaboration with Fagerlin and other scientists at U of U Heath as well as researchers elsewhere in the United States.“When people are dishonest about their COVID-19 status or what precautions they are taking, it can increase the spread of disease in their community.” Levy says. “For some people, particularly before we had COVID vaccines, that can mean death.”The researchers decided to assess how truthful Americans were being about their COVID-19 disease status and/or compliance with COVID-19 preventive measures after they noticed several media stories about people who were dishonest about their vaccination status, Fagerlin says.In the survey, conducted in December 2021, more than 1,700 people from across the country were asked to reveal whether they had ever misrepresented their COVID-19 status, vaccination status, or told others that they were following public health measures when they actually weren’t. The sample size is far larger and asked about a broader range of behaviors than previous studies on this topic, according to Fagerlin, who is also a research scientist at the Veteran Affairs Salt Lake City Healthcare System.Screening questions allowed the health service researchers and psychologists who designed the study to evenly divide the participants: one-third who had had COVID-19, one-third who had not had COVID-19 and were vaccinated, and one-third who had not had COVID-19 and were unvaccinated.Based on a list of nine behaviors, 721 respondents (42%) reported that they had misrepresented COVID-19 status or failed to follow public health recommendations. Some of the most common incidents were:Breaking quarantine rulesTelling someone they were with, or were about to see, that they were taking more COVID-19 precautions than they actually wereNot mentioning that they might have had, or knew that they had, COVID-19 when entering a doctor’s officeTelling someone they were vaccinated when they weren’tSaying they weren’t vaccinated when they actually wereAll age groups younger than 60 years and those who had a greater distrust of science were more likely to engage in misrepresentation and/or misrepresentation than others. About 60% of respondents said that they had sought a doctor’s advice for COVID-19 prevention or treatment.However, the researchers found no association between COVID-19 misrepresentation and political beliefs, political party affiliation, or religion.“Some individuals may think if they fib about their COVID-19 status once or twice, it’s not a big deal,” Fagerlin says. “But if, as our study suggests, nearly half of us are doing it, that’s a significant problem that contributes to prolonging the pandemic.”Among the reasons respondents gave for misrepresentation were:I didn’t think COVID-19 was real, or it was no big dealIt’s no one else’s businessI didn’t feel sickI was following the advice of a celebrity or other public figureI couldn’t miss work to stay homeAmong the study’s limitations, the researchers could not determine if respondents honestly answered survey questions, opening the possibility that their findings underestimated how commonly people misrepresented their health status.“This study goes a long way toward showing us what concerns people have about the public health measures implemented in response to the pandemic and how likely they are to be honest in the face of a global crisis,” says Alistair Thorpe, Ph.D., co-first author and a post-doctoral researcher in the Department of Population Health Sciences at U of U Health. “Knowing that will help us better prepare for the next wave of worldwide illness.”https://www.youtube.com/watch?v=8AbyzG9GYd8###In addition to Fagerlin and Thorpe, University of Utah Health researchers Holly Shoemaker, Frank A. Drews, Jorie M. Butler, and Vanessa Stevens contributed to this study. Other participating institutions include Middlesex Community College in Middletown, Connecticut; University of Colorado School of Medicine, Aurora; Veterans Affairs Denver Center for Innovation; University of Iowa School of Medicine, Iowa City; Salt Lake City VA Informatics Decision-Enhancement and Analytic Sciences (IDEAS) Center for Innovation; VA Salt Lake City Health Care System; and the American Heart Association.The study, “Misrepresentation and Nonadherence Regarding COVID-19 Public Health Measures,” appears in the Oct. 10, 2022, issue of JAMA Network Open. It was supported by the Jon M. Huntsman Presidential Endowment and an American Heart Association Children’s Strategically Focused Research Network Fellowship.",Survey finds more than 40% of Americans misled others about having COVID-19 and use of precautions
104,4,4_cancer_licl_gsk3_mice,https://ph.ucla.edu/news/press-release/2022/oct/ucla-fielding-school-public-health-led-research-demonstrates-importance,"An international team of researchers has demonstrated that among patients hospitalized for influenza, those who were vaccinated had less severe infections, including reducing the odds for children requiring admittance to an intensive care unit by almost half.In addition, the researchers found that deaths among hospitalized adults, 65 or older, who had been vaccinated were 38% lower compared to those who had not been vaccinated.“A common complaint about influenza vaccine is that they are typically 40-60% effective against infection - or the ‘what’s the point?’ complaint. So it is important to note that although everyone in this study was hospitalized, vaccinated individuals were less likely to be severely ill or die, suggesting that you are likely to have far less severe consequences if vaccinated,” said Dr. Annette Regan, UCLA Fielding School of Public Health assistant professor of epidemiology and lead author of the peer-reviewed research, published in the October edition of The Lancet Infectious Diseases. “This is an important point, especially in light of the upcoming influenza season coupled with ongoing COVID-19 activity, both this season and into the future.”Globally, influenza contributes to 9.5 million hospitalizations, 81.5 million hospital days, and 145,000 deaths each year. Vaccination offers the best method of preventing influenza illness, reducing illness in the general population by 40–60%, experts say.Specifically, The Lancet analysis found that three groups routinely targeted for influenza vaccination experiences less severe illness. Children who had received only part of their first series of influenza vaccines had 36% lower chances of being admitted to an intensive care unit (ICU), and children who had fully completed their first series of influenza vaccines had 48% lower chances of admission to ICU compared to unvaccinated children, the researchers found.The study – “Severity of influenza illness associated with seasonal influenza vaccination among hospitalized patients in four South American countries” – is the product of an international team of researchers from the United States, Argentina, Brazil, Chile, and Paraguay, and drew on data from all four South American countries over a period of seven years. Data were obtained through the Network for the Evaluation of Vaccine Effectiveness in Latin America and the Caribbean, influenza (REVELAC-i) which is coordinated by the Pan American Health Organization (PAHO).“Although several studies have reported drops in influenza illness following influenza vaccination, the results have focused predominantly on adults in the United States, and this study aimed to evaluate the severity of influenza illness by vaccination status in a broad range of age groups, and across multiple South American countries,” said Dr. Marta Von Horoch, a co-author who serves as coordinator of the National Immunization Program in Paraguay. “We were very pleased to work with our partners in the U.S. and across the continent, and these findings demonstrate, quite clearly, the importance of influenza vaccination for children and adults, no matter where they live.”The study – the first-ever on this scale in South America - examined influenza-related hospitalization rates and outcomes across all four countries from 2013-19. Specifically, the analysts reviewed the outcomes for some 2,747 patients hospitalized with confirmed influenza virus infection, in three age groups – children aged 6–24 months, adults aged 18–64 years, and adults aged 65 years or older.Given the reality that vaccination rates have fallen, in the U.S. and globally during the COVID-19 pandemic, including among children, the findings should help make clear the benefits of timely, pro-active immunization campaigns to the public, the researchers said.“With influenza season approaching this winter and influenza vaccines now available, these results highlight the importance of getting vaccinated for flu for anyone six months of age or older – as CDC recommends,” Regan said. “It is critical that healthcare providers and the public understand the risks of missing out on vaccinations – it is so much better to prevent a serious illness than to suffer through it, for the patient and everyone in their community.”===================================================",UCLA Fielding School of Public Health-led research demonstrates the importance of influenza vaccination globally
386,4,4_cancer_licl_gsk3_mice,https://www.hsph.harvard.edu/news/press-releases/political-ideology-of-u-s-elected-officials-linked-with-covid-19-health-outcomes/,"For immediate release: Tuesday, November 1, 2022Boston, MA – The higher the exposure to political conservatism, the higher the COVID-19 mortality rates and stress on hospital intensive care unit (ICU) capacity, according to a new study from Harvard T.H. Chan School of Public Health.“Before our study, research on how political ideology affects COVID-19 looked solely at voters’ political lean; we expanded on that research to investigate associations of COVID-19 outcomes with the voting records of federal elected representatives and the concentration of political party power at the state level. The point is not partisan analysis, but rather to understand how politics, and political polarization, are affecting population health,” said Nancy Krieger, professor of social epidemiology at Harvard T.H. Chan School of Public Health and corresponding author of the study.The study was published online in the Lancet Regional Health – Americas on November 1, 2022.Little prior research had looked at COVID-19 health outcomes in relation to U.S. congressional districts. The researchers analyzed data on COVID-19 mortality rates and stress on ICU capacity from April 2021 to March 2022, a period when adult vaccines were available, across all 435 U.S. Congressional districts. They examined three exposure variables that had not been used previously in COVID-19 research: the political ideology of U.S. elected members of Congress, as measured by their overall voting records; their votes on four key COVID-19 relief bills; and “state trifectas,” the concentration of political power at the State level, defined as the governor, House, and Senate, all under the control of one party.The study found that the higher the exposure to conservatism on each political metric, the higher the COVID-19 age-standardized mortality rates, even after adjusting for the district’s social characteristics, voters’ political lean, and vaccination rates. The same relationship held true for stress on hospital ICU capacity.For COVID-19 mortality rates, for example, models controlling for political and social metrics and vaccination rates showed that Republican trifectas were, respectively, 11% higher and conservative voter political lean 26% higher.“Our study offers new approaches to analyzing political determinants of COVID-19 metrics—such as mortality, illness, or vaccination rates—as one component of analyzing political accountability for populations’ COVID-19 burdens. It also points to the importance of analyzing political metrics in relation to population health outcomes more generally,” said Krieger.There was no funding for the study from any agency in the public, commercial, or not-for-profit sector.“Relationship of political ideology of US federal and state elected officials and key COVID pandemic outcomes following vaccine rollout to adults: April 2021-March 2022,” Nancy Krieger, Christian Testa, Jarvis T. Chen, William P. Hanage, Alecia J. McGregor, Lancet Regional Health – Americas, online November 1, 2022, doi: 10.1016/j.lana.2022.100384Visit the Harvard Chan School website for the latest news, press releases, and multimedia offerings.For more information:Todd Datztdatz@hsph.harvard.edu617.432.8413###Harvard T.H. Chan School of Public Health brings together dedicated experts from many disciplines to educate new generations of global health leaders and produce powerful ideas that improve the lives and health of people everywhere. As a community of leading scientists, educators, and students, we work together to take innovative ideas from the laboratory to people’s lives—not only making scientific breakthroughs, but also working to change individual behaviors, public policies, and health care practices. Each year, more than 400 faculty members at Harvard Chan School teach 1,000-plus full-time students from around the world and train thousands more through online and executive education courses. Founded in 1913 as the Harvard-MIT School of Health Officers, the School is recognized as America’s oldest professional training program in public health.",Political ideology of U.S. elected officials linked with COVID-19 health outcomes
83,4,4_cancer_licl_gsk3_mice,https://newatlas.com/medical/radioactive-implant-wipes-tumors-unprecedented-pre-clinical-success/,"Engineers at Duke University have developed a novel delivery system for cancer treatment and demonstrated its potential against one of the disease’s most troublesome forms. In newly published research in mice with pancreatic cancer, the scientists showed how a radioactive implant could completely eliminate tumors in the majority of the rodents, demonstrating what they say is the most effective treatment ever studied in these pre-clinical models.Pancreatic cancer is notoriously difficult to diagnose and treat, with tumor cells of this type highly evasive and loaded with mutations that make them resistant to many drugs. It accounts for just 3.2 percent of all cancers, yet is the third leading cause of cancer-related death. One way of tackling it is by deploying chemotherapy to hold the tumor cells in a state that makes them vulnerable to radiation, and then hitting the tumor with a targeted radiation beam.But doing so in a way that attacks the tumor but doesn’t expose the patient to heavy doses of radiation is a fine line to tread, and raises the risk of severe side effects. Another method scientists are exploring is the use of implants that can be placed directly inside the tumor to attack it with radioactive materials from within. They have made some inroads using titanium shells to encase the radioactive samples, but these can cause damage to the surrounding tissue.""There's just no good way to treat pancreatic cancer right now,"" said study author Jeff Schaal.Schaal and his team explored an alternative type of implant, one made from more biocompatible materials that wouldn’t post the same risks to the human body. The scientists used synthetic chains of amino acids known as elastin-like polypeptides (ELPs), which remain in a liquid state at room temperature but form a stable gel-like material in the warmer environment of the body.This substance was injected into tumors in various mouse models of pancreatic cancer along with a radioactive element called iodine-131, an isotope that is well-studied and widely used in medical treatment. In this environment, the ELP entombs the iodine-131 and prevents it leaking into the body, but allows it to emit beta radiation that penetrates into the surrounding tumor. Once the radiation is spent, the ELP biogel safely degrades into harmless amino acids.The treatment was tested in combination with a common chemotherapy drug called paclitaxel. The radioactive implants were injected into cancer tumors just beneath the skin, but with mutations known to occur in pancreatic cancer, and into tumors within the pancreas itself that are historically more difficult to treat.Across all the models tested, the scientists report a 100% response rate to the treatment. In three quarters of the models, the dual treatment completely eliminated the tumors 80% of the time. The scientists deployed the novel treatment against pancreatic cancer because they wanted to explore its potential against one of the trickiest forms of the disease, but believe these results bode well for its wider application.""We think the constant radiation allows the drugs to interact with its effects more strongly than external beam therapy allows,"" Schaal said. ""That makes us think that this approach might actually work better than external beam therapy for many other cancers, too.""There is lots to play out before that happens, with trials on larger animals the immediate next step for the researchers. They do say these findings are unparalleled in terms of how effectively the treatment was able to disintegrate the tumors, with team member Ashutosh Chilkoti describing them as “perhaps the most exciting” results against late-stage pancreatic cancer his lab has produced in almost 20 years.""We did a deep dive through over 1,100 treatments across preclinical models and never found results where the tumors shrank away and disappeared like ours did,"" said Schaal. “When the rest of the literature is saying that what we're seeing doesn't happen, that's when we knew we had something extremely interesting.""The research was published in the journal Nature Biomedical Engineering.Source: Duke University via MedicalXpress",Radioactive implant wipes tumors in unprecedented pre-clinical success
446,5,5_study_participants_brain_schizophrenia,https://www.psypost.org/2022/10/bedtime-procrastination-helps-explain-the-link-between-anxiety-and-sleep-problems-64181,"New research indicates that anxious people tend to engage in higher levels of bedtime procrastination, which in turn explains why they tend to experience more sleep problems. The findings have been published in the Journal of Clinical Psychology.Psychologists have recently begun to investigate the phenomenon of bedtime procrastination, or the tendency to put off going to bed despite having ample opportunity to fall asleep. There are a number of theories about why this occurs. For example, people who struggle with self-regulation may have trouble quitting other activities. Previous research has found that the inability to be mindfully attentive to the present is linked to bedtime procrastination.Study author Rebecca L. Campbell of the University of Arkansas wondered whether those high in anxiety were more likely to engage in bedtime procrastination because they were avoiding the negative experience of trying to fall asleep while in a state of heightened arousal.“Sleep loss is a public health crisis and if we are going to help people get more sleep, we need to understand what is getting in the way of their sleep in the first place. Bedtime procrastination might explain some challenges with getting enough sleep,” explained Campbell, who co-authored the research with Ana J. Bridges.For their study, the researchers recruited 308 adult primary care patients from two primary care clinics between August 2019 and March 2020. Most of the participants (69.8%) were female and the average age of the sample was 33.30 years.The researchers asked the patients how often in the past two weeks they had felt tense or nervous, felt irritated, and found themselves daydreaming, worrying, or staring into space. To assess bedtime procrastination, the patients were asked to respond to two questions: “In the past week, what time did you want to go to sleep?” and, “In the past week, what time did you actually go to sleep?” The patients also reported their total sleep time and problems with sleep in the past 2 weeks.Campbell and Bridges found that more anxious patients tended to show greater bedtime procrastination and tended to sleep fewer total hours per night compared to their less anxious counterparts. Those with higher symptoms of anxiety also reported more problems with sleep.Importantly, the researchers found evidence that the link between anxiety and sleep problems was mediated by bedtime procrastination. In other words, people with greater anxiety tend to experience more sleep problems, in part, because they engage in more bedtime procrastination.“One of the ways that anxiety impacts our sleep is through bedtime procrastination (putting off bedtime despite ample opportunity to go to bed),” Campbell told PsyPost. “If you are noticing anxiety in your day-to-day and you regularly stay up later than you intended, it might be time to seek professional support.”The researchers also asked the patients the extent to which they agreed with the statement “I think sleep is important for my well‐being.” Surprisingly, however, the patients’ attitude toward sleep was not associated with total sleep time, bedtime procrastination, or sleep problems.“We found that the majority of participants agreed that sleep is important,” Campbell explained. “On one hand, this fantastic because it means we do not have to convince people why sleep is essential. On the other hand, it suggests sleep loss is more complicated than a matter of motivation. We will most likely need to address a series of complex barriers to fully address sleep loss as a public health crisis.”In addition, the reason that anxiety is associated with greater bedtime procrastination is still unclear. Future research could examine whether bedtime procrastination is a form of anxious avoidance or whether people high in anxiety engage in bedtime procrastination because they are busy managing stressors.“We did not collect data on why people were delaying going to bed, just that they were going to bed later than intended,” Campbell said. “Future studies will need to address this more fully. Additionally, the relation we noted is most likely bidirectional and will need further investigation.”The study, “Bedtime procrastination mediates the relation between anxiety and sleep problems“, was published September 28, 2022.",Bedtime procrastination helps explain the link between anxiety and sleep problems
60,5,5_study_participants_brain_schizophrenia,https://institutducerveau-icm.org/en/actualite/in-the-brains-of-procrastinators/,"A research team from Inserm, CNRS, Sorbonne University and AP-HP at the Paris Brain Institute in Paris just deciphered how our brain behaves when we procrastinate. The study, conducted in humans, combines functional imaging and behavioral testing. It led to identify a region of the brain where the decision to procrastinate is made: the anterior cingular cortex. The team also developed an algorithm to predict participants’ tendency to procrastinate. This work is published in Nature Communications.Procrastination, or the tendency to postpone tasks that we are supposed to do, is an experience – often uncomfortable and even guilty – that many of us have already lived. Then why, and in which conditions, does our brain push us to procrastinate? To answer this question, a team led by Mathias Pessiglione, Inserm researcher, and Raphaël Le Bouc, neurologist at the AP-HP, from the Paris Brain Institute (Inserm/CNRS/Sorbonne University/AP-HP) conducted a study with 51 participants.In order to decipher procrastination behavior, these individuals participated in a number of tests during which their brain activity was recorded by MRI. Each participant was first asked to subjectively assign a value to rewards (cakes, flowers…) and efforts (memorizing a number, doing push-ups…). They were then asked to indicate their preferences between getting a small reward quickly or a large reward later, as well as between a small effort to be made right away or a larger effort to be made later.The imaging data revealed activation during decision making in a brain region called the anterior cingulate cortex. This region is responsible for performing a cost-benefit calculation by integrating the costs (efforts) and benefits (rewards) associated with each option.The tendency to procrastinate was then measured by two types of tests. In the first, participants were asked to decide whether to produce an effort on the same day to obtain the associated reward immediately, or to produce an effort on the following day and wait until then to obtain the reward. In the second, upon returning home, participants had to fill out several rather tedious forms and return them within a month to be compensated for their participation in the study.The MRI data and the tests scores feed a mathematical model of decision making, called “neuro-computational”, developed by the researchers.“Our model takes into account the costs and benefits of a decision, but also integrates the deadlines when they occur,” explains Raphaël Le Bouc. For example, for a task such as washing dishes, the costs are linked to the long and boring aspect of the chore and the benefits to the fact that the kitchen is clean at the end of the task. Washing the dishes is very tedious in the moment; considering doing it the next day is a little less so. Similarly, being paid immediately after a job is motivating, but knowing that you will be paid a month later is much less so. It is said that these variables, the cost of effort as well as the value of rewards, diminish with time, as they move further into the future,” adds the researcher. Thus, the more distant the deadline is, the less costly the effort seems and the less rewarding the reward seems. “Procrastination could be specifically related to the impact of the deadline on the evaluation of tasks requiring effort. More precisely, it can be explained by the tendency of our brain to count costs faster than rewards,” concludes Mathias Pessiglione.Using information about the activity of their anterior cingular cortex and data collected during behavioral testing, the researchers established a motivational profile for each of the participants. This profile described their attraction to rewards, their aversion to effort, and their tendency to devalue benefits and costs with time. Considering those profiles researchers were able to estimate the tendency to procrastinate for each of the participants. Thanks to their model they were able to predict how long it would take for each participant to return the completed form.This research could help to develop individual strategies to stop putting off chores that are within our reach. They could also help avoid the pernicious effects of procrastination in fields as varied as education, economics and health.SourcesA neuro-computational account of procrastination behaviourLe Bouc Raphaël 1,2 & Pessiglione Mathias 11Motivation, Brain and Behavior (MBB) Lab, Paris Brain Institute (ICM), Sorbonne University, Inserm, CNRS, Pitié-Salpêtrière hospital, Paris, France.2Department of Neurology, Pitié-Salpêtrière hospital, Sorbonne University, Assistance Publique – Hôpitaux de Paris (APHP), Paris, France.Nature Communications, septembre 2022DOI : https://doi.org/10.1038/s41467-022-33119-w",In the brains of procrastinators
448,5,5_study_participants_brain_schizophrenia,https://www.psypost.org/2022/10/dark-personalities-perceive-pro-environmental-behaviors-as-more-costly-and-less-beneficial-64078,"People with “dark” personality traits tend to behave in less environmentally friendly ways in everyday life, and view pro-environmental behaviors as imposing a greater burden, according to new research published in Frontiers in Psychology. The findings provide evidence that personality traits influence how people perceive the costs and benefits associated with pro-environmental behaviors.“In view of the looming climate crisis, I personally keep asking myself why people (despite better knowledge) do not behave in a more environmentally friendly way,” said study author Jana Sophie Kesenheimer, a postdoctoral researcher at the Leopold-Franzens-University of Innsbruck. “There are psychological theories (e.g. the ‘Campbell paradigm’) that take into account the cost and benefit ratios of environmentally conscious decision-making. Our aim for the study was to bring these situational costs and benefits into connection with personality and attitude.”Kesenheimer and her colleagues were particularly interested in the so-called “light triad” and “dark tetrad” of personality.The light triad is a group of three personality traits that are associated with positive characteristics such as honesty, empathy, and trustworthiness. The three traits are Kantianism, faith in humanity, and humanism. People high in these traits agree with statements such as “I prefer honesty over charm” (Kantianism), “I tend to see the best in people” (faith in humanity), and “I tend to treat others as valuable” (humanism).The dark tetrad is a group of four personality traits that are associated with harmful behaviors. The four traits are sadism, narcissism, Machiavellianism, and subclinical psychopathy. People high in these traits agree with statements such as “I hurt others for my own pleasure” (sadism), “People see me as a natural leader” (narcissism), “I love it when a tricky plan succeeds” (Machiavellianism), and “People who mess with me always regret it” (psychopathy).The initial study included 176 participants who ranged in age from 18 to 68 years. The participants completed scientific assessments of light and dark personality traits, along with a measure of pro-environmental attitudes. After completing this initial survey, the participants received a notification on their smartphones three times a day for seven consecutive days that asked them if they had “acted pro-environmentally at least once in the past 4 hours.” The participants were further asked to rate the costs and benefits associated with these behaviors.The most commonly reported pro-environmental behaviors were related to food intake, such as eating vegetarian or locally grown food. The second most commonly reported pro-environmental behaviors were related to energy and water conservation, such as dressing warmer rather than turning up the heat.People with more pro-environmental attitudes and people with stronger light personality traits tended to engage in pro-environmental behaviors more frequently. Those with stronger dark personality traits, in contrast, tended to engage in pro-environmental behaviors less frequently. Similarly, people with more pro-environmental attitudes and people with stronger light personality traits tended to view the behaviors as having greater benefits, while those with stronger dark personality traits viewed the behaviors as having less benefits.People with stronger dark personality traits, however, reported engaging in more costly behaviors, while the opposite was true among those with stronger light personality traits.But are people with dark personality traits really engaging in more costly pro-environmental action compared to those with light personality traits? The researchers were skeptical. They proposed that the observed effect was the result of differences in the perception of what is “costly.”“We originally wanted to show that personality and attitude are more influential in high-cost/low-benefit situations, but are less predictive in high-benefit/low-cost situations (because most people should be pro-environmental in the latter case),” Kesenheimer explained. “Surprisingly, it turned out that the assessment of the costs and benefits depends very much on personality and attitude. So much so that we had to get a second opinion from an independent person (in Study 2) to assess the actions of the people in the first survey.”In the second study, a sample of 159 individuals viewed and rated the pro-environmental behaviors reported by participants in the first study. The findings confirmed the researchers’ suspicions.“Above all, we were able to show that, depending on the personality and attitude of a person, there can be a very subjective perception of the costs and benefits of a situation,” Kesenheimer told PsyPost.“For example, a rather ‘dark’ personality described a great deal of effort involved in using a lid for cooking (he first had to dig the lid out of the drawer). Other people described the same situation on average as being less costly. If you ask a ‘light’ personality or a very environmentally conscious person for their assessment, he or she would probably see much lower costs and high benefits in the same behavior than the ‘dark’ person.”“We were therefore able to show that the costs and benefits of an environmentally-conscious decision-making situation are anything but clear, but depend heavily on a person’s personality and attitude: some might always see a benefit in any environmental behavior, others just generally see high costs involved.” the researcher explained.The study, “Going Green Is Exhausting for Dark Personalities but Beneficial for the Light Ones: An Experience Sampling Study That Examines the Subjectivity of Pro-environmental Behavior“, was authored by Jana Sophie Kesenheimer and Tobias Greitemeyer.",Dark personalities perceive pro-environmental behaviors as more costly and less beneficial
449,5,5_study_participants_brain_schizophrenia,https://www.psypost.org/2022/10/economic-scarcity-can-invigorate-racial-stereotypes-and-even-alter-our-mental-representations-of-black-individuals-64033,"New research provides evidence that racial stereotypes tend to be strengthened under conditions of scarcity. The findings, published in the Journal of Experimental Social Psychology, indicate that economic scarcity can influence mental representations of Black people.Previous research has indicated that when jobs are few and competition is high, people are more likely to view others in terms of race, seeing members of their own group as more deserving of scarce resources. This could lead to increased tension and conflict between different racial groups. But little is known about the psychological mechanisms driving this effect.“I became very interested in this topic because it offers a unique perspective on a pervasive social issue — the amplification of racial disparities under times of economic recession,” said study author Michael M. Berkebile-Weinberg, a PhD student in social psychology at New York University.“This phenomenon is multi-faceted, and this work in particular tries to understand how we as individuals are both impacted by and help propagate systemic forces. That interplay between the individual and the systems that operate in society is a fundamental aspect of intergroup social cognition, and a driving theme in our research going forward.”In a study of 165 mostly White participants, the researchers used a money allocation game to manipulate perceived scarcity. The participants were informed that they would be given a sum of money, and would then choose how much to allocate to their partner. They were told the endowment size would be determined randomly, but in reality the sum of money was fixed.In the scarcity condition, the participants were told they had only been given $10 out of a possible $100. In the control condition, they were told they had been given $10 out of a possible $10.When participants were led to believe that resources were scarce, they were more likely to report having knowledge of stereotypes of Black Americans as low in socioeconomic status and as threatening (e.g. uneducated and aggressive) compared to those in the control group. But scarcity had no impact on stereotypes of White American.In two additional studies, which included nearly 500 non-Black participants, the researchers used what is known as a reverse correlation procedure to examine visualizations of a Black male face.In this task, the participants were presented with 200 pairs of faces that were overlaid with different patterns of visual noise, which subtly distorted the image to produce unique facial features. They were then asked which face, out of each pair, depicted a Black person. The images selected as “Black” on each trial were averaged to create a composite image that represented the mental representation of a Black person.Berkebile and his colleagues found that visualizations of Black faces produced under perceived scarcity appeared more threatening and lower in socioeconomic status compared with a control condition, suggesting that “scarcity leads perceivers to form more stereotypical mental representations of Black people.” Visualizations of Black faces produced under perceived scarcity were also evaluated less positively by a group of independent raters.“I think the fundamental take-away from this work is that threats in our environment (like signals of economic recession) can fundamentally impact the stereotypes that we hold,” Berkebile told PsyPost. “Further, this extends past just what we think of each other, to affect how we actually see each other, making this process really difficult to control.”The researchers third study also included an implicit association test, which is often used to measure unconscious racial bias. The test works by measuring the respondent’s reaction time to certain words or images.Participants with stronger implicit racial bias tended to visualize Black individuals’ faces in a more stereotypical manner, and this was not significantly impacted by perceived scarcity. But among participants with relatively weak implicit racial bias, perceived scarcity appeared to result in a substantial increase in stereotypical visualizations.“We found an interesting and very surprising effect in the last study,” Berkebile explained. “The general effect that we reported before — that economic scarcity increased stereotyping — was actually strongest for those with less implicit bias compared to those with more implicit bias. We did not have a strong prediction for this analysis, but were surprised to see that those with less bias were impacted by scarcity the most. This analysis was exploratory, so we should replicate this finding before giving it much weight.”The study includes some limitations and points to areas for additional research. “All of the studies looked at these processes in a strict Black/White American perspective,” Berkebile said. “Future work should extend beyond these groups and test how such patterns play out for different races and other social categories altogether (like gender, for example). Also, we are yet to test the driving force behind these effects. Future work will dive into the mechanisms here to answer why these patterns emerge.”The study, “Economic scarcity increases racial stereotyping in beliefs and face representation“, was authored by Michael M. Berkebile-Weinberg, Amy R. Krosch, and David M. Amodio.",Economic scarcity can invigorate racial stereotypes and even alter our mental representations of Black individuals
450,5,5_study_participants_brain_schizophrenia,https://www.psypost.org/2022/10/expressing-gratitude-when-doing-stressful-work-reduces-teammates-stress-and-leads-to-more-efficient-cardiovascular-responses-64148,"A new social experiment conducted at the University of California, San Diego focused on cardiovascular responses of the body to a stressful task done by pairs of people in loose-tie relationships. It showed that a team member expressing gratitude improves the cardiovascular responses of teammates, making their bodies react to the task at hand as towards a challenge rather than a threat. The study was published in the Journal of Experimental Psychology: General.Many previous studies have shown that the emotion of gratitude is very beneficial in social life. It was shown to positively influence personal well-being, satisfaction in relationships, affiliative behavior and many other psychological conditions. As authors of this study state, “momentary emotional response of gratitude to another person for their kind actions helps promote a high-quality, communal relationship between the grateful person and their benefactor” and this has lead many researchers to focus on the study of expressed gratitude as a behavioral mechanisms that facilitates the relationship between the person expressing and receiving gratitude.The other aspect of this experiment is the biopsychosocial model (BPS) of challenge and threat. This model states that “when people appraise that the demands of a task exceed their own resources to complete the task, they are likely to experience a threat response, marked by a less efficient cardiovascular activation.” On the other hand, when people see their resources as exceeding the demands of the task at hand, they will experience a challenge response, marked by a more efficient cardiovascular activation pattern.The importance of this model for the study is that it allows researchers to determine whether a person perceives the situation at hand as a challenge or a threat and to what degree by measuring physiological reactions of a person using appropriate instruments (rather than having to interview the person or relying on self-report assessments).To study whether expressing gratitude influences challenge and threat responses, Yumeng Gu and her colleagues devised an experiment in which they organized participants into two-person teams or dyads. Participants were 190 mostly female undergraduate students of the University of California, San Diego, who each received $24 for their participation in this experiment and a larger study on gratitude expression.Students within each team were “same-gender, first-year students who had been living together as suitemates for approximately four months.” Teams were randomly divided into two groups – experimental and control group. In each team of the experimental group one of the students expressed gratitude by talking to her/his teammate about something that teammate did in the past for which the student felt grateful, while the control group proceeded directly to other tasks.Teammates then completed a collaborative task during which they had to design a product, marketing plan and pitch. After this, they individually presented their part of the product pitch to the evaluators (individual performance task). Physiological measurements of cardiovascular activity were taken before and during parts of this procedure.The researchers found that “when one member of a team expressed gratitude to the other prior to engaging in stressful collaborative work, the team members were buffered from inefficient (threat-patterned) cardiovascular responding” as compared to the control group. The effect was present in both teammates. Results showed that gratitude expression enhanced cardiovascular efficiency not only immediately after it was expressed and while teammates worked together, but also later when participants completed individual performance tasks (individually).This experiment provided first evidence that expression of gratitude creates positive changes in biological responses of teammates, but authors note that relationships between teammates in the study are “not strictly representative of work teammates.” Also, participants interacted face-to-face during the experiment, so it remains unknown whether similar effects would be found in situations of online collaboration.The study, “Gratitude expressions improve teammates’ cardiovascular stress responses“, was authored by Yumeng Gu, Joseph M. Ocampo, Sara B. Algoe, and Christopher Davis.",Expressing gratitude when doing stressful work reduces teammatesâ stress and leads to more efficient cardiovascular responses
451,5,5_study_participants_brain_schizophrenia,https://www.psypost.org/2022/10/feeling-low-take-a-walk-down-memory-lane-study-says-nostalgia-improves-psychological-well-being-64188,"Nostalgia, or the good feeling one gets when thinking about the past, may not be getting the respect it deserves. A recent study in the Journal of Experimental Social Psychology concludes that the more nostalgic one is, the more authentic one feels, which has positive consequences for psychological well-being. The research team found correlational and experimental support for their hypothesis. Moreover, the effect was cross-cultural; subjects from the United States, China, and the United Kingdom were included in the study.Factors that improve psychological well-being (PWB) are studied frequently and put to good use as components of therapy or valuable life advice. However, for some factors, it is not always clear why it improves PWB. Prior studies have found that nostalgic thinking improves PWB, but why was unknown. Nicholas Kelley and colleagues set out to answer this question with a series of four studies.Their hypothesis is the feeling of authenticity, generated by nostalgic thinking that increases PWB. Understanding how behaviors or cognitions improve PWB provides opportunities to establish innovative methods to increase PWB.The first of the four studies confirmed a relationship between nostalgia, PWB, and authenticity. In these studies, authenticity was defined as “the sense that one is in alignment with one’s true self.” Psychological well-being was assessed with the Brief Inventory of Thriving (BIT).The remaining studies were experimental and demonstrated cause and effect with each step. Study two demonstrated that nostalgia increased authenticity. Study three found authenticity increased PWB, and study four found authenticity increased PWB across all well-being concepts. There were 2423 participants aged 18-78, approximately 50% were American, 33% Chinese, and 17% British.The researchers posited that psychological well-being is made up of many factors and were curious if authenticity had consequences for some or all of the factors. Their results found authenticity, induced with nostalgia, resulted in statistically significant increases in all measured components of psychological well-being (social relationships, vitality, competence, meaning of life, optimism, and subjective well-being).This was true cross-culturally, with participants from the U.S., U.K., and China producing similar results. These findings demonstrate that an enjoyable walk down memory lane can induce feelings of authenticity and thus improve total well-being.The research team acknowledges there is more research to be done. For example, does feeling authentic result in behaving authentically? Future research could include subjective measures of authenticity, or subjects could keep a diary of feelings and behaviors related to nostalgia, authenticity, and well-being as they move through their daily lives.Regardless of future research, this work is a meaningful contribution to our understanding of the benefits of authenticity. Much of the prior research has been correlational. The work of Kelley and colleagues contributes experimental data to the literature. In their words, “Thus, we showed, for the first time, that nostalgia instills a general sense of psychological thriving. Our work has implications for process models of nostalgia’s benefits.”The study, “Nostalgia confers psychological well-being by increasing authenticity” , was authored by Nicholas Kelley, William Davis, William Davis, Jianning Dang, Li Liu, Tim Wildschut and Constantine Sedikides.",Feeling low? Take a walk down memory lane. Study says nostalgia improves psychological well-being
452,5,5_study_participants_brain_schizophrenia,https://www.psypost.org/2022/10/having-more-time-to-oneself-is-the-top-reported-benefit-of-being-single-study-finds-64145,"New research published in the journal Evolutionary Psychological Science suggests that people view the single life as an opportunity to focus on self-development. Having more time for themselves, being able to focus on their goals, and having no one else dictate their actions were among the most highly rated benefits of being single.Studies suggest that more and more people are living the single life. While part of this trend may be driven by difficulties obtaining a relationship partner, it seems that a good portion of people are choosing to be single. This rise in singlehood seems to contradict evolutionary theory. According to an evolutionary perspective, people are motivated toward long-term relationships since these arrangements offer the best chances of one’s genes being passed on to future generations.“Singlehood appears to be on the rise especially in Western societies,” said study author Menelaos Apostolou, a professor at the University of Nicosia. “One reason may be that people see benefits in being single, which motivated me to ask the question ‘what people perceive as beneficial in being single?'”Apostolou and his co-author Chistoforos Christoforou launched a pair of studies to examine what people consider the advantages of being single. In a first online questionnaire, the researchers asked 269 Greek-speaking men and women to write down some of the advantages enjoyed by single people. Two independent researchers then analyzed these responses and identified 84 distinct benefits.To narrow down this list, Apostolou and Christoforou conducted a follow-up study where they presented the list of 84 advantages to a larger sample of 612 Greek-speaking people. The participants were asked to rate how important each advantage would be to them if they were single.The researchers then used a statistical technique called principal component analysis to classify the 84 items into a smaller number of broader categories based on participants’ ratings. This resulted in a set of 10 factors, and the three most highly rated factors were “more time for myself”, “focus on my goals”, and “no one dictates my actions.” The other seven factors were: “no getting hurt”, “better control of what I eat”, “freedom to flirt around”, “save resources”, “peace of mind”, “no tension and fights”, and “not do things I dislike.”The analysis further revealed significant gender differences. Men rated the factor “freedom to flirt around” as a more important advantage than women did. Conversely, women gave higher ratings to “no tension and fights” and “focus on my goals.” There were also age effects — the strongest effects were that older respondents rated “more time for myself” and “not do things I dislike” as more important than younger respondents.In line with their predictions, the authors said that respondents’ emphasis on having more time for themselves, more resources, and being able to focus on their goals suggests that people find singlehood appealing partly because it allows them to develop their own strengths. The emphasis on having peace of mind and avoiding tension, fights, and getting hurt suggests that being single helps people avoid experiencing negative emotions. Finally, the “freedom to flirt around” factor suggests that singlehood is also appreciated because it allows people to engage in casual relationships.The researchers also proposed that while there are evolutionary costs to being single, there are times when it may be advantageous. For example, being single for a period of time can allow people to focus on obtaining a job promotion or pursuing their studies. “Instead of only asking whether mated or single life is better, we can ask when it is better for an individual to be single and for how long,” Apostolou and Christoforou wrote. “Considerable more research is necessary however, in order to address such questions.”Although not predicted by the researchers’ hypotheses, the factors of “better control of what I eat”, “no one dictates my actions”, and “not do things I dislike” may reflect the various compromises that intimate relationships entail.“There are potentially several benefits in being single, such as the freedom to do whatever you want,” Apostolou told PsyPost.But “I would predict that the costs of singlehood are probably higher than its benefits, which possibly explains why many singles prefer not to be single, and why most people eventually enter into a relationship,” he added. “These costs remain to be researched.”One limitation to note is that the study was conducted among Greek-speaking participants from the Republic of Cyprus, and the findings may not generalize beyond this cultural context. Cross-cultural studies may help illuminate how singlehood is perceived in different cultures.“There is not much research on singlehood, so additional studies are needed in order to understand the phenomenon,” Apostolou said. “The current study was conducted in the Greek cultural context, so some of its findings may not readily generalize to other cultural contexts.”The study, “What Makes Single Life Attractive: an Explorative Examination of the Advantages of Singlehood”, was authored by Menelaos Apostolou and Chistoforos Christoforou.","Having more time to oneself is the top reported benefit of being single, study finds"
363,5,5_study_participants_brain_schizophrenia,https://www.eurekalert.org/news-releases/969060,"New research from King’s College London has found that seeing or hearing birds is associated with an improvement in mental wellbeing that can last up to eight hours.This improvement was also evident in people with a diagnosis of depression – the most common mental illness worldwide – indicating the potential role of birdlife in helping those with mental health conditions.Published in Scientific Reports, the study used smartphone application Urban Mind to collect people’s real-time reports of mental wellbeing alongside their reports of seeing or hearing birdsong.This project was funded by the Medical Research Council (MRC), National Institute for Health and Care Research (NIHR) Maudsley Biomedical Research Centre and the NIHR Applied Research Collaboration South London.Lead author Ryan Hammoud, Research Assistant at the Institute of Psychiatry, Psychology & Neuroscience (IoPPN), King’s College London, said: “There is growing evidence on the mental health benefits of being around nature and we intuitively think that the presence of birdsong and birds would help lift our mood. However, there is little research that has actually investigated the impact of birds on mental health in real-time and in a real environment. By using the Urban Mind app we have for the first time showed the direct link between seeing or hearing birds and positive mood. We hope this evidence can demonstrate the importance of protecting and providing environments to encourage birds, not only for biodiversity but for our mental health.”The study took place between April 2018 and October 2021, with 1,292 participants completing 26,856 assessments using the Urban Mind app, developed by King’s College London, landscape architects J&L Gibbons and arts foundation Nomad Projects.Participants were recruited worldwide, with the majority being based in the United Kingdom, the European Union and United States of America.The app asked participants three times a day whether they could see or hear birds, followed by questions on mental wellbeing to enable researchers to establish an association between the two and to estimate how long this association lasted.The study also collected information on existing diagnoses of mental health conditions and found hearing or seeing birdlife was associated with improvements in mental wellbeing in both healthy people and those with depression. Researchers showed that the links between birds and mental wellbeing were not explained by co-occurring environmental factors such the presence of trees, plants, or waterways.Senior author, Andrea Mechelli, Professor of Early Intervention in Mental Health at IoPPN, King’s College London said: “The term ecosystem services is often used to describe the benefits of certain aspects of the natural environment on our physical and mental health. However, it can be difficult to prove these benefits scientifically. Our study provides an evidence base for creating and supporting biodiverse spaces that harbour birdlife, since this is strongly linked with our mental health. In addition, the findings support the implementation of measures to increase opportunities for people to come across birdlife, particularly for those living with mental health conditions such as depression.”Research partner & Landscape Architect Jo Gibbons, of J & L Gibbons said: “Who hasn’t tuned into the melodic complexities of the dawn chorus early on a spring morning? A multi-sensory experience that seems to enrich everyday life, whatever our mood or whereabouts. This exciting research underpins just how much the sight and sound of birdsong lifts the spirits. It captures intriguing evidence that a biodiverse environment is restorative in terms of mental wellbeing. That the sensual stimulation of birdsong, part of those daily ‘doses’ of nature, is precious and time-lasting.”The study, ‘Smartphone-based ecological momentary assessment reveals mental health benefits of birdlife’ was published in Scientific Reports.This study has been funded by the Medical Research Council, the National Institute for Health and Care Research (NIHR) Biomedical Research Centre at South London and Maudsley NHS Foundation Trust and King’s College London and the NIHR Applied Research Collaborative South London.After the embargo has lifted it will be available here: https://www.nature.com/articles/s41598-022-20207-6",Feeling chirpy: Being around birds is linked to lasting mental health benefits
56,5,5_study_participants_brain_schizophrenia,https://gumc.georgetown.edu/news-release/after-stroke-in-an-infants-brain-right-side-of-brain-compensates-for-loss-of-language-in-left-side/#,"Home ▸ News Release ▸ After Stroke in an Infant’s Brain, Right Side of Brain Compensates for Loss of Language in Left SideAfter Stroke in an Infant’s Brain, Right Side of Brain Compensates for Loss of Language in Left SideThese are individual scans of two healthy controls and two individuals with a left-hemisphere (LH) perinatal stroke. The orange/yellow activation shows the normal language areas of the left hemisphere in healthy individuals, as compared with the reorganized language areas in individuals with a left-hemisphere perinatal stroke. Courtesy: Elissa NewportPosted in News Release | Tagged brain, brain plasticity, strokeAs Children with Left-hemisphere Strokes Grow Up, Ability to Understand Language Shifts to Right side.WASHINGTON (October 10, 2022) — A clinical study conducted by researchers at Georgetown University Medical Center found that, for children who had a major stroke to the left hemisphere of their brain within days of their birth, the infant’s brain was “plastic” enough for the right hemisphere to acquire the language abilities ordinarily handled by the left side, while also maintaining its own language abilities as well.Media Contact Karen Teberkm463@georgetown.eduThe left hemisphere of the brain is normally responsible for sentence processing (understanding words and sentences as we listen to speech). The right hemisphere of the brain is normally responsible for processing the emotion of the voice — is it happy or sad, angry or calm. This study sought to answer the question, “What happens when one of the hemispheres is injured at birth?”The findings appear in PNAS the week of October 10, 2022.The participants in this study developed normally during pregnancy. But around birth they had a significant stroke, one that would have debilitating outcomes in adults. In infants, a stroke is much rarer, but does happen in roughly one out of every 4,000 births.The researchers studied perinatal arterial ischemic stroke, a type of brain injury occurring around the time of birth in which blood flow is cut off to a part of the brain by a blood clot. The same type of stroke occurs much more commonly in adults. Previous studies of brain injury in infants have included several types of brain injury; the focus in this study on a specific type of injury enabled the authors to find more consistent effects than in previous work.“Our most important conclusion is that plasticity in the brain, specifically the ability to reorganize language to the opposite side of the brain, is definitely possible early in life,” says Elissa Newport, Ph.D., director of the Center for Brain Plasticity and Recovery at Georgetown University Medical Center, professor in the departments of Neurology and Rehabilitation Medicine and first author of this study. “However, this early plasticity for language is restricted to one brain region. The brain is not able to reorganize injured functions just anywhere as more dramatic reorganization is not possible even in early life. This gives us great insights into the regions we might be able to focus on for potential breakthroughs in developing techniques for recovery in adults as well.”The investigators recruited people from across the United States who all had medium to large strokes to the cortex region of their left hemisphere around the time of birth. To assess long-term outcomes in their language abilities, participants were given language tests at 9 to 26 years of age and were compared to their close-in-age healthy siblings. They were also scanned in an MRI to reveal which brain areas were involved in sentence comprehension.The participants and their healthy siblings all completed the language tasks almost perfectly. The major difference was that the stroke participants processed sentences on the right side of the brain, while their siblings processed sentences on the left side. The stroke participants showed a very consistent pattern of language activation in the right hemisphere, regardless of the extent or location of damage from the stroke to the left hemisphere. Only one of the 15 participants, who had the smallest stroke, did not show clear right hemisphere dominant activation.“It is also notable that many years after their strokes our participants are all such highly functioning adults. Some are honor students, and others are working toward or have gotten their master’s degrees,” says Newport. “Their achievements are remarkable, especially since some of their parents had been told when they were born that their strokes would produce lifelong impairments.”In future studies, the researchers hope to gain a better understanding of why the left hemisphere routinely becomes dominant in healthy brains but consistently loses out to the right hemisphere when there is a significant left-hemisphere stroke. An additional question of special interest — and clinical importance — is why left hemisphere language can successfully reorganize to the right hemisphere if injuries occur very early in life, but not later. Research on stroke recovery and sentence processing in adults suggests that plasticity narrows with age, something that Newport hopes to study, as it could be of great benefit, and potential therapeutic interest, to adult stroke survivors.The researchers are very grateful to the participants and their families, who have made invaluable contributions to this work.In addition to Newport, the other authors at Georgetown University are Anna Seydell-Greenwald, Barbara Landau, Peter E. Turkeltaub, Catherine E. Chambers, Kelly C. Martin and Rebecca Rennert. Margot Giannetti and Alexander W. Dromerick are at Georgetown University and MedStar National Rehabilitation Hospital. Rebecca N. Ichord is at the Perelman School of Medicine at the University of Pennsylvania and Children’s Hospital of Philadelphia. Jessica L. Carpenter is at the University of Maryland Baltimore. William D. Gaillard and Madison M. Berl are at the Children’s National Hospital and Center for Neuroscience, Washington, DC.This work was supported by funds from Georgetown University and MedStar Health; by the Solomon James Rodan Pediatric Stroke Research Fund, the Feldstein Veron Innovation Fund, and the Bergeron Visiting Scholars Fund to the Center for Brain Plasticity and Recovery; by an American Heart Association grant 17GRNT33650054; by NIH grants P50HD105328 to the DC-IDDRC at Children’s National Hospital and Georgetown University; and by NIH Grants K18DC014558, K23NS065121, R01NS244280 and R01DC016902.Newport reports having no personal financial interests related to the study.","After Stroke in an Infantâs Brain, Right Side of Brain Compensates for Loss of Language in Left Side"
456,5,5_study_participants_brain_schizophrenia,https://www.psypost.org/2022/10/people-with-insecure-attachment-styles-tend-to-have-strong-emotional-bonds-with-pets-study-finds-64161,"New research on German dog owners finds that people with stronger relationships to their pets display more symptoms of mental disorders and distress, but proposes that this link may be fully accounted for by insecure attachment to other humans. The study was published in BMC Psychiatry.Pet ownership has long been linked to better mental health and lower levels of negative conditions such as loneliness and depression, both in the general population and in patients with physical and mental disorders. However, such findings have not been consistent as other studies reported zero or even negative effects of pet ownership on physical and mental health.A different line of research has linked emotional attachment to pets to problems in interpersonal relationships. One such study found that people with stronger attachment to pets reported lower levels of social support and higher levels of loneliness and depression. Another one found strong attachment to pets to be associated with childhood trauma and certain psychopathological traits. These results led authors of the new study to focus on the relationship between interpersonal attachment styles (i.e. what kinds of relationships with others are we comfortable in), attachment to pets and mental health.Study author Johanna Lass‑Hennemann and her colleagues conducted an online survey of 610 German dog owners to test the hypothesis that stronger emotional attachment to one’s dog is associated with higher mental health burden and insecure attachment to humans. They also aimed to “disentangle the link between emotional attachment to pets and human attachment and their perspective associations with mental health burden.”The researchers recruited respondents by distributing the link to their survey on webpages for dog owners and on social media. Study participants recruited in this way were mostly females (93%) between 18 and 73 years of age. They were asked to provide certain demographic data and dog-related information about themselves, but also to complete assessments of attachment to pets (Lexington Attachment to Pets Scale, LAPS), interpersonal attachment styles (Revised Adult Attachment Scale, R-AAS) and of symptoms of mental disorders and distress or the mental health burden, to use the words of authors (Brief Symptom inventory, BSI).Respondents who were more strongly attached to their dogs reported more symptoms of mental disorders and distress. Additionally, a stronger emotional attachment to one’s dog was associated with lower comfort with depending and trusting others (the dependence component of interpersonal attachment) and to a greater fear of being rejected and unloved (the anxiety component of interpersonal attachment). These were, in turn, associated with more pronounced symptoms of mental disorders and distress (mental health burden).Results also indicated that interpersonal attachment styles may be mediating the association between the emotional attachment to the dog and mental health burden, as it fully accounted for the link between the later two factors. Authors conclude that “stronger emotional attachment to pets might reflect a compensatory attachment strategy for people who were not able to establish secure relationships to other people during childhood. Those people may build more close relationships with pets that might be perceived as more reliable and less threatening.”These result highlight important links between emotional attachment and mental health. However, this was a correlation study and, therefore, it cannot be the basis for cause-and-effect interpretations. Almost all of the participants were women and the results might differ somewhat on a sample of men. The authors also noted that the assessment method for emotional attachment to dog used in the study assesses the intensity of attachment, but not the attachment style. Assessing the style of attachment to pet alongside intensity might provide novel insights in future studies.The paper, “The relationship between attachment to pets and mental health: the shared link via attachment to humans“, was authored by Johanna Lass‑Hennemann, Sarah K. Schäfer, M. Roxanne Sopp, and Tanja Michael.","People with insecure attachment styles tend to have strong emotional bonds with pets, study finds"
457,5,5_study_participants_brain_schizophrenia,https://www.psypost.org/2022/10/pilots-tend-to-have-less-emotional-intelligence-than-the-average-person-new-research-suggests-64172,"Being emotionally intelligent may be important for a teacher, salesman, or therapist, but what about for a pilot? A study published in Nature’s Scientific Reports suggests that pilots are less likely to be emotionally intelligent compared to the average person.Trait emotional intelligence is a concept that captures an individual’s general ability to manage, perceive, and express emotions. It has been linked with many other positive constructs, such as leadership abilities, self-control, mental strength, and the ability to manage stress well. Though aviation does not seem like a field that would require emotional intelligence, many of the aforementioned advantages could be helpful for pilots. Previous research has studied emotional intelligence in military pilots and did not compare them to a control group, and this study seeks to bridge that gap in literature.For their study, Zachary Dugger and colleagues utilized 44 pilots ranging in age from 24 to 67 years old to serve as the sample. All participants were required to have an active flight qualification and the majority of participants were either current or former military pilots. The control group was drawn from a US dataset and was matched on age, gender, ethnicity, and education as best as possible. In total, 88 control subjects were used. All 132 participants completed measures on trait emotional intelligence and demographics.Results showed that pilots scores lower in trait emotional intelligence, including the subfactors of well-being, emotionality, and sociability. Though pilots have been found to be an extraverted group, it is thought that they score lower in these constructs because their job requires careful precision. The lower scores could also be related to organizational culture.“Pilots have long been associated with a masculine culture that emphasizes aggressiveness, competition, and performance orientation,” the researchers said. “In practice, the pilot selection and training process may produce pilots, primarily male but also female, who fit within this culture.”In regard to self-control, there were no significant differences between control participants and pilots. This is likely because self-control allows people to maintain situational awareness, which is very important for people operating an aircraft.This study took steps into better understanding trait emotional intelligence in pilots. Despite this, there are limitations to note. One such limitation is that the sample was overwhelmingly male. Another limitation is that the sample size of pilots was relatively small and still consisted mainly of people with military service. Future research could focus on obtaining a more diverse sample of pilots.“Overall, the findings show that pilots tend to have lower trait [emotional intelligence] scores, indicating less confidence and reliance on their emotional world, with all the advantages and disadvantages this might entail,” the researchers concluded. “Although exploratory, these findings highlight promising avenues for future trait [emotional intelligence] research within the broader sector of international aviation. Such research will help practitioners identify new opportunities in pilot training and organizational culture, the better to equip pilots for aviation duty, ultimately leading to improved safety, performance, and all-around satisfaction.”The study, “Trait emotional intelligence in American pilots“, was authored by Zachary Dugger, K. V. Petrides, Nicole Carnegie, and Bernadette McCrory.","Pilots tend to have less emotional intelligence than the average person, new research suggests"
461,5,5_study_participants_brain_schizophrenia,https://www.psypost.org/2022/10/study-finds-brain-changes-associated-with-adhd-remission-64139,"A recent study published in NeuroImage: Clinical used state-of-the-art neuroimaging techniques to determine what brain changes may cause childhood ADHD to go into remission. Christienne Damatac and colleagues looked at brain changes in those diagnosed with ADHD over 16 years. Their findings suggest that improved hyperactivity and inattentiveness symptoms result from increased white matter density in the brain region known as the left corticospinal tract. Additionally, reduced ADHD symptoms were associated with more neural connections in the same region.ADHD is a common childhood diagnosis. However, some are fortunate enough to grow out of the challenging symptoms by adulthood, and others never do. Understanding why this is so may lead to important innovations in treating the disorder. One hypothesis is that the malfunctioning parts of the brain that result in ADHD symptoms are never able to repair themselves. Instead, for some, as the brain develops, other regions take over the responsibilities of the damaged areas. Damatac and colleagues were curious if this was so and if these changes would persist over time.Fifty-five individuals with an ADHD diagnosis in the experimental condition were examined four times over 16 years. The neuroimaging techniques used are known as diffusion tensor imaging, diffusion-weighted imaging, and fixel-based analysis. At the time of the first scans, participants were between 6 and 18 years old.The research team found that as their subjects aged, those that went into ADHD remission experienced changes in white matter that were not seen in those who did not go into remission and the healthy non-ADHD controls. Moreover, these brain differences persisted well into adulthood.Brain changes like those found here are assumed to result from experience. For example, if an individual becomes blind and then learns to read braille, the brain will change in response to this new and necessary skill. In addition, areas of the brain once responsible for processing sight may take up other jobs to help the blind person navigate the world. These findings of this study indicate that as the brains of those with ADHD mature, some individuals may repetitively engage in strategies that compensate for symptoms. These repetitive behaviors may result in the brain changes seen in those who went into remission.For children with ADHD, this research implies that remission may be possible if strategies that help to compensate for deficits are frequently practiced. Further, it suggests that investing in support in the school setting and educating parents on strategies could help pave the way to long-term remission.Damatac and team acknowledges that their imaging techniques changed over time, which may have had unknown consequences for the data. Second, some of the original participants dropped out, and those left may not be as representative as the original sample. Finally, the sample size was too small to claim cause and effect; there may be other factors that led to persistent ADHD.These limitations aside, this research provides neurological evidence that consistent use of strategies to cope with ADHD symptoms may be a way out. Findings like this are sure to be used to support better funding and education for those helping children with ADHD.The study, “Longitudinal changes of ADHD symptoms in association with white matter microstructure: A tract-specific fixel-based analysis“, was authored by Christienne Damatac, Sourena Soheili-Nezhad, Guilherme Blazquez Freches, Marcel Zwiers, Sanne de Bruijn, Seyma Ikde, Christel M. Portengen, Amy Abelmann, Janneke Dammers, Daan van Rooij, Sophie Akkermans, Jilly Naaijen, Barbara Franke, Jan Buitelaa, Christian Beckmann, and Emma Sprooten.",Study finds brain changes associated with ADHD remission
462,5,5_study_participants_brain_schizophrenia,https://www.psypost.org/2022/10/study-finds-twelvefold-higher-mortality-risk-among-psychopathic-female-offenders-64037,"New research highlights the health dangers associated with an antisocial lifestyle. A study of psychopathic female offenders revealed a mortality risk that was 12 times higher compared to the general population. The findings were published in the journal Frontiers in Psychiatry.Antisocial personality disorder (ASPD) is a condition defined by antisocial behavior, aggression and impulsivity, and a lack of concern for others. People with ASPD are often involved in criminal activity and tend to experience difficulty forming long-term relationships. In its extreme form, ASPD is often referred to as psychopathy.A body of past research has revealed high rates of premature death among people with ASPD, presumably due to the dangerous lifestyle associated with the disorder. Research in men has additionally found that the degree of psychopathy matters — as psychopathy increases, life expectancy decreases.Study author Olli Vaurio and colleagues conducted a study to investigate whether mortality risk may differ between men and women with psychopathy. The researchers note that psychopathy manifests differently in men and women, which could lead to gender differences in mortality risk. For example, psychopathic women are less likely to display physical aggression and more likely to show relational or verbal aggression compared to psychopathic men.To study the risk of mortality in psychopathic women, the researchers obtained data from a sample of female criminal offenders and compared this to national mortality rates. The sample included 57 Finnish women who had undergone forensic psychiatric evaluation for having committed severe crimes. For the current analysis, participants were excluded if they met criteria for a major mental illness like schizophrenia or delusional disorder.Based on their scores on the Psychopathy Checklist-Revised (PCL-R), the women were categorized into one of two groups: low psychopathy (41 women) and high psychopathy (16 women). The women were followed for between 17 and 25 years, and during the follow-up period, 6 deaths occurred in the high psychopathy group, and 16 deaths occurred in the low psychopathy group. The average age at the time of death was 52.7 years old.The researchers obtained mortality data collected from Statistics Finland, for the years 1984–2013. They then calculated the ratio of observed to expected number of deaths for both the low and high psychopathy groups. It was found that both groups had higher mortality compared to the general Finnish population.Mortality was especially high in the high psychopathy group, where it was twelve times higher than in the general population. For the low psychopathy group, mortality was more than six times higher than in the general population. In line with previous research conducted among psychopathic men, mortality risk increased with higher PCL-R scores.Death by natural causes was more common than death by unnatural causes. For the high psychopathy group, this included cancer (2 cases), cerebrovascular disease (1 case), and liver disease (1 case).The authors discuss how some of these fatalities may relate to an antisocial lifestyle. For example, psychopathy involves an impulsivity and recklessness which may influence health behaviors. This may lead to higher rates of cigarette smoking and alcohol consumption, which can lead to lung and liver disease. More generally, a volatile lifestyle may lead to fewer healthcare visits.“Although effective treatment options for adult antisocial psychopaths are scarce, many of those suffering from its consequences could benefit from information about common health issues and targeted measures to alleviate them,” Vaurio and colleagues say. “There is also some evidence that early interventions, such as parent management training (PMT) or adequate treatment of conduct disorder, may be beneficial in preventing the possible later life complications in children and young people (CYP) at risk of psychopathy.”Overall, the findings support past evidence that antisocial personality is tied to increased mortality. Notably, the sample size of the study was small, with only 16 psychopathic women. Future studies with larger samples may help unearth differences in mortality risk between men and women with psychopathy.The study, “Female Psychopathy and Mortality”, was authored by Olli Vaurio, Markku Lähteenvuo, Hannu Kautiainen, Eila Repo-Tiihonen, and Jari Tiihonen.",Study finds twelvefold higher mortality risk among psychopathic female offenders
465,5,5_study_participants_brain_schizophrenia,https://www.psypost.org/2022/10/unique-therapy-that-alters-memory-processes-could-reduce-psychological-disturbances-following-romantic-betrayal-64138,"A novel technique that uses a beta blocker to interfere with memory reconsolidation shows promise in the treatment of adjustment disorder following romantic betrayal, according to new research published in the Journal of Affective Disorders.Adjustment disorder is a condition that can occur in response to a significant life event or change. While it is normal to feel some degree of anxiety or distress in such situations, people with adjustment disorder experience more intense and long-lasting symptoms that interfere with their ability to cope. These may include difficulty sleeping, depressed mood, social withdrawal, and difficulty concentrating. In severe cases, adjustment disorder can lead to self-harm or suicidal thoughts.“There is no recognized empirically-based treatment for adjustment disorders,” said study author Alain Brunet, a clinical psychologist and psychiatry professor at McGill University. “This is an oddity. We were interested in determining if the good clinical results we had obtained in treating PTSD with Reconsolidation Therapy applied to a broader set of trauma-like conditions, hence our interest for adjustment disorder.”“Romantic Betrayal (a form of adjustment disorder) seemed like an interesting topic to study because, first, it is very distressing. Second, it is one of the most common reason why individuals seek professional help. Finally, there is very little help available for romantically betrayed individuals who do not wish to return with their partner.”Propranolol is a beta blocker that is often prescribed for high blood pressure, migraines, and certain anxiety disorders. But the drug has also been shown to weaken the emotional tone of memories by blocking adrenergic pathways.“Reconsolidation Therapy consist in recalling a bad memory under the influence of propranolol with the help of a trained therapist,” Brunet explained. “This treatment approach is a translational treatment stemming from the research in neuroscience which stipulates that a recalled memory needs to be saved again to long-term memory storage in order to persist. Interfering with the storage process will yield a degraded (less emotional) memory.”In the new study, Brunet and his colleagues recruited adults who met the DSM-5 criteria for adjustment disorder. The participants had all experienced a romantic betrayal event, such as infidelity, that occurred during a monogamous long-term relationship.The researchers asked the participants to write a first-person narrative of their romantic betrayal/abandonment event. The participants were told to focus on the most emotionally provocative aspects of the event and to include stress-related reactions, such as feeling tense, trembling, and sweating. During treatment sessions, the participants ingested propranolol before reading their narrative out loud once. Fifty-five participants completed at least one treatment session, while 48 completed all five sessions.To assess clinically significant symptoms, the participants completed a widely used questionnaire known as the Impact of Event Scale — Revised (IES-R) before, during, and after the treatment phase. The researchers observed a large drop in IES-R scores immediately following the first treatment. The declines in IES-R scores continued over the course of the treatment phase. Thirty-five participants who completed a follow-up survey provided evidence that the improvements in symptom endured up to 4 months.“Our study suggests that Reconsolidation Therapy works with adjustment disorder, in that it is clearly superior to a wait-list group (subjects were their own control),” Brunet told PsyPost. “The magnitude of the pre-post treatment improvement compares to results we obtained in our PTSD research.”Brunet said he was surprised by how high the IES-R scores were prior to treatment. “Looking at the severity of symptoms, were surprised at how painful adjustment disorder can be,” the researcher explained. “Adjustment disorder is no ‘wimpy’ disorder. This is clearly a misconception.”The study utilized a within-subjects open-label design, which limits the ability to draw strong conclusions about causality. However, the findings provide an important foundation for future research. “In spite of its moderate size, the study is important in that it provides the treatment ‘effect sizes’ required to launch a placebo-controlled randomized controlled trial,” Brunet said.The study, “Treatment of adjustment disorder stemming from romantic betrayal using memory reactivation under propranolol: A open-label interrupted time series trial“, was authored by Michelle Lonergan, Daniel Saumier, Sereena Pigeon, Pierre E. Etienne, and Alain Brunet.",Unique therapy that alters memory processes could reduce psychological disturbances following romantic betrayal
466,5,5_study_participants_brain_schizophrenia,https://www.psypost.org/2022/10/victims-of-childhood-abuse-are-biologically-older-than-their-peers-in-midlife-study-indicates-64085,"New research has found that individuals who suffer physical or sexual abuse in childhood age faster than their non-abused peers. Published in the journal Psychoneuroendocrinology, the researchers obtained participants aged 32-49 years and, using blood tests, found evidence to support the hypothesis that childhood trauma can shorten the lifespan.Interested in the consequences of childhood trauma, Gloria Graf and colleagues sought to investigate if evidence of childhood abuse could be seen in the biomarkers of aging. Previous research has already discovered that individuals abused in childhood experience more health problems as they age. If this is so, Graf and colleagues hypothesized it could be due to faster biological aging. As biological age increases, so does vulnerability to disease.To determine if those abused in childhood are aging faster than their non-abused peers, the researchers found 357 test subjects from a pool of individuals who had experienced court-documented childhood neglect and physical or sexual abuse. The study also included 200 control subjects who were matched with test subjects based on childhood economic and demographic similarities.Two blood tests were used to assess the biological aging of both the experimental and control subjects. The first is known as the Klemera-Doubal method Biological Age (KDM BA). Results of a KDM BA test can identify at what age certain biological markers would be seen as typical. For example, a KDM BA result of 53 for someone who is 49 indicates the individual is aging faster than their years. The second test, PhenoAge, measures mortality risk, and those with high mortality risk should have high biological age.The results of the KDM BA test indicated that those abused in childhood are more likely to have biological markers that indicate they are older than their actual age. Within the abused group, women aged faster than men, and minority sub-groups aged slower than Caucasians.The PhenoAge test did not reveal statistically significant results. The researchers hypothesized this could be because of the age of the subjects. Regardless of abuse and faster aging, those in both groups were not yet old enough to demonstrate large differences in mortality risk.Graf and colleagues report that these results are large enough to consider the necessity of follow-up care throughout the lifespan for victims of childhood abuse. If preventative care was provided to support physical and mental health long after childhood, it might be possible to decrease disease and increase length of life.The researchers acknowledge the limitations of their work. First, science is still developing the standard methodology for identifying how fast or slow someone is aging. The methods chosen are well-researched and respected but are not yet considered ‘standard.’ Second, subjects in the experimental group are those who experienced abuse significant enough to make it into the judicial system. Those in the control group may have also experienced undocumented abuse. This unknown factor could have had consequences for their results.Despite the acknowledged limitations, the researchers found their results meaningful, stating: “In sum, our results contribute support for the hypothesis that childhood maltreatment disrupts healthy aging processes.”The findings are also in line with other research, which has indicated that individuals exposed to adverse childhood experiences – such as neglect, witnessing intimate partner violence, and parental death — tend to be biologically older than their counterparts.The study, “Biological aging in maltreated children followed up into middle adulthood”, was authored by Gloria Graf, X. Li, D. Kwan, Daniel Belskey, and Cathy Widom.","Victims of childhood abuse are biologically older than their peers in midlife, study indicates"
468,5,5_study_participants_brain_schizophrenia,https://www.psypost.org/2022/11/mothers-who-spend-more-time-on-social-media-sites-about-motherhood-experience-higher-stress-hormone-levels-study-finds-64206,"A study published in the journal Biological Psychology suggests that exposure to social media content about motherhood can trigger a sense of threat among mothers, activating the body’s stress response. The study found that more time spent on social networking sites devoted to motherhood was associated with increased cortisol output among mothers.During social interactions, people frequently fall victim to social comparison — they begin comparing themselves to the people around them and making self-judgments. These self-evaluations can lead to negative feelings, particularly when they stem from upward social comparisons — comparisons to people who seem better off than oneself.Social self-preservation theory says that when a social situation threatens a person’s self-concept, this activates the hypothalamic-pituitary-adrenal (HPA) axis and stimulates the release of cortisol. Accordingly, study author Nataria T. Joseph and her co-authors wanted to test whether engaging in social comparison has a measurable impact on a person’s cortisol levels.“This project is the third of a series of projects that we executed together, with the aim of examining the complex nature of and multifactorial, biopsychosocial implications of social media use among first time mothers,” explained Joseph (@_NoCrystalStair), an associate professor at Pepperdine University who holds the Blanche E. Seaver Professor of Social Science professorship.“My collaborator Dr. Lauren Amaro‘s initial interest in the online exchanges that occur between mothers regarding parenting sparked the overall series of studies,” Joseph continued. “This last study in the series was focused on the biological health implications of these exchanges and based in my previous work using ecological momentary assessment to study daily life moments and cortisol hormone levels.”Joseph and her colleagues launched a study to explore how social comparisons occurring within the context of technology — such as social networking sites — might influence momentary cortisol levels. Since women are particularly affected by social comparison, and since social comparison and self-evaluation are common during motherhood, the researchers focused their study on mothers.“Our prior research has shown that social networking sites for moms include a range of messages about motherhood, including both positive and negative emotion,” said co-author Theresa de los Santos. “Social networking sites also provide potential community for support in which a mother might feel belonging within particular groups.“However, intensive mothering norms that influence social roles and identities can clash through social comparison practices to lower parenting satisfaction. This led us to investigate in our current study how comparisons and emotions that mothers experience on these sites and in other technology-based interactions influence a mother’s health through monitoring of the stress hormone, cortisol.”The researchers used a technique called ecological momentary assessment (EMA) to capture naturalistic experiences in daily life. The study sample consisted of 47 mothers with an average age of 34 who reported being exposed to “online content about motherhood” for at least 6 days a week. Mothers were asked to indicate how many minutes a day they spent on social networking sites or online forums in the past week. They then participated in a 4-day study where they completed short surveys and provided saliva samples four times a day — upon awakening, 4 hours after awakening, 9 hours after awakening, and at bedtime.At each survey, the participants indicated the time of their most recent technology-mediated social exposure (TMSE) about motherhood. They were asked to include “communication on social networking sites, online forums, email, texts, or any other technology and passive exposure to communications about motherhood or parenting.” The participants also responded to questions concerning the emotions they had experienced during the TMSE interaction, and any social comparisons they had made regarding motherhood.“Participants were very active on social media, with 55.3% spending at least 2 hours daily on social networking sites for mothers and 46.8% using social networking sites for mothers at least 4 times a day,” said de los Santos. “This gave us a tremendous amount of data about their technology-driven, day-to-day interactions as well as their biological data at various points throughout the day.”The researchers found that higher negative emotions during TMSE was associated with higher cortisol. Participants who self-reported more time spent on social networking sites or online forums in the past week also had higher momentary cortisol levels, as well as higher average daily cortisol output. The authors note that on these online sites, mothers often express negative emotions like anger and sadness. Exposure to these negative emotions might lead mothers to feel a heightened sense of threat, triggering self-preservation.“Social comparison is very frequent in contemporary society, and inevitable as we interact with others,” Joseph explained. “They are especially salient when we face new roles in life, such as becoming a mother for the first time. So, we hope that putting a spotlight on social comparison will make individuals pause and reflect on their social comparison tendencies and find healthy ways of making comparisons so that individuals don’t have to experience negative emotions when comparing themselves to others.”“We believe there are responsible ways of engaging with social exchanges on social media and other websites. First, being one’s authentic self on these platforms not only validates one’s self but also contributes to creating an online community in which individuals can see that others struggle and are flawed. Second, if an individual monitors his or her emotions as he or she interacts with these online platforms, that individual will be better able to recognize when his or her social comparisons are becoming unhealthy.”The results also revealed that, at a momentary level, higher engagement in social comparison during TMSE was tied to lower cortisol.Interestingly, these findings suggest that social comparison may not be detrimental on its own. While experiencing increased negative emotions during TMSE was associated with heightened cortisol levels, engaging in social comparison without increased negative feelings was not. Moreover, downward social comparisons — comparisons to mothers who are doing worse than themselves — appeared to alleviate the stress response. Mothers who reported engaging in more downward social comparisons than other participants had lower momentary cortisol levels.“We were surprised that social comparisons were associated with lower cortisol (an important stress hormone with many implications) as we hypothesized that it would be associated with higher cortisol,” Joseph told PsyPost. “One must keep in mind though, that we controlled for negative emotions in our analyses. So, the results really show that social comparisons that are not accompanied by spikes in negative emotion are associated with lower cortisol. Thus, there are some healthy ways of engaging in social comparison that protect a person’s self schema and emotional wellbeing. These healthy ways of comparing are associated with lower cortisol.”Joseph and her colleagues warn that their study results have worrying implications for mothers since heightened cortisol can damage a person’s health over time and even increase mortality risk. The results also have implications for young children, since mothers with high cortisol levels tend to have children with high cortisol levels as well.“There are certainly benefits and drawbacks to TMSEs. Based on our H.O.M.E.S. (Health Outcomes for Mothers’ Exchanges on Social Media) program and other research in the area, my advice is for mothers to first decide if the online space is the best place to seek support, given their existing tendencies to compare and their existing interpersonal relationships,” Amaro told PsyPost.“For instance, a person who is prone to upward comparison, or the practice of seeing others as ‘doing better’ while perhaps feeling inadequate in a particular role can find herself experiencing more negative emotion and less satisfaction in her parenting when engaging with these sites. She might be best suited to an in-person mom group or to relying on family members or neighbors when she needs support.”“The second step in deciding whether to go online for support or not is to determine your intention or need,” Amaro continued. “If a mom needs practical information (hey Facebook group, which pediatricians do you recommend?), online mom groups can be a wealth of knowledge and advice, though discernment is necessary. If it’s friendship or reduced feelings of isolation, some online mom groups can also be wonderfully encouraging places to make “real-life” friends, but some are not. Moms should explore the culture of a group prior to engaging. It’s always worth questioning why and what you’re scrolling.”Future studies will be needed to potentially replicate the results and expand on the current findings. The authors suggest that potential interventions for mothers might include, “educating mothers on limiting TMSE engagement, pursuing healthier self-evaluation that recognizes strengths and weaknesses in parenting thereby promoting healthier social comparison, and the management of negative emotion when engaging messages about motherhood.”The study, “Naturalistic social cognitive and emotional reactions to technology-mediated social exposures and cortisol in daily life”, was authored by Nataria T. Joseph, Theresa de los Santos, and Lauren Amaro.","Mothers who spend more time on social media sites about motherhood experience higher stress hormone levels, study finds"
469,5,5_study_participants_brain_schizophrenia,https://www.psypost.org/2022/11/new-study-links-suffering-from-long-lasting-severe-depression-to-reduction-in-brain-volume-64201,"A study on a large sample of patients found chronic, long-lasting depression to be associated with reduced brain volume. The reduced volume was found in brain regions relevant for planning one’s behavior, focusing attention, thinking, learning and remembering and also in regions relevant for regulating emotions. The study was published in Neurobiology and Treatment of Depression.Depression, also called major depressive disorder, is a mood disorder that causes a persistent feeling of sadness and loss of interest. It changes the way a person feels, thinks and behaves. For many people suffering from it, depressive episodes become a recurring event. More than half of patients with depression experience a relapse after 2 years and the probability of recurrent depressive episodes rises to 90% after 3-4 episodes. Studies have indicated that recurring depressive episodes might be linked to structural changes in the brain, but the existing results are not uniform.Ms. Hannah Lemke and her colleagues analyzed the data of 681 patients from the Marburg-Muenster-Affective-Cohort Study (MACS) in order to better link properties of the course of depressive disorder with specific changes in the brain structure. Patient data were collected at two sites in Germany – Muenster and Marburg.Patients participated in a clinical diagnostic interview (Structured Clinical Interview-I) that focused on the number and duration of hospitalizations and the duration of the disease, completed an assessment of depressive symptoms (Hamilton Depression Rating Scale, HDRS) and the current medication regime, and underwent brain imaging using magnetic resonance imaging and voxel-based morphometry.The results showed that an adequate description of the course of a depressive disorder needs to focus on two components – hospitalization i.e., the number and duration of lifetime hospitalizations, and duration of illness i.e., the time since the first psychiatric episodes and the number and duration of lifetime depressive episodes.These two components were found to be linked with specific changes in the brain structure. Longer durations of illness were associated with lower volumes of grey mass in the left hippocampal and dorsolateral prefrontal cortex regions of the brain. Higher hospitalization scores were associated with significantly decreased gray mass volume in the dorsolateral prefrontal cortex (on both sides) and left insula regions of the brain.An important strength of this study is that it involved a very large and diverse group of patients suffering from depression. However, the duration of illness assessment was based on self-reports, which can be affected by difficulties of remembering the previous course of illness. Hospitalization for depression is done when symptoms of depression are severe or when suicidal ideas/behavior is present.This means that the association between brain structure alterations and the hospitalization component of depression course may be the result of severity of depressive symptoms and not of the hospitalization itself. Authors conclude that stress-related mechanisms may be underlying the described effects.The study, “Association of disease course and brain structural alterations in major depressive disorder”, was authored by Hannah Lemke, Lina Romankiewicz, Katharina Förster, Susanne Meinert,Lena Waltemate, Stella M. Fingas, Dominik Grotegerd, Ronny Redlich, Katharina Dohm, Elisabeth J. Leehr, Katharina Thiel, Verena Enneking, Katharina Brosch, Tina Meller, Kai Ringwald, Simon Schmitt, Frederike Stein, Olaf Steinsträter, Jochen Bauer, Walter Heindel, Andreas Jansen, Axel Krug, Igor Nenadic, Tilo Kircher, and Udo Dannlowski.",New study links suffering from long-lasting severe depression to reduction in brain volume
470,5,5_study_participants_brain_schizophrenia,https://www.psypost.org/2022/11/pedestrians-give-panhandler-more-than-twice-as-much-money-when-he-wears-a-suit-versus-jeans-experiment-finds-64199,"In a field experiment, pedestrians gave more than twice as much money to a panhandler when he wore clothes that signaled a higher social class versus a lower social class. Findings from a follow-up study suggest that this was due to inferences about the man’s competence, trustworthiness, humanity, and similarity to the self. The study was published in the journal Frontiers in Psychology.People make assumptions about others based on their social class. For example, people tend to perceive low-status individuals, like those experiencing homelessness or poverty, as lower in warmth and competence. These assumptions appear to influence behavior, leading people to ostracize members of low-status groups.Study author Bennett Callaghan and his colleagues wanted to explore how visible status symbols influence compassionate responding toward others. Are people more giving toward those who emanate high social status versus low social status? Some theoretical accounts suggest that compassionate responding involves judging whether or not a person “deserves” the help. Since low-status individuals are viewed as less trustworthy and competent, they may also be viewed as less deserving of help.“My coauthors and I first got interested in this topic based on research showing how social class and inequality can influence even brief social interactions or conversations,” explained Callaghan (@bennettcallag), an associated researcher at the Stone Center on Socio-Economic Inequality at the City University of New York (CUNY) Graduate Center.“Specifically, we were studying social class signaling: processes through which individuals can identify, with some accuracy, the social class (or socioeconomic status) of others through exposure to very brief and superficial cues (e.g., an accent, 60 seconds of video, or social media profile pictures). Research shows that, consciously or not, when those in power perceive and act on these cues, it leads to all sorts of negative outcomes and denied opportunities for individuals lower in social class, such as discrimination in hiring.”“We began this project to test the limits of just how powerful these cues and perceptions were: whether, instead of looking at something like hiring, we instead looked at behavior that many might think of as being purely selfless and prosocial — helping others — and did so in ‘real-world’ contexts with people sharing their own resources.”In an initial field study, Callaghan and his team tested whether people would be more likely to help a panhandler wearing clothes that signal high social class or low social class. The experiment was conducted in six busy, downtown areas in New York City and Chicago. Across various trials, the confederate stood in the street holding a paper cup and a cardboard sign with a message about homelessness.In the high-status trials, the confederate was dressed in high-status symbols — a suit, dress shirt, a tie, and slick hair. In the low-status trials, he was dressed in low-status symbols — jeans and a t-shirt. Throughout the experiment, research assistants counted the number of passersby, the number of people who donated their money, and the number of people who engaged with the confederate.The results revealed that when the confederate was wearing a suit and tie, he received 2.55 times the amount of money he received when he wore jeans. He was also approached by a larger number of donors when wearing high-status clothing, although this effect was marginally significant.“While we expected that the displaying high-status symbols would lead to an increase in giving, I was still surprised by the size of this difference — a more than two-fold increase in donations,” Callaghan explained. “I was also somewhat surprised by some of the different ways in which people interacted with me, as the confederate, in the two conditions. For instance, when I was dressed in high-status clothing, several individuals gave donations of $5 or $10 and one dropped a business card in my cup rather than give a one-time donation.”Notably, this means that the confederate earned more money when signaling higher socioeconomic status (SES). Interestingly, passersby were equally likely to interact with the confederate — whether they gave him money or not — in both conditions, suggesting that the status symbols influenced the quality of interactions with the panhandler, but not the number of them.The findings provide an indication “of just how powerful an influence social class exerts in our lives and how inequality permeates every aspect of our lived experience: even superficial symbols of social class can have large impacts on our willingness to help others in the moment — and whether we even see these others as deserving of help in the first place,” Callaghan told PsyPost.Next, a follow-up study shed light on why high-status symbols might elicit more compassionate responding. A final sample of 492 people completed an online survey where they viewed photographs of the confederate from Study 1. Depending on the condition, the confederate was pictured panhandling in either high-status or low-status clothing. Participants were asked to rate the target according to various social attributes.The results showed that participants perceived the target in high-status clothing to have higher SES compared to the target in low-status clothing. They also rated the high-status target as higher in competence, warmth, similarity to the self, and humanity.These findings suggest a potential mechanism to explain participants’ behavior in the field experiment. One interpretation is that passersby gave more money to the high-status panhandler because they perceived him to be more deserving of the money. Perceptions that he was more competent and more trustworthy may have led passersby to believe he was more likely to use the money for the intended purpose, such as personal advancement or care, as opposed to using the donations to gain wealth or purchase drugs or alcohol.Alternatively, participants may have judged the high-status confederate to be in a temporary state of need, and thus more likely to reciprocate the altruism at a later time. This would be consistent with an evolutionary concept called reciprocal altruism, which contends that people are motivated to help others who are likely to return the favor in the future.The authors of the study say their findings are evidence that symbols of social status can influence the way people judge others on basic human traits. Moreover, they can affect the tendency to respond to others’ suffering with compassion.“This research provides a further demonstration of the myriad ways in which inequality reproduces itself, and though compassion and generosity are potentially powerful tools to increase others’ wellbeing and promote equality, in this instance, it ironically increased inequality by directing these tendencies toward those who already had access to higher-status symbols (and, thus, might be presumed to be better off in the first place),”“Relatedly, I hope this research leads us to think critically about the way we approach solving social issues such as homelessness. Relying exclusively on the kindness of individuals, such as charitable donations, is more likely to be subject to the types of biases we show here, and while charity obviously has its place in addressing issues of poverty and inequality, we believe this research also shows the need for robust policy solutions and structural changes that ensure that everybody receives the help they need.”But the study, like all research, includes some caveats.“One major caveat in this research is that we do not know, exactly, how to interpret each individual’s behavior in the field experiment: for ethical reasons, we did not intentionally mislead people into thinking that I, as the confederate, was unhoused or that donations would go directly towards helping me,” Callaghan explained. “It is possible that some portion of passersby thought I was collecting on behalf of a charity, for example, and the likelihood of making this inference might depend somewhat on whether or not I was wearing a suit. Future research, then, might help to address this ambiguity by explicitly varying whether donations are going directly to the individual displaying status symbols or whether that individual is collecting on behalf of a third-party.”“The other major caveat concerns the confederate in the field study itself: we know from previous research that important social identity characteristics, such as race and gender, can influence these types of processes in complex ways. Since I collected donations, these results cannot necessarily be generalized beyond targets from advantaged social identity groups, such as those who are generally perceived to be White and male.“It is also possible that, even though I followed a standardized procedure, my own behavior could have contributed to the difference in donations in subtle ways that I was unaware of; future research should definitely investigate whether these same results hold for people from other various and intersecting social identity groups, ideally in a way that does not involve anybody familiar with the study and its hypotheses collecting donations.”The study, “The influence of signs of social class on compassionate responses to people in need“, was authored by Bennett Callaghan, Quinton M. Delgadillo, and Michael W. Kraus.","Pedestrians give panhandler more than twice as much money when he wears a suit versus jeans, experiment finds"
311,5,5_study_participants_brain_schizophrenia,https://www.cbs.mpg.de/2055357/20221007-01,"How the mother's mood influences her baby's ability to speakCommunicating with babies in infant-directed-speech is considered an essential prerequisite for successful language development of the little ones. Researchers at the Max Planck Institute for Human Cognitive and Brain Sciences have now investigated how the mood of mothers in the postpartum period affects their child’s development. They found that even children whose mothers suffer from mild depressive mood that do not yet require medical treatment show early signs of delayed language development. The reason for this could be the way the women talk to the newborns. The findings could help prevent potential deficits early on.If mothers are in a depressive mood two months after birth, their children are less able to process speech sounds on average at six months of age. © shutterstock/GrooveZ If mothers are in a depressive mood two months after birth, their children are less able to process speech sounds on average at six months of age. © shutterstock/GrooveZUp to 70 percent of mothers develop postnatal depressive mood, also known as baby blues, after their baby is born. Analyses show that this can also affect the development of the children themselves and their speech. Until now, however, it was unclear exactly how this impairment manifests itself in early language development in infants.In a study, scientists at the Max Planck Institute for Human Cognitive and Brain Sciences in Leipzig have now investigated how well babies can distinguish speech sounds from one another depending on their mother's mood. This ability is considered an important prerequisite for the further steps towards a well-developed language. If sounds can be distinguished from one another, individual words can also be distinguished from one another. It became clear that if mothers indicate a more negative mood two months after birth, their children show on average a less mature processing of speech sounds at the age of six months. The infants found it particularly difficult to distinguish between syllable-pitches. Specifically, they showed that the development of their so-called Mismatch Response was delayed than in those whose mothers were in a more positive mood. This Mismatch Response in turn serves as a measure of how well someone can separate sounds from one another. If this development towards a pronounced mismatch reaction is delayed, this is considered an indication of an increased risk of suffering from a speech disorder later in life.""We suspect that the affected mothers use less infant-directed-speech,"" explains Gesa Schaadt, postdoc at MPI CBS, professor of development in childhood and adolescence at FU Berlin and first author of the study, which has now appeared in the journal JAMA Network Open. ""They probably use less pitch variation when directing speech to their infants."" This also leads to a more limited perception of different pitches in the children, she said. This perception, in turn, is considered a prerequisite for further language development.The results show how important it is that parents use infant-directed speech for the further language development of their children. Infant-directed speech that varies greatly in pitch, emphasizes certain parts of words more clearly - and thus focuses the little ones' attention on what is being said - is considered appropriate for children. Mothers, in turn, who suffer from depressive mood, often use more monotonous, less infant-directed speech. ""To ensure the proper development of young children, appropriate support is also needed for mothers who suffer from mild upsets that often do not yet require treatment,"" Schaadt says. That doesn't necessarily have to be organized intervention measures. ""Sometimes it just takes the fathers to be more involved.""The researchers investigated these relationships with the help of 46 mothers who reported different moods after giving birth. Their moods were measured using a standardized questionnaire typically used to diagnose postnatal upset. They also used electroencephalography (EEG), which helps to measure how well babies can distinguish speech sounds from one another. The so-called Mismatch Response is used for this purpose, in which a specific EEG signal shows how well the brain processes and distinguishes between different speech sounds. The researchers recorded this reaction in the babies at the ages of two and six months while they were presented with various syllables such as ""ba,"" ""ga"" and ""bu.",How the mother's mood influences her baby's ability to speak
357,5,5_study_participants_brain_schizophrenia,https://www.eurekalert.org/news-releases/967846,"A study by the universities of Zurich and Mainz has shown that teaching children how to manage their attention and impulses in primary school has a positive long-term effect on their later educational success.Self-regulation, i.e., the ability to manage attention, emotions and impulses, as well as to pursue individual goals with perseverance, is not a skill that we usually associate with young children. However, the school closures due to the pandemic and the increased usage of digital media by children have now shown how important these abilities are, especially for children.Studies show that people who demonstrated self-regulation as children go on to have on average higher income, better health and greater life satisfaction. They also show that the ability to exert self-regulation can already be trained in a targeted manner in childhood. How can the training of self-regulation skills be integrated into the standard elementary school day without taking up too much teaching time? Is it possible to teach young pupils an abstract self-regulation strategy in an appropriate way? Does teaching such skills have the potential to improve long-term educational success?Self-regulation improves even with short training unitsAn international team from the Department of Economics at the University of Zurich (Switzerland) and from the Johannes Gutenberg University Mainz (Germany) examined these questions. Using a randomized controlled study in elementary schools involving more than 500 first graders, the research team was able to show that even a short training unit led to a significant and sustainable improvement in self-regulation. The training did not just affect self-regulation abilities; the children had significantly improved reading ability and an improved focus on careless mistakes one year after the training, and were also considerably more likely to be admitted to a selective academic secondary school (Gymnasium) three years after the training.“Our study has shown how the training of this skill can be explicitly embedded in primary school teaching at an early stage. An increase in self-regulation enables children to take on more responsibility for their own learning and to set goals on their own and work toward them,” says Ernst Fehr, professor in the Department of Economics at the University of Zurich. According to the last author, children’s comprehensive key skills that are of fundamental importance for good educational attainment and a successful later life can be improved thanks to the simple scalability of the program.Easily integrated into the regular timetableDue to concerns from previous practical experience, the study authors designed the training units in an extremely cost-effective and time-saving manner, in such a way that they could be introduced in any primary school setting: the training unit lasted only five hours, and teachers participated in a three-hour training session and received completely developed teaching materials which they could integrate directly into the regular class schedule.The training units were based on the MCII Strategy (“Mental Contrasting with Implementation Intentions”), which has already been the subject of excellent research studies in adults and older students. The teachers presented the abstract strategy in a playful manner using a picture book and the role model of a hurdle jumper. In a first step, the children imagined the positive effects of reaching a goal. They contrasted them with the obstacles that might face them on the way (“Mental Contrasting”). The children then identified specific behaviors to face the obstacles and develop “when-then” plans (“Implementation Intention”).Positive effect on society“The special feature of our study is the long-term ripple effects that this short training unit can have. These effects benefit the child, and they are transferred in many ways to society as a whole over the course of the child’s life,” says first author Daniel Schunk, professor of public and behavioral economics at Johannes Gutenberg University Mainz. “The fact that early investments in such fundamental skills not only benefit the child alone, but also society, should be given more attention in education policy.”Contacts:Ernst FehrProfessor of Microeconomics and Experimental EconomicsUniversity of ZurichDepartment of EconomicsPhone: +41-44-634-3709E-mail: ernst.fehr@econ.uzh.chDaniel SchunkProfessor of Public and Behavioral EconomicsJohannes Gutenberg University MainzResearch Priority Program “Interdisciplinary Public Policy”Phone: +49-6131-39-27297E-mail: daniel.schunk@uni-mainz.de",Early self-regulation boosts childrenâs educational success
85,5,5_study_participants_brain_schizophrenia,https://news.ki.se/heres-how-the-brain-works-when-we-choose-to-help-someone-in-danger,"“Our findings indicate that the brain’s defence system plays a greater role in helping behaviour than was previously thought. These results contradict the conventional wisdom that we need to suppress our own fear system in order to help others who are in danger”, says Andreas Olsson, professor at the Department of Clinical Neuroscience, Karolinska Institutet and the study's last author.The capacity for empathy for the distress of others has long been considered the driving force of helping. But when we decide to help someone who is in danger, we have to consider not only the other person’s distress but the risk we pose to ourselves by helping – for example, by rushing into a burning building to save someone or helping someone who fell on a train track.Measured brain activityStudies in other animals suggest that the brain’s defence systems are important for helping others, but very little is known about these processes in humans. Researchers from Karolinska Institutet have now investigated this in more detail.The study included 49 healthy volunteers who were asked to decide whether they wanted to help another, unknown person avoid the discomfort of a mild electric shock. But if they decided to help the person, there was a risk that they might receive a shock themselves. The unknown person was visible to the participant on a screen. During the task, the activity in the participants’ brains was imaged with an fMRI scanner.The participants were also told how soon the shock would be delivered, so that the researchers could measure their reactions relative to the proximity of the threat.An evolutionarily ancient region of the brain",Hereâs how the brain works when we choose to help someone in danger
455,5,5_study_participants_brain_schizophrenia,https://www.psypost.org/2022/10/people-systematically-underestimate-how-positively-recipients-will-respond-to-social-support-study-finds-64043,"Have you ever given a stranger a compliment in passing? If you have, they probably appreciated more than you would think. A study published in Psychological Science suggests that people severely underestimate how positively recipients feel about receiving support.Social support is an integral part of a balanced and healthy life. It can have profound effects of well-being by making the recipient feeling important, loved, and cared for by others. It can even improve physical health in some cases. Despite the benefits, many people hesitate to reach out due to being unsure about how the other person will react. People may fear a negative or indifferent reaction, as well as believe their offer of social support will not have any significant impact. This study seeks to better understand the potential gap between perceived and experienced benefits for the recipient.For their research, James A. Dungan and colleagues conducted four studies to test their hypotheses. Study 1 tested expectations and interest in expressing support. Dungan and colleagues collected data through Amazon’s Mechanical Turk and utilized a sample of 100 participants. Participants were asked to identify five people who may be going through a difficult time that they could express support to, why they thought the person needed support, and how they would feel providing that support.In Study 2, Dungan and colleagues tested messages of support by utilizing a sample of 120 University of Chicago students, asked them to identify a person on campus that they believed could use their social support, and had them write an email expressing support. Participants rated how they believed the recipient would feel receiving it, and it was sent anonymously to the recipient. The recipients were asked to complete a survey about the experience of receiving the supportive email.Study 3 measured in-person support and utilized 50 pairs of strangers from the Chicago area to participate. Participants were told to introduce themselves to each other and then put in separate rooms to complete an online survey. Recipients described a difficulty they were facing, and expressers read it. Participants were put back together, and expressers were told to do their best to express support.Study 4 measured differing perspectives on support and utilized 300 participants recruited online to complete an online survey. Participants were told to complete a survey from the perspective of someone expressing or receiving support. They were told to vividly imagine what it would be like to provide or receive support and answered measures on how it felt.Results across the studies showed that even when people could recognize that someone needed social support, there were high levels of hesitation to providing it due to expectations and misconceptions around how the support would be received and responded to. This mismatch seemed to come from people who are considering providing support focusing on the competency aspect of how their support may come off, while recipients focused primarily on how warm or kind it was.“Whether facing a global pandemic or dealing with life’s tribulations, people rely on support from others to manage adversity. Our studies suggest that even when people recognize that support is needed, they may be overly reluctant to express it because they hold miscalibrated expectations of their recipients’ response,” the researchers explained.Additionally, the relationship between the provider and the recipient was a big factor, as many providers did not feel it was helpful or appropriate to provide support to acquaintances or strangers. Contrary to this, results showed that in Study 3, participants received positive benefits from getting support from a complete stranger in a lab setting. Overall, results point to a significant mismatch and suggest that social support is more appreciated than many realize.This study took important steps toward better understanding how social support feels from both giver and recipient. Despite this, there are limitations to note. One such limitation is that the population was all from the US and consisted of people who answered online ads for participants. Future research could target people experiencing significant distress to see if they benefit as much.“Each day offers opportunities to reach out and show some form of support, however large or small, to a person in need,” Dungan and colleagues concluded. “Our experiments suggest that undervaluing the positive impact of expressing support could create a psychological barrier to expressing it more often. Withholding support because of misguided fears of saying or doing the wrong thing could leave both recipients and expressers of support less happy than they could be. Understanding how these psychological barriers restrain prosocial behavior could help to encourage more routine expressions of social support, to everyone’s benefit.”The study, “Too Reluctant to Reach Out: Receiving social Support Is More Positive Than Expressers Expect“, was authored by James Dungan, David Munguia Gomez, and Nicholas Epley.","People systematically underestimate how positively recipients will respond to social support, study finds"
460,5,5_study_participants_brain_schizophrenia,https://www.psypost.org/2022/10/study-cognitive-control-in-children-with-adhd-finds-abnormal-neural-connectivity-patterns-in-multiple-brain-regions-64090,"A new study has identified abnormal brain connectivity in children with ADHD. The findings have been published in Psychiatry Research: Neuroimaging.Functional connectivity is a measure of the correlation between neural activity in different brain regions. When brain regions show similar patterns of activity at the same time when performing specific tasks, it is an indication that they are communicating with each other. Researchers are using functional connectivity to better understand how the brain works, and to identify potential targets for new therapies.“Attention Deficit Hyperactivity Disorder (ADHD) is highly prevalent in children worldwide,” said study author Uttam Kumar, an additional professor at the Center of Biomedical Research at the Sanjay Gandhi Post-Graduate Institute of Medical Sciences.“Presently there is no cure for ADHD, but its symptoms can be managed therapeutically. Thus, it is important to work on these children to increase our understanding towards their brain functioning so behavioral intervention, parent training, peer and social skills training, and school-based intervention/training can be developed effectively.”For their new study, the researchers investigated functional brain connectivity during an arrow flanker task in children with and without ADHD. The arrow flanker task is a cognitive control task that has been used extensively in research to study attention and executive function. The task requires participants to identify the direction of an arrow (e.g., left or right) while ignoring the direction of surrounding arrows. The task is considered to be a measure of cognitive control because it requires participants to inhibit the automatic tendency to respond to the distractors.The study included 16 healthy children and 16 non-medicated male children with ADHD, who were recruited from the outpatient unit of the Department of Psychiatry at King George Medical University.“ADHD is a condition which is almost always associated with poor academic performance and social connectivity with peers,” Kumar told PsyPost. “Early identification and intervention of this multi-factorial neuropsychiatric condition in the children will help to improve their academic performance and at some extent bring them into the mainstream.”“The technique multi-voxel pattern analysis (MVPA) used in this study will help in predicting the sensitive functional biomarkers of patient severity, identifying patterns of brain activity or structure that reliably predict disease onset as well as prediction of clinical outcomes.”The researchers observed abnormal patterns of brain connectivity pattern in multiple regions, including the cerebellum, left dorsolateral prefrontal cortex, right supplementary motor area, and right inferior frontal gyrus. “The ADHD group showed connectivity impairments in all the four selected seed regions. This finding could explain the inability of ADHD children to modulate according to task demands,” they wrote.The findings indicate that “frontal-subcortical (striatal and cerebellar) and frontoparietal networks are crucially affected” in ADHD. But it is still unclear how “this circuit influences the academic and learning skills in ADHD children,” Kumar said. “That need to be further explored.”“ADHD children are good in creative skills; their performance automatically increases when they are involved with the things they like most,” the researcher added. “This skill is important to integrate while planning interventions.”The study, “Altered functional connectivity in children with ADHD while performing cognitive control task“, was authored by Uttam Kumar, Amit Arya, and Vivek Agarwal.",Study on cognitive control in children with ADHD finds abnormal neural connectivity patterns in multiple brain regions
533,5,5_study_participants_brain_schizophrenia,https://www.theverge.com/2022/10/24/23420502/video-game-kid-brain-function-fmri,"Kids who play video games have better memory and better control over their motor skills than kids who don’t, according to a new study looking at adolescent brain function.Video games might not be responsible for those differences — the study can’t say what the causes are — but the findings add to a bigger body of work showing gamers have better performance on some tests of brain function. That lends support to efforts to develop games that can treat cognitive problems.“This study adds to our growing understanding of the associations between playing video games and brain development,” said Nora Volkow, director of the National Institute on Drug Abuse, in a statement.The study used data from the Adolescent Brain Cognitive Development (ABCD) study, which launched in 2018 and is tracking brain development in thousands of children in the United States as they grow into adulthood. Participants periodically go through a battery of assessments, including brain imaging, cognitive tasks, mental health screenings, physical health exams, and other tests.To study video games and cognition, the research team on this new study pulled from the first set of assessments in the ABCD study. It included data on 2,217 children who were nine and 10 years old. The ABCD study asked participants how many hours of video games they played on a typical weekday or weekend day. The research team divided the group into video gamers (kids who played at least 21 hours per week) and non-video gamers (kids who played no video games per week). Kids who only played occasionally weren’t included in the study. Then, the research team looked at the kids’ performance on tests that measure attention, impulse control, and memory.The video gamers did better on the tests, the study found. They also had differences in brain activity patterns from the non-gamers — they had more activity in brain regions involved with attention and memory when they were performing the tests. Notably, there were no differences between the two groups on measures of mental health (more evidence rebutting widespread concerns that video games are bad for emotional well-being).This study adds to a large body of work showing differences in the brains of gamers compared with non-gamers and hinting that gamers have an edge on certain types of brain function. Companies are trying to leverage those differences to develop video games that treat cognitive conditions. Akili Interactive, for example, has a prescription video game to treat ADHD, and DeepWell Digital Therapeutics wants to find the therapeutic value in existing games.But despite all that work, it’s still not clear why there are differences between gamers and non-gamers in this age group. It could be that video games cause the improvements in cognition. It could also be that people who already have better attention for tasks like the ones in this study are more drawn to video games. There are many different types of video games, as well — this new study, for example, didn’t ask what games the gamers played.",Kids who play video games score higher on brain function tests
427,5,5_study_participants_brain_schizophrenia,https://www.nih.gov/news-events/news-releases/video-gaming-may-be-associated-better-cognitive-performance-children,"Video gaming may be associated with better cognitive performance in childrenAdditional research necessary to parse potential benefits and harms of video games on the developing brain.A study of nearly 2,000 children found that those who reported playing video games for three hours per day or more performed better on cognitive skills tests involving impulse control and working memory compared to children who had never played video games. Published today in JAMA Network Open, this study analyzed data from the ongoing Adolescent Brain Cognitive Development (ABCD) Study, which is supported by the National Institute on Drug Abuse (NIDA) and other entities of the National Institutes of Health.“This study adds to our growing understanding of the associations between playing video games and brain development,” said NIDA Director Nora Volkow, M.D. “Numerous studies have linked video gaming to behavior and mental health problems. This study suggests that there may also be cognitive benefits associated with this popular pastime, which are worthy of further investigation.”Although a number of studies have investigated the relationship between video gaming and cognitive behavior, the neurobiological mechanisms underlying the associations are not well understood. Only a handful of neuroimaging studies have addressed this topic, and the sample sizes for those studies have been small, with fewer than 80 participants.To address this research gap, scientists at the University of Vermont, Burlington, analyzed data obtained when children entered the ABCD Study at ages 9 and 10 years old. The research team examined survey, cognitive, and brain imaging data from nearly 2,000 participants from within the bigger study cohort. They separated these children into two groups, those who reported playing no video games at all and those who reported playing video games for three hours per day or more. This threshold was selected as it exceeds the American Academy of Pediatrics screen time guidelines, which recommend that videogaming time be limited to one to two hours per day for older children. For each group, the investigators evaluated the children’s performance on two tasks that reflected their ability to control impulsive behavior and to memorize information, as well as the children’s brain activity while performing the tasks.The researchers found that the children who reported playing video games for three or more hours per day were faster and more accurate on both cognitive tasks than those who never played. They also observed that the differences in cognitive function observed between the two groups was accompanied by differences in brain activity. Functional MRI brain imaging analyses found that children who played video games for three or more hours per day showed higher brain activity in regions of the brain associated with attention and memory than did those who never played. At the same time, those children who played at least three hours of videogames per day showed more brain activity in frontal brain regions that are associated with more cognitively demanding tasks and less brain activity in brain regions related to vision.The researchers think these patterns may stem from practicing tasks related to impulse control and memory while playing videogames, which can be cognitively demanding, and that these changes may lead to improved performance on related tasks. Furthermore, the comparatively low activity in visual areas among children who reported playing video games may reflect that this area of the brain may become more efficient at visual processing as a result of repeated practice through video games.While prior studies have reported associations between video gaming and increases in depression, violence, and aggressive behavior, this study did not find that to be the case. Though children who reported playing video games for three or more hours per day did tend to report higher mental health and behavioral issues compared to children who played no video games, the researchers found that this association was not statistically significant, meaning that the authors could not rule out whether this trend reflected a true association or chance. They note that this will be an important measure to continue to track and understand as the children mature.Further, the researchers stress that this cross-sectional study does not allow for cause-and-effect analyses, and that it could be that children who are good at these types of cognitive tasks may choose to play video games. The authors also emphasize that their findings do not mean that children should spend unlimited time on their computers, mobile phones, or TVs, and that the outcomes likely depend largely on the specific activities children engage in. For instance, they hypothesize that the specific genre of video games, such as action-adventure, puzzle solving, sports, or shooting games, may have different effects for neurocognitive development, and this level of specificity on the type of video game played was not assessed by the study.“While we cannot say whether playing video games regularly caused superior neurocognitive performance, it is an encouraging finding, and one that we must continue to investigate in these children as they transition into adolescence and young adulthood,” said Bader Chaarani, Ph.D., assistant professor of psychiatry at the University of Vermont and the lead author on the study. “Many parents today are concerned about the effects of video games on their children’s health and development, and as these games continue to proliferate among young people, it is crucial that we better understand both the positive and negative impact that such games may have.”Through the ABCD Study, researchers will be able to conduct similar analyses for the same children over time into early adulthood, to see if changes in video gaming behavior are linked to changes in cognitive skills, brain activity, behavior, and mental health. The longitudinal study design and comprehensive data set will also enable them to better account for various other factors in the children’s families and environment that may influence their cognitive and behavioral development, such as exercise, sleep quality, and other influences.The ABCD Study, the largest of its kind in the United States, is tracking nearly 12,000 youth as they grow into young adults. Investigators regularly measure participants’ brain structure and activity using magnetic resonance imaging (MRI) and collect psychological, environmental, and cognitive information, as well as biological samples. The goal of the study is to understand the factors that influence brain, cognitive, and social-emotional development, to inform the development of interventions to enhance a young person’s life trajectory.The Adolescent Brain Cognitive Development Study and ABCD Study are registered service marks and trademarks, respectively, of the U.S. Department of Health and Human ServicesAbout the National Institute on Drug Abuse (NIDA): NIDA is a component of the National Institutes of Health, U.S. Department of Health and Human Services. NIDA supports most of the world’s research on the health aspects of drug use and addiction. The Institute carries out a large variety of programs to inform policy, improve practice, and advance addiction science. For more information about NIDA and its programs, visit www.nida.nih.gov.About the National Institutes of Health (NIH): NIH, the nation's medical research agency, includes 27 Institutes and Centers and is a component of the U.S. Department of Health and Human Services. NIH is the primary federal agency conducting and supporting basic, clinical, and translational medical research, and is investigating the causes, treatments, and cures for both common and rare diseases. For more information about NIH and its programs, visit www.nih.gov.NIH…Turning Discovery Into Health®",Video gaming may be associated with better cognitive performance in children
578,5,5_study_participants_brain_schizophrenia,https://www.uq.edu.au/news/article/2022/09/research-shows-water-fluoridation-safe-children,"Research from The University of Queensland has found no link between community water fluoridation and adverse effects on children’s brain development.Professor Loc Do“We found emotional and behavioural development, and functions such as memory and self-control, were at least equivalent to those who had no exposure to fluoridated water,” Professor Do said.“In other words, there was no difference in child development and function related to fluoridated water.“This finding shows that consuming water with fluoride at levels used for public supplies in Australia is safe and it supports continuing and expanding fluoridation programs.”Currently, approximately 90 per cent of the Australian population has access to fluoridated water, although in Queensland it is 71 per cent.Many regional Queensland areas and Aboriginal and Torres Strait Islander communities are not covered by a fluoridation program.“A small but vocal group of people sometimes claims that water fluoridation can have adverse neurodevelopment effects, especially in young children,” Professor Do said.“This concern can impact community and public health support for the practice, but our research provides reassurance that it is safe and supports its expansion into more communities.“This is an important message because fluoride is extremely effective in preventing tooth decay and its use in water and toothpaste is credited with significant improvements in child dental health in Australia.”Dental caries (also known as tooth decay or dental cavities) is the most common chronic childhood disease worldwide causing pain and infection and can lead to tooth extraction.The UQ study followed up child participants of Australia’s National Child Oral Health study 2012-2014 when they were aged 12 to 17 years.It measured their emotional and behavioural development using a Strengths and Difficulties Questionnaire and executive brain function using the Behaviour Rating Inventory of Executive Function - both instruments widely used in population health surveys.The study was funded by a National Health and Medical Research Council Project grant, and it is a collaboration between The University of Queensland, University of Adelaide and University of Western Australia in Australia, along with the University of Bristol in the UK.The study is published in theMedia: Professor Loc Do,",Research shows water fluoridation is safe for children
412,5,5_study_participants_brain_schizophrenia,https://www.nature.com/articles/s41598-022-20841-0,"Power calculation and study registrationA power calculation was conducted for an interaction effect (repeated measures ANOVA), in G*Power 3.1.9.7 with f = 0.10, α = 0.05, power = 0.90, 4 groups, correlation between repeated measures r = 0.60, resulting in a minimum required total sample size of N = 288 (n = 72 per group). According to the general rule of thumb for Cohen’s f statistic, f ≥ 0.10 < 0.25 is a small effect, f ≥ 0.25 < 0.40 is a medium effect, and, f ≥ 0.40 a large effect (see Cohen, 1988)25. We opted for a small effect size as a similar study as ours, conducted by van Hedger et al.16, also using a repeated-measures ANOVA data analysis approach, reported interaction effects type ([2] natural vs. urban soundscapes) by time ([2] pre-to-post exposure) on mood, whereby Cohen’s d for negative affect was between 0.36 and 0.40. These results were non-significant, as the study was underpowered for detecting small effects. The interaction effects observed concerning cognition in that paper, applying the same tests as in the present paper, were large (d between 0.71 to 0.76). Since we were interested in detecting effects on mood and to study yet unknown effects on state paranoia, we opted for and intermediate effect size between small and medium.The study was pre-registered at aspredicted.org (study name: “Sounds_Online”, trial identifier: #67702, https://aspredicted.org/d5j7j.pdf) on 06/04/2021.Recruitment and in- and exclusion criteriaThe study was programmed using Inquisit 526 (https://www.millisecond.com) and accordingly run on the Millisecond server. Participants were recruited from the crowdsourcing platform Prolific and received 10€ reimbursement for their full participation. Adult individuals were pre-screened on Prolific (i.e., visibility of the study only for candidates with a suited profile) concerning fluent German language skills (as this was the study language), having no diagnosed lifetime mental illness, and having no hearing difficulties. Pre-screened individuals could then access the study, where in- and exclusion criteria were checked further. This included no regular substance or drug intake, no suicidal thoughts, or tendencies, and availability of headphones for the purpose of the study.Study procedureAfter providing informed consent, sociodemographic information was assessed, including education, income, and further variables, which were assessed for potential additional or exploratory analyses, but for the sake of conciseness are not reported in this paper. Psychosis liability was assessed. For an according overview on sample characteristics, see Table 2. Hereafter, pre-test assessments were conducted, including an assessment of mood (depression, anxiety), paranoia, the digit-span, and n-back tasks. Participants were randomized to one of four sound conditions: (1) low diversity traffic noise soundscape n = 83, (2) high diversity traffic noise soundscape n = 69, (3) low diversity birdsong soundscape n = 63, or (4) high diversity birdsong soundscape n = 80, (for details on the stimuli, see “Stimuli” section). The soundscapes each lasted for exactly 6 min. Participants were instructed to set their audio system volume to 80% (which was piloted with members of our research unit beforehand and deemed to be an optimal average volume) and to listen to the sounds until the end, when participants were required to continue by clicking with their mouse. Participants were told that a code, consisting of two spoken digits (in German), would be audible towards the end of the sound presentation, which they were required to type in correctly afterwards. This was implemented to assure listening-compliance and attention. After the sound presentation, the pre-test measures were repeated. Finally, several items to assess perceived sound quality, including beauty, pleasantness, and monotony (vs. diversity) were presented.Table 2 Descriptive sample data and between-group differences for socio-demographic variables. Full size tableSampleInitially, N = 401 individuals started the survey. Of those, n = 76 quit during the sociodemographic assessment, n = 24 lacked pre-test data, and n = 6 lacked post-test data. These n = 106 cases were excluded from the analyses, resulting in a final sample of N = 295. Of these, some participants had incomplete post-test data (n = 10 missing digit span, 5 missing n-back, and n = 8 missing the qualitative assessments [sound rating]).For detailed information and inferential statistics comparing the groups at baseline see Table 2. The participants were in their middle to late twenties on average and there were in tendency more males than females. Net income was mostly reported to be in the lowest category (i.e., < 1.250€, 40–50% of participants of all groups), but also between 5 and 20% of participants did not wish to reveal their monthly net income. Positive symptom frequency levels did not differ significantly between the groups, albeit there were relatively marked descriptive differences (p = 0.058). The values were mostly similar and within a confidence interval range that has previously been reported for healthy individuals19. Due to the trend-level nature of the differences in positive symptom frequency, we decided to repeat the main analyses, controlling for this variable as covariate in the repeated measures ANOVAs.MeasuresFor all mood and the paranoia scales, item scores were computed (i.e., summing up responses on all items and dividing this by the number of items). This way, the interpretation of scores is facilitated, as it corresponds to the Likert-scale of the respective measure.Psychosis liabilityPsychosis-liability or sub-clinical psychosis levels was assessed using the Community Assessment of Psychic Experiences (CAPE)19, in its German version, to assesses lifetime positive, negative and depressive symptoms (http://www.cape42.homestead.com/index.html). The CAPE, including the German version, has been validated extensively17. Items refer to the lifetime prevalence of specific symptoms, rated on an ordinal response scale for frequency (categories: 1 = ‘never’, 2 = ‘sometimes’, 3 = ‘often’, 4 = ‘nearly always’). The total scale consists of 42 items, whereby the positive symptom scale includes 20 (e.g., ‘Do you ever feel as if things in magazines or on TV were written especially for you?’), the negative symptom scale 14 (e.g., ‘Do you ever feel that your mind is empty?’), and the depressive symptom scale 8 items (e.g., ‘Do you ever feel like a failure?’). To test for comparative baseline levels across all groups in psychosis liability, mean frequency scores for the positive symptom subscale was used, for which Mossaheb and colleagues have provided descriptive data for individuals with ultra-high risk for psychosis (n = 84) vs without risk (i.e., healthy controls; n = 81)19. The positive dimension (frequency) of the CAPE had excellent internal consistency in the present sample, with Cronbach’s α = 90.Mood and paranoid symptomsMood was assessed with the State Trait Anxiety Depression Inventory (STADI)27. The scale contains 40 items, whereby the same 20 items are once presented in trait and once in state format. Only the latter was used in the present study. The scale differentiates between depression (low euthymia [inverted items], dysthymia) and anxiety (hyperarousal and worry), whereby each of the subscales is assessed by 5 items. The response format is a 4-point Likert (1 = ‘not at all’, 4 = ‘strongly applies’). Internal consistency (Cronbach’s α) at pre-test was good both for the state anxiety (0.85) and depression (0.86) scales.Paranoia was assessed with a brief, change sensitive state version of the paranoia checklist, which has been validated and comparable to the long, state adapted 18-item version17. The scale comprises 3 statements (e.g., ‘I need to be on my guard against others’, ‘Strangers and friends look at me critically’, ‘People try to upset me’), rated on an 11-point Likert-scale (each from 1 to 11) for the degree of agreement to the statement, associated distress and conviction, at present. The latter two categories were only presented if the rating of agreement to the statement was > 1 (which accordingly often results in a large amount of missing data). In the present study, only agreement was evaluated. Internal consistency at pre-test was acceptable with Cronbach’s α = 0.78.CognitionTo assess digit span cognitive performance, both the forward and backward version were used, as available in Inquisit 526 [retrieved from https://www.millisecond.com] which is based on the original task reported by Woods et al.28. Two parameters are recommended for evaluation: the two-error maximum length (TE_ML) and the maximum length recalled (ML). The two-error maximum length is defined as the last digit span a participant gets correct before making two consecutive errors while the maximum length is the digit span that a participant recalled correctly during all trials irrespective of the number of errors in-between. Starting with a successive visual presentation of 3 digits, the participants need to correctly recall a by 1 digit increasing sequence of digits and reproduce it by clicking on the correct digits in correct order. After two wrongly recalled sequences of the same length, the digit span is decreased by 1 digit until the digit span length again reaches the starting point of 3. The total amount of trials is 14 making the shortest span possible 3 digits long and the longest span 16 digits long. The participants were explicitly reminded not to use any memory assisting methods such as paper and pencil. The dual n-back task, also available in the Inquisit 526 (retrieved from https://www.millisecond.com) was assessed. The task is based on the original work by Jaeggi et al.29. It consists of 4 experimental blocks demanding 2-back and 3-back level performance. While performing the task, subjects pay attention to their computer screen while also listening to a computer audio. On each trial a blue square appears in one out of eight grid-like locations around a central fixation cross, while at the same time a (German) letter is presented via the headphones. In the 2-back block condition, the subjects are instructed to press the “A” button on their keyboard when the current square position matches the square position from two trials before. Subjects are also instructed to press the “L” button on their keyboard if the spoken letter matched the letter two trials before. The same instruction, but having to match stimuli 3 trials back, is provided for the 3-back condition. In the present study, participants trained each condition once, and then went on with the experimental blocks. The performance parameter was the so-called d prime value calculated as the proportion of ((visual_TotalHits − visual_TotalFA) + (auditory_TotalHits − auditory_TotalFA)/2)/number of total experimental blocks. The highest possible d prime (greatest sensitivity) was 6.93 and the lowest was 0. Visual hits are defined as correct responses with respect to the location of the square and auditory hits are defined as the correct responses with respect to the spoken letter. Visual false alarms (FA) are defined as responses in the absence of a target in the visual domain, thus with respect to the location of the square and auditory false alarms are responses in the absence of a target in the auditory domain, thus with respect to the spoken letter.Soundscape perceptionThe participant’s perception of the soundscapes was assessed using a one item questionnaire per dimension (diversity/monotony, pleasantness, and beauty). Participants were asked to report on a 0 to 100 visual scale how diverse/monotone, beautiful, and pleasant they had perceived the soundscape they had listened to during the experiment. The items have been formulated by the authors themselves while the use of an aesthetic rating of the soundscapes per se was a replication from the van Hedger et al.16 study where we exchanged the “like–dislike” affective response with a more detailed aesthetic rating splitting the response up into a pleasantness and a beauty dimension. The dimension of diversity/monotony has been to perform a manipulation check on diversity for the soundscapes used in the present study.StimuliThe soundscapes for all four categories have been generated in the same way. Single sound snippets were gathered and then adapted and merged within the audio software Steinberg Cubase10. An exemplary visualization of the resulting soundscape can be seen in the Supplementary Material (see Supplementary Fig. 1). For the nature category a database of birdsong recordings (https://www.xeno-canto.org/explore/region) from a central European origin was used. For the low diversity birdsong condition, eight recordings from the same two species were used (common chiffchaff & wood warbler). For the high diversity birdsong condition, the same approach was chosen, but recordings from eight different bird species were used to create the soundscape (garden warbler, honey buzzard, woodlark, Eurasian sparrow hawk, coal tit, greenshank, common crane, and black woodpecker). In both birdsong conditions, additionally subtle water and wind sounds were played in the background, to create a constant auditory experience. For the traffic noise conditions, sound snippets from eight car recoding’s (https://freesound.org/search/?q=city) were used for the low diversity traffic noise condition while audio-snippets from eight diverse sources of noise pollution associated with the city were used for the high diversity traffic noise condition (ambulance siren, construction, trucks, train, motorcycle, airplane, bus and fire-fighter siren). In both traffic noise soundscapes, a constant subtle traffic flow was audible in the background.To ensure that all soundscapes were perceived with a similar loudness level, all soundscapes were engineered to have a similar loudness value. The loudness values from all four conditions range between 19.4 and 27.8 loudness units relative to full scale (LUFS). All soundscapes had a duration of 6 min. Prior to the experiment the soundscapes have been presented to a small set of pilot participants rating the similarity of the audio level ensuring a comfortable audio level across all conditions. As a result, at the beginning of the experiment, participants were instructed to set their headphone loudness level to 80%. Soundscapes can be accessed openly via this link https://osf.io/4y3vh/.Statistical analysesAnalyses were run in SPSS 27 (IBM Corp., 2020). To test the differences of all measures at baseline, several univariate analyses of variance (ANOVA) were run. In order to test the effects of high vs. low diverse traffic noise vs. birdsong soundscapes on mood, paranoia, and cognition, repeated measures analyses of variance (ANOVA) were run testing for a 2 (timepoint: pre vs. post) × 2 (soundscape type: birdsong vs. traffic noise) × 2 (diversity: low vs. high) interaction effect. The analyses were once run with all participants, and then only with those who entered at least one of the digits (control of compliance of listening to audio, see “Study procedure” section) correctly, to check for the robustness of findings. In order to further check for the robustness of effects on mood and cognition repeated measures ANOVAs were run controlling for baseline sample differences on sample characteristics or outcomes (i.e., state paranoia, age and positive symptoms) as covariates. To check the robustness of effects on paranoia a univariate analysis of covariance (ANCOVA) with paranoia at baseline as covariate and post-test paranoia as outcome, predicted by type and diversity as factors, was computed. Significant interactions (i.e., of interest were the type × time and type × diversity × time) interactions identified for any of the outcomes were followed up by subsequent detailed post-hoc-tests. To explore mean differences between the qualitative ratings of soundscapes (i.e., beauty, pleasantness, and monotony vs. diversity), a one-way multivariate analysis of variance (MANOVA) was conducted. In case of significant omnibus tests indicating global differences across the qualitative sound rating dimensions, follow-up between group t-tests were conducted. Due to the exploratory nature of the study, no p-level correction was applied.The partial eta squared effect size was used to interpret the ANOVA based analyses, with the corresponding rule of thumb defining η2 = 0.01 as a small effect size, η2 = 0.06 as a medium effect size and η2 = 0.14 as a large effect size30. Cohen’s d effect size was used to interpret post-hoc test effect sizes, with the corresponding rule of thumb defining a value of ≥ 0.2 as a small effect size, a value of ≥ 0.5 as a medium effect size and a value of ≥ 0.8 as a large effect size. The criteria for interpreting the effect size for Hedge’s g stem from the corresponding rule of thumb with the same definition30.Ethics statementAll procedures performed in studies involving human participants were in accordance with the ethical standards of the institutional and/or national research committee and with the 1964 Helsinki Declaration and its later amendments or comparable ethical standards. Informed consent was obtained from all participating subjects. The experimental protocol was approved by the ethical committee from the University Clinic Hamburg Eppendorf.",Birdsongs alleviate anxiety and paranoia in healthy participants
411,5,5_study_participants_brain_schizophrenia,https://www.nature.com/articles/s41598-022-20562-4,"ParticipantsA total of 487 visitors of the NEMO Science Center in Amsterdam aged 8 years or older volunteered to participate in this study. Data was collected during a 2-week Science Live exhibition, during which we tested all visitors interested in volunteering during all opening hours of the NEMO Science Center. While this somewhat restricted our control over the age and the total number of participants, it yielded a final sample size that largely exceeded that of prior studies (e.g., between 30 and 103 participants in references25 and28). Forty-five participants were excluded: 17 participants were excluded because of administrative issues (e.g., accidental reuse of a participant number), seven due to technical issues (e.g., task crash), seven because of language issues (e.g., unable to understand the instructions), six because they worked together or received help from a parent, five participants because they did not finish the tasks in sequence (e.g., with a long break to visit an exhibition show), and two participants because they talked on the phone during the word learning task. As such, 439 participants were included in the main analyses (401 performed the task in Dutch and 38 in English). As the landmark test did not run on all laptops due to technical issues, the number of included participants that completed this task was only 331. Participants were classified as children (8–11 years; mean = 9.33; SD = 1.15), adolescents (12–17 years; mean = 13.19; SD = 1.43), younger adults (18–44 years; mean = 32.73; SD = 8.27) or older adults (> = 45 years [range 46–77]; mean = 53.30; SD = 8.23 ) based on their age (and presumed associated differences changes in dopaminergic functioning:38,45,46). Supplementary Information (SI): Appendix 1 shows demographics and the distribution of participants over age groups and conditions. Participants in the first testing week performed a word learning task with a deep encoding, and participants in the second week performed a shallow encoding task. For participants within each age-group, age distributions were similar across the different novelty and level of processing conditions (for novelty and level of processing respectively, children: p = 0.598 and p = 0.405; adolescents: p = 0.568 and p = 0.155; young adults: p = 0.077 and p = 0.815; old adults: p = 0.658 and p = 0.733). Also sex distributions were similar over conditions (Pearson Chi-Square for novelty and level of processing respectively, children: p = 0.216 and p = 0.821; adolescents: p = 1 and p = 0.128; young adults: p = 0.214 and p = 0.853; older adults: p = 0.285 and p = 0.241).All participants or a participant’s parent in case of minors, gave written informed consent. Participants could choose to perform the tasks in Dutch or English. The study was approved by the Psychology Research Ethics committee (CEP) of Leiden University, the Netherlands. All procedures were in line with the Declaration of Helsinki (1964, and later amendments), and followed relevant COVID-19 guidelines and regulations.General procedureThroughout all procedures the experimenters were wearing a mask and gloves as a safety regulation regarding the COVID-19 pandemic. For data collection we used six laptops in two spacious testing rooms that allowed for social distancing (> = 1.5 m). The experimenter stayed in the testing room throughout the entire procedure to start the tasks and to answer questions. The entire experimental procedure took approximately 15–25 min.Data was collected at the NEMO Science Center in Amsterdam. Upon arrival, participants were asked to disinfect their hands as part of the COVID-19 protocol. Before participation, participants or their parents read the information letter and were given the opportunity to ask questions. After giving written informed consent, the participants were seated before they performed a series of tasks on a laptop.Stimuli and apparatusThe VEs were created using Unity Version 2017.2.21f1 (Unity Technologies, 2017), and were matched in size, path length, and number of intersections. Both VEs consisted of fantasy islands with unusual landmarks (such as a slot machine) at intersections or road endpoints, including land and a body of water (see Fig. 1). The VEs were presented on laptops running on Windows 10 (Microsoft, 2015). Participants could move forward using the W key on the keyboard and the mouse to determine the heading direction. During exploration the X, Y and Z coordinates of the moving agent were logged for all timepoints with a sampling rate of about 15 Hz. The VAS I, VAS 2 and word learning task were programmed and presented using Open Sesame 3.3.347, the landmark task and NS questionnaire were created using E-Prime 3.0 software (Psychology Software Tools, Pittsburgh, PA).Figure 1 Screenshots of the two virtual environments. The environments contained landmarks at intersections and road endpoints, and were matched in size, number of intersections, number of landmarks, and path length. Full size imageFor the word learning task fifteen Dutch neutral nouns were chosen from the CELEX lexical database and translated to English for non-Dutch speakers48. The same words were used in the novel and familiar, and shallow and deep encoding conditions. Four words referred to an animal (“alive”) and eleven words referred to a non-living thing (“not alive”). Similarly, four words started with a closed letter (e.g., “Boat”) and eleven with an open letter (e.g., “Wolf”).Participants were reminded of the response keys and task during the encoding, recall and recognition phase of the word learning and landmark tasks. The response keys were shown below the word, in the location corresponding to the keyboard, and in the semantic task the response keys were further accompanied by the picture of a cow (to indicate a living thing) and a chair (to indicate a non-living thing). These reminders were included to lift the working memory load, especially because this otherwise could have made the task disproportionally difficult for the younger children.Landmarks were objects from the Unity Asset store, and included a wide range of easily recognizable objects, such as an airplane and desk chair. Pictures of the landmarks presented on a grey background were used in the landmark memory test. During this test also lures were presented, which consisted of objects that were not part of either of the two VEs.Exploration phases and affective ratingsParticipants received scripted verbal instructions regarding how to navigate through the VE. The ‘W’ key (for ‘walk’) could be used to move forward, and the mouse could be used to look around and determine heading direction. The space bar could be used to jump, although there was no function in jumping, as one could not jump on top of things. Participants were instructed that they could navigate freely but should try to stay on the paths. During the first familiarization phase, participants explored the VE for 3 min. After exploration, they were asked to indicate their happiness (“How happy are you?”, from 1 = extremely unhappy to 9 = extremely happy) and arousal (“How aroused are you?”, from 1 = very calm to 9 = very excited) on a visual analogue scale (VAS) with Self-Assessment Manikins49. They could use the number keys to indicate their answers, and completing the ratings took less than 1 min.During the second exploration phase participants explored either the same (i.e., familiar) or a new VE for another 3 min (i.e., novelty and VEs were counterbalanced). After this exploration, participants were asked to rate their happiness and arousal levels again on the same two VAS as before the first exploration. See Fig. 2 for the experimental task sequence.Figure 2 Experimental task sequence. Tasks are shown in sequential order from top to bottom. During the first exploration phase participants explored one of the two virtual environments (counterbalanced between participants). Participants filled out Visual Analogue Scales to report current mood and arousal state44 In the second exploration phase participants either explored the same (familiar condition) environment again or a new one (novel condition). The depth of encoding during the word task was varied between subjects, with participants either performing a semantic (deep encoding condition) or shallow encoding task. After a short distractor task, memory ways tested with free recall and a recognition test. After a visuomotor adaptation task (not reported here) landmark memory was tested with a recognition test with confidence judgments. Finally, adults filled out the full Novelty Seeking scale of the TPQ35,36,45, while children answered NS-related questions (non-standardized). Full size imageExperimental tasksFor the word task, instructions were shown on the screen. During the encoding phase, fifteen nouns were shown in a random sequence (we believe this number of items to be sufficient to identify individual and condition differences, as the smaller 10-word learning list from the Consortium to Establish a Registry for Alzheimer’s Disease [CERAD] has been shown to be a sensitive measure for detecting mild cognitive impairment and identifying early symptoms of Alzheimer’s disease, suggesting that relatively short word lists are sufficient to robustly identify individual differences in memory performance50,51. Also other neuropsychological test batteries use relatively short word lists, such as the California Verbal Learning Test [CVLT;52] which uses 16 words, or the Rey Auditory Verbal Learning Test [R-AVLT] which uses 15 words53). In the first week of data collection, word learning involved a deep encoding task in which participants had to judge whether the shown word represented a living (e.g., a cow) or a non-living (e.g., a chair) thing. During the second test week, word learning involved a shallow encoding task in which participants had to indicate whether the first letter of the shown word had an open (such as a “W”) or closed (such as an “O”) shape. Each word was presented for a duration of 3000 ms (irrespective of whether a response was given or not). In between words a fixation cross was shown for 500 ms. After the encoding phase, participants performed a series of nine simple math problems (e.g., 4 – 3 or 7 + 1) in a distractor task. The solution to all problems varied between 1 and 9. Next, participants were prompted to enter as many words as they could remember from the encoding phase. They were instructed to press ENTER, to continue entering words or to press ESC + ENTER to continue if they could not remember any more words. In the following recognition test all 15 words from the encoding phase were randomly shown, interspersed with 10 lures (new words that were not presented during encoding). Participants had to indicate for each word whether it was old (“press X”) or new (“press N”). Each word was shown until a response was given. All phases of the word task were finished in 3–4 min. Recall was quantified by the percentage correctly remembered words, while recognition was quantified by the corrected hit rate (CHR = percentage old hits – percentage new false alarms). Next, participants performed a visuomotor adaptation task, which was completed in 2–3 min (results published in54).The landmark test assessed memory for landmarks that participants could have encountered during the second exploration phase. In total 35 landmarks were shown, of which 20 were present in the second VE (i.e., “old) and 15 were lures (i.e., “new”). Participants had to indicate for each landmark whether they saw it before (“press X for old”) or not (“press N for new”). When participants indicated “old” they were further asked to indicate whether they thought the landmark was “sure old” (“press X”), “probably old” (“press N”) or whether they guessed (“press M “). Each landmark was shown until a response was given and the test had a duration of approximately 2–3 min. As an estimate of landmark recollection, the “sure” CHR was calculated (i.e., “sure” old hits—new false alarms).Novelty seeking questionnaireFinally, participants reported their sex (male; female; other), age in years, and handedness (right; left; ambidextrous). Adults (> 17) subsequently filled out the 34 items of the NS scale of the Tridimensional Personality Questionnaire39,40,55, whereas children and adolescents filled out a simplified and abbreviated (20 item) version of the questionnaire. Each question remained on the screen until a response (“X” = yes; “M” = no) was given. All questions could be answered in about 2–5 min. Afterwards feedback was shown on basis of the total NS score (i.e., with a subdivision into low, medium, and high scorers). These cut-off scores were only used to provide the participants feedback and were not used in any analyses.AnalysesMemory performanceRecall and CHR for words were subjected to 2*2*4 ANOVAs with Novelty (novel; familiar), Encoding type (shallow; deep) and Age group (children; adolescents; younger adults; older adults) as between-subject factors. As we expected the effects of age on memory performance to be quadratic, with performance peaking in adolescents or young adults, we followed up a main effect of age group with a quadratic contrast38. In line with our hypothesis that older adults would show diminished effects of novelty, an interaction between novelty and age was followed up with three 2*2*2 ANOVAs with Novelty (novel; familiar), Encoding type (shallow; deep), and Age (either older adults vs. children, older adults vs. adolescents, or older vs. younger adults) as factors. As the groups between conditions were unequal, we also included Encoding type in this analysis, but the main effect and interactions with this factor are not interpreted. For all analyses, the α-criterion was set at 0.05, and Bonferroni-Holm correction was applied to compensate for multiple testing.Roaming entropy, and other measures of explorationRoaming entropy (RE) during the first and second exploration round was defined for each participant. In this analysis the Z-coordinates were omitted, as the VEs consisted of only one location for each of the XY coordinates (although it was possible that people jumped at a location, they could not climb on anything). As there was a very high number (6.31 million) of possible locations the likelihood that the same coordinates were visited during the 3-min exploration was small, therefore the individual paths were smoothed using a Gaussian filter with a width of 100. Then a likelihood matrix p j was calculated for each of the two VEs, where the likelihood that someone visited each of the XY positions j was calculated, by dividing the total number of visits to that location by the total number of visited locations for all participants. See Fig. 3A for the map of one of the VEs, and 3B for a heatmap depicting the number of visits for all XY coordinates for that VE.Figure 3 Maps of one of the virtual environments. (A) Depicts the map of one the VEs. (B) Shows the number of visits per XY-coordinate in a heatmap for all participants that explored that island. The spawn point in the top left is visible as a highly visited region. Outlines of landmarks can be recognized at some ends of paths. Individual navigation traces show that some people left the paths and used short-cuts to other paths. This data was used to calculate a probability matrix reflecting the likelihood that each of the locations was visited (see main text). Note, the number of visits per XY-coordinate is relatively low, despite smoothing. The probability that each location was visited was used to calculate the roaming entropy (RE) of individuals. Full size imageRoaming entropy (RE i ) was calculated per participant and exploration round by summating over the product between the individual’s path (p ij ) and the log of the probability that each location was visited (p j ) divided by the log of the number of possible locations (k):$$RE_{i} = - \sum\limits_{j = 1}^{k} {\left( {\frac{{p_{ij} \log (p_{j} )}}{\log (k)}} \right)}$$High RE indicates that the participant explored more of the less-often-walked paths, while a lower value reflects higher concordance to the often-walked paths.In addition to RE, we calculated the total distance travelled (in Unity meters) as the sum of Euclidian distances between successive datapoints (2D) and counted the number of landmarks that were encountered in the second exploration round for each participant by defining regions of interest (ROIs) for each of the landmarks for which memory was tested. These ROIs consisted of rectangular bounding boxes around the landmarks. ROIs could overlap in case landmarks were close to each other. For each participant it was determined which ROIs were visited. The total number of ROIs visited provides an additional measure of exploration, as it reflects how many regions were visited by the participant. A GLM including novelty (novel; familiar) and encoding type (deep; shallow) as categorical predictors, and RE for round 2 and age as continuous predictors of word recall was ran, to investigate whether exploration behavior as quantified by RE could predict later word recall above and beyond the effects of novelty and encoding type. We chose to include only RE and not distance travelled, or landmarks encountered in this model, as these measures were found to be positively correlated (see SI: Appendix 5). RE is the most commonly used measure of exploration and the only of these three measures for which we found a novelty effect. The GLM was ran on centered data to reduce multicollinearity. Multicollinearity was shown to be low, with all variance inflation factor (VIF) values < 1.15. It is of note that we ran our task on different laptops with varying specifications, which resulted in different sampling rates between participants, but as participants were randomly distributed over the laptops, and RE exhibits a similar pattern of results as the other exploration measures (e.g., distance traveled, and landmarks encountered) we believe that potential effects of these differences were minimal.PreprintA previous version of this manuscript was published as a preprint https://psyarxiv.com/r2tdn/",Effects of exploring a novel environment on memory across the lifespan
359,5,5_study_participants_brain_schizophrenia,https://www.eurekalert.org/news-releases/968159,"In “emulated” clinical trial, longer work weeks were strongly linked to larger rise in depression symptoms, pushing some first-year resident physicians into moderate or severe depression rangeThe more hours someone works each week in a stressful job, the more their risk of depression rises, a study in new doctors finds.Working 90 or more hours a week was associated with changes in depression symptom scores three times larger than the change in depression symptoms among those working 40 to 45 hours a week.What’s more, a higher percentage of those who worked a large number of hours had scores high enough to qualify for a diagnosis of moderate to severe depression -- serious enough to warrant treatment – compared with those working fewer hours.The research team, based at the University of Michigan, used advanced statistical methods to emulate a randomized clinical trial, accounting for many other factors in the doctors’ personal and professional lives.They found a “dose response” effect between hours worked and depression symptoms, with an average symptom increase of 1.8 points on a standard scale for those working 40 to 45 hours, ranging up to 5.2 points for those working more than 90 hours. They conclude that, among all the stressors affecting physicians, working a large number of hours is a major contributor to depression.Writing in the New England Journal of Medicine, the team from Michigan Medicine, U-M’s academic medical center, report their findings from studying 11 years’ worth of data on more than 17,000 first-year medical residents. The recently graduated doctors were in training at hundreds of hospitals across the United States.The data come from the Intern Health Study, based at the Michigan Neuroscience Institute and the Eisenberg Family Depression Center. Each year, the study recruits new medical school graduates to take part in a year of tracking of their depressive symptoms, work hours, sleep and more while they complete the first year of residency, also called the intern year.The impact of high numbers of work hoursThis study comes as major national organizations, such as the National Academy of Medicine and the Association of American Medical Colleges, grapple with how to address the high rates of depression among physicians, physicians-in-training and other health care professionals. Though the interns in the study reported a wide range of previous-week work hours, the most common work hour levels were between 65 to 80 hours per week.The Accreditation Council for Graduate Medical Education, which sets national standards for residency programs, currently sets an 80-hour limit on residents’ work weeks, but that can be averaged over four weeks and there are possible exceptions. ACGME also limits the length of a single shift and the number of days in a row that residents can work. Studies have shown mixed results about the impact these limits have had on resident wellness and patient safety risks.The authors say their findings point to a clear need to further reduce the number of hours residents work each week on average.“This analysis suggests strongly that reducing the average number of work hours would make a difference in the degree to which interns’ depressive symptoms increase over time, and reduce the number who develop diagnosable depression,” says Amy Bohnert, Ph.D., the study’s senior author and a professor at the U-M Medical School. “The key thing is to have people work fewer hours; you can more effectively deal with the stresses or frustrations of your job when you have more time to recover.”Yu Fang, M.S.E., the study’s lead author and a research specialist at the Michigan Neuroscience Institute, notes that the number of hours is important, but so are the training opportunities that come from time spent in hospitals and clinics. “It is important to use the time spent at work for supervised learning opportunities, and not low-value clinical service tasks,” she says.A population ripe for studyThe new study uses a design called an emulated clinical trial, which simulates a randomized clinical trial in situations where conducting a real randomized trial is not feasible. Because nearly all interns nationwide start at about the same time of year and are subject to varying work schedules set by their programs, studying people going through this stage of medical training is ideal for emulating a clinical trial.This opportunity is what led Intern Health Study founder Srijan Sen, M.D., Ph.D. to launch the research project in the first place: New physicians entering the most stressful year of their careers make a perfect group in which to study the role of many factors in the risk or onset of depression.The authors suggest that studies parallel to this work on physicians should be conducted in other high-stress and high-work-hour jobs. “We would expect that the negative effect of long work hours on physician mental health would be present in other professions,” says Sen.The average age of the doctors in the study was 27, and just over half were women. One in five were training in surgical disciplines, and 18% were from racial or ethnic groups traditionally underrepresented in the medical profession.Less than 1 in 20 met the criteria for moderate to severe depression at the start of intern year. In all, 46% had a stressful life event such as a family death or birth, or getting married, during their intern year, and 37% said they had been involved in at least one medical error during the year.In analyzing the results, the researchers adjusted for gender, neuroticism, pre-internship history of depression, early family environment, age, year they began internship, marital status, whether they had children, and stressful life events and medical errors during the intern year.Make a difference for today’s residents“National initiatives on clinician well-being have put increasing emphasis on the complex set of factors that affect clinician well-being, including the electronic health record, regulatory burden, resilience, workplace violence and culture,” says Sen, the director of the EFDC and the Eisenberg Professor of Depression and Neurosciences. “I think this emphasis has inadvertently led to the feeling that the problem is infinitely complicated and making real progress is hopeless. This paper demonstrates how big of an impact that the single factor of work hours has on clinician depression and well-being.”Sen is part of National Academy of Medicine’s Working Group on Navigating the Impacts of COVID-19 on Clinician Well-Being, part of a larger effort that recently issued a National Plan for Health Workforce Well-Being.Bohnert notes that residency directors running training programs for new doctors could reduce work hours by prioritizing efforts that increase efficiency and decreases unnecessary work.Fang also notes that the data from U.S. residents may apply to junior doctors, as they’re called, in other nations. The Intern Health Study now enrolls interns in China and Kenya as well.Supported by the National Institute of Mental Health (MH101459)Work Hours and Depression in U.S. First-Year PhysiciansN Engl J Med 2022; 387:1522-1524DOI: 10.1056/NEJMc2210365http://www.nejm.org/doi/full/10.1056/NEJMc2210365","In stressful jobs, depression risk rises with hours worked, study in new doctors finds"
502,5,5_study_participants_brain_schizophrenia,https://www.technologynetworks.com/neuroscience/news/researchers-reduce-nightmare-frequency-by-altering-dreamers-emotions-367019,"An innovative study has shown that modulating dreamers’ emotions using therapy and audio cues could reduce the frequency of terrifying nightmares.Researchers combined a well-established therapeutic approach for treating nightmares with an innovative and subliminal stimulus that enhanced the benefits of the therapy. The research, conducted by scientists at the University of Geneva, is published in Current Biology.No more nightmares?Nightmares are distressing experiences that virtually all of us have experienced. But for nearly 5% of the adult population, bad dreams become a much more serious issue. These people experience them more than once a week on average.If such regular nightmares start interfering with one’s waking life, leading international classification texts for mental health disorders, such as the Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition (DSM-V), start treating scary dreams as a recognized condition, called nightmare disorder.Why do we have nightmares? The reasons why nightmares occur remain unclear. The authors write that two main models exist – one suggesting that, like insomnia disorders and post-traumatic stress disorder, clinically significant nightmares are related to a state of hyper-arousal. Evidence suggests that the brains of individuals with frequent nightmares are more active during sleep.Another theory relates to memory and fear extinction. This is a process by which the brain unlearns associations between scary events and fear if the association is no longer useful. For example, if we have a painful experience at the dentist, we might come to associate the type of music being played in the waiting room with fear. Under the fear extinction theory, a normal dreamer might hear that music, as the brain tries to reassociate it with a new scenario with a lower level of fear involved. Nightmares, however, are proposed to be a malfunction of this process, where the level of fear is not reduced, meaning the fear stimulus enters our sleeping mind.Nightmares are so common that psychotherapists have developed a bespoke process for treating them, called imagery rehearsal therapy (IRT). First author of the study Dr. Sophie Schwartz explained how IRT works in an interview with Technology Networks. “IRT is a cognitive behavioral technique that encompasses the following steps: recalling the nightmare, changing the negative story line towards a more positive ending and rehearsing the rewritten dream scenario during the day.”IRT practice takes just 5–10 minutes a day, said Schwartz. “A partial remission of nightmare frequency and severity has been observed after regular use of the technique for two to three weeks.”But for around 30% of people with regular nightmares, IRT doesn’t help. To tackle this intractable issue, Schwartz and colleagues recruited 36 patients with nightmare disorder to explore an innovative combination approach.The science of InceptionWhile 18 patients received IRT treatment as standard, the other half of the patient population were unknowingly subject to an additional intervention – the Inception-like targeted memory reactivation (TMR).During these individuals’ IRT sessions, just when they were asked to imagine a happier version of their nightmare, a sound was played – a neutral piano chord. The researchers didn’t explain the purpose of the tone, but the TMR group were told to play the same sound while they practiced their IRT over the next two weeks. While they slept, all participants wore a headband device, the sleep tracker Dreem, enabling researchers to establish when the participants were in rapid eye movement (REM) dream sleep. Unbeknownst to the participants, the researchers had adapted the device to enable it to play the TMR-linked chord to all participants when they entered deep dream sleep.The idea behind TMR is to help the brain associate cues together and boost learning. The researchers hoped that, without realizing it, the tone would help the sleeping participants recall the positive version of their nightmare scenario that they practiced during IRT, improving their sleep. One strength of this approach is that it largely eliminates “expectation effects”. These are confounding factors in experimental design that arise from patients expecting an intervention to make them feel better and getting a placebo effect boost from that expectation. With no knowledge that they were to be played the sound as they slept, the dreamers were free from such effects.Dream a little deeperWhile the participants who received IRT alone had fewer nightmares, those that also received the TMR tone were comparatively sound sleepers, said Schwartz. “The patients reported having fewer nightmares (disturbing bad dreams) over the two weeks of active treatment and still so after three months.”At that three month-mark, TMR dreamers who had been plagued by almost three nightmares a week now had less than half a nightmare over the same period on average. Dreamers who were given just IRT had 1.5 nightmares per week on average, a significantly higher rate.The researchers also surveyed the dreamers, through questionnaires and dream reports, on the emotional quality of their dream world, taking in nightmares and regular dreams. Those played the positively associated sound reported significantly higher levels of joy in their dreams compared to those who only had IRT.The exact method by which this combination of techniques improves sleep quality remains to be deciphered. The participants in the trial didn’t directly recall the positive scenario that was rehearsed in the IRT sessions – a finding that was confirmed using a machine learning technique called latent semantic analysis to scan the dream diaries that participants produced.While these unanswered riddles may keep the research team awake at night, what’s clear is that for those living with disruptive nightmares, this new technique has the potential to help them sleep more soundly.Reference: Schwartz S, Clerget A and Perogamvros L. Enhancing imagery rehearsal therapy for nightmares with targeted memory reactivation. Curr. Biol. 2022; 32: 1–9. doi:10.1016/j.cub.2022.09.032",Researchers Reduce Nightmare Frequency by Altering Dreamersâ Emotions
423,5,5_study_participants_brain_schizophrenia,https://www.newscientist.com/article/2343471-dyslexia-linked-to-42-genetic-variants-in-biggest-study-of-its-kind/,"Better understanding the genetic variants associated with dyslexia could lead to a test that assesses whether a child is predisposed to have itDyslexia has been linked to 42 genetic variants Shutterstock/Patryk KosmiderIn the largest genetic study of dyslexia to date, scientists have identified 42 genetic variants that may influence a person’s chance of having it.Dyslexia, which affects up to 1 in 10 people in the UK alone to some extent, often affects a person’s ability to read, write and spell.Although the exact cause is unknown, previous research suggests dyslexia is inherited, with smaller studies linking it to a few genes.AdvertisementTo get a wider view of dyslexia’s potential genetic associations, Michelle Luciano at the University of Edinburgh, UK, and her colleagues carried out a genome-wide association study on 1.1 million adults, mainly of European origin, of whom 51,000 said they had been diagnosed with dyslexia. These studies involve scanning markers across the complete sets of genomes of many people to find any variations associated with a particular trait or condition.The researchers identified 42 genetic variants that tended to differ between the participants who did and did not have dyslexia. The more of these variants an individual had, the higher their chance of having dyslexia, the results suggest.Referring to dyslexia, Luciano says: “It’s a complex trait and like lots of complex traits, it’s influenced by many genes, and each of them by itself has a very small effect on the increasing genetic predisposition of dyslexia.”To confirm their results, the researchers created a scoring system for a separate group of 2800 adults and teenagers, based on their dyslexia-associated variants.A higher score was linked to these participants showing more signs of dyslexia on a reading and spelling test. It isn’t known whether any of the 2800 participants had been diagnosed with dyslexia.Read more: Chronic fatigue syndrome linked to almost 200 genetic variantsAround one-third of the 42 genetic variants have previously been linked to conditions such as attention deficit hyperactivity disorder. The remaining links were more novel, with some of these variants also being associated with a lower pain threshold and being ambidextrous.The genetic variants may alter a neurodevelopmental process, says In-Hyun Park at Yale University. This could then affect multiple connections between a person’s neurons, which may cause dyslexia, pain sensitivity and ambidexterity, he says.The 42 genetic variants may not cause dyslexia per se, but could make it more likely to occur if combined with certain learning styles, says Luciano. For example, phonics teaches children to match certain letters with sounds and may be particularly helpful for people with dyslexia, she says.“When people think of genetics, the first thing they might think is that it’s something that’s fixed and we know that that’s simply not the case,” she says. “Genes operate within environments, so the environment is really important to consider.”While much more research is required, the findings could one day lead to a genetic test that identifies children who are predisposed to have dyslexia, says Luciano. This could allow for interventions that reduce the development of reading and writing difficulties, she says.Journal reference: Nature Genetics, DOI: https://doi.org/10.1038/s41588-022-01192-y",Dyslexia linked to 42 genetic variants in biggest study of its kind
409,5,5_study_participants_brain_schizophrenia,https://www.nature.com/articles/s41380-022-01832-z,"Friedman NP, Miyake A. The relations among inhibition and interference control functions: a latent-variable analysis. J Exp Psychol Gen. 2004;133:101–35.Sawa A, Snyder SH. Schizophrenia: diverse approaches to a complex disease. Science 2002;296:692–5.Chong HY, Teoh SL, Wu DBC, Kotirum S, Chiou CF, Chaiyakunapruk N. Global economic burden of schizophrenia: a systematic review. Neuropsychiatr Dis Treat. 2016;12:357–73.Fasseeh A, Németh B, Molnár A, Fricke FU, Horváth M, Kóczián K, et al. A systematic review of the indirect costs of schizophrenia in Europe. Eur J Public Health. 2018;28:1043–9.Ross CA, Margolis RL, Reading SAJ, Pletnikov M, Coyle JT. Neurobiology of Schizophrenia. Neuron 2006;52:139–53.Andreasen NC. Negative symptoms in schizophrenia. Défin Reliab Arch Gen Psychiatry. 1982;39:784–8.McCutcheon RA, Reis Marques T, Howes OD. Schizophrenia-An Overview. JAMA Psychiatry. 2020;77:201–10.Andersen KAA, Carhart‐Harris R, Nutt DJ, Erritzoe D. Therapeutic effects of classic serotonergic psychedelics: a systematic review of modern‐era clinical studies. Acta Psychiatr Scand. 2021;143:101–18.Reiff CM, Richman EE, Nemeroff CB, Carpenter LL, Widge AS, Rodriguez CI, et al. Psychedelics and psychedelic-assisted psychotherapy. Am J Psychiatry. 2020;177:391–410.Patra S. Return of the psychedelics: psilocybin for treatment resistant depression. Asian J Psychiatry. 2016;24:51–2.Melle I, Olav Johannesen J, Haahr UH, Ten Velden Hegelstad W, Joa I, Langeveld J, et al. Causes and predictors of premature death in first-episode schizophrenia spectrum disorders. World Psychiatry J World Psychiatr Assoc WPA. 2017;16:217–8.Tiihonen J, Lönnqvist J, Wahlbeck K, Klaukka T, Niskanen L, Tanskanen A, et al. 11-year follow-up of mortality in patients with schizophrenia: a population-based cohort study (FIN11 study). Lancet 2009;374:620–7.Chesney E, Goodwin GM, Fazel S. Risks of all-cause and suicide mortality in mental disorders: a meta-review. World Psychiatry J. 2014;13:153–60.Ross CA, Margolis RL. Neurogenetics: insights into degenerative diseases and approaches to schizophrenia. Clin Neurosci Res. 2005;5:3–14.Hanson E, Healey K, Wolf D, Kohler C. Assessment of Pharmacotherapy for Negative Symptoms of Schizophrenia. Curr Psychiatry Rep. 2010;12:563–71.Stahl SM, Buckley PF. Negative symptoms of schizophrenia: a problem that will not go away. Acta Psychiatr Scand. 2007;115:4–11.Foussias G, Siddiqui I, Fervaha G, Agid O, Remington G. Dissecting negative symptoms in schizophrenia: opportunities for translation into new treatments. J Psychopharmacol. 2015;29:116–26.Milev P, Ho BC, Arndt S, Andreasen NC. Predictive values of neurocognition and negative symptoms on functional outcome in schizophrenia: a longitudinal first-episode study with 7-year follow-up. Am J Psychiatry. 2005;162:495–506.Austin SF, Mors O, Budtz-Jørgensen E, Secher RG, Hjorthøj CR, Bertelsen M, et al. Long-term trajectories of positive and negative symptoms in first episode psychosis: a 10 year follow-up study in the OPUS cohort. Schizophr Res. 2015;168:84–91.Díaz-Caneja CM, Pina-Camacho L, Rodríguez-Quiroga A, Fraguas D, Parellada M, Arango C. Predictors of outcome in early-onset psychosis: a systematic review. NPJ Schizophr. 2015;1:14005.Ventura J, Subotnik KL, Gitlin MJ, Gretchen-Doorly D, Ered A, Villa KF, et al. Negative symptoms and functioning during the first year after a recent onset of schizophrenia and 8years later. Schizophr Res. 2015;161:407–13.Best MW, Grossman M, Oyewumi LK, Bowie CR. Examination of the Positive and Negative Syndrome Scale factor structure and longitudinal relationships with functioning in early psychosis: PANSS factor structure and functioning. Early Interv Psychiatry. 2016;10:165–70.Lieberman JA, Stroup TS, McEvoy JP, Swartz MS, Rosenheck RA, Perkins DO, et al. Effectiveness of antipsychotic drugs in patients with chronic schizophrenia. N Engl J Med. 2005;353:1209–23.Kirkpatrick B, Fenton WS, Carpenter WT, Marder SR. The NIMH-MATRICS consensus statement on negative symptoms. Schizophr Bull. 2006;32:214–9.Laughren T, Levin R. Food and drug administration perspective on negative symptoms in schizophrenia as a target for a drug treatment claim. Schizophr Bull. 2006;32:220–2.Fusar-Poli P, Smieskova R, Kempton MJ, Ho BC, Andreasen NC, Borgwardt S. Progressive brain changes in schizophrenia related to antipsychotic treatment? A meta-analysis of longitudinal MRI studies. Neurosci Biobehav Rev. 2013;37:1680–91.Selemon LD, Goldman-Rakic PS. The reduced neuropil hypothesis: a circuit based model of schizophrenia. Biol Psychiatry. 1999;45:17–25.Kaar SJ, Angelescu I, Marques TR, Howes OD. Pre-frontal parvalbumin interneurons in schizophrenia: a meta-analysis of post-mortem studies. J Neural Transm. 2019;126:1637–51.Osimo EF, Beck K, Reis Marques T, Howes OD. Synaptic loss in schizophrenia: a meta-analysis and systematic review of synaptic protein and mRNA measures. Mol Psychiatry. 2019;24:549–61.Yan J, Cui Y, Li Q, Tian L, Liu B, Jiang T, et al. Cortical thinning and flattening in schizophrenia and their unaffected parents. Neuropsychiatr Dis Treat. 2019;15:935–46.Winterburn JL, Voineskos AN, Devenyi GA, Plitman E, de la Fuente-Sandoval C, Bhagwat N, et al. Can we accurately classify schizophrenia patients from healthy controls using magnetic resonance imaging and machine learning? A multi-method and multi-dataset study. Schizophr Res. 2019;214:3–10.Jiang Y, Luo C, Li X, Duan M, He H, Chen X, et al. Progressive reduction in gray matter in patients with schizophrenia assessed with MR Imaging by using causal network analysis. Radiology 2018;287:633–42.Kelly S, Jahanshad N, Zalesky A, Kochunov P, Agartz I, Alloza C, et al. Widespread white matter microstructural differences in schizophrenia across 4322 individuals: results from the ENIGMA Schizophrenia DTI Working Group. Mol Psychiatry. 2018;23:1261–9.Narr KL, Toga AW, Szeszko P, Thompson PM, Woods RP, Robinson D, et al. Cortical thinning in cingulate and occipital cortices in first episode schizophrenia. Biol Psychiatry. 2005;58:32–40.Kubota M, Miyata J, Yoshida H, Hirao K, Fujiwara H, Kawada R, et al. Age-related cortical thinning in schizophrenia. Schizophr Res. 2011;125:21–9.Sun D, Stuart GW, Jenkinson M, Wood SJ, McGorry PD, Velakoulis D, et al. Brain surface contraction mapped in first-episode schizophrenia: a longitudinal magnetic resonance imaging study. Mol Psychiatry. 2009;14:976–86.Provenzano FA, Guo J, Wall MM, Feng X, Sigmon HC, Brucato G, et al. Hippocampal pathology in clinical high-risk patients and the onset of schizophrenia. Biol Psychiatry. 2020;87:234–42.Kraguljac NV, Anthony T, Monroe WS, Skidmore FM, Morgan CJ, White DM, et al. A longitudinal neurite and free water imaging study in patients with a schizophrenia spectrum disorder. Neuropsychopharmacology 2019;44:1932–9.Kirkpatrick B, Buchanan RW, Ross DE, Carpenter WT. A separate disease within the syndrome of schizophrenia. Arch Gen Psychiatry. 2001;58:165.Nenadic I, Yotter RA, Sauer H, Gaser C. Patterns of cortical thinning in different subgroups of schizophrenia. Br J Psychiatry. 2015;206:479–83.Walton E, Hibar DP, van Erp TGM, Potkin SG, Roiz-Santiañez R, Crespo-Facorro B, et al. Prefrontal cortical thinning links to negative symptoms in schizophrenia via the ENIGMA consortium. Psychol Med. 2018;48:82–94.Sugihara G, Oishi N, Son S, Kubota M, Takahashi H, Murai T. Distinct patterns of cerebral cortical thinning in schizophrenia: a neuroimaging data-driven approach. Schizophr Bull. 2016;sbw176.Bodnar M, Hovington CL, Buchy L, Malla AK, Joober R, Lepage M. Cortical thinning in temporo-parietal junction (TPJ) in non-affective first-episode of psychosis patients with persistent negative symptoms. Jiang T, editor. PLoS ONE. 2014;9:e101372.Sanfilipo M, Lafargue T, Rusinek H, Arena L, Loneragan C, Lautin A, et al. Volumetric measure of the frontal and temporal lobe regions in schizophrenia: relationship to negative symptoms. Arch Gen Psychiatry. 2000;57:471.Crow TJ. Molecular pathology of schizophrenia: more than one disease process? BMJ. 1980;280:66–8.Correll CU, Schooler NR. Negative symptoms in schizophrenia: a review and clinical guide for recognition, assessment, and treatment. Neuropsychiatr Dis Treat. 2020;16:519–34.Harvey RC, James AC, Shields GE. A systematic review and network meta-analysis to assess the relative efficacy of antipsychotics for the treatment of positive and negative symptoms in early-onset schizophrenia. CNS Drugs. 2016;30:27–39.Krause M, Zhu Y, Huhn M, Schneider-Thoma J, Bighelli I, Nikolakopoulou A, et al. Antipsychotic drugs for patients with schizophrenia and predominant or prominent negative symptoms: a systematic review and meta-analysis. Eur Arch Psychiatry Clin Neurosci. 2018;268:625–39.Németh G, Laszlovszky I, Czobor P, Szalai E, Szatmári B, Harsányi J, et al. Cariprazine versus risperidone monotherapy for treatment of predominant negative symptoms in patients with schizophrenia: a randomised, double-blind, controlled trial. Lancet 2017;389:1103–13.Wagner E, Siafis S, Fernando P, Falkai P, Honer WG, Röh A, et al. Efficacy and safety of clozapine in psychotic disorders-a systematic quantitative meta-review. Transl Psychiatry. 2021;11:487.Raymond N, Lizano P, Kelly S, Hegde R, Keedy S, Pearlson GD, et al. What can clozapine’s effect on neural oscillations tell us about its therapeutic effects? A scoping review and synthesis. Biomark Neuropsychiatry. 2022;6:100048.Barnes TRE, Paton C. Do antidepressants improve negative symptoms in schizophrenia? BMJ. 2011;342:d3371.Kantrowitz JT, Woods SW, Petkova E, Cornblatt B, Corcoran CM, Chen H, et al. D-serine for the treatment of negative symptoms in individuals at clinical high risk of schizophrenia: a pilot, double-blind, placebo-controlled, randomised parallel group mechanistic proof-of-concept trial. Lancet Psychiatry. 2015;2:403–12.Singer P, Dubroqua S, Yee B. Inhibition of glycine transporter 1: The yellow brick road to new schizophrenia therapy? Curr Pharm Des. 2015;21:3771–87.Stauffer VL, Millen BA, Andersen S, Kinon BJ, LaGrandeur L, Lindenmayer JP, et al. Pomaglumetad methionil: No significant difference as an adjunctive treatment for patients with prominent negative symptoms of schizophrenia compared to placebo. Schizophr Res. 2013;150:434–41.Kishi T, Iwata N. NMDA receptor antagonists interventions in schizophrenia: meta-analysis of randomized, placebo-controlled trials. J Psychiatr Res. 2013;47:1143–9.Rezaei F, Mohammad-karimi M, Seddighi S, Modabbernia A, Ashrafi M, Salehi B, et al. Memantine add-on to risperidone for treatment of negative symptoms in patients with stable schizophrenia: randomized, double-blind, placebo-controlled study. J Clin Psychopharmacol. 2013;33:336–42.Deutsch SI, Schwartz BL, Schooler NR, Brown CH, Rosse RB, Rosse SM. Targeting alpha-7 nicotinic neurotransmission in schizophrenia: a novel agonist strategy. Schizophr Res. 2013;148:138–44.Walling D, Marder SR, Kane J, Fleischhacker WW, Keefe RSE, Hosford DA, et al. Phase 2 trial of an alpha-7 nicotinic receptor agonist (TC-5619) in negative and cognitive symptoms of schizophrenia. Schizophr Bull. 2016;42:335–43.Müller N. Inflammation in schizophrenia: pathogenetic aspects and therapeutic considerations. Schizophr Bull. 2018;44:973–82.Monte AS, de Souza GC, McIntyre RS, Soczynska JK, dos Santos JV, Cordeiro RC, et al. Prevention and reversal of ketamine-induced schizophrenia related behavior by minocycline in mice: Possible involvement of antioxidant and nitrergic pathways. J Psychopharmacol (Oxf). 2013;27:1032–43.Inta D, Lang UE, Borgwardt S, Meyer-Lindenberg A, Gass P. Microglia activation and schizophrenia: lessons from the effects of minocycline on postnatal neurogenesis, neuronal survival and synaptic pruning. Schizophr Bull. 2016;sbw088.Solmi M, Veronese N, Thapa N, Facchini S, Stubbs B, Fornaro M, et al. Systematic review and meta-analysis of the efficacy and safety of minocycline in schizophrenia. CNS Spectr. 2017;22:415–26.Zhang L, Zheng H, Wu R, Kosten TR, Zhang XY, Zhao J. The effect of minocycline on amelioration of cognitive deficits and pro-inflammatory cytokines levels in patients with schizophrenia. Schizophr Res. 2019;212:92–8.Deakin B, Suckling J, Barnes TRE, Byrne K, Chaudhry IB, Dazzan P, et al. The benefit of minocycline on negative symptoms of schizophrenia in patients with recent-onset psychosis (BeneMin): a randomised, double-blind, placebo-controlled trial. Lancet. Psychiatry. 2018;5:885–94.Kishimoto T, Horigome T, Takamiya A. Minocycline as a treatment for schizophrenia: is the discussion truly finished? Lancet. Psychiatry. 2018;5:856–7.Flanagan TW, Nichols CD. Psychedelics as anti-inflammatory agents. Int Rev Psychiatry. 2018;30:363–75.Nkadimeng SM, Steinmann CML, Eloff JN. Anti-inflammatory effects of four psilocybin-containing magic mushroom water extracts in vitro on 15-lipoxygenase activity and on lipopolysaccharide-induced cyclooxygenase-2 and inflammatory cytokines in human u937 macrophage cells. J Inflamm Res. 2021;14:3729–38.Stoll W. 11. Lysergsäure-diäthylamid, ein Phantastikum aus der Mutterkorngruppe. Schweiz Arch Neurol Psychiatr. 1947;60:279–323.Condrau C. Klinische Erfahrungen an Geisteskranken mit Lysergsäure- Diäthylamide. Acta Psychiat Scand. 1949;24:9–32.Katzenelbogen S, Fang AD. Narcosynthesis effects of sodium amytal, methedrine and L.S.D-25. Dis Nerv Syst. 1953;14:85–8.Cholden LS, Kurland A, Savage C. Clinical reactions and tolerance to LSD in chronic schizophrenia. J Nerv Ment Dis. 1955;122:211–21.Abramson HA, Hewitt MP, Lennard H, Turner WJ, O’neill FJ, Merlis S. The stablemate concept of therapy as affected by LSD in schizophrenia. J Psychol. 1958;45:75–84.Bender L. D-lysergic acid in the treatment of the biological features of childhood schizophrenia. Dis Nerv Syst. 1966;7 Suppl:43–6.Mogar RE, Aldrich RW. The use of psychedelic agents with autistic schizophrenic children. Behav Neuropsychiatry. 1969;1:44–50.Yensen R, Dryer D. Thirty years of psychedelic research: the spring grove experiment and its sequels.Kefauver E, Harris O. Drug amendments of 1962, 87 P.L. 781; 76 Stat. 780, 1962 (Kefauver Harris Amendments).Hall W. Why was early therapeutic research on psychedelic drugs abandoned? Psychol Med. 2021;1–6.Roth BL. Multiple serotonin receptors: clinical and experimental aspects. Ann Clin Psychiatry J Am Acad Clin Psychiatr. 1994;6:67–78.Hollister LE, Hartman AM. Mescaline, lysergic acid diethylamide and psilocybin: comparison of clinical syndromes, effects on color perception and biochemical measures. Compr Psychiatry. 1962;3:235–41.Wolbach AB, Miner EJ, Isbell H. Comparison of psilocin with psilocybin, mescaline and LSD-25. Psychopharmacologia 1962;3:219–23.Halberstadt AL. Recent advances in the neuropsychopharmacology of serotonergic hallucinogens. Behav Brain Res. 2015;277:99–120.Glennon RA. Arylalkylamine drugs of abuse: an overview of drug discrimination studies. Pharm Biochem Behav. 1999;64:251–6.Preller KH, Vollenweider FX. Phenomenology, structure, and dynamic of psychedelic states. Curr Top Behav Neurosci. 2018;36:221–56.Preller KH, Burt JB, Ji JL, Schleifer CH, Adkinson BD, Stämpfli P, et al. Changes in global and thalamic brain connectivity in LSD-induced altered states of consciousness are attributable to the 5-HT2A receptor. eLife. 2018;7:e35082.Hermle L, Gouzoulis-Mayfrank E, Spitzer M. Blood flow and cerebral laterality in the mescaline model of psychosis. Pharmacopsychiatry 1998;31 Suppl 2:85–91.Dittrich A. The standardized psychometric assessment of altered states of consciousness (ASCs) in humans. Pharmacopsychiatry 1998;31 Suppl 2:80–4.Vollenweider FX, Vollenweider-Scherpenhuyzen MF, Bäbler A, Vogel H, Hell D. Psilocybin induces schizophrenia-like psychosis in humans via a serotonin-2 agonist action. Neuroreport 1998;9:3897–902.Davis M. Mescaline: excitatory effects on acoustic startle are blocked by serotonin2 antagonists. Psychopharmacology 1987;93:286–91.González-Maeso J, Weisstaub NV, Zhou M, Chan P, Ivic L, Ang R, et al. Hallucinogens recruit specific cortical 5-HT(2A) receptor-mediated signaling pathways to affect behavior. Neuron 2007;53:439–52.Jha S, Rajendran R, Fernandes KA, Vaidya VA. 5-HT2A/2C receptor blockade regulates progenitor cell proliferation in the adult rat hippocampus. Neurosci Lett. 2008;441:210–4.Catlow BJ, Song S, Paredes DA, Kirstein CL, Sanchez-Ramos J. Effects of psilocybin on hippocampal neurogenesis and extinction of trace fear conditioning. Exp Brain Res. 2013;228:481–91.Gouzoulis-Mayfrank E, Heekeren K, Neukirch A, Stoll M, Stock C, Obradovic M, et al. Psychological effects of (S)-ketamine and N,N-dimethyltryptamine (DMT): a double-blind, cross-over study in healthy volunteers. Pharmacopsychiatry 2005;38:301–11.Müller F, Borgwardt S. Acute effects of lysergic acid diethylamide (LSD) on resting brain function. Swiss Med Wkly. 2019;149:w20124.Quednow BB, Kometer M, Geyer MA, Vollenweider FX. Psilocybin-induced deficits in automatic and controlled inhibition are attenuated by ketanserin in healthy human volunteers. Neuropsychopharmacol Publ Am Coll Neuropsychopharmacol 2012;37:630–40.Chindo BA, Adzu B, Yahaya TA, Gamaniel KS. Ketamine-enhanced immobility in forced swim test: a possible animal model for the negative symptoms of schizophrenia. Prog Neuropsychopharmacol Biol Psychiatry. 2012;38:310–6.Ke X, Ding Y, Xu K, He H, Wang D, Deng X, et al. The profile of cognitive impairments in chronic ketamine users. Psychiatry Res. 2018;266:124–31.Luo Y, Yu Y, Zhang M, He H, Fan N. Chronic administration of ketamine induces cognitive deterioration by restraining synaptic signaling. Mol Psychiatry. 2020.Javitt DC. Glutamate and schizophrenia: phencyclidine, N-methyl-D-aspartate receptors, and dopamine-glutamate interactions. Int Rev Neurobiol. 2007;78:69–108.Cilia J, Hatcher P, Reavill C, Jones DNC. (+/-) Ketamine-induced prepulse inhibition deficits of an acoustic startle response in rats are not reversed by antipsychotics. J Psychopharmacol. 2007;21:302–11.Geyer MA, Vollenweider FX. Serotonin research: contributions to understanding psychoses. Trends Pharm Sci. 2008;29:445–53.Halberstadt AL, Geyer MA. Serotonergic hallucinogens as translational models relevant to schizophrenia. Int J Neuropsychopharmacol. 2013;16:2165–80.Marona-Lewicka D, Nichols CD, Nichols DE. An animal model of schizophrenia based on chronic LSD administration: old idea, new results. Neuropharmacology 2011;61:503–12.Halberstadt AL, Powell SB, Geyer MA. Role of the 5-HT 2 A receptor in the locomotor hyperactivity produced by phenylalkylamine hallucinogens in mice. Neuropharmacology 2013;70:218–27.Smart RG, Bateman K. Unfavourable reactions to LSD: a review and analysis of the available case reports. Can Med Assoc J. 1967;97:1214–21.Vardy MM, Kay SR. LSD psychosis or LSD-induced schizophrenia? A multimethod inquiry. Arch Gen Psychiatry. 1983;40:877–83.Solursh LP, Clement WR. Hallucinogenic drug abuse: manifestations and management. Can Med Assoc J. 1968;98:407–10.Abraham HD, Aldridge AM. Adverse consequences of lysergic acid diethylamide. Addiction 1993;88:1327–34.Eveloff HH. The LSD syndrome. A review. Calif Med. 1968;109:368–73.Osmond H, Smythies J. Schizophrenia: a new approach. J Ment Sci. 1952;98:309–15.Strassman RJ. Adverse reactions to psychedelic drugs. A review of the literature. J Nerv Ment Dis. 1984;172:577–95.Anastasopoulos G, Photiades H. Effects of LSD-25 on relatives of schizophrenic patients. J Ment Sci. 1962;108:95–8.Langs RJ, Barr HL. Lysergic acid diethylamide (LSD-25) and schizophrenic reactions. A comparative study. J Nerv Ment Dis. 1968;147:163–72.De Gregorio D, Comai S, Posa L, Gobbi G. d-Lysergic acid diethylamide (LSD) as a model of psychosis: mechanism of action and pharmacology. Int J Mol Sci. 2016;17:E1953.Giannini AJ, Eighan MS, Loiselle RH, Giannini MC. Comparison of haloperidol and chlorpromazine in the treatment of phencyclidine psychosis. J Clin Pharm. 1984;24:202–4.Leptourgos P, Fortier-Davy M, Carhart-Harris R, Corlett PR, Dupuis D, Halberstadt AL, et al. Hallucinations under psychedelics and in the schizophrenia spectrum: an interdisciplinary and multiscale comparison. Schizophr Bull. 2020;46:1396–408.Hays P, Tilley JR. The differences between LSD psychosis and schizophrenia. Can Psychiatr Assoc J. 1973;18:331–3.Nayani TH, David AS. The auditory hallucination: a phenomenological survey. Psychol Med. 1996;26:177–89.Kometer M, Schmidt A, Jäncke L, Vollenweider FX. Activation of serotonin 2A receptors underlies the psilocybin-induced effects on α oscillations, N170 visual-evoked potentials, and visual hallucinations. J Soc Neurosci. 2013;33:10544–51.Shanon B. The antipodes of the mind: charting the phenomenology of the Ayahuasca experience. Oxford; New York: Oxford University Press; 2002. 475 p.Sanz C, Zamberlan F, Erowid E, Erowid F, Tagliazucchi E. The experience elicited by hallucinogens presents the highest similarity to dreaming within a large database of psychoactive substance reports. Front Neurosci. 2018;12:7.Krebs TS, Johansen PØ. Psychedelics and mental health: a population study. PloS One. 2013;8:e63972.Johansen PØ, Krebs TS. Psychedelics not linked to mental health problems or suicidal behavior: a population study. J Psychopharmacol. 2015;29:270–9.Lev-Ran S, Feingold D, Goodman C, Lerner AG. Comparing triggers to visual disturbances among individuals with positive vs negative experiences of hallucinogen-persisting perception disorder (HPPD) following LSD use: comparing triggers to HPPD type I and II. Am J Addict. 2017;26:568–71.Lerner G, Rudinski A, Bor D, Goodman O. C. Flashbacks and HPPD: a clinical-oriented concise review. Isr J Psychiatry Relat Sci. 2014;51:296–301.Lerner AG, Shufman E, Kodesh A, Kretzmer G, Sigal M. LSD-induced Hallucinogen Persisting Perception Disorder with depressive features treated with reboxetine: case report. Isr J Psychiatry Relat Sci. 2002;39:100–3.Doyle MA, Ling S, Lui LMW, Fragnelli P, Teopiz KM, Ho R, et al. Hallucinogen persisting perceptual disorder: a scoping review covering frequency, risk factors, prevention, and treatment. Expert Opin Drug Saf. 2022;1–11.Espiard ML, Lecardeur L, Abadie P, Halbecq I, Dollfus S. Hallucinogen persisting perception disorder after psilocybin consumption: a case study. Eur Psychiatry. 2005;20:458–60.Lev-Ran S, Feingold D, Frenkel A, Lerner AG. Clinical characteristics of individuals with schizophrenia and hallucinogen persisting perception disorder: a preliminary investigation. J Dual Diagn. 2014;10:79–83.Breier A, Su TP, Saunders R, Carson RE, Kolachana BS, de Bartolomeis A, et al. Schizophrenia is associated with elevated amphetamine-induced synaptic dopamine concentrations: Evidence from a novel positron emission tomography method. Proc Natl Acad Sci. 1997;94:2569–74.Frankle WG, Paris J, Himes M, Mason NS, Mathis CA, Narendran R. Amphetamine-induced striatal dopamine release measured with an agonist radiotracer in schizophrenia. Biol Psychiatry. 2018;83:707–14.Lahti A. Subanesthetic doses of ketamine stimulate psychosis in schizophrenia. Neuropsychopharmacology 1995;13:9–19.Cramer SC, Sur M, Dobkin BH, O’Brien C, Sanger TD, Trojanowski JQ, et al. Harnessing neuroplasticity for clinical applications. Brain. 2011;134:1591–609.Pittenger C, Duman RS. Stress, depression, and neuroplasticity: a convergence of mechanisms. Neuropsychopharmacology 2008;33:88–109.Olson DE. Psychoplastogens: a promising class of plasticity-promoting neurotherapeutics. J Exp Neurosci. 2018;12:117906951880050.Andrade R. Serotonergic regulation of neuronal excitability in the prefrontal cortex. Neuropharmacology 2011;61:382–6.Beique JC, Imad M, Mladenovic L, Gingrich JA, Andrade R. Mechanism of the 5-hydroxytryptamine 2A receptor-mediated facilitation of synaptic activity in prefrontal cortex. Proc Natl Acad Sci. 2007;104:9870–5.Kurrasch-Orbaugh DM, Parrish JC, Watts VJ, Nichols DE. A complex signaling cascade links the serotonin2A receptor to phospholipase A2 activation: the involvement of MAP kinases: serotonin2A receptor-coupled AA release. J Neurochem. 2003;86:980–91.Qu Y, Chang L, Klaff J, Balbo A, Rapoport SI. Imaging brain phospholipase A2 activation in awake rats in response to the 5-HT2A/2C Agonist (±)2,5-dimethoxy-4-iodophenyl-2-aminopropane (DOI). Neuropsychopharmacology 2003;28:244–52.Dakic V, Minardi Nascimento J, Costa Sartore R, Maciel R, de M, de Araujo DB, et al. Short term changes in the proteome of human cerebral organoids induced by 5-MeO-DMT. Sci Rep. 2017;7:12863.Li N, Lee B, Liu RJ, Banasr M, Dwyer JM, Iwata M, et al. mTOR-dependent synapse formation underlies the rapid antidepressant effects of NMDA antagonists. Science 2010;329:959–64.Ly C, Greb AC, Cameron LP, Wong JM, Barragan EV, Wilson PC, et al. Psychedelics promote structural and functional neural plasticity. Cell Rep. 2018;23:3170–82.Inserra A, De Gregorio D, Gobbi G. Psychedelics in psychiatry: neuroplastic, immunomodulatory, and neurotransmitter mechanisms. Nader M, editor. Pharmacol Rev. 2021;73:202–77.Kadriu B, Greenwald M, Henter ID, Gilbert JR, Kraus C, Park LT, et al. Ketamine and serotonergic psychedelics: common mechanisms underlying the effects of rapid-acting antidepressants. Int J Neuropsychopharmacol. 2021;24:8–21.Cameron LP, Benson CJ, DeFelice BC, Fiehn O, Olson DE. Chronic, intermittent microdoses of the psychedelic N,N-Dimethyltryptamine (DMT) produce positive effects on mood and anxiety in rodents. ACS Chem Neurosci. 2019;10:3261–70.Dong C, Ly C, Dunlap LE, Vargas MV, Sun J, Hwang IW, et al. Psychedelic-inspired drug discovery using an engineered biosensor. Cell 2021;184:2779–92.e18.Berman RM, Cappiello A, Anand A, Oren DA, Heninger GR, Charney DS, et al. Antidepressant effects of ketamine in depressed patients. Biol Psychiatry. 2000;47:351–4.Feder A, Parides MK, Murrough JW, Perez AM, Morgan JE, Saxena S, et al. Efficacy of intravenous ketamine for treatment of chronic posttraumatic stress disorder: a randomized clinical trial. JAMA Psychiatry. 2014;71:681.Ampuero E, Rubio FJ, Falcon R, Sandoval M, Diaz-Veliz G, Gonzalez RE, et al. Chronic fluoxetine treatment induces structural plasticity and selective changes in glutamate receptor subunits in the rat cerebral cortex. Neuroscience 2010;169:98–108.Kraus C, Castrén E, Kasper S, Lanzenberger R. Serotonin and neuroplasticity—links between molecular, functional and structural pathophysiology in depression. Neurosci Biobehav Rev. 2017;77:317–26.Vaidya VA, Marek GJ, Aghajanian GK, Duman RS. 5-HT2A receptor-mediated regulation of brain-derived neurotrophic factor mRNA in the hippocampus and the neocortex. J Neurosci J Soc Neurosci. 1997;17:2785–95.Autry AE, Monteggia LM. Brain-derived neurotrophic factor and neuropsychiatric disorders. Daws LC, editor. Pharmacol Rev. 2012;64:238–58.Browne CA, Lucki I. Antidepressant effects of ketamine: mechanisms underlying fast-acting novel antidepressants. Front Pharmacol [Internet]. 2013;4. http://journal.frontiersin.org/article/10.3389/fphar.2013.00161/abstract. Accessed 26 Jan 2022.Duman RS, Aghajanian GK, Sanacora G, Krystal JH. Synaptic plasticity and depression: new insights from stress and rapid-acting antidepressants. Nat Med. 2016;22:238–49.Savalia NK, Shao LX, Kwan AC. A dendrite-focused framework for understanding the actions of ketamine and psychedelics. Trends Neurosci. 2021;44:260–75.Sarkar A, Kabbaj M. Sex differences in effects of ketamine on behavior, spine density, and synaptic proteins in socially isolated rats. Biol Psychiatry. 2016;80:448–56.Tizabi Y, Bhatti BH, Manaye KF, Das JR, Akinfiresoye L. Antidepressant-like effects of low ketamine dose is associated with increased hippocampal AMPA/NMDA receptor density ratio in female Wistar–Kyoto rats. Neuroscience 2012;213:72–80.Buchborn T, Schröder H, Höllt V, Grecksch G. Repeated lysergic acid diethylamide in an animal model of depression: normalisation of learning behaviour and hippocampal serotonin 5-HT 2 signalling. J Psychopharmacol. 2014;28:545–52.Cini FA, Ornelas I, Marcos E, Goto-Silva L, Nascimento J, Ruschi S, et al. d-Lysergic acid diethylamide has major potential as a cognitive enhancer [Internet]. Neuroscience; 2019. http://biorxiv.org/lookup/doi/10.1101/866814. Accessed 26 Jan 2022.Cameron LP, Benson CJ, Dunlap LE, Olson DE. Effects of N, N -dimethyltryptamine on rat behaviors relevant to anxiety and depression. ACS Chem Neurosci. 2018;9:1582–90.Hibicke M, Landry AN, Kramer HM, Talman ZK, Nichols CD. Psychedelics, but not ketamine, produce persistent antidepressant-like effects in a rodent experimental system for the study of depression. ACS Chem Neurosci. 2020;11:864–71.Hutten NRPW, Mason NL, Dolder PC, Theunissen EL, Holze F, Liechti ME, et al. Mood and cognition after administration of low LSD doses in healthy volunteers: a placebo controlled dose-effect finding study. Eur Neuropsychopharmacol. 2020;41:81–91.Holze F, Vizeli P, Ley L, Müller F, Dolder P, Stocker M, et al. Acute dose-dependent effects of lysergic acid diethylamide in a double-blind placebo-controlled study in healthy subjects. Neuropsychopharmacology 2021;46:537–44.Yaden DB, Griffiths RR. The subjective effects of psychedelics are necessary for their enduring therapeutic effects. ACS Pharm Transl Sci. 2021;4:568–72.Gassaway MM, Jacques TL, Kruegel AC, Karpowicz RJ, Li X, Li S, et al. Deconstructing the Iboga alkaloid skeleton: potentiation of FGF2-induced glial cell line-derived neurotrophic factor release by a novel compound. ACS Chem Biol. 2016;11:77–87.Cao D, Yu J, Wang H, Luo Z, Liu X, He L, et al. Structure-based discovery of nonhallucinogenic psychedelic analogs. Science 2022;375:403–11.Moreno FA, Wiegand CB, Taitano EK, Delgado PL. Safety, tolerability, and efficacy of psilocybin in 9 patients with obsessive-compulsive disorder. J Clin Psychiatry. 2006;67:1735–40.Ona G, Bouso JC. Potential safety, benefits, and influence of the placebo effect in microdosing psychedelic drugs: a systematic review. Neurosci Biobehav Rev. 2020;119:194–203.Szigeti B, Kartner L, Blemings A, Rosas F, Feilding A, Nutt DJ, et al. Self-blinding citizen science to explore psychedelic microdosing. eLife 2021;10:e62878.Rootman JM, Kryskow P, Harvey K, Stamets P, Santos-Brault E, Kuypers KPC, et al. Adults who microdose psychedelics report health related motivations and lower levels of anxiety and depression compared to non-microdosers. Sci Rep. 2021;11:22479.Higgins GA, Carroll NK, Brown M, MacMillan C, Silenieks LB, Thevarkunnel S, et al. Low doses of psilocybin and ketamine enhance motivation and attention in poor performing rats: evidence for an antidepressant property. Front Pharm. 2021;12:640241.Liechti ME, Holze F. Dosing psychedelics and MDMA. Curr Top Behav Neurosci. 2021;4:3–21.Gartz J. Biotransformation of tryptamine in fruiting mycelia of Psilocybe cubensis. Planta Med. 1989;55:249–50.Johnson JR, Burnell-Nugent M, Lossignol D, Ganae-Motan ED, Potts R, Fallon MT. Multicenter, double-blind, randomized, placebo-controlled, parallel-group study of the efficacy, safety, and tolerability of thc:cbd extract and thc extract in patients with intractable cancer-related pain. J Pain Symptom Manag 2010;39:167–79.Russo EB. Taming THC: potential cannabis synergy and phytocannabinoid-terpenoid entourage effects: phytocannabinoid-terpenoid entourage effects. Br J Pharm. 2011;163:1344–64.Matsushima Y, Shirota O, Kikura-Hanajiri R, Goda Y, Eguchi F. Effects of Psilocybe argentipes on marble-burying behavior in mice. Biosci Biotechnol Biochem. 2009;73:1866–8.Zhuk O, Jasicka-Misiak I, Poliwoda A, Kazakova A, Godovan VV, Halama M, et al. Research on acute toxicity and the behavioral effects of methanolic extract from psilocybin mushrooms and psilocin in mice. Toxins 2015;7:1018–29.Carter OL, Burr DC, Pettigrew JD, Wallis GM, Hasler F, Vollenweider FX. Using psilocybin to investigate the relationship between attention, working memory, and the serotonin 1A and 2A receptors. J Cogn Neurosci. 2005;17:1497–508.Carter OL, Hasler F, Pettigrew JD, Wallis GM, Liu GB, Vollenweider FX. Psilocybin links binocular rivalry switch rate to attention and subjective arousal levels in humans. Psychopharmacology 2007;195:415–24.Müller CP, Jacobs BL. Handbook of the behavioral neurobiology of serotonin. 1st ed. London: Academic Press; 2010. (Handbook of behavioral neuroscience).Wing LL, Tapson GS, Geyer MA. 5HT-2 mediation of acute behavioral effects of hallucinogens in rats. Psychopharmacol (Berl). 1990;100:417–25.Moreno JL, Holloway T, Rayannavar V, Sealfon SC, González-Maeso J. Chronic treatment with LY341495 decreases 5-HT(2A) receptor binding and hallucinogenic effects of LSD in mice. Neurosci Lett. 2013;536:69–73.Schmid Y, Enzler F, Gasser P, Grouzmann E, Preller KH, Vollenweider FX, et al. Acute effects of lysergic acid diethylamide in healthy subjects. Biol Psychiatry. 2015;78:544–53.Marek GJ, Aghajanian GK. LSD and the phenethylamine hallucinogen DOI are potent partial agonists at 5-HT2A receptors on interneurons in rat piriform cortex. J Pharm Exp Ther. 1996;278:1373–82.Rasmussen K, Aghajanian GK. Effect of hallucinogens on spontaneous and sensory-evoked locus coeruleus unit activity in the rat: reversal by selective 5-HT2 antagonists. Brain Res. 1986;385:395–400.Geyer MA, Swerdlow NR, Mansbach RS, Braff DL. Startle response models of sensorimotor gating and habituation deficits in schizophrenia. Brain Res Bull. 1990;25:485–98.Halberstadt AL, Geyer MA. Multiple receptors contribute to the behavioral effects of indoleamine hallucinogens. Neuropharmacology 2011;61:364–81.Canal CE, Morgan D. Head-twitch response in rodents induced by the hallucinogen 2,5-dimethoxy-4-iodoamphetamine: a comprehensive history, a re-evaluation of mechanisms, and its utility as a model. Drug Test Anal. 2012;4:556–76.Schreiber R, Brocco M, Audinot V, Gobert A, Veiga S, Millan MJ.1-(2,5-dimethoxy-4 iodophenyl)-2-aminopropane)-induced head-twitches in the rat are mediated by 5-hydroxytryptamine (5-HT) 2A receptors: modulation by novel 5-HT2A/2C antagonists, D1 antagonists and 5-HT1A agonists.J Pharmacol Exp Ther. 1995;273:101–12.Holloway T, Moreno JL, González-Maeso J. HSV-mediated transgene expression of chimeric constructs to study behavioral function of GPCR heteromers in mice. J Vis Exp. 2016.Corne SJ, Pickering RW, Warner BT. A method for assessing the effects of drugs on the central actions of 5-hydroxytryptamine. Br J Pharm Chemother. 1963;20:106–20.Corne SJ, Pickering RW. A possible correlation between drug-induced hallucinations in man and a behavioural response in mice. Psychopharmacologia 1967;11:65–78.Yamamoto T, Ueki S. Behavioral effects of 2,5-dimethoxy-4-methylamphetamine (DOM) in rats and mice. Eur J Pharm. 1975;32:156–62.Hesselgrave N, Troppoli TA, Wulff AB, Cole AB, Thompson SM. Harnessing psilocybin: antidepressant-like behavioral and synaptic actions of psilocybin are independent of 5-HT2R activation in mice. Proc Natl Acad Sci USA. 2021;118:e2022489118.Shao LX, Liao C, Gregg I, Davoudian PA, Savalia NK, Delagarza K, et al. Psilocybin induces rapid and persistent growth of dendritic spines in frontal cortex in vivo. Neuron 2021;109:2535–44.e4.Odland AU, Kristensen JL, Andreasen JT. Investigating the role of 5-HT2A and 5-HT2C receptor activation in the effects of psilocybin, DOI, and citalopram on marble burying in mice. Behav Brain Res. 2021;401:113093.de la Fuente Revenga M, Zhu B, Guevara CA, Naler LB, Saunders JM, Zhou Z, et al. Prolonged epigenomic and synaptic plasticity alterations following single exposure to a psychedelic in mice. Cell Rep. 2021;37:109836.Studerus E, Kometer M, Hasler F, Vollenweider FXAcute. subacute and long-term subjective effects of psilocybin in healthy humans: a pooled analysis of experimental studies. J Psychopharmacol. 2011;25:1434–52.Mithoefer MC, Feduccia AA, Jerome L, Mithoefer A, Wagner M, Walsh Z, et al. MDMA-assisted psychotherapy for treatment of PTSD: study design and rationale for phase 3 trials based on pooled analysis of six phase 2 randomized controlled trials. Psychopharmacol 2019;236:2735–45.Jones CA, Watson DJG, Fone KCF. Animal models of schizophrenia. Br J Pharm. 2011;164:1162–94.Ozawa K, Hashimoto K, Kishimoto T, Shimizu E, Ishikura H, Iyo M. Immune activation during pregnancy in mice leads to dopaminergic hyperfunction and cognitive impairment in the offspring: a neurodevelopmental animal model of schizophrenia. Biol Psychiatry. 2006;59:546–54.Borrell J, Vela JM, Arévalo-Martin A, Molina-Holgado E, Guaza C. Prenatal immune challenge disrupts sensorimotor gating in adult rats. Implications for the etiopathogenesis of schizophrenia. Neuropsychopharmacol Publ Am Coll Neuropsychopharmacol. 2002;26:204–15.Lipina TV, Zai C, Hlousek D, Roder JC, Wong AHC. Maternal immune activation during gestation interacts with Disc1 point mutation to exacerbate schizophrenia-related behaviors in mice. J Neurosci. 2013;33:7654–66.Talukdar PM, Abdul F, Maes M, Binu VS, Venkatasubramanian G, Kutty BM, et al. Maternal immune activation causes schizophrenia-like behaviors in the offspring through activation of immune-inflammatory, oxidative and apoptotic pathways, and lowered antioxidant defenses and neuroprotection. Mol Neurobiol. 2020;57:4345–61.Bitanihirwe BKY, Weber L, Feldon J, Meyer U. Cognitive impairment following prenatal immune challenge in mice correlates with prefrontal cortical AKT1 deficiency. Int J Neuropsychopharmacol. 2010;13:981–96.Murray BG, Davies DA, Molder JJ, Howland JG. Maternal immune activation during pregnancy in rats impairs working memory capacity of the offspring. Neurobiol Learn Mem. 2017;141:150–6.Tunstall B, Beckett S, Mason R. Ultrasonic vocalisations explain unexpected effects on pre-pulse inhibition responses in rats chronically pre-treated with phencyclidine. Behav Brain Res. 2009;202:184–91.Clapcote SJ, Lipina TV, Millar JK, Mackie S, Christie S, Ogawa F, et al. Behavioral phenotypes of Disc1 missense mutations in mice. Neuron 2007;54:387–402.Hikida T, Jaaro-Peled H, Seshadri S, Oishi K, Hookway C, Kong S, et al. Dominant-negative DISC1 transgenic mice display schizophrenia-associated phenotypes detected by measures translatable to humans. Proc Natl Acad Sci USA. 2007;104:14501–6.Lipina TV, Niwa M, Jaaro-Peled H, Fletcher PJ, Seeman P, Sawa A, et al. Enhanced dopamine function in DISC1-L100P mutant mice: implications for schizophrenia. Genes Brain Behav. 2010;9:777–89.Krystal JH, Karper LP, Seibyl JP, Freeman GK, Delaney R, Bremner JD, et al. Subanesthetic effects of the noncompetitive NMDA antagonist, ketamine, in humans. Psychotomimetic, perceptual, cognitive, and neuroendocrine responses. Arch Gen Psychiatry. 1994;51:199–214.Cohen S. Social ties and susceptibility to the common cold. JAMA J Am Med Assoc. 1997;277:1940.Javitt DC, Zukin SR. Recent advances in the phencyclidine model of schizophrenia. Am J Psychiatry. 1991;148:1301–8.Castañé A, Santana N, Artigas F. PCP-based mice models of schizophrenia: differential behavioral, neurochemical and cellular effects of acute and subchronic treatments. Psychopharmacology 2015;232:4085–97.He J, Zu Q, Wen C, Liu Q, You P, Li X, et al. Quetiapine attenuates schizophrenia-like behaviors and demyelination in a MK-801-induced mouse model of schizophrenia. Front Psychiatry. 2020;11:843.Hamm JP, Peterka DS, Gogos JA, Yuste R. Altered cortical ensembles in mouse models of schizophrenia. Neuron 2017;94:153–67.e8.Deyama S, Duman RS. Neurotrophic mechanisms underlying the rapid and sustained antidepressant actions of ketamine. Pharm Biochem Behav. 2020;188:172837.Zanos P, Gould TD. Mechanisms of ketamine action as an antidepressant. Mol Psychiatry. 2018;23:801–11.Bogenschutz M. NIH workshop on psychedelics as therapeutics. 2022.Epstein J, Stern E, Silbersweig D. Mesolimbic activity associated with psychosis in schizophrenia. Symptom-specific PET studies. Ann NY Acad Sci. 1999;877:562–74.Barrett FS, Doss MK, Sepeda ND, Pekar JJ, Griffiths RR. Emotions and brain function are altered up to one month after a single high dose of psilocybin. Sci Rep. 2020;10:2214.Vaupel DB, Nozaki M, Martin WR, Bright LD, Morton EC. The inhibition of food intake in the dog by LSD, mescaline, psilocin, -amphetamine and phenylisopropylamine derivatives. Life Sci. 1979;24:2427–31.Potkin SG, Alva G, Fleming K, Anand R, Keator D, Carreon D, et al. A PET study of the pathophysiology of negative symptoms in schizophrenia. Positron Emiss Tomogr Am J Psychiatry. 2002;159:227–37.Zhang M, Palaniyappan L, Deng M, Zhang W, Pan Y, Fan Z, et al. Abnormal thalamocortical circuit in adolescents with early-onset schizophrenia. J Am Acad Child Adolesc Psychiatry. 2021;60:479–89.Schilling TM, Bossert M, König M, Wirtz G, Weisbrod M, Aschenbrenner S. Acute effects of a single dose of 2 mA of anodal transcranial direct current stimulation over the left dorsolateral prefrontal cortex on executive functions in patients with schizophrenia—a randomized controlled trial. PloS One. 2021;16:e0254695.",Could psychedelic drugs have a role in the treatment of schizophrenia? Rationale and strategy for safe implementation
291,5,5_study_participants_brain_schizophrenia,https://www.aan.com/PressRoom/Home/PressRelease/5025,"Press ReleaseEMBARGOED FOR RELEASE UNTIL 4 PM ET, November 02, 2022In Young Adults, Moderate to Heavy Drinking Linked to Higher Risk of StrokeRisk Increases with More Years of DrinkingMINNEAPOLIS – People in their 20s and 30s who drink moderate to heavy amounts of alcohol may be more likely to have a stroke as young adults than people who drink low amounts or no alcohol, according to a study published in the November 2, 2022, online issue of Neurology®, the medical journal of the American Academy of Neurology. The risk of stroke increased the more years people reported moderate or heavy drinking. “The rate of stroke among young adults has been increasing over the last few decades, and stroke in young adults causes death and serious disability,” said study author Eue-Keun Choi, MD, PhD, of Seoul National University in the Republic of Korea. “If we could prevent stroke in young adults by reducing alcohol consumption, that could potentially have a substantial impact on the health of individuals and the overall burden of stroke on society.” The study looked at records from a Korean national health database for people in their 20s and 30s who had four annual health exams. They were asked about alcohol consumption each year. They were followed for an average of six years. They were asked the number of days per week they drank alcohol and the number of standard drinks per time. People who drank 105 grams or more per week were considered moderate or heavy drinkers. This is equal to 15 grams per day, or slightly more than one drink per day. A standard drink in the United States contains about 14 grams of alcohol, which is equivalent to 12 ounces of beer, five ounces of wine or 1.5 ounces of liquor. More than 1.5 million people were included in the study. A total of 3,153 had a stroke during the study. People who were moderate to heavy drinkers for two or more years of the study were about 20% more likely to have a stroke than people who were light drinkers or did not drink alcohol. Light drinkers were those who drank less than 105 grams per week, or less than 15 grams per day. As the number of years of moderate to heavy drinking increased, so did the risk of stroke. People with two years of moderate to heavy drinking had a 19% increased risk, people with three years had a 22% increased risk and people with four years had a 23% increased risk. These results were after researchers accounted for other factors that could affect the risk of stroke, such as high blood pressure, smoking and body mass index. The association was mainly due to an increased risk of hemorrhagic stroke, or stroke caused by bleeding in the brain. For any type of stroke, people with four years of moderate to heavy drinking had a stroke rate of 0.51 per 1,000 person-years, compared to 0.48 for three years of drinking, 0.43 for two years, 0.37 for one year and 0.31 for none. “Since more than 90% of the burden of stroke overall can be attributed to potentially modifiable risk factors, including alcohol consumption, and since stroke in young adults severely impacts both the individual and society by limiting their activities during their most productive years, reducing alcohol consumption should be emphasized in young adults with heavy drinking habits as part of any strategy to prevent stroke,” Choi said. A limitation of the study was that only Korean people were included, so the results may not apply to people of other races and ethnicities. In addition, people filled out questionnaires about their alcohol consumption, so they may not have remembered correctly. The study was supported by the Korea Medical Device Development Fund and the Korea National Research Foundation. Learn more about stroke at BrainandLife.org, home of the American Academy of Neurology’s free patient and caregiver magazine focused on the intersection of neurologic disease and brain health. Follow Brain & Life® on Facebook, Twitter and Instagram. When posting to social media channels about this research, we encourage you to use the hashtags #Neurology and #AANscience.The American Academy of Neurology is the world's largest association of neurologists and neuroscience professionals, with 38,000 members. The AAN is dedicated to promoting the highest quality patient-centered neurologic care. A neurologist is a doctor with specialized training in diagnosing, treating and managing disorders of the brain and nervous system such as Alzheimer's disease, stroke, migraine, multiple sclerosis, concussion, Parkinson's disease and epilepsy.For more information about the American Academy of Neurology, visit AAN.com or find us on Facebook, Twitter, LinkedIn, Instagram and YouTube.","Legal Marijuana Access Tied To Lower Risk Of Lung Injuries From Contaminated Vapes, Study Indicates"
1,5,5_study_participants_brain_schizophrenia,http://www.exeter.ac.uk/news/research/title_932902_en.html,"Positive childhood experiences of blue spaces linked to better adult well-beingNew research based on data from 18 countries concludes that adults with better mental health are more likely to report having spent time playing in and around coastal and inland waters, such as rivers and lakes (also known collectively as blue spaces) as children.The finding was replicated in each of the countries studied.Mounting evidence shows that spending time in and around green spaces such as parks and woodlands in adulthood is associated with stress reduction and better mental health. However, we know far less about the benefits of blue spaces, or the role childhood contact has in these relationships in later life.Data came from the BlueHealth International Survey (BIS), a cross-sectional survey co-ordinated by the University of Exeter’s European Centre for Environment and Human Health. The current analysis used data from over 15,000 people across 14 European Countries and 4 other non-European countries/regions (Hong Kong, Canada, Australia and California).Respondents were asked to recall their blue space experiences between the ages of 0-16 years including how local they were, how often they visited them, and how comfortable their parents/guardians were with them playing in these settings, as well as more recent contact with green and blue spaces over the last four weeks, and mental health over the last two weeks.The research, published in the Journal of Environmental Psychology, found that individuals who recalled more childhood blue space experiences tended to place greater intrinsic value on natural settings in general, and to visit them more often as adults – each of which, in turn, were associated with better mental wellbeing in adulthood.Valeria Vitale, Lead author and PhD Candidate at Sapienza University of Rome, said: “In the context of an increasingly technological and industrialized world, it’s important to understand how childhood nature experiences relate to wellbeing in later life.“Our findings suggest that building familiarity and confidence in and around blue spaces during childhood may stimulate an inherent joy of nature and encourage people to seek out recreational nature experiences, with beneficial consequences for adult mental health.”Dr Leanne Martin, Co-author and Postdoctoral Research Associate at the University of Exeter’s European Centre for Environment and Human Health, said: “Water settings can be dangerous for children, and parents are right to be cautious. This research suggests though that supporting children to feel comfortable in these settings and developing skills such as swimming at an early age can have previously unrecognised life-long benefits.”Dr Mathew White, Co-author and Senior Scientist at the University of Vienna, said: “The current study is adding to our growing awareness of the need for urban planners and local bodies responsible for managing our green and blue spaces to provide safe, accessible access to natural settings for the healthy mental and physical development of our children.“If our findings are supported by longitudinal research that tracks people’s exposures over the entire life-course, it would suggest that further work, policies and initiatives encouraging more blue space experiences during childhood may be a viable way to support the mental health of future generations.”The study is entitled ‘Mechanisms underlying childhood exposure to blue spaces and adult subjective well-being: An 18-country analysis’ and published in the Journal of Environmental Psychology. The project was funded by the European Union’s Horizon 2020 research and innovation programme under Grant Agreement No. 66773.",Positive childhood experiences of blue spaces linked to better adult well-being
124,6,6_fintech_companies_crypto_startups,https://spectrum.ieee.org/4d-printing-microscale,"Edd Gent is a freelance science and technology writer based in Bangalore, India. His writing focuses on emerging technologies across computing, engineering, energy and bioscience. He's on Twitter at @EddytheGent and email at edd dot gent at outlook dot com. His PGP fingerprint is ABB8 6BB3 3E69 C4A7 EC91 611B 5C12 193D 5DFC C01B. His public key is here. DM for Signal info.",Micro 4D Printing Builds on Programmable Matter
236,6,6_fintech_companies_crypto_startups,https://techcrunch.com/2022/08/10/5-takeaways-from-coinbases-disappointing-q2-results/,"Shares of U.S. crypto exchange Coinbase teetered this morning after its second-quarter results missed both top- and bottom-line expectations, off more than 5% in pre-market trading then jumping as much as 7% after markets opened.The company, once hugely profitable in the wake of its 2021 direct listing thanks to a run in crypto-related trading activities, is now working to limit costs and brave the ongoing “winter” in its market and stick to prior profitability targets for the full year.The Coinbase report — read TechCrunch’s initial look here — is replete with fascinating data, making it difficult to detail in just one column. To wrap our minds around what Coinbase reported yesterday and what its notes on the future mean for the crypto startup economy in the back half of 2022, we’re digging deeper today.What follows are five takeaways from Coinbase’s report that stood out to us. Of course, let us know if you think we missed something critical. To work!",5 takeaways from Coinbaseâs disappointing Q2 results
268,6,6_fintech_companies_crypto_startups,https://techcrunch.com/2022/10/16/fintech-decacorn-brex/,"Welcome to The Interchange! If you received this in your inbox, thank you for signing up and your vote of confidence. If you’re reading this as a post on our site, sign up here so you can receive it directly in the future. Every week, I’ll take a look at the hottest fintech news of the previous week. This will include everything from funding rounds to trends to an analysis of a particular space to hot takes on a particular company or phenomenon. There’s a lot of fintech news out there and it’s my job to stay on top of it — and make sense of it — so you can stay in the know. — Mary AnnHello, hello. By the time you’re reading this, we’ll be two days away from TechCrunch Disrupt! Soooo exciting!But first, let’s talk about fintech.Last week’s big news was corporate spend management startup Brex’s announcement that it was laying off 11% of its staff, or 136 people. It was also revealed that the startup’s CFO, Adam Swiecicki, is departing to join Rippling as its CFO. Notably, workforce platform unicorn Rippling recently entered the corporate management space, making it a direct competitor with Brex.First off, it’s rare — and refreshing — for a company to actually proactively share news of a layoff, so it’s interesting that Brex got ahead of any gossip and let me know firsthand of its plans. And as Alex Wilhelm pointed out on Friday’s Equity podcast, the layoffs appear to mostly be related to Brex’s move earlier this year to no longer work with SMBs and nonprofessionally funded startups. In other words, the company said it primarily let go of people who were focused on serving that group. Still, it must suck for those employees — especially considering those groups that it no longer works with were initially Brex’s bread and butter.Bigger-picture-wise, the news of Brex’s layoffs show that even decacorns have not been immune to this downturn. The company earlier this year confirmed a $300 million Series D extension at a staggering $12.3 billion valuation. And while the company claims to be “in a strong financial position with many years of runway,” it adds that its shift away from SMBs to focus more on enterprise customers — and, by default, any related layoffs — will put the company “on a path to sustainable profitability over the next few years.”Side note: Brex aside, it still blows my journalist mind that companies in general can raise hundreds of millions of dollars in funding and yet not be profitable. I am doubtful that I could ever be a venture-backed startup founder. The pressure of having to provide returns to investors who poured that kind of money into my company and the pressure of not wanting to ever have to lay off staff would likely make me lose sleep at night! Guess that’s why I’m a journalist and not a startup founder!Anyway…speaking of Disrupt and Brex, I will be interviewing co-founder and co-CEO Henrique Dubugras and Anu Hariharan, managing director of YC’s growth fund, YC Continuity, live in a Fireside Chat on October 19! I’ll also be talking to Ramp CEO and co-founder Eric Glyman, Airbase CEO and founder Thejo Kote and Anthemis partner Ruth Foxe Blader in a session called “How to Compete without Losing Your Mind and Runway When Cash Is Expensive” that same day. And lastly, I’ll be chatting with Rippling CEO and co-founder Parker Conrad about his company’s plans to “go global.” Come see us! (Get 15% off here).Oh, and if you want to hear me talk about everything from “The Good and Ugly Sides of Fintech, What Great Journalism Really Means, & Why Startups Represent Hope,” check out this episode of the Fintech Leaders podcast I recently recorded with VC Miguel Armaza.VCs clamor to fund real estate investing startupsHello! It’s Anita Ramaswamy reporting from the fintech desk here at TechCrunch alongside Mary Ann. We’ve been seeing a lot of interest — and funding news — in the real estate and proptech spaces lately. Specifically, there have been a number of startups raising rounds for real estate investing apps that aim to help broaden access to the asset class to retail investors by giving them tools to bypass hurdles like large up-front capital requirements that are typically necessary to invest in property.Fintor is one such example. The startup recently closed on a $6.2 million funding round at an $80 million valuation for its platform that offers fractionalized shares in residential properties to investors for as little as $5. We’ve also covered similar platforms such as Landa, Nada and Arrived Homes, all of which have raised new funding in 2022.The surge in interest among retail investors for access to real estate might seem counterintuitive given that rising interest rates make real estate seem less attractive than it has been for the past few years. But these startups are likely more focused on long-term, secular demand growth for real estate as a part of a diversified portfolio rather than getting caught up in concerns around short-term volatility.Here’s what Fintor founder and CEO Farshad Yousefi had to say about the current market environment in an email to TechCrunch:While recent media headlines have mainly focused on the volatility of the market, there are still present opportunities for investors to take part in investing in real estate with the right type of strategic approach. For example, Atlanta has seen an incredible near 12% year-over-year growth in rental rates, directly boosting investors’ cash flows. Additionally, when looking across the board at the top MSAs, major institutional investors have seen a near 50% jump in renewal rent growth. This drastic upward trend in tenant retention clearly demonstrates where rental demand is going.For a deeper dive into real estate tech and how it’s changing the investing landscape, check out my article in TC+ this week:Weekly NewsPlaid announced last week that it added two new features to its identity verification product. Via email, Plaid’s head of identity and fraud (and former Cognito CEO) Alain Meier told me: “With our new autofill feature, users can be verified in as little as 10 seconds. On the back end, we’re building more intelligence to our risk and fraud models with behavioral analytics to stay ahead of fraudsters.”The behavioral analytics piece is particularly interesting because say you know your SSN/phone number by heart, then your typing behavior would be very different than if you were to copy and paste it from a document. Plaid acknowledges this sort of technology is not new but claims it’s not usually combined with the other fraud detection features that Plaid offers.Going after Square? TechRadar reports: “Following its acquisition of now Zettle in 2018, PayPal has announced a brand new POS device that’s designed to accommodate the needs of small and medium businesses. The newly launched Zettle Terminal connects to the internet via Wi-Fi or a free-of-charge pre-loaded SIM card on the 3G and 4G networks to enable business owners to operate on the go. This ‘completely mobile’ approach should appeal to multi-location vendors, as it doesn’t require additional setup or manual connection at every new location.”As reported by Christine Hall: “Greenlight Financial Technology, a venture-backed fintech company focused on providing a debit card, banking app and financial education to children, added another layer to its subscription plan with the introduction of family safety features. Greenlight Infinity, priced at $14.98 per month for the whole family, includes location sharing to see where anyone in the family is and do check-ins; SOS alerts to emergency contacts and/or 911 with one tap; and crash detection with automatic 911 dispatch wherein if a crash is detected while driving, driver and trip information is provided to emergency services.”TC+ editor Alex Wilhelm dug deep on some Q3 funding numbers, and what he discovered when it comes to the fintech sector wasn’t pretty. He writes: “Looking at Q3 2022 data from CB Insights, it’s clear that the fintech funding boom is behind us; even more, global fintech funding activity is now back to where it was before 2021, indicating that last year was more aberration than new normal for the startup category.”Sarah Perez reports that Apple “is taking a big step toward offering more banking services to its customers. The company announced on Oct. 13 it’s partnering with Goldman Sachs to soon launch a new Savings account feature for its Apple Card credit cardholders which will allow them to save and grow their ‘Daily Cash’ — the cashback rewards that are earned from their Apple Card purchases. In the months ahead, Apple says cardholders will be able to automatically save this cash in a new, high-yield Savings account from partner Goldman Sachs which is accessible with Apple Wallet. Customers will be able to transfer their own money into this account, as well.”The Los Angeles Times reported that “credit cards and digital payment apps such as PayPal offer some distinct advantages over cash, including the ability to recover money paid to scammers. But Zelle, a digital payment network owned by seven major banks, isn’t so protective of its users. If you use Zelle to pay someone who proves to be a con artist, you have only a slim chance of recovering the money from your bank. The same is true if you send money to the wrong person. If you hit Send, the money is probably gone — just as if you’d lost a $20 bill on the street.” Meanwhile, there was chatter on Twitter that Zelle actually had substantially more transaction volume in 2021 than Venmo and CashApp. Hmm. I’m still trying to find evidence of that.Funding and M&ASeen on TechCrunchFormer VC brings smart financial advice to people who really need it, instead of just the rich: In announcing this $24.4 million raise led by GGV Capital, Northstar CEO and co-founder Will Peng told me: “The time from the first meeting to the term sheet was about a month.”With $67M in new capital, NorthOne is doubling down on SMBs as some fintech companies pull backOh look, TripActions raised at a $9.2B valuation after reported $12B IPO filingGetaway launches a way for you to enjoy, and own, vacation homesEgyptian consumer money app Telda raises $20M from GFC, Sequoia Capital and BlockAirwallex raises $100M to power cross-border business banking, valuation stays flat at $5.5BCharli D’Amelio-endorsed fintech Step borrows $300M to bring crypto to teensThis company wants to improve your credit by gamifying financial literacyGoHenry, the banking service for under-18s, raises $55M after passing 2M usersAnd elsewhereVC firm QED acquires fintech executive search companyAstra raises $10M in Series A funding; $30M credit lineCorporate card startup Mercantile raises $22 million to target an unusual niche: professional associationsFinancial Finesse launches venture arm supporting ‘fintech for the greater good’Well, that’s it for this week! Once again, thanks for your continued support — and I really hope to see some of you IRL at Disrupt! xoxo, Mary Ann",Even decacorns have their challenges
585,6,6_fintech_companies_crypto_startups,https://techcrunch.com/2022/06/09/shield-ai-raises-165m-at-a-2-3b-valuation-to-fuel-development-of-its-military-autonomous-flying-systems/,"Technology built with defense in mind is getting some significant and serious traction at the moment, spurred by world events, advances in technology and a growing appetite from end users to invest in more innovative ways to protect themselves. In the latest development,Technology built with defense in mind is getting some significant and serious traction at the moment, spurred by world events, advances in technology and a growing appetite from end users to invest in more innovative ways to protect themselves. In the latest development,The funding is coming in at a $2.3 billion valuation, Shield AI said. The company has been on a strong pace on that front: It follows on from a $210 million-$300 millionThe funding is coming in at a $2.3 billion valuation, Shield AI said. The company has been on a strong pace on that front: It follows on from a $210 million-$300 millionDoug Philippone at Snowpoint Ventures led the round, with Riot Ventures, Disruptive (a returning backer; it led Shield AI’s Series D) and Homebrew (it led Shield AI’s seed round). The company’s other investors include Point72 Ventures, Andreessen Horowitz, Breyer Capital and SVB Capital.Doug Philippone at Snowpoint Ventures led the round, with Riot Ventures, Disruptive (a returning backer; it led Shield AI’s Series D) and Homebrew (it led Shield AI’s seed round). The company’s other investors include Point72 Ventures, Andreessen Horowitz, Breyer Capital and SVB Capital.Philippone is an interesting person to lead on this latest round: In addition to being an investor, he is also Palantir’s global defense lead, a job he’s been in for the last 14 years. This is important not least because Palantir arguably was one of the key companies to change the game for how startups, spurred by the tech boom out of Silicon Valley, both engaged and started to win defense contracts and raised huge sums from VCs to fuel that growth.Philippone is an interesting person to lead on this latest round: In addition to being an investor, he is also Palantir’s global defense lead, a job he’s been in for the last 14 years. This is important not least because Palantir arguably was one of the key companies to change the game for how startups, spurred by the tech boom out of Silicon Valley, both engaged and started to win defense contracts and raised huge sums from VCs to fuel that growth.Another influential startup changing the conversation around funding defense tech is Anduril,Another influential startup changing the conversation around funding defense tech is Anduril,Shield AI is based out of San Diego, which you could say is a little like the Silicon Valley of the defense industry. It’s the home port of the U.S. Pacific fleet, and according toShield AI is based out of San Diego, which you could say is a little like the Silicon Valley of the defense industry. It’s the home port of the U.S. Pacific fleet, and according toAnd if you don’t follow the defense industry, but have at least seen or heard ofAnd if you don’t follow the defense industry, but have at least seen or heard of“China’s military is Netflix; the U.S. military is Blockbuster. China is Amazon; the U.S. is Barnes & Noble. China is Tesla; the U.S. is General Motors,”“China’s military is Netflix; the U.S. military is Blockbuster. China is Amazon; the U.S. is Barnes & Noble. China is Tesla; the U.S. is General Motors,”And on the company’s home page, it describes Hivemind, its AI-based autonomous software platform, as what else? “A Top Gun for every aircraft.”And on the company’s home page, it describes Hivemind, its AI-based autonomous software platform, as what else? “A Top Gun for every aircraft.”As with a lot of other companies (maybe every company) in autonomous transportation, be it in the air or on the ground, Shield AI has a mix of software and hardware that is already usable, and then products that are still in development. Some will be used in purely autonomous systems and some in tandem with humans.As with a lot of other companies (maybe every company) in autonomous transportation, be it in the air or on the ground, Shield AI has a mix of software and hardware that is already usable, and then products that are still in development. Some will be used in purely autonomous systems and some in tandem with humans.In the case of Shield AI, the company says that Hivemind and its Nova drone (or small-unmanned aircraft system, sUAS, in more formal terminology) have been in use since 2018. Ryan Tseng tells us that the specifics of exactly where and how are classified, as are most of the company’s other activities, but they are part of the U.S. Department of Defense Program of Record.In the case of Shield AI, the company says that Hivemind and its Nova drone (or small-unmanned aircraft system, sUAS, in more formal terminology) have been in use since 2018. Ryan Tseng tells us that the specifics of exactly where and how are classified, as are most of the company’s other activities, but they are part of the U.S. Department of Defense Program of Record.It’s also working on a vertical take-off and landing (VTOL) aircraft called V-BAT that will be soon equipped with Hivemind. The software is being integrated into other aircraft, too, such as the F-16 fighter jet pictured above, where it will act as a co-pilot alongside a human, with the aim for it to be used also across F-22s, F-18s and other models. In the meantime, Tseng said in an interview that its V-BAT craft also have been operational since 2018 around the globe.It’s also working on a vertical take-off and landing (VTOL) aircraft called V-BAT that will be soon equipped with Hivemind. The software is being integrated into other aircraft, too, such as the F-16 fighter jet pictured above, where it will act as a co-pilot alongside a human, with the aim for it to be used also across F-22s, F-18s and other models. In the meantime, Tseng said in an interview that its V-BAT craft also have been operational since 2018 around the globe.“The DoD and international militaries are acquiring V-BAT at a rapid rate so we’re ramping production as quickly as possible,” he said — one reason for this funding. V-BAT beat out 13 competitors to win a major Navy Program of Record, he added. Its selling point is its ability to withstand challenging conditions. “The unique design and controls allow it to take off and land in high winds, on crowded flight decks, aboard moving vessels with landing zones as small as 12’ x 12’.”“The DoD and international militaries are acquiring V-BAT at a rapid rate so we’re ramping production as quickly as possible,” he said — one reason for this funding. V-BAT beat out 13 competitors to win a major Navy Program of Record, he added. Its selling point is its ability to withstand challenging conditions. “The unique design and controls allow it to take off and land in high winds, on crowded flight decks, aboard moving vessels with landing zones as small as 12’ x 12’.”The bigger strategy is to build a “swarming” capability for its devices — essentially to use a number of them in concert as a way of evading jamming technologies from adversaries. This, Tseng said, is on track for coming to market by the end of 2023 (although since a lot of what they do is classified, they may not actually make anything public until it’s already being used).The bigger strategy is to build a “swarming” capability for its devices — essentially to use a number of them in concert as a way of evading jamming technologies from adversaries. This, Tseng said, is on track for coming to market by the end of 2023 (although since a lot of what they do is classified, they may not actually make anything public until it’s already being used).Taking both Anduril’s recent landmark round and this latest round for Shield AI, we’re in a moment right now where VCs — working themselves in a challenging financial climate — have changed their tune when it comes to backing companies in the defense space, which includes not just companies like these building military technology, but also those working in cybersecurity and other kinds of technology that helps with resilience. This could include, interestingly, alternative energy tech and of course products that can be used by more than just governments but enterprises as well.Taking both Anduril’s recent landmark round and this latest round for Shield AI, we’re in a moment right now where VCs — working themselves in a challenging financial climate — have changed their tune when it comes to backing companies in the defense space, which includes not just companies like these building military technology, but also those working in cybersecurity and other kinds of technology that helps with resilience. This could include, interestingly, alternative energy tech and of course products that can be used by more than just governments but enterprises as well.“The fundraising climate has never been more favorable for defense technology companies,” Tseng told TechCrunch. “Supporting defense was taboo in many circles. We were rejected by many early investors because defense was considered too controversial. Today, there is growing recognition that investment in defense contributes to security, stability and peace, all of which are foundational to a flourishing society.”“The fundraising climate has never been more favorable for defense technology companies,” Tseng told TechCrunch. “Supporting defense was taboo in many circles. We were rejected by many early investors because defense was considered too controversial. Today, there is growing recognition that investment in defense contributes to security, stability and peace, all of which are foundational to a flourishing society.”As noted by others who are investing in this space right now, or building for it, there has indeed been a noticeable shift in how people view companies like Shield AI and what they are trying to develop. That is still a challenge, though, which might be one reason why a company like Shield goes through the work of putting out messaging to people who may never actually be customers to still take in what they are trying to do.As noted by others who are investing in this space right now, or building for it, there has indeed been a noticeable shift in how people view companies like Shield AI and what they are trying to develop. That is still a challenge, though, which might be one reason why a company like Shield goes through the work of putting out messaging to people who may never actually be customers to still take in what they are trying to do.“Many people don’t realize the scope of conflict in the world — before Ukraine, 84 million people were displaced by violence and persecution, up from 39 million in 2011,” Tseng said. “There aren’t that many opportunities to contribute to technologies that meaningfully address humanity’s great challenges — or that create the general conditions for human achievement. When you work on AI pilots for defense — you are working on the most important and disruptive defense technology of the next thirty years — and are empowering our country and allies to advance security, stability and peace.”“Many people don’t realize the scope of conflict in the world — before Ukraine, 84 million people were displaced by violence and persecution, up from 39 million in 2011,” Tseng said. “There aren’t that many opportunities to contribute to technologies that meaningfully address humanity’s great challenges — or that create the general conditions for human achievement. When you work on AI pilots for defense — you are working on the most important and disruptive defense technology of the next thirty years — and are empowering our country and allies to advance security, stability and peace.”That is bolstered also by the fact that adversaries are also hot on the heels building their own similar systems. China is aiming for military parity by 2027 in the Pacific, Tseng pointed out, meaning they aim to exceed the U.S. by 2028. And he added that there have been reports that it is already benchmarking their prototypes against Shield AI’s pilot.That is bolstered also by the fact that adversaries are also hot on the heels building their own similar systems. China is aiming for military parity by 2027 in the Pacific, Tseng pointed out, meaning they aim to exceed the U.S. by 2028. And he added that there have been reports that it is already benchmarking their prototypes against Shield AI’s pilot.Tseng also may be biased but has a very different idea of why autonomous matters more in this context. “Waymo engineers get to build minivans that plod through the suburbs at 25 mph, we get to work autonomous fighter jets that fly 1,000+ mph, dodge missiles and find threats,” he said.Tseng also may be biased but has a very different idea of why autonomous matters more in this context. “Waymo engineers get to build minivans that plod through the suburbs at 25 mph, we get to work autonomous fighter jets that fly 1,000+ mph, dodge missiles and find threats,” he said.All this is spelling not just an opportunity in the business sense, but a wider one, too, for those backing Shield AI.All this is spelling not just an opportunity in the business sense, but a wider one, too, for those backing Shield AI.“Investors are flocking to quality. This round is a reflection of Shield AI’s success in creating great products, building a business with strong fundamentals and dominant technological leadership — with an AI pilot proven to be the world’s best in numerous military evaluations,” said Philippone in a statement. “We love that they are leveraging an AI and software backbone across a variety of aircraft to deliver truly game-changing value to our warfighters. The work they are doing today is just the tip of the iceberg.”“Investors are flocking to quality. This round is a reflection of Shield AI’s success in creating great products, building a business with strong fundamentals and dominant technological leadership — with an AI pilot proven to be the world’s best in numerous military evaluations,” said Philippone in a statement. “We love that they are leveraging an AI and software backbone across a variety of aircraft to deliver truly game-changing value to our warfighters. The work they are doing today is just the tip of the iceberg.”",Shield AI raises $165M at a $2.3B valuation to fuel development of its military autonomous flying systems
235,6,6_fintech_companies_crypto_startups,https://techcrunch.com/2022/08/09/techcrunch-roundup-bridge-round-bingo-saas-sales-smarts-tracking-monthly-expenses/,"I have limited boating experience, but I do know that being a few degrees off your desired heading can be the difference between enjoying a relaxing dinner with friends and waiting in the dark for a Coast Guard rescue.Similarly, in a down market, SaaS startups that help clients make incremental improvements to cash flow are in a much better position to ride things out.Full TechCrunch+ articles are only available to members.Use discount code TCPLUSROUNDUP to save 20% off a one- or two-year subscription.“In a downturn, money saved is worth even more than money earned,” writes Sahil Mansuri, CEO of Bravado. He advises companies to shift sales strategies from driving growth to helping customers stretch their precious resources: “If you can frame your product as a way to boost revenue or cut costs, people will find a budget.”Mansuri, who started out in software sales during the Great Recession, shares multiple strategies that managers can use to “tailor your approach, show prospects unexpected opportunities and focus on the money.”Thanks very much for reading,Walter ThompsonEditorial Manager, TechCrunch+@yourprotagonist4 principles for building an MVP even if you can’t write a single line of codeYou don’t need a degree in computer science or an MBA to launch a successful startup.Non-technical founders have enjoyed much success by either recruiting a partner who has relevant experience or hiring a freelancer to help them spin up an MVP, according to Magnus Grimeland, founder and CEO of early-stage VC firm Antler.However, “you can be a completely non-technical founder, but you can’t be a completely non-product founder,” he notes.“You must understand your product, through and through, and be able to answer three simple questions: What’s the problem? What’s the solution? How will the customer use the solution?”The party’s over: Tips for tracking and reporting monthly startup expenses and revenueI suspect that Craigslist’s furniture sales listings are a reliable economic indicator for San Francisco Bay Area startups: There seem to be more standing desks and Aeron chairs available than a few months ago, and they are priced to move.Companies are under tremendous pressure to reduce and control spending because investors are demanding transparency, says Jason Richelson, co-founder and CEO of Bookkeep.“CEOs who once got away with marketing themselves as visionaries will also need to think and act like accountants.”Bridge rounds are the late-stage rageData released by Carta shows that more late-stage startups are looking for bridge rounds to stay in business as they work toward landing a more substantial tranche of capital.“Why? Because mega-rounds were so popular last year,” wrote Alex Wilhelm in The Exchange, who found that “the later stage a startup is, the more likely it was in Q2 2022 to raise bridge capital.”5 reasons why Ukraine’s fintech sector is growing despite warUkraine’s fintech sector has proven to be remarkably resilient since Russia’s February invasion has killed and injured thousands, and destroyed much of the nation’s infrastructure.Despite the war, Ukrainian impact entrepreneur Vadym Synegin says his country is creating regulatory frameworks and infrastructure that will continue to drive growth.“I’m sure many investors think the country’s IT sector is a risky investment right now, but it’s still business as usual at fintech companies here,” he writes in a TC+ column.“They have proven their resilience even in wartime conditions, and impressively, 90% of Ukrainian tech startups are still hiring.","TechCrunch+ roundup: Bridge round bingo, SaaS sales smarts, tracking monthly expenses"
586,6,6_fintech_companies_crypto_startups,https://techcrunch.com/2022/06/10/meet-felix-williams-a-vc-who-started-his-own-firm-at-the-age-of-19/,"Felix Williams is the founder and managing director ofFelix Williams is the founder and managing director ofTechCrunch sat down with Williams to learn more about how he got into venture capital, and his plans for the future.TechCrunch sat down with Williams to learn more about how he got into venture capital, and his plans for the future.When did you first become interested in venture capital? How did you break into it?When did you first become interested in venture capital? How did you break into it?While growing up, I had no idea what venture capital was. The concept made sense; who wouldn’t want to be invested in ‘Google’ in the early days, but the idea of an industry that did precisely that was foreign to me until about 16 years old. At the time, there was a fund in St. Louis, iSelect Fund, that was growing rapidly and needed some help doing Excel/database work. I’d have to say that performing that grunt work was the best thing that has happened to me in my professional life. In a few weeks, I was engulfed in the venture and startup worlds. Reading about activity in the ecosystem became my dopamine hit, and I was hooked. I felt like the luckiest teenager in the world, having gotten the opportunity to watch some of the best and brightest people I had ever seen try and solve the problems we are afflicted with, the problems we see on the news every day. The notion of work that I felt in my previous job at a national tutoring chain fell away and was replaced with a sense of purpose.While growing up, I had no idea what venture capital was. The concept made sense; who wouldn’t want to be invested in ‘Google’ in the early days, but the idea of an industry that did precisely that was foreign to me until about 16 years old. At the time, there was a fund in St. Louis, iSelect Fund, that was growing rapidly and needed some help doing Excel/database work. I’d have to say that performing that grunt work was the best thing that has happened to me in my professional life. In a few weeks, I was engulfed in the venture and startup worlds. Reading about activity in the ecosystem became my dopamine hit, and I was hooked. I felt like the luckiest teenager in the world, having gotten the opportunity to watch some of the best and brightest people I had ever seen try and solve the problems we are afflicted with, the problems we see on the news every day. The notion of work that I felt in my previous job at a national tutoring chain fell away and was replaced with a sense of purpose.When did you start your venture firm? What challenges did you face? Did you find it difficult to be taken seriously because of your age at the time? How old were you exactly?When did you start your venture firm? What challenges did you face? Did you find it difficult to be taken seriously because of your age at the time? How old were you exactly?Lagomaj was born a week or two before my 19th birthday. At the time, our path forward wasn’t always clear. For example, I was mistaken for an intern in multiple meetings and generally not taken too seriously at networking or industry events. It wasn’t unusual for founders to take calls mid-pitch or check their messages when it was my turn to ask questions. I learned quickly that the best way to go about working was to build a rapport with someone via email or phone before a face-to-face meeting. Referrals and testimonials went a long way in establishing credibility with people outside my growing network, but that network is what kept me going. I was inspired by the people in my life. There is something very special about working with individuals who devote their lives to working on huge problems. Passion drives the best innovators that we’ve ever known, and there were times where founders and I were able to share a common passion, and those deals have turned out to be some of my favorite ones to have been involved with. As we’ve started building a more robust reputation, my age has become less of an obstacle and more of an advantage as some of my viewpoints are often different from the typical GP.Lagomaj was born a week or two before my 19th birthday. At the time, our path forward wasn’t always clear. For example, I was mistaken for an intern in multiple meetings and generally not taken too seriously at networking or industry events. It wasn’t unusual for founders to take calls mid-pitch or check their messages when it was my turn to ask questions. I learned quickly that the best way to go about working was to build a rapport with someone via email or phone before a face-to-face meeting. Referrals and testimonials went a long way in establishing credibility with people outside my growing network, but that network is what kept me going. I was inspired by the people in my life. There is something very special about working with individuals who devote their lives to working on huge problems. Passion drives the best innovators that we’ve ever known, and there were times where founders and I were able to share a common passion, and those deals have turned out to be some of my favorite ones to have been involved with. As we’ve started building a more robust reputation, my age has become less of an obstacle and more of an advantage as some of my viewpoints are often different from the typical GP.What is your firm’s investment thesis? How much have you raised? What are some of your portfolio companies?What is your firm’s investment thesis? How much have you raised? What are some of your portfolio companies?While we don’t disclose how much we’ve deployed or how much has been committed to the fund, I can say that we have done more than 45 deals with check sizes ranging anywhere from a few hundred thousand to $5 million, most of the time landing somewhere in the middle. In 2021, we invested more than we did in 2017-2020 combined. We currently have a presence in St. Louis, Austin and Southern California, and spend time looking nationally for primarily B2B deals involving early-stage companies. Our fund is especially keen on aligning incentives with the entrepreneurs we work with through a multi-decade investing horizon, participation through multiple rounds, and our willingness to do deals outside of a normal-priced round. For example, we completed a cutting-edge research and development facility with one of our portfolio companies that is seeking to transform how we produce and think about food. That deal is quite different from what most VC funds will take on, but we believed it to be critical to the advancement of a better food system, and we pursued it. Unlike some other funds, we do not consider ourselves an Impact or ESG fund. Our mission is to find passionate people doing extraordinary things for the world we live in, and when you do that, you output ESG gains. I am proud to say that most companies in the portfolio are working towards at least one UN sustainable development goal.While we don’t disclose how much we’ve deployed or how much has been committed to the fund, I can say that we have done more than 45 deals with check sizes ranging anywhere from a few hundred thousand to $5 million, most of the time landing somewhere in the middle. In 2021, we invested more than we did in 2017-2020 combined. We currently have a presence in St. Louis, Austin and Southern California, and spend time looking nationally for primarily B2B deals involving early-stage companies. Our fund is especially keen on aligning incentives with the entrepreneurs we work with through a multi-decade investing horizon, participation through multiple rounds, and our willingness to do deals outside of a normal-priced round. For example, we completed a cutting-edge research and development facility with one of our portfolio companies that is seeking to transform how we produce and think about food. That deal is quite different from what most VC funds will take on, but we believed it to be critical to the advancement of a better food system, and we pursued it. Unlike some other funds, we do not consider ourselves an Impact or ESG fund. Our mission is to find passionate people doing extraordinary things for the world we live in, and when you do that, you output ESG gains. I am proud to say that most companies in the portfolio are working towards at least one UN sustainable development goal.An early win we had was with Agrible and itsAn early win we had was with Agrible and itsWhy did you open an office in Austin?Why did you open an office in Austin?We think that Austin complements our presence in St. Louis well. Both cities have a growing tech scene that is not yet saturated with VC firms, and each has different focuses at the core of their startup ecosystems. In St. Louis, we see an exceptionally robust hard science market, especially bioscience, while in Austin, the focus and growth that we have seen has been more software-centric. Austin too, has many macro trends going for it, such as its desirability for young professionals, a culture that facilitates growth and a considerably strong talent pool. The city has been tremendously welcoming, and we are grateful to be part of its story.We think that Austin complements our presence in St. Louis well. Both cities have a growing tech scene that is not yet saturated with VC firms, and each has different focuses at the core of their startup ecosystems. In St. Louis, we see an exceptionally robust hard science market, especially bioscience, while in Austin, the focus and growth that we have seen has been more software-centric. Austin too, has many macro trends going for it, such as its desirability for young professionals, a culture that facilitates growth and a considerably strong talent pool. The city has been tremendously welcoming, and we are grateful to be part of its story.What are your long-term goals/plans?What are your long-term goals/plans?Over the next few years, our top priority is to build an engine that can invest at scale using data and software to augment the human decision-making process. Although we’ve been operating for a few years now, I am not shy in telling people we are still in the development stages. Our processes and thesis will continue to evolve as we bring in new team members with much more experience than I have. We’re building capabilities on both the ventures and support sides for post-investment portfolio company guidance. In the next two years, like many of our portfolio companies, we plan to forgo profitability and invest heavily in the infrastructure that will position us well in the decades to come. Every day, we refine our offering to investors and portfolio companies. Every day, we will continue that process to ensure that when we go out for our next big fundraising round down the road, we will be poised to do well. It is our opinion that venture as it stands now won’t last forever, and we want to be positioned well for when that paradigm shift starts to manifest itself.Over the next few years, our top priority is to build an engine that can invest at scale using data and software to augment the human decision-making process. Although we’ve been operating for a few years now, I am not shy in telling people we are still in the development stages. Our processes and thesis will continue to evolve as we bring in new team members with much more experience than I have. We’re building capabilities on both the ventures and support sides for post-investment portfolio company guidance. In the next two years, like many of our portfolio companies, we plan to forgo profitability and invest heavily in the infrastructure that will position us well in the decades to come. Every day, we refine our offering to investors and portfolio companies. Every day, we will continue that process to ensure that when we go out for our next big fundraising round down the road, we will be poised to do well. It is our opinion that venture as it stands now won’t last forever, and we want to be positioned well for when that paradigm shift starts to manifest itself.Although 2022 has certainly been interesting as allocators re-evaluate their portfolios, our conviction in particular technology and trends has never been higher. We’re ecstatic about continuing to invest in companies and partnerships at the confluence of innovation and market adoption.Although 2022 has certainly been interesting as allocators re-evaluate their portfolios, our conviction in particular technology and trends has never been higher. We’re ecstatic about continuing to invest in companies and partnerships at the confluence of innovation and market adoption.","Why Felix Williams, who started a VC firm at 19, believes his youth gives him an advantage as an investor"
20,6,6_fintech_companies_crypto_startups,https://arstechnica.com/information-technology/2022/10/passkeys-microsoft-apple-and-googles-password-killer-are-finally-here/,"For years, Big Tech has insisted that the death of the password is right around the corner. For years, those assurances have been little more than empty promises. The password alternatives—such as pushes, OAUTH single-sign ons, and trusted platform modules—introduced as many usability and security problems as they solved. But now, we’re finally on the cusp of a password alternative that’s actually going to work.The new alternative is known as passkeys. Generically, passkeys refer to various schemes for storing authenticating information in hardware, a concept that has existed for more than a decade. What’s different now is that Microsoft, Apple, Google, and a consortium of other companies have unified around a single passkey standard shepherded by the FIDO Alliance. Not only are passkeys easier for most people to use than passwords; they are also completely resistant to credential phishing, credential stuffing, and similar account takeover attacks.On Monday, PayPal said US-based users would soon have the option of logging in using FIDO-based passkeys, joining Kayak, eBay, Best Buy, CardPointers, and WordPress as online services that will offer the password alternative. In recent months, Microsoft, Apple, and Google have all updated their operating systems and apps to enable passkeys. Passkey support is still spotty. Passkeys stored on iOS or macOS will work on Windows, for instance, but the reverse isn’t yet available. In the coming months, all of that should be ironed out, though.What, exactly, are passkeys?Passkeys work almost identically to the FIDO authenticators that allow us to use our phones, laptops, computers, and Yubico or Feitian security keys for multi-factor authentication. Just like the FIDO authenticators stored on these MFA devices, passkeys are invisible and integrate with Face ID, Windows Hello, or other biometric readers offered by device makers. There’s no way to retrieve the cryptographic secrets stored in the authenticators short of physically dismantling the device or subjecting it to a jailbreak or rooting attack.Even if an adversary was able to extract the cryptographic secret, they still would have to supply the fingerprint, facial scan, or—in the absence of biometric capabilities—the PIN that’s associated with the token. What’s more, hardware tokens use FIDO’s Cross-Device Authentication flow, or CTAP, which relies on Bluetooth Low Energy to verify the authenticating device is in close physical proximity to the device trying to log in.Until now, FIDO-based security keys have been used mainly to provide MFA, short for multi-factor authentication, which requires someone to present a separate factor of authentication in addition to the correct password. The additional factors offered by FIDO typically come in the form of something the user has—a smartphone or computer containing the hardware token—and something the user is—a fingerprint, facial scan, or other biometric that never leaves the device.AdvertisementSo far, attacks against FIDO-compliant MFA have been in short supply. An advanced credential phishing campaign that recently breached Twilio and other top-tier security companies, for instance, failed against Cloudflare for one reason: Unlike the other targets, Cloudflare used FIDO-compliant hardware tokens that were immune to the phishing technique the attackers used. The victims who were breached all relied on weaker forms of MFA.But whereas hardware tokens can provide one or more factors of authentication in addition to a password, passkeys rely on no password at all. Instead, passkeys roll multiple authentication factors—typically the phone or laptop and the facial scan or fingerprint of the user—into a single package. Passkeys are managed by the device OS. At the user’s option, they can also be synced through end-to-end encryption with a user’s other devices using a cloud service provided by Apple, Microsoft, Google, or another provider.Passkeys are “discoverable,” meaning an enrolled device can automatically push one through an encrypted tunnel to another enrolled device that’s trying to sign in to one of the user’s site accounts or apps. When signing in, the user authenticates themselves using the same biometric or on-device password or PIN for unlocking their device. This mechanism completely replaces the traditional username and password and provides a much easier user experience.“Users no longer need to enroll each device for each service, which has long been the case for FIDO (and for any public key cryptography),"" said Andrew Shikiar, FIDO's executive director and chief marketing officer. ""By enabling the private key to be securely synced across an OS cloud, the user needs to only enroll once for a service, and then is essentially pre-enrolled for that service on all of their other devices. This brings better usability for the end-user and—very significantly—allows the service provider to start retiring passwords as a means of account recovery and re-enrollment.”Ars Review Editor Ron Amadeo summed things up well last week when he wrote: ""Passkeys just trade WebAuthn cryptographic keys with the website directly. There's no need for a human to tell a password manager to generate, store, and recall a secret—that will all happen automatically, with way better secrets than what the old text box supported, and with uniqueness enforced.""","PasskeysâMicrosoft, Apple, and Googleâs password killerâare"
234,6,6_fintech_companies_crypto_startups,https://techcrunch.com/2022/08/09/selling-sunset-christine-quinn-crypto-credit-score-luxury-real-estate-platform-realopen-realscore/,"For fans of both reality television and web3 (hopefully that group includes more than just this reporter), Christine Quinn’s move to leave the Oppenheim Group and co-found a brokerage with her husband to serve the crypto-rich was quite the bombshell. Now, RealOpen CMO Quinn and CEO Christian Dumontet, who married Quinn in a swan-filled soiree on Selling Sunset season three, have finally shared some long-awaited juicy details about their company’s product roadmap (!!!).The pair sat down for an exclusive interview with TechCrunch to discuss RealOpen’s latest product, RealScore, a crypto credit scoring system for buyers and sellers of luxury real estate. Their brokerage primarily serves high-net-worth clients who want to purchase property using cryptocurrency. The RealScore software they have developed serves as a tool for both parties in a transaction to assess the strength of an offer, taking into account the mix of tokens used in the offer and attempting to predict their volatility, according to Dumontet, who previously founded and bootstrapped Foodler and sold it to Grubhub for over $50 million in 2017.Before we got into how RealScore works, Quinn explained why a client would want to buy a house using crypto instead of cash in the first place. Crypto “whales” who hold a significant portion of their wealth in digital currency prefer to move fast in business, and buying property is no exception, Quinn said.“I’m finding a lot of clients who actually want to close quickly because of the volatility [of crypto],” she said. “Because of that, we can choose the day, down to the minute, that they want to do so. We live in an instant gratification generation where people want things quickly. With a traditional home mortgage, you’re looking at a four-week close, or it could be longer sometimes [due to] inspection contingencies.”Some of Quinn’s clients value speed so highly that they are okay with just seeing a property over FaceTime before agreeing to move forward with a deal, she said.“I’ve seen people who have said, I’m good with the contingencies, I don’t care if there are termites or if I have to fix the chimney,” Quinn added.Buyers pay RealOpen’s sellers in cash, which means they have to convert their crypto into dollars before closing a transaction, Dumontet explained. But they usually can’t wait until the last minute to liquidate their funds, because sellers need to be able to evaluate the buyer’s ability to pay before agreeing to the transaction, which can be complicated if the buyer holds crypto at the time of making the offer. For the buyer, converting crypto to cash triggers a taxable event that “can’t be undone,” Dumontet explained.“The process of shopping for a home can take months if they’re looking at various properties. If it’s one of the more particular buyers, they’ve lost participation in the crypto market. They may have an excess of cash because they want to buy a property at price X, but they want to have a buffer above that because they’re not quite sure what the property prices are and what the seller will accept,” he said.That’s where RealScore comes in — it is essentially the software engine that powers RealOpen’s brokerage. By using RealScore, buyers can defer converting their crypto into cash until the very instant the transaction closes without having to explain to a seller what the value of their offer is beforehand, according to Dumontet.The platform allows buyers and sellers to see algorithmic predictions about the likelihood of price movement across the various crypto assets involved in a transaction based on historical data, Dumontet explained. The RealScore for an offer is calculated using correlation coefficients that illustrate how different types of digital currencies are related, helping a buyer decide if they should diversify the mix of assets that make up their offer and when to formally extend one, he said.Ultimately, this allows both parties to come to a shared understanding of an offer, which often results in a quicker transaction process, Quinn added, noting that RealOpen can complete “Know Your Customer” (KYC) diligence on a customer in a matter of minutes and close a transaction in a single day. RealScore’s analytics can also be a helpful tool for Quinn and Dumontet to use to motivate buyers who were already considering purchasing a specific property to take the leap.“It gives us an opportunity to go to our clients and say, hey, actually, right now is a really good time if you want to do that transaction. So it helps us as well so that we have a universal language between our clients,” Quinn said.Quinn said the RealOpen platform has over $150 million worth of exclusive listings, primarily in Miami, which has emerged as a hub for cryptocurrency. Outside of Miami, Quinn said, RealOpen’s crypto focus has attracted a strong pipeline of listings from sellers all over the world.The brokerage contracts with a network of ~50 local agents licensed in each jurisdiction, Dumontet added. The company employs three full-time engineers and has a four-person management team, including the co-founders, he said.While RealOpen’s main focus is on buyers in the crypto world, the RealScore platform can also help both parties assess all-cash offers. Since the company officially launched in April 2022, it has closed just under a dozen transactions for its customers, Quinn said, though she did not share details on how many of those offers included crypto.RealOpen says it can facilitate transactions in all cryptocurrencies, though Dumontet noted that bitcoin, ethereum and stablecoins are the most popular choices among buyers. Both Quinn and Dumontet, who was an early adopter of crypto in 2013 when Foodler began accepting payments in bitcoin, said they are confident that the asset class is here to stay.Next up on RealOpen’s product roadmap are features tailored toward people looking to buy investment properties using crypto, not just homes they themselves will occupy, Dumontet said.“If you look at the traditional [real estate platforms such as] Zillow and Redfin, the user interface is all the same. You input your parameters, price range, geographies, bedrooms, bathrooms, house type, and then you see your list and go from there … But when you think about asset diversification, real estate is an excellent investment. It’s got leverage, and interest rate arbitrage, which you don’t see on other types of assets,” he added.Quinn, meanwhile, hinted at even bigger long-term ambitions for RealOpen, saying she hopes to eventually diversify beyond real estate and bridge digital assets with all sorts of physical goods.“One of my girlfriends is actually a diamond dealer. She’s the one who designed my ring, and she said, ‘a lot of my clients have tons of cryptocurrency, and they would love to buy diamonds for their wife, how can I do this?’ And I said, well, the process is exactly the same as it is for people who want to buy cars and stuff like that,” Quinn said.All eyes will certainly be on Quinn as she continues building the venture that pulled her away from the television series and brokerage that brought her massive fame, but in her usual style, she seems confident that she is up for the challenge.“I think the Oppenheim Group will not exist in seven years. It can only scale for so long. Sotheby’s, Berkshire Hathaway, I think those will be around forever, but I think boutique brokerages will be a thing of the past. So for me, it was just all about getting on the forefront,” Quinn said of starting RealOpen.",âSelling Sunsetâ star Christine Quinnâs brokerage debuts new crypto credit scoring platform
224,6,6_fintech_companies_crypto_startups,https://techcrunch.com/2022/07/31/how-fintech-startups-are-navigating-the-extension-round-rush/,"As the fintech venture market goes, so goes the venture market itself. Why? Because fintech investment has historically made up around one-fifth of every venture dollar invested — at least in recent years. And after both fintech investing and venture capital itself went a bit bonkers last year, both are dealing with a new, more conservative reality.For fintech startups, the downturn is real, and many upstart companies — we learned during our recent fintech investor survey — are looking to avoid de-novo rounds that include a new valuation (no one wants to raise a down round!). Therefore, extension rounds are an attractive option for many founders.But as TechCrunch has reported, while extension rounds are popular even beyond fintech today, there are often more startups hunting for the round type than there are checks. So, to better understand the market for fintech extension rounds today, we have one more set of answers from a group of fintech venture investors we surveyed. Here’s the question we posed:How popular are extension rounds proving? Are you seeing more companies opt to raise extensions rather than new rounds compared to, say, 2021 and 2020?Eight investors answered: Paul Stamas of General Atlantic, Alda Leu Dennis of Initialized Capital, Michael Gilroy of Coatue, Justin Overdorff of Lightspeed Venture Partners, Addie Lerner of Avid Ventures, David Jegen of F-Prime Capital, Nik Milanović of The Fintech Fund, Jay Ganatra of Infinity Ventures. (Their answers have been lightly edited for clarity.)Michael Gilroy, general partner and co-head of fintech, Coatue",How fintech startups are navigating the extension-round rush
231,6,6_fintech_companies_crypto_startups,https://techcrunch.com/2022/08/08/bitmain-matrixport-john-ge/,"The collapse of Three Arrow Capital and the counterparties wrapped in the crypto hedge fund’s troubles have drawn questions about the soundness of the heady digital asset investment space. For the industry’s survivors, watching their rivals fall to pieces overnight has been an alarming experience.To understand where the industry might be going after the market turmoil, we spoke with John Ge, chief executive officer at Matrixport, a Singapore-based digital asset manager with over $10 billion in assets under management and custody.Ge was formerly the head of investment and financing as well as a founding partner at Bitmain, the world’s biggest maker of Bitcoin mining machines. Together with Bitmain’s co-founder and former CEO Jihan Wu, Ge co-founded Matrixport in 2018.Three Arrow Capital, known as 3AC in the crypto community, was one of the world’s largest crypto hedge funds before its fall from grace. Its success was predicated on a risky strategy: it borrowed aggressively from crypto lenders and in turn invested that money in other crypto projects.When cryptocurrency prices began to plummet earlier this year, the firm, as well as other similar outfits that bet on rising crypto prices, failed to repay their creditors and plunged into liquidation. The crypto market is down by $1.8 trillion since its peak in November, led by the slide in Bitcoin and Ethereum prices.The recent market crash is “inevitable”, Ge says in an interview with TechCrunch. “The core issue is that we saw players whose business model is like a black box. They borrow money from investors without giving transparency over how the money will be used.”The other problem is that these crypto managers are acting both as the player and referee, Ge contends. “Many of them are providing both asset management and proprietary trading. An asset manager should not be doing proprietary trading, and if it does, it needs to follow stringent leverage requirements.”“Even the most conservative investment strategy has risks and may result in losses, but the principle is to be transparent with your customers, not fraudulent, deceptive, or misleading,” the founder says.Matrixport, which serves individuals as well as over 500 institutions across Asia, Europe and North America, was exposed to 3AC and has lodged a claim alongside other creditors. But Ge assures that the firm’s exposure is “relatively small” when compared to the exposure other industry players faced and is considered “minor” when compared relative to Matrixport’s equity.As to how to restore investor confidence in the crypto sphere, Ge believes regulators are on the right track to bring more oversight over consumer-facing crypto products and protection for retail investors, as is the case in Singapore.But it’s “unrealistic” to have regulators design risk control models for institution-focused asset managers. “The pace of regulations tends to fall behind that of industry development.”Ge thinks investors have “lost a certain level of confidence” in the crypto market and the industry will take time to recover. On the other hand, he thinks competition has waned for survivors like Matrixport because “many of the other players are gone.”Matrixport told Bloomberg last year that it planned to go public in three to five years and Ge said that plan “hasn’t changed.” It’s too early to say which market the company is floating its shares but the U.S. is a “likely” option given investors there are more “welcoming of crypto innovation.”",Bitmain co-founder welcomes crypto regulation to restore market confidence
230,6,6_fintech_companies_crypto_startups,https://techcrunch.com/2022/08/08/5-reasons-why-ukraines-fintech-sector-is-growing-despite-war/,"Ukrainians have often pioneered market-leading companies and built products that positively impact society, especially in the fintech sector.Despite the hurdles of war, the Ukrainian fintech community is working to create better infrastructure and regulation for the country, which can attract valuable companies and institutional investors from different backgrounds.It’s a valuable marketI’m sure many investors think the country’s IT sector is a risky investment right now. But it’s still business as usual at fintech companies here. They have proven their resilience even in wartime conditions, and impressively, 90% of Ukrainian tech startups are still hiring.This March, Ukraine’s President Volodymyr Zelenskyy signed a bill to establish a regulatory framework for cryptocurrency in the country. While the bill doesn’t let you use digital assets as a form of payment, it seeks to create proper conditions to establish a strong cryptocurrency market.Cryptocurrency exchanges can now claim a license to operate legally in the country. Banks will be able to open accounts for crypto companies, which can opt for different licenses depending on what they do.The Ukrainian IT sector is still growingIn the first five months of 2022, Ukraine’s IT sector generated roughly $3.2 billion from exports — 27% more than the same period in 2021, and accounting for almost half of the country’s total volume of export services.One of the main goals of the Ukrainian government is to increase the IT sector’s share of the country’s GDP from the current 4% to 10% by 2024.The government’s backing and the strong growth together make for a strong signal of how the IT sector is ripe for investments.Global finance and tech firms are supporting UkraineEarlier this month, the Ukrainian Ministry of Digital Transformation presented Digital4Freedom — a global initiative that is the primary source of charitable donations for tech companies to support Ukraine.Digital4Freedom is part of the global UNITED24 effort and allows anyone across the globe to make monetary contributions to the restoration of the country’s economy.The program consists of nine projects presented to 40 companies, of which the vast majority have agreed to help with monetary contributions or technology solutions.Amazon, for example, will reportedly provide over $100 million in cloud hosting services for Ukrainian state registers and is planning to develop solutions for deploying artificial intelligence in courts.Crypto exchange Binance will become an official partner of the Ministry of Digital Transformation of Ukraine to offer educational projects in the IT, web3 and finance fields.Meta, with the support of the Ministry of Digital Transformation, recently launched a $1.5 million assistance program for the recovery of the Ukrainian economy, helping small- and medium-sized businesses with a specialized training centre.Startups are an integral part of the Ukrainian economyUkrainian businesses expect a reduction in the production of goods and services due to the war. What’s more, company executives predict the next 12 months will bring inflation and devaluation of the hryvnia.Ukrainian tech startups are dedicating their efforts to elevating the industry to new heights. They are set to become the foundational pillars for a new layer of technology companies that will add significant value to the economy.Fintech is growing fast despite setbacksThis year, Ukrainian lawmakers introduced the Law of Ukraine on Payment Services (LPS), which will provide a better regulatory environment for the fintech sector, including for payment services.Additionally, the Ukrainian Association of Fintech and Innovation Companies (UAFIC) became the first non-EU member to join the European Digital Finance Association (EDFA). Both organizations are collaborating to strengthen the fintech landscape in Ukraine.I believe Ukraine’s resilience in the face of Russian aggression, and the tremendous growth of the fintech landscape despite the crisis has proven that investors should not hesitate to invest in the fast-growing industry.Despite the difficult situation in the country, the industry continues to develop. While attending numerous conferences and fintech events, I meet investors from all over the world interested in companies from Ukraine. All this tells us that the prospects for fintech in Ukraine look good, and that now is an excellent time to invest in it.",5 reasons why Ukraineâs fintech sector is growing despite war
183,6,6_fintech_companies_crypto_startups,https://techcrunch.com/2020/01/29/all-eyes-are-on-the-next-liquidity-event-when-it-comes-to-space-startups/,"At the FAA’s 23rd Annual Commercial Space Transportation Conference in Washington, DC on Wednesday, a panel dedicated to the topic of trends in VC around space startups touched on public versus private funding, the right kinds of space companies that should even be considering venture funding and, perhaps most notably, the big L: Liquidity.Moderator Tess Hatch, vice president at Bessemer Venture Partners, addressed the topic in response to an audience question that noted while we’ve heard a lot about how much money will flow into space-related startups from the VC community, we haven’t actually seen much in the way of liquidity events that prove out the validity of these investments.“In 2008, a company called Skybox was created and a handful of years later Google acquired the company for $500 million,” Hatch said. “Every venture capitalist’s ears perked up and they thought ‘Hey, that’s pretty good ROI in a short amount of time — maybe the space thing is an investable area,’ and then a ton of venture capital investments flooded into space startups, and all of these venture capitalists made one, or maybe two investments in the area. Since then, there have not been many — if any — liquidity events: Perhaps Virgin Galactic going public via the SPAC (special purpose vehicle) on the New York Stock Exchange late last year would be the second. So we’re still waiting; we’re still waiting for those exits, we are still waiting for companies to pave the path for the 400+ startups in the ecosystem to return our investment.”Hatch added that she’s looking at a number of companies who have the potential to break this somewhat prolonged exit drought in 2020, including five that are either quite mature in terms of their development, naming SpaceX, Rocket Lab, Planet and Spire as all likely candidates to have some kind of liquidity event in 2020, with the mostly likely being an IPO.Space as an industry was described to me recently as a “maturing” startup market by Space Angels CEO Chad Anderson, by virtue of the distribution of activity in terms of the overall investment rounds in the sector. There is indeed a lot of activity with early-stage companies and seed rounds, but the fact remains that there hasn’t been much in the way of exits, and it’s also worth pointing out that corporate VCs haven’t been as acquisitive in space as some of their consumer and enterprise technology counterparts.The panel touched on a lot more apart from liquidity, which actually only came up toward the end of the discussion. Panelists included Astranis CEO and co-founder John Gedmark; Capella Space CEO and founder Payam Banazadeh and Rocket Lab VP of Global Commercial Launch Services Shane Fleming. Both Gedmark and Banazadeh addressed aspects of the risks and benefits of seeking VC as a space technology company.“Not every space business is a venture-backable business,” said Banazadeh earlier in the conversation. “But there are a lot of space businesses that are specifically going after raising venture money, and that’s dangerous for everyone — because at the end of the day, venture is looking at high risk, high return. The ‘high return’ comes from being able to get substantial amounts of revenue in a market that’s bigenough for those revenues to be coming from. But if your idea is to go build, maybe, some very specific part in a satellite, then you have to make the case of why you’ll be able to make those returns for the investors, and in a lot of cases, that’s just not possible.”Banazadeh also concedes that doing any kind of space technology development is expensive, and the money has to come from somewhere. Gedmark talked about one popular source, government funding and grants, and why that often isn’t as obviously a positive thing for startups as it might seem.“Small government grants can be great, and obviously a fantastic source of non-dilutive capital,” Gedmark said. “But there is a little bit of a trick there, or something to be aware of: I think people are often surprised how much time is spent in the early days of a startup refining the exact idea and the product, and if you’re not certain that you have that product market fit […] then, the government grant can be extremely dangerous, because they will fund you to do something that is sort of similar to what to what you’re doing, but it really prevents you changing your approach later; you’re going to end up spending time executing on the specific project of the program manager on the government side and you’re executing on what they want.”VC funds, on the other hand, come with the built-in expectation that you’re going to refine and potentially even change direction altogether, Gedmark says. Depending on the terms of the public funding you’re seeking, that flexibility may not be part of the arrangement, which ultimately could be more important than a bit of equity dilution.",All eyes are on the next liquidity event when it comes to space startups
225,6,6_fintech_companies_crypto_startups,https://techcrunch.com/2022/08/01/crypto-fraud-scam-forsage-ponzi-pyramid-scheme-sec-regulators-crackdown/,"U.S. regulators are seizing their moment during this ongoing crypto bear market to crack down on bad actors in the space as many investors are already souring on the asset class.The U.S. Securities and Exchange Commission charged 11 people today in connection with Forsage, a crypto project that raised over $300 million from “millions of retail investors worldwide,” the agency announced today. The individuals charged include the project’s four founders — Vladimir Okhotnikov, Jane Doe aka Lola Ferrari, Mikhail Sergeev, and Sergey Maslakov — who were last sighted in Russia, Georgia and Indonesia. Several members of the “Crypto Crusaders,” a group that promoted the scheme in at least five different U.S. states were also charged, according to the announcement.Forsage was launched in January 2020 as a website that allowed retail investors to transact on the Ethereum, Tron and Binance blockchains, the SEC complaint says. In June 2020, Forsage was the most popular decentralized application on Ethereum and consumed so much bandwidth on the chain that it caused gas fees to spike. At the peak of its popularity in July 2020, over $20 million worth of ETH was sent to the platform in a single day, Dune Analytics data shows.According to the SEC, the project has operated as a pyramid scheme for more than two years and used assets from new investors to pay off old ones, typical of a Ponzi scheme structure. Operating a pyramid scheme, a fundamentally unsustainable business model wherein participants recruit others to buy in with the promise of quick returns, is illegal in the U.S.“Fraudsters cannot circumvent the federal securities laws by focusing their schemes on smart contracts and blockchains,” wrote Carolyn Welshhans, acting chief of the SEC’s Crypto Assets and Cyber Unit, a division of the SEC that rebranded to include crypto in its title and embarked on a hiring spree in May this year.This isn’t the first time Forsage has been in regulators’ crosshairs. The Securities and Exchange Commission of the Philippines sent the company a cease-and-desist order in 2020 for operating as a fraud and in 2021, the Montana Commissioner of Securities and Insurance did the same. Despite these warnings, the defendants kept promoting the scheme and denied that they were operating a pyramid scheme on various social media platforms, the SEC says.Besides the founders, Cheri Beth Bowen, Ronald R. Deering, Samuel D. Ellis, Mark F. Hamlin, Carlos L. Martinez, Alisha R. Shepperd and Sarah L. Theissen were all also charged with violating federal securities laws in connection with Forsage, according to the SEC complaint. Ellis and Theissen, the agency says, have agreed to settle the charges.The charges come at a time of heightened regulatory scrutiny over the digital asset space, particularly from the SEC itself. Coinbase has been locked in a battle with the agency over its sale of cryptocurrencies listed on its platform that it insists are commodities, not securities.Meanwhile, U.S. Senators Kirsten Gillibrand and Cynthia Lummis are seeking to build consensus in Congress for their bill that would classify most cryptocurrencies as commodities, bringing the industry largely under the jurisdiction of the U.S. Commodity Futures Trading Commission rather than leaving it open to the stricter SEC.",SEC spears âCrypto Crusadersâ over alleged pyramid scheme
168,6,6_fintech_companies_crypto_startups,https://techcrunch.com/2019/05/02/unshackled-ventures-has-20m-to-invest-exclusively-in-immigrant-founders/,"Unshackled Ventures isn’t like other venture capital funds.The firm invests in immigrant founders and helps them secure visas so they can ditch their corporate job and launch the startup of their dreams. Today, Unshackled is announcing its sophomore fund of $20 million, topping its debut effort by $15.5 million.“The point is to take the burden off of founders because they are not immigration experts, they are experts at building satellites or extracting protein from plants,” Unshackled founding partner Nitin Pachisia told TechCrunch. “These are people that if you go to a workspace, you’ll see them show up on nights and weekends because they want to build something but they can’t.”Immigrants looking to start their own businesses face a huge barrier. Take Jyoti Bansal for example. He famously waited seven years before launching AppDynamics, a business that later sold to Cisco for $3.7 billion days before its initial public offering. Why? Because as an Indian immigrant with H-1B visa status, he could work for startups but wasn’t legally allowed to start his own. It wasn’t until receiving an employment authorization document (EAD), a part of the green card process, that Bansal could finally found AppDynamics. If Bansal had the opportunity to pitch to Unshackled, which provides bespoke immigration solutions to each founder, he could have launched AppDynamics years prior.Immigrant founders, according to a 2018 study by the National Foundation for American Policy, are responsible for 55% of U.S. billion-dollar companies, or “unicorns,” as they are known. Uber, SpaceX, WeWork, Palantir Technologies, Stripe, Slack, Moderna Therapeutics, Robinhood, Instacart, Houzz, Credit Karma, Tanium, Zoox and CrowdStrike all count at least one immigrant co-founder.“The difference between success and failures is oftentimes who you know and when,” Unshackled founding partner Manan Mehta told TechCrunch. “We can bring those resources at just 1/200th the size of Andreessen Horowitz to immigrants at day zero.”“We’re creating the best place for immigrants to start their companies,” he added. “And guess what? We’re keeping American innovation in America.”The firm was founded by Mehta, the son of immigrants, and Pachisia, an Indian immigrant, in 2015. Since then, the duo have written pre-seed checks to 31 companies with a 100% success rate in procuring visas to keep talent working in the U.S. Startups in its portfolio include the very recent Y Combinator graduate Career Karma, Starsky Robotics, Plutoshift, Togg, Hype, Lily AI and more.“I didn’t think it was possible to start a company on a visa in the U.S., let alone scale one to hit the next major milestone so quickly,” Plutoshift founder Prateek Joshi said in a statement. “That all changed when we met the Unshackled team.”Mehta and Pachisia say its startups have gone on to raise $54 million in follow-on investments from top investors like First Round Capital, NEA and Shasta.In addition to supporting companies based in Silicon Valley, the investors search far and wide for aspiring immigrant founders, as well as respond to every single cold email they receive. Recently, they joined the Rise of the Rest tour, a trip hosted by Steve Case and JD Vance that showcases startups in underrepresented geographies, and they make frequent visits to college campuses across the U.S.Unshackled’s limited partners include Bloomberg Beta, Jerry Yang’s AME Cloud Ventures and Emerson Collective.“I think the name represents the feeling that you’re a little bit shackled to a framework or a policy that doesn’t necessarily encourage entrepreneurship,” Mehta said. “When if you take a step back, immigrants are probably more entrepreneurial than native-born people.”",Unshackled Ventures has $20M to invest exclusively in immigrant founders
248,6,6_fintech_companies_crypto_startups,https://techcrunch.com/2022/09/02/an-action-plan-for-founders-fundraising-in-fintechs-choppy-waters/,"This past year has seen a wholesale shift in how the market feels about fintech. A year ago, nearly every investor had a fintech thesis, companies were racing to go public and investors at nearly every stage of the market were fighting to jam money into the hands of founders.That’s not true any longer. The collapse in valuations on the public market has been extreme. A significant number of the biggest fintech companies to go public in the last couple of years are now worth less than the money they’d raised. And that drop in confidence has now permeated to all stages of the market.Understandably, many early founders are unprepared to contemplate that the valuation of their idea — which will likely take about a decade to come to fruition — is now worth 75% less than it would have been six months ago.But the long-term outlook of the sector remains unchanged for most investors and founders. The good news is, we’re still seeing deals getting done. The founders who are succeeding in this environment have adapted to the new reality quickly.Money tends to attract money, so find ways to get the ball rolling.Here are four strategies that the best early-stage fintech founders are now employing to fundraise:Recognize that bid/ask spreads are going to be wideIt’s not you; it’s the market. The best founders recognize that the goal is to close a round, not to maximize the price or minimize dilution.Minimizing dilution is good but not at the cost of losing a deal.Plan for a long fundraiseWhile quick deals with proven founders and exceptional teams still happen, the average fundraising round, including diligence and paperwork, can now take up to four to five months. The days of the Notion-doc-over-a-weekend are firmly in the past.",An action plan for founders fundraising in fintechâs choppy waters
245,6,6_fintech_companies_crypto_startups,https://techcrunch.com/2022/08/29/meet-the-judges-for-the-minneapolis-minn-techcrunch-live-pitch-off/,"TechCrunch Live is hosting a special, extended event focused on the great city of Minneapolis, Minnesota on September 7. I hope you can join us. We have an agenda packed with insiders who can speak to the growing startup ecosystem in the Twin Cities. But for the pitch-off, we’ve recruited two outsiders to judge the local startups: Mahati Sridhar, vice president, Rise of the Rest Seed Fund and Sarah Hinkfuss, partner, Bain Capital Ventures.Both Mahati and Sarah offer considerable investment and startup experience. We’re thrilled to have their participation.To help highlight what the city has to offer, we’re enlisting the help of local startups! Like past City Spotlights, this one will feature a pitch-off with local Minneapolis startups pitching to VCs. The winner gets fast-tracked into Startup Battlefield 200, which includes free exhibition space at TechCrunch Disrupt 2022. Applications are closed, but everyone can register for the event here.Mahati Sridhar, vice president, Rise of the Rest Seed FundMahati is a vice president on the investment team at Revolution’s Rise of the Rest Seed Fund. She joined the firm in 2021 and focuses on sourcing, due diligence and supporting existing portfolio companies. Prior to joining Revolution, Mahati was an associate at Bull City Venture Partners, a Durham-based venture capital fund where she worked on seed and Series A software and internet investments in the Southeast and Mid-Atlantic. Mahati is also a Venture for America Fellow and served her fellowship in Charlotte where she helped launch CFV Ventures, an early-stage fintech-focused venture fund. She started her career in Investment Banking at SunTrust Robinson Humphrey where she worked on the healthcare coverage team. Mahati received a B.S. in Business Administration from UNC-Chapel Hill and an MBA from Columbia Business School. Mahati originally hails from Raleigh but currently calls New York City home.Sarah Hinkfuss, partner, Bain Capital VenturesSarah Hinkfuss works with growth-stage founders across both application software and fintech. She is particularly interested in backing founders who have personal experience in the market they are creating. Ms. Hinkfuss joined Bain Capital in 2020 as a vice president on the Tech Opportunities team. Prior to joining Bain Capital, Ms. Hinkfuss worked as an associate in KKR’s Growth Equity group in San Francisco. Prior to that she was a senior vice president at Applied Predictive Technologies, an enterprise SaaS company acquired by Mastercard in 2015. Ms. Hinkfuss received an MBA from Stanford Graduate School of Business, where she was an Arjay Miller Scholar and Siebel Scholar. She graduated cum laude with a BA in Economics and Environmental Science and Public Policy from Harvard College, where she was a Hoopes Prize recipient and Weatherhead Research Fellow.AgendaTechCrunch Live in Minneapolis, MinnesotaRaising capital outside of the coasts with Anna Mason (Rise of the Rest Seed Fund) + Andrew Leone (Dispatch)Andrew Leone’s Dispatch provides businesses with an on-demand courier delivery service. Headquartered in the greater Minneapolis, Minnesota area, the startup is quickly becoming a shining star in the area’s exploding startup ecosystem. It’s the type of startup that captures the attention of local investors, but outsiders as well including Anna Mason, managing partner at Revolution’s Rise of the Rest fund.Who’s writing checks in MSP with Mary Grove (Bread & Butter Fund) and Justin Kaufenberg (Rally Ventures)A panel on the growth opportunities for Minnesota from the perspective of VC funds – what’s needed in the market, what are they funding right, where startups should look for funding.Building a fintech company with Atif Siddiqi (Branch) and Ryan Broshar (Matchstick Ventures)Minneapolis has a growing number of fintech companies, and Branch is among the best positioned. Hear from its CEO and founder Atif Siddiqi and one of the company’s early investors, Ryan Broshar, managing director and partner at Matchstick Ventures.Pitch-offJudges: Mahati Sridhar and Sarah Hinkfuss",Meet the judges for the Minneapolis TechCrunch Live pitch-off
217,6,6_fintech_companies_crypto_startups,https://techcrunch.com/2022/07/14/systemiq-vc-70-million-fund-2/,"Systemiq Capital, a backer of early-stage climate tech startups, says it has secured $70 million to kick off its second fund.The London-based VC aims to raise as much as $130 million more; that would mark quite a step up for the firm, which says it has funneled $30 million into 19 startups since 2018.As far as putting that money to use, Systemiq says it is out to fund founders who are focused on making large industries and cities “more efficient and sustainable.” In practice, it’ll fund key areas like regenerative land use, oceans, transportation and the circular economy.Systemiq’s past deals include climate data company Jupiter, shipping data firm Nautilus Labs (whose co-founder later launched Bedrock) and ESG-focused investing startup OpenInvest. Last year, OpenInvest sold to J.P. Morgan, the world’s top funder of fossil fuels.Systemiq was co-founded by McKinsey veterans Jeremy Oppenheim and Martin Stuchtey. The consulting giant, which pulls in an estimated $10 billion a year in revenue, also has a lengthy history of work with many of the world’s top polluters.Systemiq partner and former Goldman executive Irena Spazzapan will steer the second fund, along with Oppenheim and former Unilever CEO Paul Polman, the firm said.",Systemiq secures $70 million to fund early-stage climate tech founders
252,6,6_fintech_companies_crypto_startups,https://techcrunch.com/2022/09/12/is-the-glass-half-empty-or-half-full-in-the-seed-market/,"New data from Carta indicates that valuations for very early-stage startups are holding up better than we might have expected in the current slowdown.But while it appears that the price at which investors are willing to put capital into various startup sectors is at times becoming more expensive, the pace at which deals are happening is slowing enough that the changing value of seed deals actually makes sense.Call it the glass half-full/glass half-empty seed market. If you are bullish, there’s good news aplenty. And if you are bearish, well, we have enough data to make that argument as well.",Is the glass half-empty or half-full in the seed market?
390,6,6_fintech_companies_crypto_startups,https://www.independent.co.uk/tech/ethereum-merge-crypto-energy-environment-b2167637.html,"For free real time breaking news alerts sent straight to your inbox sign up to our breaking news emails Sign up to our free breaking news emails Please enter a valid email address Please enter a valid email address SIGN UP I would like to be emailed about offers, events and updates from The Independent. Read our privacy notice Thanks for signing up to theBreaking News email {{ #verifyErrors }}{{ message }}{{ /verifyErrors }}{{ ^verifyErrors }}Something went wrong. Please try again later{{ /verifyErrors }}One of the most highly-anticipated moments in the history of crypto has finally happened after years of build up.At 7.45am BST on Thursday, Ethereum completed what has been dubbed ‘The Merge’. In doing so, the energy consumption of the world’s second largest cryptocurrency dropped by more than 99 per cent in an instant.“After bitcoin’s whitepaper release, Ethereum’s merge is the most consequential event in crypto history,” tweeted Erik Voorhees, founder of the popular crypto platform ShapeShift.The Ethereum Merge involved a complex network transition from proof-of-work – a technoogy pioneered by bitcoin – to proof-of-stake, meaning vast computing power is no longer needed to support transactions and the minting of new units of the crptocurrency.Before the Merge, Ethereum’s electricity requirements were equivalent to that of a small country, leading to environmental concerns about the partial reliance on fossil fuels to support its network.Crypto advocates now hope that Ethereum will face less regulatory challenges and broader adoption as scrutiny shifts away from its environmental impact.“The Merge is a significant step forward bringing Ethereum into the mainstream, and it marks an evolution of blockchain technology,” Gilbert Verdian, CEO of blockchain firm Quant.“A smaller carbon footprint has long been an industry goalpost... Now that this is happening, we will see more institutional adoption where Financial Services turn to decentralised infrastructure. It can offer safe and secure transaction processing at a fraction of the cost, when compared to the enormous expense and burden of today’s infrastructure.”In the hours leading up to the switch, the price of Ethereum (ETH) remained steady as major exchanges temporarily suspended deposits and withdrawals of the cryptocurrency.Other leading cryptocurrencies like bitcoin (BTC) and Cardano (ADA) also saw little price movement, with the overall crypto market shifting by less than 1 per cent.View moreIt followed a major price dip on Wednesday, which wiped nearly $100 billion of value from the market and pushed it below $1 trillion amid concerns that there may be issues with the Ethereum Merge.No major issues appeared to transpire, though developers continue to assess whether all aspects of the event occurred as expected.Ethereum co-founder Vitalik Buterin said earlier this year that the Merge was only part one of a multi-part plan to transform the cryptocurrency, with the next steps referred to as “the Surge, the Verge, the Purge and the Splurge”. All are aimed at making Ethereum’s new proof-of-stake blockchain more scalable and secure.",âHistoricâ moment for crypto as Ethereum Merge finally completed
256,6,6_fintech_companies_crypto_startups,https://techcrunch.com/2022/09/19/fintech-app-portabl-raises-2-5m-to-help-consumers-securely-store-financial-data/,"Fintech Portabl announced the closing of a $2.5 million seed round today led by Harlem Capital Partners. Portabl, founded by Nate Soffio and Alex Yenkalov, also launches its beta today for institutional use.It provides identity management and protection for financial services, banking and consumer apps, but Soffio calls it a financial digital passport, which helps with user identification, making the task less cumbersome for both consumers and financial services. He said the company’s goal is to wean people from passwords, helping consumers obtain more ownership over their economic data by granting control over who can access it.The app works like this: Portabl stores the information used to access existing financial apps. Every time an app is opened, a Portabl log-in will appear and, within two clicks, simply enable users to log in.“We recognize the fact that if you’re a consumer, having a say over who has access to your identity and financial life has been historically confusing and cumbersome — at worst, adversarial and exclusionary,” Soffio told TechCrunch. “We believe that by enabling consumers to own their data, securely hold it, and share it for access or updates, that’s the right way to make good on a lot of the promises you hear about in open banking.”Yenkalov noted the emergence of decentralized identifiers, verifiable credentials and zero-knowledge proofs, saying that the industry is closer than ever to enabling financial organizations to benefit from consumers owning and sharing their own data.“In a way, by putting users in charge of their authentic data, Portabl is turning them into secure APIs of themselves,” he told TechCrunch. “This has enormous potential to transform consumer-provider interactions in the financial world.”Calling the fundraising journey “this sort of weird hopscotch situation,” Soffio said he started building the app early last year while attending the Wharton School of Business. In his spare time, he pitched competitions and accelerators. Eventually, he met Yenkalov, who helped him continue shaping the hypothesis of the app. Thanks to a series of warm introductions, the duo managed to start scraping together money and met with investors.Soffio said Portabl chose Harlem Capital to lead the round after a call he’ll never forget: Yenkalov, a Ukrainian citizen, was trapped in the country as the war with Russia broke out in the middle of a fundraising call with the firm. Soffio remembers air raid sirens going off and said for the first few minutes of the call, the conversation was not about business but instead about finding a way to support Yenkalov’s escape from the country.“It’s a VC firm, and their mission is to make great investments,” Soffio said. “They put all that aside and said, ‘Hey, this is a global crisis going on — what else can we do for you?’ For me, that was something to remember forever, frankly.”Harlem Capital Partner Brandon Bryant said what initially drew the fund to Portabl was the idea that the verification of identity in fintech is still unsolved. “Their platform lets you as a consumer create your own identity credentials one time and bring them with you to every fintech application,” Bryant told TechCrunch. “We think this will be a big unlock for the industry.”Carl Vogel, a partner at Sixth Man Ventures who also invested in Portabl’s seed round, expressed similar sentiments. He said the app “realized that creating user-owned financial identities can create enormous value for not only users but can also create a meaningful product and operational improvements for financial institutions.”“What also excited us was that Portabl’s solution spans both traditional financial institutions and web3-native companies looking to securely onboard and maintain users,” Vogel continued. “We could not be more excited to partner with Portabl on their journey.”Soffio said the company plans to use the money to help expand the team and accelerate its growth. It’s also working on its SOC2 and ISO/IEC 27001 security certifications (the former is a voluntary standard for managing consumer data while the latter is an international rubric for managing information) and is leaning into blockchain to grant consumers “bulletproof records of their data.”As a child, Soffio learned the importance of verifiable identity information. He was born in Colombia, adopted and raised in Stamford, Connecticut. He described his neighborhood as primarily immigrant and said there was always an underlying fear and anxiousness about having proper documentation.“I grew up witnessing various types of financial exclusion due to poverty, immigration, cash reliance and a lot of anxiety around documentation and forming relationships with brick-and-mortar banks,” Soffio said. “Those are the things that keep otherwise good people locked out of using basic services.”He studied anthropology during his undergraduate years at Yale and always planned to attend law school. His first job out of university was as a paralegal, where he was tasked with building databases, fostering his love for information gathering. He then spent a decade working for various software and fintech startups, holding roles focused on product management fraud and anti-money laundering.He soon realized the link between data management, identity issues and access to essential financial services. From there, he dropped everything, scrapped plans for law school and went on the journey of learning about identity, naturally leading him to Wharton.“Historically, lots of people have been left out of the system not because they’re bad, but because they’re hard to make sense of,” he said. “We want to make that a thing of the past by standardizing how both traditional and alternative data can be owned, shared and trusted.”This article was updated to clarify what the app Portabl does.",Fintech app Portabl raises $2.5M to help consumers securely store financial data
247,6,6_fintech_companies_crypto_startups,https://techcrunch.com/2022/09/01/to-reach-fintechs-next-level-infrastructure-providers-must-address-these-pain-points/,"We’ve all seen the headlines: Fintech is struggling. Since last year, valuations are down 70%-80%, deal activity is down 67% and layoffs have plagued many former industry favorites.But fintech is resilient. Innovation continues to drive new developments in lending, payments, crypto and, in particular, infrastructure, showing that the industry still has lots of room for growth. And even though investment activity decreased this year, it still remains well above where it was in 2019 and 2020.Infrastructure providers have a unique opportunity to be a bright spot amidst all the doom and gloom. Over the years, infrastructure has enabled fintech companies and non-financial services companies alike to seamlessly integrate financial products into their platforms.However, as the market grew crowded, infrastructure providers have started competing over who can develop the least expensive product and sign the most fintech companies. The infrastructure market is overlooking a pivotal opportunity to build additional product capabilities that address pain points arising from the struggles of fintech.Infrastructure providers can help connect fintech companies with incumbent banks so that they can both reap the benefits of the interest rate environment.Infrastructure providers must reprioritize and find a way to grow their capabilities for their current customers instead of just signing new ones. To do this, they’ll have to take a closer look at the problems those customers deal with on a daily basis. What does a fintech company do when it’s under a fraud attack? What does a new compliance order in the U.K. mean for their business? How do they retain customers who are terrified by news of skyrocketing interest rates and inflation?These are the questions the leaders of the fintech industry face daily, and infrastructure providers need to understand how they can help answer them.Identifying and addressing pain pointsThe influx of prodigious amounts of cash in the financial infrastructure sector has crowded the space with newcomers. Addressing specific fintech pain points is not only a way to help the fintech industry out; it’s also a way for infrastructure providers to differentiate themselves and show that they provide real value.International coverageThe draw of additional customers and revenue streams has caused fintech companies to explore international waters. In an increasingly globalized world, international coverage is no longer optional.Infrastructure providers must meet their customers’ appetite for global growth by ensuring that their platform is available in countries outside the U.S. They also need to ensure their platform helps fintech companies stay compliant with rapidly changing global regulations — more on that below.Regulatory scrutiny","To reach fintechâs next level, infrastructure providers must address these pain points"
246,6,6_fintech_companies_crypto_startups,https://techcrunch.com/2022/08/31/solid-63m-embedded-fintech/,"Solid, which rebranded from Wise in 2021, raised a $63 million Series B round of funding to continue providing its fintech-as-a-service offering for companies wanting to launch and scale their own fintech products.The San Mateo-based company works with fintech and vertical SaaS companies and offers banking, payments, cards and cryptocurrency products via easy-to-integrate APIs.We last profiled the company in 2020, before its name change, and after it had picked up both a $5.7 million seed and a $12 million Series A.Arjun Thyagarajan, co-founder and CEO at Solid, told TechCrunch that the company spent the last 18 months working with early customers on product-market fit.Traditional fintech infrastructure was not built for the modern company, he explained, and that results in companies needing dozens of point solutions and often spending millions in upfront and ongoing maintenance costs, all before launching an actual product.Instead, by utilizing APIs and a few lines of code, customers can embed fintech products and get them up-and-running quickly.“Customers aren’t looking for UI/UX, but really for DI/DX, that developer interface with a powerful dashboard that is self-service,” he said. “We understood what they were looking for — that demand for modern infrastructure. They work with banks, but those often don’t have tools for launching the fintech experience or the building blocks to make it easy to put together.”It was also during that time that they decided to rebrand as they solidified their business-to-business focus. The new Solid name also resonated more with customers, Thyagarajan added.Over the past year, Solid grew 10x in revenue, doubled its customers to 100 and became profitable. Year to date, the company processed $2 billion in transactions.After being in sort of a stealth mode during the past 18 months, Thyagarajan said the company was now on a journey to get to 100 customers. To do that, he and co-founder Raghav Lal thought it was time to go after new funding. They started the process in May and closed the Series B at the end of July.“We saw early signs of product market fit, so our thought process was to do the Series B when we were ready for hypergrowth, and now we have cash in the bank,” Thyagarajan said. “We are going after the mid-market, so we had to go back and fine-tune our product as we figured out what businesses need. The key was to build our technology from the ground up to own the complete experience so we could give customers what they want.”FTV Capital led the new investment and was joined by existing investor Headline. To date, Solid has raised $80.7 million. Thyagarajan didn’t disclose a specific valuation with the new round, but did reveal it was 5x over Solid’s Series A valuation.The capital infusion will help accelerate Solid’s entrance into some new verticals like travel, logistics, construction, healthcare, education and the gig economy. The company is looking at where money is moving and identified 40 to 50 different verticals where there is an impedance in how money moves, but they want to benefit their customers.The company is also focusing on mid-market and larger companies, which is another reason why Thyagarajan said the investment was important.“We are talking to Fortune 1000 companies and they feel more comfortable working with companies with a strong balance sheet,” he added. “A lot of work has been under the radar, so we are getting the brand out and showcasing we are the ‘AWS of fintech,’ a one-stop shop. Our goal is to be alongside them as a partner, not just a vendor.”",Solid banks $63M for easier deployment of embedded fintech products
443,6,6_fintech_companies_crypto_startups,https://www.protocol.com/climate/data-center-climate-risk-assessment,"Benjamin Pimentel ( @benpimentel) covers crypto and fintech from San Francisco. He has reported on many of the biggest tech stories over the past 20 years for the San Francisco Chronicle, Dow Jones MarketWatch and Business Insider, from the dot-com crash, the rise of cloud computing, social networking and AI to the impact of the Great Recession and the COVID crisis on Silicon Valley and beyond. He can be reached at bpimentel@protocol.com or via Google Voice at (925) 307-9342.",Data centers arenât prepared for the climate crisis
226,6,6_fintech_companies_crypto_startups,https://techcrunch.com/2022/08/02/identity-verification-company-youverify-extends-seed-funding-to-2-5m-as-it-expands-across-africa/,"This past month has seen several African fintechs such as Flutterwave and Union54 make headlines for compliance checks issues and fraud allegations. Both unlinked events re-emphasize the importance of know your customer (KYC) and anti-money laundering (AML) checks and why regulators enforce strict policies that financial institutions need to be held accountable to while operating across the continent and globally.For the many startups whose services help keep the operations of financial institutions such as banks and fintechs in check, this period highlights their relevance more than ever. In the latest development, Youverify, a Lagos and San Francisco–based identity verification company helping African banks and startups automate KYC and other compliance procedures, is announcing that it has secured a $1 million seed round extension. The startup raised a $1.5 million round in 2020, bringing its total seed raise to $2.5 million.Africa-focused VCs Orange Ventures and LoftyInc Capital, the two investors who co-led its initial seed round, also led the extension. Additional investment came from Octerra Capital, Plug & Play Venture, Syntax Ventures, HTTP Investors, Afer Group and Fronesyz Capital.The proliferation of financial services in Africa is beginning to attract more scrutiny from regulators. According to reports, transactions worth $116 billion will be made through digital payment channels this year, requiring stringent measures to prevent identity theft and fraud. Therefore, the rise in focus on maintaining transparency in financial regulations and improving strategies for KYC and AML by implementing regulatory technologies has become a significant growth factor for the market. And as regtech demand globally increases, so will Africa’s, with reports saying it will reach about $1.2 billion in the next five years.Youverify came into Africa’s regtech scene when founder and CEO Gbenga Odegbami founded the company in 2018. Launched in the Nigerian market, Youverify first provided API for address and identity verification to several financial institutions. Now it has added more KYC products and expanded into new markets such as Ghana, Côte d’Ivoire, South Africa, Kenya and Uganda.“The way our customers see us is that we help them automate their KYC and compliance issues,” said Odegbami on a call with TechCrunch.In addition to verifying identities beyond Nigeria’s bank verification number (BVN) and addresses, Odegbami says Youverify layers KYC and compliance products such as transaction monitoring. He further explained that these offerings cater to issues some fintech platforms have faced recently: alleged AML issues in the case of Flutterwave in Kenya and Ping Express in the U.S. and fraud in the case of Union54’s chargebacks. In the latter, Youverify claims it could’ve prevented large-scale chargeback fraud by identifying the pattern of transactions to flag fraud, blocking the virtual cards and tying them back to fraudsters committing the multiple fake chargebacks.“They [Union54] grew faster than they could put in place the proper transaction monitoring and fraud detection systems that will identify transactions happening from their customers,” the CEO said of the chargeback situation Union54 has dealt with over the past couple of months. “A system like ours will be able to identify previous and new patterns in such a way that we would’ve been able to help such the company.”It wasn’t until last year that Youverify started dealing with fintechs. Initially, most of its customers were governmental bodies, big corporations like Bolt and banks. Nearly two-thirds of Nigeria’s commercial banks, such as Standard Chartered, Standard Bank and Fidelity Bank, use the platform’s identity verification and KYC products, Youverify said.However, in a bid to serve more clients, the company launched its proprietary technology, the Youverify OS (YVOS), which provides a single platform for automating due diligence and combines risk and compliance management with its core identity verification platform to deliver these fintechs an enterprise-grade compliance solution. With its other product, vFORM, a low and no-code tool, businesses can create a custom process for onboarding new customers using a drag-and-drop builder.As a result of diversifying its clientele and demand for its KYC products, Odegbami said Youverify’s customer base increased by 300% to serve more than 400 banks and high-growth startups. In the last 24 months, Youverify’s application processes have grown by more than 1,000% to more than 5 million applications that have helped its clients hire talent, sell financial products, and remotely onboard ride-hailing drivers. The company’s YouID digital identity platform added more than 500,000 users, with 600 service providers on its marketplace waitlist across the continent. Odegbami said the Lagos-based identity verification company crossed an ARR of over $1 million last year.Youverify isn’t the only identity verification company in Africa. Similar providers include Smile Identity and YC-backed companies IdentityPass and Dojah. Without mincing words, Odegbami said his company is a “market leader” because it came into the market much earlier and possesses more experience, and provides more data sets than the others.Over the next 18 months, Youverify plans to grow its footprint to cover 30 countries, especially in the southern, eastern and francophone parts of Africa, where Odegbami says the company will be recruiting aggressively. It also intends to increase the number of IDs it can verify, from 400 million to 2 billion, and develop new automated compliance products for the gaming, travel, healthcare and telecommunications industries.",Identity verification company Youverify extends seed funding to $2.5M as it expands across Africa
244,6,6_fintech_companies_crypto_startups,https://techcrunch.com/2022/08/21/a-conversation-with-andreessen-horowitzs-fintech-leads/,"Welcome to The Interchange! If you received this in your inbox, thank you for signing up and your vote of confidence. If you’re reading this as a post on our site, sign up here so you can receive it directly in the future. Every week, I’ll take a look at the hottest fintech news of the previous week. This will include everything from funding rounds to trends to an analysis of a particular space to hot takes on a particular company or phenomenon. There’s a lot of fintech news out there and it’s my job to stay on top of it — and make sense of it — so you can stay in the know. — Mary AnnLast month, Andreessen Horowitz — one of venture capital’s largest and most prominent players — announced that its “headquarters will be in the cloud” going forward.Founded in 2009 in Menlo Park, California, the firm — also known as a16z — has for years been a symbol of Silicon Valley investing.Its new philosophy in this post-COVID era of remote work is that there is no longer a need for a centralized HQ. This philosophy extends to its fintech team. And let’s face it, fintech is opening so many doors in general — making a lot more things possible in terms of running a company or just operating in general, globally. Many may underestimate just how much the pandemic really pushed this acceleration in the financial services world and people are now kind of commenting, “Oh, there’s this slowdown and, like, look at how much decreased investment is in fintech.” You have to put it in perspective — we’re still way, way up from 2020 in terms of how much money is going into this space. And fintech is still taking almost a fifth of all venture capital dollars. I believe this is because it impacts everyone on a daily basis. If financial services are easier to access or if it’s easier for a business to operate or make payments or accept payments, then that’s all because of fintech.I sat down (virtually, that is) with a16z general partners Angela Strange and Anish Acharya to learn more about why the pair believes we’re experiencing the “unbundling of Silicon Valley,” what sectors of fintech have the most potential and how the new era of remote work has led to so.much.opportunity for financial technology startups.Read more here.Reporter’s note: The interview with Angela and Anish interview took place weeks prior to publication, and I learned after publication that a recent analysis reportedly revealed that of the firm’s fintech portfolio of 42 companies, only four had female co-founders. I have reached out to the partners on the topic this weekend but I had not heard back at the time of this newsletter going live. Of course, it is the weekend so I was not expecting a response quite so soon. If I hear back, I’ll update you next weekend!Weekly NewsMy fintech partner in crime, the oh-so-talented Natasha Mascarenhas, ended the week with a scoop about Stripe laying off some of the employees that support TaxJar, a tax compliance startup that it acquired last year. According to Natasha: “The layoffs – conducted over the last month – are related to Stripe’s decision to wind down TaxJar-focused go-to-market efforts in late July. Sources estimate the number of employees impacted by the workforce reduction is between 45 to 55 folks, at least a portion of whom were invited to take 30 days to apply to internal jobs at Stripe…According to LinkedIn, TaxJar’s co-founder Matt Anderson left Stripe in July, followed by folks in the sales, marketing and partnerships teams.” Read more here.The world of expense management just got (even more) competitive. Corporate spend and cash management company Rho announced that it is adding expense management to its offerings with “custom controls designed to make expenses less painful.”Via email, the company told me it believes that “offering the full suite” is crucial in the world of fintech today. Specifically, a spokesperson said: “When looking at the landscape, there are ten different providers for every individual process: spend management (ex. Brex), expenses (ex. Expensify), and banking services (ex. Mercury). Cobbling together different platforms for these separate functions creates friction for finance users. Rho believes in the power of integrating spend management and business banking services. Each corporate finance process — AP, commercial banking, spend management/cards, treasury management — works better when they work together in a single, connected view.”The startup in December raised a $75 million Series B funding round led by Dragoneer Investment Group.Speaking of spend management, Airbase announced the appointment of Philip Lacor as its chief revenue officer. Lacor most recently served as CRO for no-code platform company Unqork, where he handled all go-to-market efforts, including sales, pre-sales consulting, customer success, revenue operations and channel partners. He also led the company’s expansion into APAC. Before that, he was CRO at Envoy.Mexican fintech Covalto, which serves Mexican SMEs and was formerly known as Credijusto, has agreed to publicly list on a U.S. stock exchange via a SPAC at an estimated pro-forma valuation of $547 million. The transaction is believed to be the first time a Mexican fintech has agreed to publicly list on a U.S. stock exchange. In a press release, the company said it was merging with LIV Capital Acquisition Corp. II, a special purpose acquisition company launched by Mexico City–based fund LIV Capital. Upon closing of the transaction, LIVB will be renamed Covalto and remain listed on Nasdaq under the new ticker symbol “CVTO.” The company said originations grew at a 152% CAGR from 2015 to 2021. More here.While we’re on the topic of public markets, something interesting is happening with fintech stocks, according to F-Prime’s Fintech Index. The Index has picked up a 41% increase, compared to EMCloud’s 19.5%, Nasdaq’s 15.6%, and S&P’s 12.2% increases. Notably, it said, Affirm is leading the pack and is up 67%. Insurance stocks are up 46% overall, led by Lemonade and Oscar Health. Payment stocks are up 44% driven by Wise and Mercado Libre. Wealth and asset management stocks are also up 32%, driven by Coinbase and Bakkt Holdings. Meanwhile, banking and lending stocks are up 24%.Speaking of Affirm. The buy now, pay later giant announced an expanded, multiyear partnership with BigCommerce that “makes Affirm the preferred and recommended pay-over-time partner for BigCommerce’s tens of thousands of merchants,” according to the two companies. As a result of this partnership, BigCommerce’s merchants can activate Affirm as a payment option at checkout directly within the BigCommerce merchant dashboard.On August 12, PayPal announced that “all eligible PayPal account holders in the U.S. can now transfer, send and receive cryptocurrency with PayPal.” TechCrunch had reported that the move would be taking place in early June.Robinhood rolled out a couple of new features last week. For one, it launched advanced charts, with the goal of “giving all customers customizable, quick, simple and in-depth analysis right in the app.” The company said that advanced charts were “the number one most requested feature” from its active customers. The company also launched Cash Card Offers, a new benefit that allows all Cash Card customers to earn cash back “automatically” when spending at retailers such as Chevron, Nike, Five Guys, Macy’s, and others.From PitchBook on payments: “The accelerated digitalization of financial services, as well as the shift to online services, has benefited fintech startups over the last few years. The COVID-19 pandemic helped speed up these trends as consumers turned to digital financial services in lieu of face-to-face interactions…Digital payments, one of the earliest financial segments to go digital, has continued to see rapid disruption during this period. Checkout platforms have benefited from demand for online and contactless transactions, remote working has driven a need for payroll software providers, and corporate credit card providers like Ramp and Brex have reportedly seen revenues surge.”Hello Alice, which says it is “helping over 1,000,000 small businesses grow,” has announced a new Small Business Mastercard. The card was launched on August 16 in partnership with Mastercard and First National Bank of Omaha, and offers small business owners features such as a rewards program featuring the ability to earn points by completing “business-advancing activities” on the Hello Alice platform. The company says it recently completed a Small Business Capital Access Study and found that 78% of owners claim access to capital is limiting their ability to manage their day-to-day operations, with Black (84%) and multiracial (82%) owners overindexing on this claim. In its words, Hello Alice designed the card “to meet the needs of small business owners where they are, breaking longstanding barriers for those who have traditionally been denied access.”Funding and M&ASeen on TechCrunchPomelo exits stealth mode with $20M seed to rethink international money transferTiger Global doubles down on Indian savings and investments app JarPastel, a Nigerian bookkeeping and digital platform for merchants, raises $5.5M led by TLcomHighbeam inks $7M to shine light on e-commerce-specific banking needsFunding Circle co-founder unveils new Super Payments fintech venture with $27M investmentRocketplace raises $9M in seed funding to build the ‘Fidelity for crypto’Social investment platform eToro to acquire fintech startup Gatsby for $50MYC-backed Arc, a digital bank for ‘high-growth’ SaaS startups, lands $20M Series ASeen elsewherePayments company AtoB raises $155M in Series B to ‘modernize trucking industry’Ecuadorian ‘unicorn’ Kushki buys finance service startup in Mexican expansion. TechCrunch covered the company’s $100 million raise at a $1.5 billion valuation in June.Agora raises $20M Series A led by Insight Partners ‘to accelerate the growth of real estate firms with digital transformation’Closinglock announces $4M in funding led by LiveOak Venture PartnersICYMI: Digital credit fintech Kapital raises $30 million in debt and equity to grow in Mexico and ColombiaAnother busy fintech week down in the books. Thanks, as always, for your support in reading and sharing this newsletter of mine! Have a wonderful week ahead. xoxo, Mary Ann",A conversation with Andreessen Horowitzâs fintech leads
267,6,6_fintech_companies_crypto_startups,https://techcrunch.com/2022/10/12/investing-platform-raises-10m-to-offer-climate-investing-from-a-different-angle-to-esg-ratings/,"Investing platform raises $11M to offer climate investing from a different angle to ESG ratingsNet Purpose, a platform for sustainable investors, has raised $11 million in a Series A round led by ETF Partners, funding that will be used to expand its product and team, says the company.The company is benefitting from a shift to investing in sustainable products. According to the UN Principles for Responsible Investment, $120 trillion is committed to invest sustainably, with allocations growing at 22% year on year.New investors M-Tech Capital and Exceptional Ventures joined the round, and existing investors Jim O’Neill, former chair of Goldman Sachs Asset Management; Kevin Gould, co-founder IHS Markit; the Louis Family; Illuminate Financial and Revent increased their commitments.Currently, investors rely on reported and estimated data and ESG ratings. These tend to measure financial risk, not social and environmental return. Net Purpose claims its platform looks more at social and environmental performance based on factual reporting.Indeed, there are claims that MSCI, the largest ESG rating company, doesn’t even try to measure the impact of a corporation.Sam Duncan, Net Purpose founder and CEO said: “Net Purpose’ core differentiator is that we provide investment-grade facts on the social and environmental performance of companies and investment portfolios and no black box ratings or scores. Facts measure social and environmental performance, not financial risk. Net Purpose also has more and higher quality data than any other provider.”Net Purpose competitors are ESG Ratings providers like MSCI and Sustainalytics.",Investing platform raises $11M to offer climate investing from a different angle to ESG ratings
260,6,6_fintech_companies_crypto_startups,https://techcrunch.com/2022/10/07/convective-capitals-35-million-answer-to-the-increasing-threat-of-wildfires/,"Wildfires have become an ever-increasing threat as houses are built closer together and the growing impacts of climate change wreak havoc on natural landscapes. Entrepreneurs, in response, have started to develop tech meant to minimize the scale and damage of these natural disasters. Convective Capital is a new VC firm looking to back them.Bill Clerico, the former co-founder and CEO of fintech WePay, launched the firm this year and has since raised $35 million for a first fund to back early-stage startups creating tech that can help detect and contain wildfires. The fund plans to invest in roughly 15 companies by writing seven-figure checks.While some of these startups could fall under the umbrella of climate tech, Clerico said focusing on wildfire solutions fits more into the climate resilience category. He thinks the distinction makes it a more straightforward investment opportunity because increasing wildfires are already a problem today as opposed to climate tech meant to prevent or minimize future problems.The thesis turned out to be a polarizing one to pitch as some investors understood the need immediately while others thought focusing on wildfire could prove to be too niche.“It made fundraising easier in that there were people who had witnessed the economic opportunity first hand, and understood the impacts of fire there,” Clerico said. “Or they had experienced it personally in various ways…or folks didn’t.”Clerico left WePay after a decade to pursue angel investing and become a volunteer fire fighter. While spending time at his home in Mendocino, California, he saw the impacts of the growing wildfire crisis as one of the natural disasters closed down the road to his house and he found himself struggling to get home insurance.“Just watching wildfires become the really big crisis that it is, and having a real vested interest in the outdoors, led me down this path,” Clerico said. “I started thinking about how technology could be a solution.”Clerico had made about 50 angel investments and admitted he was bored at the thought of hearing another fintech pitch. He started backing fire tech companies and began to specialize in the category. He decided to raise a fund so he could invest in the sector at scale.He estimated that there are currently 200 startups focused on this area, claiming that Convective Capital has probably talked to all of them. These startups are tackling different areas of the wildfire crisis ranging from Pano, a startup that uses camera systems and AI to help emergency response teams detect fires earlier, to Rain Industries, a company that creates autonomous drones to help put out fires.“What was missing though is that community coming together and acknowledging that fire tech is thing,” Clerico said. “It doesn’t exist as a category like fintech exists. Part of what we want to do as a firm is bring awareness to that category and really just help these founders get to know each other.”Maxwell Brodie, the co-founder and CEO of Rain Industries, told TechCrunch that while most likely everyone in California has been impacted by wildfires, it’s nice to see that someone like Clerico is actually trying to do something about it. He hopes that his involvement can help draw more builders and investors into the category.“All larger pools of resources [and capital] seek out where new emerging growth is going to come from,” Brodie said. “What [Clerico] is doing with Convective Capital is proving that there is a new emerging area of growth and everyone should be paying attention to that, especially late-stage investors.”While Clerico doesn’t believe that any sectors can really be recession proof, he thinks firetech is an area that isn’t really tied to broader market conditions. And while many of the companies he backs aren’t preventing the wildfires from starting, he hopes they can help mitigate some of the damage.“To leverage the power of startups to have a positive impact is a really inspiring challenge,” he said. “And one of the reasons why myself and my partners have gone after this.”",Convective Capitalâs $35 million answer to the increasing threat of wildfires
257,6,6_fintech_companies_crypto_startups,https://techcrunch.com/2022/09/20/sardine-fintech-crypto-fraud-startup-a16z-series-b-funding/,"As fintechs become more efficient, so too do fraudsters.“Faster instant payments mean faster fraud,” Sardine CEO and co-founder Soups Ranjan told TechCrunch. That’s the thesis behind his startup, which uses behavioral, financial and device-specific user data to detect fraud on behalf of its clients in the crypto and fintech industries.Those conditions also mean a faster fundraising process for Sardine, evidently. The company announced it has raised $51.5 million in a Series B round led by Andreessen Horowitz’s (a16z) Growth Fund after closing $19.5 million for its Series A earlier this year. A16z was a new investor in the Series A, with the fintech-focused GP Angela Strange leading Sardine’s previous round and Growth Fund partner Alex Immerman taking the lead this time around.The other Series B participants were a mix of new and existing investors including XYZ, Nyca Partners, Sound Ventures, Activant Capital, Visa, Google Ventures, Eric Schmidt, Vikram Pandit, The General Partnership, NAventures, ING Ventures, ConsenSys, Cross River Digital Ventures, Alloy Labs and Uniswap Labs Ventures, according to the company.Sardine has grown considerably since it announced the Series A back in February, growing its roster of clients from ~50 to ~135 today, Ranjan said. Its customers include crypto exchanges FTX and Blockchain.com as well as fintechs with broader mandates such as Wealthsimple and Digit, he added.After participating as one of 10 startups in the FIS Fintech Accelerator program this summer, the startup is making a push into “core banking processes” and is in discussions with large banks in the U.S. and Europe, Ranjan said.It’s easier to understand why a fintech or crypto startup might want to beef up its fraud prevention capabilities, but Ranjan explained that even for big banks, the standard KYC (“Know Your Customer”) compliance process isn’t equivalent to a fraud protection program; 90% of fraud detected on Sardine’s customers’ platforms comes from individuals who have already passed the KYC process, he said.Sardine does have competition from other startups in the identity-verification space, such as Socure, which told TechCrunch last year that it counts three of the top five global banks as customers. Socure, which counts Tiger Global as its lead investor, was valued at $4.5 billion during its last publicly announced fundraise in November 2021, a Series D round. Sardine didn’t share the valuation from its latest fundraise, but the startup is significantly earlier-stage than Socure.Ranjan described Sardine’s differentiation in the market as stemming from his team’s experience and the company’s focus on fintechs in particular. Ranjan himself previously worked as Coinbase’s director of data science and risk and Revolut’s head of crypto, and the company’s head of banking partnerships came to the startup from Zelle.“If you actually peek underneath the hood at any of these traditional fraud prevention vendors, you will find that the APIs don’t even have support for the identity of an individual, because they’re all built or designed for the e-commerce checkout experience,” Ranjan said. Rather than analyzing a customer’s shipping address and shopping cart, Sardine looks at device intelligence and behavioral biometric data that helps identify whether an individual engaging in a transaction is really who they say they are, he continued.Another major differentiator for Sardine from competitors like Socure is its instant ACH and card onramp to crypto, which allows its customers to purchase over 30 different crypto assets instantly rather than having to wait the traditional few days to access their funds. It also offers direct fiat to NFT checkout in partnership with Tom Brady’s company, Autograph, and plans to expand that product to other NFT marketplaces, according to Ranjan.Banks and card issuers typically use fraud detection algorithms for crypto that aren’t nearly granular enough, Ranjan said, meaning around half of the customers who attempt to transact using fiat-to-crypto onramps through traditional platforms are declined as fraudulent.When Sardine launched its NFT checkout product in partnership with Autograph earlier this month, its conversion rate was much higher, around 98%, Ranjan said. It’s too early to tell if there are any chargebacks, or instances of fraud that went undetected, from that launch, he added, noting that Sardine is one of the first companies to even offer such instantaneous access to crypto through ACH.“One of the reasons why folks haven’t dabbled or launched ACH to crypto, or even direct ACH to NFT, has been that there is no one [else] taking on the fraud risk liability,” Ranjan said. He declined to share details around the chargeback rates Sardine sees across its older products, but said that the platform allows customers to access some, but not all, of their crypto instantly.“Sardine is taking on the fraud risk. [The transaction] typically settles into two-plus days, so for that period of time, we’re taking on the settlement risk, and we’re taking on the third-party fraud risk, as in, if somebody connects a stolen bank account,” Ranjan explained.Venture capitalist Andrew Steele, who led Activant Capital’s investment in Sardine, thinks the company is uniquely positioned to assume and manage risk in a way that enables instant transactions.“Identity and fraud are usually completely separate things,” Steele said. “We’ve invested in identity platforms. We’ve also invested in fraud platforms, and typically they’re completely separate. Identity to me is a moment in time. It’s when you onboard someone, it’s how you make sure that they are who they say they are. And then fraud is usually a transaction-based thing, but both are completely separate and siloed. Typically, that lack of connection means that you have limited data and you can’t really take on risk in the way we’re talking about [with Sardine].”",Sardine raises $51.5M led by a16z to sniff out fishy fintech transactions
206,6,6_fintech_companies_crypto_startups,https://techcrunch.com/2021/12/28/sectors-where-new-zealand-startups-are-poised-to-win/,"As a remote island nation in the middle of the South Pacific, New Zealand is experiencing the stirrings of a burgeoning startup scene. The country has historically been capital-starved, but recent investments from the government and foreign investors have significantly increased access to early-stage venture capital funding. Now, certain industries are emerging as potential areas where New Zealand can win in the tech space.Deep tech, medtech/biotech, climate tech, and crypto and blockchain are all areas that investors say they’re either actively investing in or watching for signs of scale.Note: All monetary amounts are listed in New Zealand dollars unless otherwise stipulated.Deep techNew Zealand has the right mix of deep tech-focused capital and resources, strong engineering schools and major success stories that are helping create technologically sophisticated, globally scalable startups.During the first half of this year, total investment in New Zealand’s early-stage sector increased 78% from the first half of 2020, 42% of which went directly to deep tech startups, according to PwC. Much of that funding came from New Zealand Growth Capital Partners (NZGCP), a government entity established to create a vibrant early-stage environment in New Zealand, via its Elevate fund of funds program that provides capital to New Zealand VCs investing in Series A and B rounds.Last October, Elevate allocated $20 million to a fund managed by deep tech VC firm Pacific Channel. More recently, Elevate committed $17 million to Nuance Connected Capital’s fund focused on deep tech innovations, as well as $45 million to GD1’s Fund 3, which focuses on deep tech as well as connected hardware, enterprise software and health tech.New Zealanders make really good founders. … There’s something about just growing up on a farm or, you know, playing beer float out in the lakes and rivers; New Zealand is just really resourceful. Blockchain NZ Chair Bryan VenturaThen there are groups, like Outset Ventures, formerly LevelTwo, that are geared toward helping seed and pre-seed deep tech startups get going. Outset is home to New Zealand’s only two deep tech unicorns, Rocket Lab and LanzaTech, both of which have spun out numerous other deep tech companies. Outset continues to be a resource for seed-stage startups that need not only money, but also connections to larger companies and access to workshops and labs. Just last year, Outset and Icehouse Ventures, a VC firm, partnered to raise a $12 million fund, which launched this April, for early-stage science and engineering startups. Imche Fourie, CEO of Outset, said the company has already made 17 investments from that fund.*Notable deep tech companies out of New Zealand include Foundry Lab, a startup that creates metal castings quickly and cheaply with a microwave; Soul Machines, a platform that creates lifelike digital avatars that animate autonomously, responding to interactions and interpreting facial expressions, with personalities that evolve over time; and Dennisson Technologies, a wearables company that’s developing soft exosuits that incorporate 4D material technology to actively assist people with limited mobility due to physical or neurological disability.The most Kiwi of deep tech startups, however, is Halter — a company that spun out of Rocket Lab and produces solar-powered smart cow collars that allow farmers to remotely shift and virtually fence and monitor cows in order to optimize pasture time. Founder Craig Piggott grew up on a dairy farm watching his parents struggle with the relentless work. After studying engineering at Auckland University of Technology and working at Rocket Lab, Piggott combined his experience to come up with a somewhat wacky and ambitious hardware and software play. Rocket Lab founder Peter Beck, who backed Halter, told TechCrunch he thinks it will be a globally scalable billion-dollar company.The epitome of New Zealand’s deep tech scene is its over $1.75 billion space industry. Rocket Lab, which is now a U.S.-owned company after a SPAC merger, put New Zealand on the map as a place with minimal air traffic, clear skies and favorable aerospace regulations. As a result, companies are forming that can either send payloads into space or provide services to existing space companies.Outset-backed Zenno Astronautics, for example, is developing a fuel-free satellite propulsion system that uses magnets powered by solar panels. Dawn Aerospace is working toward a remotely piloted aircraft that could take off like a normal airplane, drop a satellite payload and be back home in 15 minutes. And Astrix Autonautics, a startup founded by three Auckland University students, is being trialed by Rocket Lab to see if they can more than double the power-to-weight ratios of solar arrays used today.",Sectors where New Zealand startups are poised to win
11,6,6_fintech_companies_crypto_startups,https://apnews.com/article/cryptocurrency-biden-technology-united-states-ae9cf8df1d16deeb2fab48edb2e49f0e,"FILE - Treasury Secretary Janet Yellen speaks after touring the IRS New Carrolton Federal Building, Sept. 15, 2022, in Lanham, Md. The Biden administration is moving one step closer to developing a central bank digital currency, otherwise known as the digital dollar. It's a step that administration officials said would help reinforce the U.S. role as a leader in the world financial system. (AP Photo/Alex Brandon, File)FILE - Treasury Secretary Janet Yellen speaks after touring the IRS New Carrolton Federal Building, Sept. 15, 2022, in Lanham, Md. The Biden administration is moving one step closer to developing a central bank digital currency, otherwise known as the digital dollar. It's a step that administration officials said would help reinforce the U.S. role as a leader in the world financial system. (AP Photo/Alex Brandon, File)WASHINGTON (AP) — The Biden administration is moving one step closer to developing a central bank digital currency , known as the digital dollar, saying it would help reinforce the U.S. role as a leader in the world financial system.The White House said on Friday that after President Joe Biden issued an executive order in March calling on a variety of agencies to look at ways to regulate digital assets, the agencies came up with nine reports, covering cryptocurrency impacts on financial markets, the environment, innovation and other elements of the economic system.Treasury Secretary Janet Yellen said one Treasury recommendation is that the U.S. “advance policy and technical work on a potential central bank digital currency, or CBDC, so that the United States is prepared if CBDC is determined to be in the national interest.”“Right now, some aspects of our current payment system are too slow or too expensive,” Yellen said on a Thursday call with reporters laying out some of the findings of the reports.ADVERTISEMENTCentral bank digital currencies differ from existing digital money available to the general public, such as the balance in a bank account, because they would be a direct liability of the Federal Reserve, not a commercial bank.According to the Atlantic Council nonpartisan think tank, 105 countries representing more than 95% of global gross domestic product already are exploring or have created a central bank digital currency.The council found that the U.S. and the U.K. are far behind in creating a digital dollar or its equivalent.Treasury, the Justice Department, the Consumer Finance Protection Bureau, the Securities and Exchange Commission and other agencies were tasked with contributing to reports that would address various concerns about the risks, development and usage of digital assets. Several reports will come out in the next weeks and months.Eswar Prasad, a trade professor at Cornell who studies the digitization of currencies, said Treasury’s report “takes a positive view about how a digital dollar might play a useful role in increasing payment options for individuals and businesses” while acknowledging the risks of its development.He said the report sets the stage for the creation of agency regulations and legislation “that can improve the benefit-risk tradeoff associated with cryptocurrencies and related technologies.”ADVERTISEMENTThe Blockchain Association, which lobbies lawmakers on Capitol Hill, said in a statement that the White House reports are “a missed opportunity to cement U.S. crypto leadership.”“These reports focus on risks — not opportunities,” the statement reads, “and omit substantive recommendations on how the United States can promote its burgeoning crypto industry, including job creation, improvements to the financial system, and expanded access for all Americans.”On Capitol Hill, lawmakers have submitted various pieces of legislation to regulate cryptocurrency and other digital assets.Sheila Warren, CEO of the Crypto Council for Innovation, said in an emailed statement that the report “seem to kick the can down the road” she said, “we don’t see clear recommendations.”ADVERTISEMENTThe director of the National Economic Council, Brian Deese, told reporters that “we’ve seen in recent months substantial turmoil in cryptocurrency markets and these events really highlight how, without proper oversight, cryptocurrencies risk harming everyday Americans’ financial stability and our national security.”“It is why this administration believes that now more than ever,” he said, “prudent regulation of cryptocurrencies is needed.”He said on Friday that the Administration plans to “execute a comprehensive action plan with priority steps to mitigate key risks of cryptocurrencies — among others, money laundering and financing for terrorism.”___Follow the AP’s coverage of cryptocurrency at https://apnews.com/hub/cryptocurrency .",Treasury recommends exploring creation of a digital dollar
241,6,6_fintech_companies_crypto_startups,https://techcrunch.com/2022/08/16/european-climate-vc-contrarian-targets-100-million-seed-fund/,"From ‘literally zero’ experience to $100M, this VC is raising his second climate tech seed fundIf you ask me, climate tech investor Contrarian Ventures isn’t so contrarian anymore.The five-year-old firm is targeting $100 million for its second seed-stage fund, and it’s doing so smack in the middle of a climate-tech dealmaking boom. So, if anything, it’s trendy.But when the seed-stage VC — a backer of e-bike maker Zoomo and solar data firm PVcase — debuted with a $13.6 million fund in 2017, its focus was “obviously contrarian,” founding partner Rokas Peciulaitis told TechCrunch, as the “industries in vogue at the time were AI and Fintech.”The launch also marked an unexpected pivot for Peciulaitis, who says he dove into the scene with “literally zero climate tech sector experience.” He’d recently left an inflation-trading job at Bank of America, where the work was “not fulfilling in the slightest,” Peciulaitis said in a nod to the bank’s reputation as a major funder of fossil fuels.In 2017, PitchBook recorded 578 climate tech deals globally, altogether worth $12.5 billion. The sector has since tripled in size, as climate change–driven extreme weather events occupy evermore space in our collective consciousness. To that point: PitchBook tracked 1,130 climate tech deals globally in 2021, topping $44.8 billion in value. Climate tech is cool now, but Peciulaitis’s Lithuania-based venture firm is sticking with its name anyways.Like any venture capital firm, Contrarian says that it stands out through its emphasis on “developing excellent relationships with founders.” Materially, the firm invests in tech that could help decarbonize transportation, industrial processes, energy and buildings.Contrarian has completed 21 deals to date, and this year it expanded beyond Lithuania with new partners in Berlin and London. The firm backs emerging startups in Europe as well as Israel, but nowhere else in the Middle East. Currently, the firm does not invest in agriculture-related tech, though the category has a significant carbon footprint of its own.In an email, Contrarian said it counts London-based tech VC Molten Ventures among its limited partners. The firm declined to share a full list of its LPs, but stated that none of them were fossil fuel companies.","From âliterally zeroâ experience to $100M, this VC is raising his second climate tech seed fund"
220,6,6_fintech_companies_crypto_startups,https://techcrunch.com/2022/07/20/ok-dont-fear-the-long-shots-are-still-getting-venture-funding/,"OK, don’t fear: The long shots are still getting venture fundingHello and welcome back to Equity, a podcast about the business of startups, where we unpack the numbers and nuance behind the headlines.This is our Wednesday show, where we niche down to a single topic, think about a question and unpack the rest. This week, Natasha and Alex asked: How do founders hold two ideas in their heads: both that there is an economic downturn, but also that things are looking up for many industries?After a series of episodes about the tensions within the downturn, this is a “good news, despite” episode.We started with a vibe check based on recent interviews with recently venture-backed founders, before getting into the bright spots from Q2 2022 data.Then we spent some time talking about specific sectors enjoying fresh cash right now, including climate and European edtech.Geographically, Africa continues to be one to watch. The continent is set to have its best year yet.In the second half of the show, lean back and enjoy the riffing: We talk VC vacation homes, good news and somehow end with tater tots.We had a great time, and hope you like this show. We’re back Friday with our regular news roundup!Equity drops every Monday at 7 a.m. PDT and Wednesday and Friday at 6 a.m. PDT, so subscribe to us on Apple Podcasts, Overcast, Spotify and all the casts.","OK, donât fear: The long shots are still getting venture funding"
219,7,7_food_farmers_agriculture_production,https://techcrunch.com/2022/07/19/crop-one-emirate-worlds-largest-vertical-farm-in-dubai/,"Crop One Holdings and Emirates Flight Catering announced this week they opened Emirates Crop One, what they say is “the world’s largest vertical farm.”The over 330,000-square-foot facility is located in Dubai, United Arab Emirates near Al Maktoum International Airport at Dubai World Central. It has the capacity to produce over 2 million pounds of leafy greens annually.The facility got its start in 2018 when Crop One, an indoor vertical farming company, and Emirates Flight, the airline Emirates catering arm, signed a $40 million joint venture to build Emirates Crop One. AgFunder reported the $40 million was a majority debt funded.Dubbed ECO 1, the farm uses 95% less water than field-grown produce and is guaranteed an output of three tons per day, according to the companies. Passengers on Emirates and other airlines will be able to eat the leafy greens, which include lettuces, arugula, mixed salad greens and spinach on their flights starting this month.Those local to the United Arab Emirates will be able to buy the produce at stores under the Bustanica brand. The greens require no pre-washing and are grown without pesticides, herbicides or chemicals.“We are proud to bring Crop One’s best-in-class technology to this innovative food production facility alongside our joint venture partner,” said Craig Ratajczyk, CEO at Crop One., in a written statement.” ECO 1 will address growing supply chain challenges and food security issues, while introducing millions of new consumers to the benefits of vertically farmed produce. It’s our mission to cultivate a sustainable future to meet global demand for fresh, local food, and this new farm is the manifestation of that commitment. This new facility serves as a model for what’s possible around the globe.”This is Crop One’s second vertical farm after its flagship facility in Millis, Massachusetts.Emirates Crop One joins vertical farms being built all over the world. In May, Bowery Farms opened its vertical farm in Pennsylvania. Though it did not give a size for the facility, my colleague Brian Heater wrote that it was suspected to be 156,000 square feet. Earlier this year, Upward Farms was planning a 250,000-square-foot vertical farm, also in Pennsylvania, that was poised to open in mid-2023.","Crop One, Emirate open âworldâs largest vertical farmâ in Dubai"
214,7,7_food_farmers_agriculture_production,https://techcrunch.com/2022/07/13/bowery-farming-found-podcast/,"Welcome back to Found, the TechCrunch podcast that brings you the stories behind the startups.This week’s guest, Bowery Farming founder and CEO Irving Fain wants you to taste the best strawberry you’ve ever had, grown only a few miles from your urban home. As the leading and largest vertical farming company in the U.S, their goal is to make agriculture possible in urban spaces while also making it possible to grow a wide array of crops from anywhere in the world. Darrell and Jordan talk to him about how agtech companies all have a space in the fight against climate change, what led him to start Bowery, and how they are innovating and scaling thoughtfully.Subscribe to Found to hear more stories from founders each week.Connect with us:",Vertical farming founder is reimagining agriculture from the ground up
317,7,7_food_farmers_agriculture_production,https://www.cell.com/iscience/fulltext/S2589-0042(22)01513-9,"Further information and requests for resources and reagents should be directed to and will be fulfilled by the lead contact, Ellard Hunting, [email protected]Observations were carried out at our field station at the University of Bristol, School of Veterinary Sciences, Langford United Kingdom, which is equipped with an electric field monitor to continuously measure atmospheric PG (Boltek EFM 100 Field Mill, calibrated in a capacitor plat setup). This site features several honeybee hives that are used for research. In the event of overcrowding of a beehive, the original queen leaves the hive with a fraction of the workers (on average around 12,000 bees) (), resulting in occasional swarming events near PG measuring equipment at our study site. When honeybees swarm, they usually cluster on a limb of a tree for several days while scout bees search for suitable cavities to nest in. After an appropriate nest is found, the swarm will collectively migrate. An electric field monitor was placed near the swarm. A camera (AKASO V50×, 30 fps) was positioned next to the field mill with an upward orientation to record the swarm in flight ( Figure 1 B). A second electric field monitor was placed in an open field, 50m away from the other electric field monitor and swarm to ascertain any dynamics in the electric field monitor was caused by the presence of a swarm. For about three minutes, part of the swarm passed over the electric field monitor.Modelling of electric fields was performed using finite element analysis within COMSOL Multiphysics® v. 5.4 (COMSOL AB, Stockholm, Sweden) utilising the “Electrostatics” interface within the “AC/DC” module. The three-dimensional geometry consisted of a 60 m × 60 m x 40 m (length, width, height) cuboid within which the model operated. The two runs made for differently sized and charged ellipsoids represented insect swarms of honeybees and locusts. The honeybee swarm was represented as an ellipsoid with semi-axes 2 m x 1 m x 0.5 m ellipsoid at a height of 3 m, and the locust swarm an ellipsoid with semi-axes 4 m x 1 m x 1 m at a height of 15 m. The swarm charge was distributed evenly as a volume charge within the ellipsoids, with the total charge calculated respectively as the sum of 500 bees each carrying +100 pC (order of magnitude from ()) and 1000 locusts carrying +850 pC each. An approximately 8 m tall deciduous tree was included in each model to provide scale and an electrical landmark for comparison (). The remainder of the model domain was assigned as air. The upper surface of this air column was given an electrical potential typical of a 40 m altitude in fair-weather conditions (+4 kV), with the bottom surface defined as zero potential, equivalent to the established surface (first meter) atmospheric potential gradient of 100 V/m (). The surface of the tree was also defined as ground (). Meshing of the geometry was physics-controlled, set to “extremely fine”. The relative permittivity, ε, was defined as ε= 12 for living trees (), ε= 1 for air, and ε= 80 inside the insect swarms (likely an overestimate, based on the permittivity of water). Model outputs presented for this study were produced by plotting data from two-dimensional slices through the centre of the three-dimensional dataset.Quantification and statistical analysisThe video recording capturing the swarm event was cropped to a 500 by 220-pixel window to remove foreground objects obstructing the view of the swarm and was analysed using a custom script in Python 3.8.1. The video was converted to black and white pixels such that the background was white and any non-background objects, comprising the swarm, were black. The ratio of black to white pixels was calculated for each frame of the video resulting in a proxy measure for bee density, defined as relative pixel density here. At points during the swarm’s passage, flowering heads of grass entered the frame, increasing the relative pixel density. Such affected data were therefore removed. The relative pixel densities were filtered by a moving mean over 10 data points (corresponding to 0.5 s) to emphasise the long-term trend of the data. These data were compared to data collected by the electric field monitor using cross correlation and linear regression in PAST v.4. Since insect charges can be expected to influence PGs directly, data were aligned (zero lag) based on the highest cross correlation coefficient.",Observed electric charge of insect swarms and their contribution to atmospheric electricity
202,7,7_food_farmers_agriculture_production,https://techcrunch.com/2021/09/14/agbiome-lands-166m-for-safer-crop-protection-technology/,"AgBiome, developing products from microbial communities, brought in a $116 million Series D round as the company prepares to pad its pipeline with new products.The company, based in Research Triangle Park, N.C., was co-founded in 2012 by a group including co-CEOs Scott Uknes and Eric Ward, who have known each other for over 30 years. They created the Genesis discovery platform to capture diverse microbes for agricultural applications, like crop protection, and screen the strains for the best assays that would work for insect, disease and nematode control.“The microbial world is immense,” said Uknes, who explained that there is estimated to be a trillion microbes, but only 1% have been discovered. The microbes already discovered are used by humans for things like pharmaceuticals, food and agriculture. AgBiome built its database in Genesis to house over 100,000 microbes and every genome in every microbe was sequenced into hundreds of strains.The company randomly selects strains and looks for the best family of strains with a certain activity, like preventing fungus on strawberries, and creates the product.Its first fungicide product, Howler, was launched last year and works on more than 300 crop-disease combinations. The company saw 10x sales growth in 2020, Uknes told TechCrunch. As part of farmers’ integrated pest program, they often spray fungicide applications 12 times per year in order to yield fruits and vegetables.Due to its safer formula, Howler can be used as the last spray in the program, and its differentiator is a shorter re-entry period — farmers can spray in the morning and be able to go back out in the field in the afternoon. It also has a shorter pre-harvest time of four hours after application. Other fungicides on the market today require seven days before re-entry and pre-harvest, Uknes explained.AgBiome aims to add a second fungicide product, Theia, in early 2022, while a third, Esendo was submitted for Environmental Protection Agency registration. Uknes expects to have 11 products, also expanding into insecticides and herbicides, by 2025.The oversubscribed Series D round was co-led by Blue Horizon and Novalis LifeSciences and included multiple new and existing investors. The latest investment gives AgBiome over $200 million in total funding to date. The company’s last funding round was a $65 million Series C raised in 2018.While competitors in synthetic biology often sell their companies to someone who can manufacture their products, Uknes said AgBiome decided to manufacture and commercialize the products itself, something he is proud of his team for being able to do.“We want to feed the world responsibly, and these products have the ability to substitute for synthetic chemicals and provide growers a way to protect their crops, especially as consumers want natural, sustainable tools,” he added.The company has grown to over 100 employees and will use the new funding to accelerate production of its two new products, building out its manufacturing capacity in North America and expanding its footprint internationally. Uknes anticipates growing its employee headcount to 300 in the next five years.AgBiome anticipates rolling up some smaller companies that have a product in production to expand its pipeline in addition to its organic growth. As a result, Uknes said he was particular about the kind of investment partners that would work best toward that goal.Przemek Obloj, managing partner at Blue Horizon, was introduced to the company by existing investors. His firm has an impact fund focused on the future of food and began investing in alternative proteins in 2016 before expanding that to delivery systems in agriculture technology, he said.Obloj said AgBiome is operating in a $60 billion market where the problems include products that put toxic chemicals into the ground that end up in water systems. While the solution would be to not do that, not doing that would mean produce doesn’t grow as well, he added.The change in technology in agriculture is enabling Uknes and Ward to do something that wasn’t possible 10 years ago because there was not enough compute or storage power to discover and sequence microbes.“We don’t want to pollute the Earth, but we have to find a way to feed 9 billion people by 2050,” Obloj said. “With AgBiome, there is an alternative way to protect crops than by polluting the Earth or having health risks.”",AgBiome lands $116M for safer crop protection technology
30,7,7_food_farmers_agriculture_production,https://cosmosmagazine.com/science/antibiotic-potato-fungal-infections/,"Antibiotic resistance is a worry for human and plant health alike. The more we use existing antibiotics in clinical settings and in agriculture, the more antibiotic resistance develops. Without new antibiotic compounds, we will struggle to treat once-controlled diseases in both humans and plants.Exciting new research from Europe details the discovery of a new antifungal antibiotic called solanimycin hiding within a bacteria which causes disease in potatoes.Most therapeutic antibiotics actually come from soil microbes, so this discovery broadens the search for new compounds to plant-based microorganisms.“We have to look more expansively across much more of the microbial populations available to us,” said Dr. Rita Monson a microbiologist at the University of Cambridge and one of the study’s authors.Blackleg in potatoes one of the symptoms of Dickeya solani. Credit: Liudmyla Liudmyla/Getty ImagesThe bacteria itself, known as Dickeya solani, has been known for over 15 years, with another antibiotic called oocydin A originally isolated from the bacteria and used as an effective fungicide against many plant ailments.Get an update of science stories delivered straight to your inbox. Get a daily dose of science Get a weekly Cosmos Catch-upFrom this, and a sequencing of the bacterium’s genome, the researchers realised the potential for other antibiotic compounds to be hiding in the background. When they switched off the genes responsible for producing oocydin A and when the bacteria were within an acidic background – such as that found inside a potato – it would switch on genes responsible for creating solanimycin.“It’s an antifungal that we believe that will work by killing fungal competitors, and the bacteria benefit so much from this,” said Monson. “But you don’t turn it on unless you’re in a potato.”Antiobiotic resistance: an arms race going on millions of yearsComputer illustration of Candida fungi (yeast). C. albicans is found on the skin and mucous membranes of the mouth, genitals, respiratory and digestive tracts. Credit: Kateryna Koc/Science Photo/Library/Getty ImagesSolanimycin has been demonstrated to act against fungal disease in both plants and also on a common organism known as Candida albicans, which is present inside the body naturally, but can become out of control and cause dangerous infections.Monson and colleagues want to understand more about solanimycin and how it works, so they have teamed up with chemists to investigate the molecular structure in detail. From there, they hope to progress to testing the compound in plant and animal models.",New antibiotic hiding in diseased potatoes thwarts fungal infections in plants and humans
513,7,7_food_farmers_agriculture_production,https://www.theguardian.com/environment/2022/oct/01/scotland-vertical-farming-boost-tree-stocks-hydroponics,"It is a long way from the romance of a sun-dappled Highland glen. Picture instead a white cube equipped with the computer-controlled automation you would sooner expect to see in an Amazon or Ikea warehouse.Scotland’s state forestry agency believes this prefabricated structure, erected at an agricultural research centre near Dundee, could play a significant part in its quest to help combat climate heating by greatly expanding the country’s forest cover.Forestry and Land Scotland (FLS) wants to plant tens of millions of new trees in the coming years – conifers such as Norway and sitka spruce, douglas fir and Scots pine, and broadleaf varieties such as oak, alder and birch.This white cube, held up by steel ribs and girders, can help it do so at a remarkable speed and efficiency, producing saplings six times faster than it takes to grow them naturally outdoors. In the open, it would take about 18 months to bring a tree seedling up to 40-50mm in height; in these units, that growing time is about 90 days.“Essentially, this isn’t a building. It’s a machine; it’s a growing machine,” said Georgia Lea, a communications manager for Intelligent Growth Solutions (IGS), the Edinburgh-based firm that has designed the system.The “vertical farm” uses hydroponics, where plants are grown indoors in very tightly controlled conditions. The type of light, temperature, humidity and nutrition can be tailored for each plant, far more so than in a greenhouse or polytunnel. “It’s better than a summer’s day. It’s totally an optimal environment,” Lea said.The unit can hold eight towers each carrying 52 trays of seedlings. Photograph: Murdo MacLeod/The GuardianIn the IGS unit at the James Hutton Institute, an agricultural sciences research centre at Invergowrie, technicians use iPads to control stacks of tightly packed shelves held in a cluster of automated towers standing 9 metres tall. Robots deliver trays of seedlings to the shelves. On the underside of each shelf, rows of LED lights shine on the chillis, birch, alder, strawberries and basil on the shelf below, tuned to provide the exact spectrum of light each crop needs.Plastic drainpipes feed in water that carries fertiliser, with doses controlled by computer. The water is harvested from Tayside’s copious rainfall, cleaned and reused in a closed-loop system. The building is entered through a pressurised airlock designed to protect its microclimate.This particular unit covers 42 square metres and can hold eight towers, each carrying 52 trays of seedlings. In theory, that can allow FLS to grow 3m seedlings at a time, at a significantly faster pace and using a fraction of the area needed for conventional growing.FLS, previously known as the Forestry Commission, believes Scotland would be the first country in the world where the state forester uses hydroponics for its tree stocks. It hopes the Scottish government will soon agree to approve the purchase of one of these multimillion-pound units.After running three batches designed to prove the concept worked for trees, tweaking the “recipe” of light and nutrition to suit each species, FLS is on its sixth trial run. Many thousands of spruce, pine and broadleaves from earlier trials are now “hardening off” for years at its open-air nursery at Newton near Elgins, before being taken to the plantations carpeting parts of the Highlands.FLS ran three batches designed to prove the concept worked for trees, and is now on its sixth trial run. Photograph: Murdo MacLeod/The Guardian“We came along, looked at the system and were blown away by it. It has real potential,” said Kenny Hay, FLS’s tree seed resource manager.Until it began working with FLS, IGS had been growing pak choi, carrots, seed potatoes and herbs, alongside edible nasturtiums, marigolds and strawberry runners, as samples to show to potential buyers of its systems.Sign up to Down to Earth Free weekly newsletter The planet's most important stories. Get all the week's environment news - the good, the bad and the essential Privacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy . We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply.The tree trials threw up one problem: earlier batches grew too fast, leaving saplings too soft to withstand the wind once they were planted out at Newton. FLS and IGS have slowed things down, to ensure the saplings are stronger just above their roots. “We’re doing everything and anything to see what this system does,” Hay said.These experiments have a much greater success rate than normal methods, he said. Traditionally, seeds would be scattered by machine across a nursery bed, known as broadcast sowing. Up to 50% of those seeds may fail to produce saplings. In these optimised towers, the survival rate is about 95%.Hay predicts that if FLS buys and runs its own growing tower, it could produce up to 60% of the 24m new trees the agency needs each year – chiefly commercially planted conifers to meet the UK’s timber needs, of which 80% is currently imported.Alder seedlings at the vertical farm. Photograph: Murdo MacLeod/The GuardianIn the Cairngorm mountains to the north of Invergowrie, conservation-minded landowners such as the National Trust for Scotland at Mar Lodge, or Anders Povlsen at Glenfeshie, are allowing their forests to regenerate slowly using naturally fallen seeds, a strategy that fits far more closely with the ethics of rewilding.FLS feels a greater sense of urgency. The UK’s central and devolved governments have set a challenging target of planting 30,000 hectares (74,131 acres) of new forestry every year by 2025, to help meet their net zero ambitions.The latest official data shows the UK is some way off meeting that target. Planting rates fell sharply 20 years ago, and the UK failed to restock the mature forests that had been felled.With that in mind, Hay is delighted by the potential of vertical farming. “These are great-looking trees. I have no issues whatsoever that we can make these trees work,” he said.",âA growing machineâ: Scotland looks to vertical farming to boost tree stocks
327,7,7_food_farmers_agriculture_production,https://www.cnbc.com/2022/10/24/beyond-meats-steak-substitute-coming-to-grocery-stores.html,"Beyond Meat is launching a steak substitute in grocery stores on Monday.The new product will roll out nationwide at more than 5,000 Kroger and Walmart stores, as well as Albertsons , Ahold Delhaize , Jewel-Osco, Sprouts and other local grocers.The announcement concludes a rocky month for the maker of meat alternatives. Beyond ousted Chief Operating Officer Doug Ramsey after he was arrested for allegedly biting another man's nose. The company also announced plans to cut 19% of its workforce, or roughly 200 employees, as well as the departure of its chief financial officer and the elimination of the chief growth officer role.Amid the chaos, Beyond and Yum Brands' Taco Bell started testing meatless carne asada using its Beyond Steak product at restaurants in Dayton, Ohio.The Beyond Steak that will be sold in grocery stores comes packaged in bite-sized pieces. It uses faba bean protein as its base and contains 21 grams of protein per serving, according to the company. It also has lower saturated fat content than beef steak and contains no cholesterol. The 10-ounce package will retail for $7.99.Historically, product launches have boosted Beyond's sales, driving new and returning customers to try the item. That would be welcome news for the company, which has seen its sales slide and investors lose hope in its long-term growth prospects.In the second quarter, Beyond reported U.S. grocery sales rose just 2.2% while restaurant revenue was off 2.4%. This year, shares of the company have lost 80% of their value, shrinking its market value to $821 million. At its record high in July 2019, Beyond was valued at $13.4 billion.",Beyond Meat is rolling out its steak substitute in grocery stores
31,7,7_food_farmers_agriculture_production,https://edition.cnn.com/2022/09/17/business-food/purple-tomato-gmo-scn-trnd/index.html,"CNN —It tastes like a tomato, smells like a tomato, and even looks (mostly) like a tomato. There’s just one catch: It’s purple.The USDA has approved a genetically modified purple tomato, clearing the path for the unique fruit to be sold in American stores next year.“From a plant pest risk perspective, this plant may be safely grown and used in breeding,” the agency said in a September 7 news release.The approval moves the purple tomato one step closer to widespread distribution. In addition to its unique color, the purple tomato also has health benefits and a longer shelf life than garden variety red tomatoes, scientists say.The tomato was developed by a team of scientists, including British biochemist Cathie Martin, who is a professor at the University of East Anglia and a project leader at the John Innes Centre in Norwich, England.Martin worked on pigment production in flowers for over 20 years, she told CNN. “I wanted to start projects where we could look and see whether there were health benefits for this particular group of pigments,” she said.The pigments that drew Martin’s interest are anthocyanins, which give blueberries, blackberries and eggplants their rich blue-purple hues. With funding from a German consortium, she decided to engineer tomatoes that were rich in anthocyanins, hoping to “increase the antioxidant capacity” of the fruits.By comparing regular tomatoes to the engineered purple tomatoes, she would be able to easily identify whether the anthocyanins were linked to any specific health benefits.To engineer the purple tomatoes, the scientists used transcription factors from snapdragons to trigger the tomatoes to produce more anthocyanin, creating a vibrant purple color.Martin and her colleagues published the first results of their research in 2008 in an article in Nature Biotechnology.The results were “stunning,” she said. Cancer-prone mice that ate the purple tomatoes lived around 30% longer than those that ate normal tomatoes, according to the study.Martin said there are “many explanations” as to why anthocyanin-rich tomatoes may have health benefits. There are “probably multiple mechanisms involved,” she said. “It’s not like a drug, where there’s a single target. It’s about them having antioxidant capacity. It also may influence the composition of the microbiome, so it’s better able to deal with digestion of other nutrients.”And in 2013, Martin and colleagues released a study that found the purple tomatoes had double the shelf life of their red cousins.Martin established a spinout company, Norfolk Plant Sciences, to bring the purple tomatoes to market. Nathan Pumplin, the CEO of Norfolk’s US-based commercial business, told CNN that the purple tomato “strikes a cord with people in this very basic way.”The distinctive purple color means that “it takes no imagination to see that it’s different,” Pumplin said. “It really allows people to make a choice.”FDA approval and commercialization are next stepsIn the past, forays into genetically modified foods have often focused on engineering crops that are more sustainable to produce, he added. But for consumers, the benefits of eating a genetically modified food are murky.“It’s very abstract, hard to understand,” Pumplin said. “But a purple tomato – you either choose or choose not to consume.” The difference between the GMO (Genetically Modified Organism) product and the non-modified tomato are stark – and the possible health benefits for consumers are also clear.Pumplin says that consumers are “warming up” to genetically modified foods across the world.“We look at the problems facing our society as far as sustainability, climate change, health tied to diet and nutrition, and what’s clear from the response from our announcement is that it’s a really important topic to a lot of people,” he said. “I’m encouraged that a lot of people are starting to relook at biotechnology in light of the important challenges.”At the same time, “GMOs are not a silver bullet,” he said. “It’s one tool in our toolbox as plant scientists, as scientists, agronomists, to improve the food production system.”The next steps for the purple tomato are FDA approval and commercialization, Pumplin said. “We need to breed excellent, delicious purple tomatoes. We need to work with producers to produce them and distribute them.”Norfolk will begin to launch limited test markets in 2023 to identify which consumers are most interested in purple tomatoes.As for the taste? The purple tomato is indistinguishable from your standard red tomato, Pumplin said.“It tastes like a great tomato,” he said.","A new, genetically modified purple tomato may hit the grocery market stands"
205,7,7_food_farmers_agriculture_production,https://techcrunch.com/2021/12/16/phytoform-climate-crop-technology/,"Climate change is affecting the way farmers grow their crops, and Phytoform, which knows all about the harsh United Kingdom growing conditions, is out to make crops more impervious.The biotechnology company with headquarters in London and Boston announced $5.7 million in funding, led by Eniac Ventures, to scale its artificial intelligence genome editing technology aimed at improving crops.Phytoform was started by William Pelton and Nicolas Kral in 2017 while the two were finishing up their PhD degrees. CTO Kral was studying plant development biology, while CEO Pelton is a crop scientist. He told TechCrunch that his grandfather was a farmer, and he grew up hearing about crops failing due to the U.K.’s weather and other factors.“We are lucky to be where we are with genetics,” Pelton said. “You can do the whole plant genome for a few hundred dollars and synthesize the genome as well. Nic and I were PhD students and decided to give it a go, taking technology and applying it to some problems we see.”Current breeding methods to improve crops usually take decades to develop, and genetically modified organism technologies are limited, he said.Phytoform’s approach to developing new climate-resilient crops involves machine learning and genome editing technologies to determine the number of possible DNA sequence combinations in plants and identify new traits, while also reducing agriculture’s climate footprint. Then those traits are implemented directly into crop varieties using footprint-free CRISPR genome editing.The success of CRISPR technology in agriculture has been well documented over the years as a way to manipulate plant genomes to make them more pest and climate resistant and grow more consistent products.Pelton and Kral say utilizing this technology for plants couldn’t come faster as the Food and Agriculture Organization of the United Nations estimates up to 14% of food globally is lost every year and climate change will only get worse amid drought, heat and pest conditions increase in the next few years.Joining Eniac in the round were Wireframe Ventures, Fine Structure Ventures and FTW Ventures, existing investors Pale Blue Dot, Refactor Capital and Backed VC and a group of angel investors, including Jeff Dean, Ian Hogarth and Rick Bernstein.“At Eniac we are big believers in the future of sustainable agriculture with existing bets that include Vence and Iron Ox,” said Vic Singh, general partner at Eniac Ventures, in a written statement. “As the climate crisis worsens, we’ve looked closely at how computational genomics can be leveraged to reduce food waste and bring higher quality diverse produce to market. We were immediately drawn to Phytoform’s founders Will and Nick’s approach of using deep machine learning on plant genetics combined with genome editing to offer a portfolio of optimized produce to consumers.”The new funding will enable Phytoform to expand its team and boost its work introducing traits in tomatoes and potatoes so that there is greater yield and fewer crop losses along the food supply chain.The co-founders say 2022 “is going to be a great year” for the company as it prepares to bring its tomato and potato program to market. They are also working on three other programs and have expanded their geographies from just the U.S. to Australia and the U.K.In addition, they have focused efforts on partnerships with individuals and companies at the beginning of the supply chain — seed breeders and producers — and future plans to expand that to food producers.Meanwhile, the company’s initial business model will be royalty revenue from the sale of the seeds, but expects that to change as its customer base evolves all the way to the consumer, Pelton said.Phytoform started the year with six employees and has now doubled its headcount, Kral said. In addition, it has achieved good technological growth with proof of concept for its AI process both in software and web lab capabilities.“We still have quite a big process on the way to get the tomato program in the late stages of development, but we will continue to do more trials,” he added. “Current technology is taking a decade to generate new varieties, but we are proving we can cut that down.”",Phytoform plants $5.7M into climate-resistant crop technology
153,7,7_food_farmers_agriculture_production,https://techcrunch.com/2017/09/06/after-scraping-monsanto-deal-deere-agrees-to-buy-precision-farming-startup-blue-river-for-305m/,"Five months after abandoning its proposed purchase of Monsanto’s precision planting subsidiary due to anti-trust concerns, agricultural equipment giant Deere and Company announced that it will spend $305 million to acquire ag-tech startup Blue River Technology. Founded in 2011 and based in Sunnyvale, Blue River develops machine learning technology for precision farming and counts Monsanto’s venture capital arm, Monsanto Growth Ventures, among its investors.According to Crunchbase, Blue River raised a total of $30.35 million in funding and its other backers included Data Collective, Pontifax AgTech, Innovation Endeavors and Khosla Ventures.Precision agriculture refers to farming practices that rely on computer vision, machine learning and smart devices to improve yield. It’s attractive to growers because it automates labor-intensive parts of the farming process, allowing them to cut labor costs, and also reduces the waste of pesticides, water and fertilizer.Precision farming is an important part of Deere’s growth strategy and it agreed to buy Precision Planting LLC from Monsanto in November 2015, but gave up on the deal in May after the U.S. Department of Justice filed an anti-trust lawsuit. The Department of Justice claimed that the deal would have been worth about $190 million and that Deere and Precision Planting together would have held at least 86 percent of the high-speed precision planting market.Monsanto Growth Ventures participated in Blue River’s $17 million Series B, which was announced in December 2015.Though Deere is based in Moline, Illinois, Blue River’s 60-person team will remain in Sunnyvale. The acquisition is expected to be completed next month.Blue River’s See & Spray equipment uses computer vision and machine learning to help growers reduce the use of herbicides. Its technology analyzes each plant and refers to an image library to determine if it is a weed. If it is, the See & Spray then sprays herbicide directly onto the offending plant, avoiding the crop or surrounding soil. This distinguishes See & Spray from other computer vision-based weed detection methods that analyze growth patterns and colors in patches of plants and makes it much more precise. Other Blue River machines include the LettuceBot, which automatically culls lettuce plants so the others have enough space to thrive, and remote-sensing drones.In a statement, John May, Deere’s chief information technology officer and president of agricultural solutions, said the company will be able to integrate Blue River’s technology in a wide range of products. “As a leader in precision agriculture, John Deere recognizes the importance of technology to our customers. Machine learning is an important capability for Deere’s future.”","After scrapping Monsanto deal, Deere agrees to buy precision farming startup Blue River for $305M"
485,7,7_food_farmers_agriculture_production,https://www.sciencealert.com/scientists-created-living-synthetic-cells-by-harvesting-bacteria-for-parts?utm_source=ScienceAlert+-+Daily+Email+Updates&utm_campaign=78601d00d9-RSS_EMAIL_CAMPAIGN&utm_medium=email&utm_term=0_fe5632fb09-78601d00d9-364810597,"Researchers at the University of Bristol in the UK have taken a major step forward in synthetic biology by designing a system that performs several key functions of a living cell, including generating energy and expressing genes.Their artificially constructed cell even transformed from a sphere shape to a more natural amoeba-like shape over the first 48 hours of 'life', indicating that the proto-cytoskeletal filaments were working (or, as the researchers put it, were ""structurally dynamic over extended time scales"").Building something that comes close to what we might think of as alive is no walk in the park, not least thanks to the fact even the simplest of organisms rely on countless biochemical operations involving mind-bendingly complex machinery to grow and replicate.Scientists have previously focused on getting artificial cells to perform a single function, such as gene expression, enzyme catalysis, or ribozyme activity.If scientists crack the secret to custom building and programming artificial cells capable of mimicking life more closely, it could create a wealth of possibilities in everything from manufacturing to medicine.While some engineering efforts focus on redesigning the blueprints themselves, others are investigating ways to reduce existing cells to scraps that can then be reconstructed into something relatively novel.To perform this latest bottom-up bioengineering feat, researchers used two bacterial colonies – Escherichia coli and Pseudomonas aeruginosa – for parts.These two bacteria were mixed with empty microdroplets in a viscous liquid. One population was captured inside the droplets and the other was trapped at the droplet surface.The scientists then burst open the bacteria membranes by bathing the colonies in lysozyme (an enzyme) and melittin (a polypeptide which comes from honeybee venom).The bacteria spilled their contents, which were captured by the droplets to create membrane-coated protocells.The scientists then demonstrated that the cells were capable of complex processing, such as the production of the energy storage molecule ATP through glycolysis, and the transcription and translation of genes.""Our living-material assembly approach provides an opportunity for the bottom-up construction of symbiotic living/synthetic cell constructs, says first author, chemist Can Xu.""For example, using engineered bacteria it should be possible to fabricate complex modules for development in diagnostic and therapeutic areas of synthetic biology as well as in biomanufacturing and biotechnology in general.""In the future, this kind of synthetic cell technology could be used to improve ethanol production for biofuels and food processing.Combined with knowledge based on advanced models of basic biology, we could mix-and-match some structures while redesigning others completely to engineer whole new systems.Artificial cells could be programmed to photosynthesize like purple bacteria, or generate energy from chemicals just like sulfate-reducing bacteria do.""We expect that the methodology will be responsive to high levels of programmability,"" the researchers say.This paper was published in Nature.",Scientists Created 'Living' Synthetic Cells by Harvesting Bacteria For Parts
588,7,7_food_farmers_agriculture_production,https://techcrunch.com/2022/04/25/ghanaian-agtech-farmerline-to-use-new-funding-to-strengthen-its-infrastructure-help-farmers-create-wealth/,"A McKinsey and Co.A McKinsey and Co.Noting critical gaps in the region’s agri-food space, Ghanaian agritechNoting critical gaps in the region’s agri-food space, Ghanaian agritech$12.9 million pre-Series A funding$12.9 million pre-Series A fundingFarmerline was founded in 2013 byFarmerline was founded in 2013 byThe equity round was led by Acumen Resilient Agriculture Fund (ARAF) and FMO, the Dutch entrepreneurial development bank, with participation from Greater Impact Foundation. Debt lenders included DEG, Rabobank, Ceniarth, Rippleworks, Mulago Foundation, Whole Planet Foundation, the Netri Foundation and Kiva.The equity round was led by Acumen Resilient Agriculture Fund (ARAF) and FMO, the Dutch entrepreneurial development bank, with participation from Greater Impact Foundation. Debt lenders included DEG, Rabobank, Ceniarth, Rippleworks, Mulago Foundation, Whole Planet Foundation, the Netri Foundation and Kiva.Attah told TechCrunch that the agtech will use its first equity funding to build physical infrastructure like warehouses and distribution networks.Attah told TechCrunch that the agtech will use its first equity funding to build physical infrastructure like warehouses and distribution networks.“We think of ourselves as the Amazon of farmers… a digital and physical infrastructure powering a marketplace that allows the movement of goods and services to and from rural areas,” said Attah.“We think of ourselves as the Amazon of farmers… a digital and physical infrastructure powering a marketplace that allows the movement of goods and services to and from rural areas,” said Attah.“We plan to use the funding to strengthen our infrastructure, that is warehouses and distribution channels. Having a network of partners that can help us quickly move inputs like fertilizer and seeds to rural areas, and farm produce from rural areas, is important and part of what we do. We don’t intend to bring all of the logistics and storage in-house, but we want to be more efficient and that means working with the right partners,” he said.“We plan to use the funding to strengthen our infrastructure, that is warehouses and distribution channels. Having a network of partners that can help us quickly move inputs like fertilizer and seeds to rural areas, and farm produce from rural areas, is important and part of what we do. We don’t intend to bring all of the logistics and storage in-house, but we want to be more efficient and that means working with the right partners,” he said.Farmerline delivers quality farm inputs and education on the best farming practices through partner agribusinesses.Farmerline delivers quality farm inputs and education on the best farming practices through partner agribusinesses.Farmerline delivers quality farm inputs and education on the best farming practices through partner agribusinesses.Greater reachGreater reachFarmerline works with agribusinesses (usually small retail shops that stock farm inputs) to ensure that farmers get access to high-quality supplies. These shop owners, usually the first point of knowledge for the farmers, are used by Farmerline to distribute educational material and to gather farmers together for training. The partnering shops use the startup’sFarmerline works with agribusinesses (usually small retail shops that stock farm inputs) to ensure that farmers get access to high-quality supplies. These shop owners, usually the first point of knowledge for the farmers, are used by Farmerline to distribute educational material and to gather farmers together for training. The partnering shops use the startup’s“We are tapping into that network of agribusiness, and in a way, we are tapping into a network of trust — the relationship that these shop owners have with farmers to help us expand,” said Attah.“We are tapping into that network of agribusiness, and in a way, we are tapping into a network of trust — the relationship that these shop owners have with farmers to help us expand,” said Attah.The partnership with retailers, said Attah, emerged after Farmerline realized that working directly with the farmers would amount to “competing with local businesses, and it didn’t make any sense. The cost of going door to door to each farmer was really high,” he said.The partnership with retailers, said Attah, emerged after Farmerline realized that working directly with the farmers would amount to “competing with local businesses, and it didn’t make any sense. The cost of going door to door to each farmer was really high,” he said.“Working with the agribusinesses made our businesses scalable, and it also helped us make more impact especially during the pandemic when we couldn’t travel — they became our eyes and ears on the ground. We sent trucks full of fertilizer and seeds to them that they would then distribute to farmers. That model worked really well.”“Working with the agribusinesses made our businesses scalable, and it also helped us make more impact especially during the pandemic when we couldn’t travel — they became our eyes and ears on the ground. We sent trucks full of fertilizer and seeds to them that they would then distribute to farmers. That model worked really well.”Using Mergdata, Farmerline can tell the performance of their partnering agribusinesses (retail shops), and develop a credit scoring program that guides the extension of business expansion loans.Using Mergdata, Farmerline can tell the performance of their partnering agribusinesses (retail shops), and develop a credit scoring program that guides the extension of business expansion loans.According to Attah, the startup more than doubled its direct-reach last year to 79,000 farmers, up from 36,000 in 2020 and 8,000 in 2019.According to Attah, the startup more than doubled its direct-reach last year to 79,000 farmers, up from 36,000 in 2020 and 8,000 in 2019.Moreover, through third party licensing for Mergdata — which is now used by 180 clients including governments, non-governmental organizations and agri-companies to ensure transparency in their supply chain and traceability — the agtech has digitized over 1 million farmers in 26 countries across the globe. Benin, in West Africa, uses the platform as a national market information system.Moreover, through third party licensing for Mergdata — which is now used by 180 clients including governments, non-governmental organizations and agri-companies to ensure transparency in their supply chain and traceability — the agtech has digitized over 1 million farmers in 26 countries across the globe. Benin, in West Africa, uses the platform as a national market information system.","Ghanaian agtech Farmerline to use new funding to strengthen its infrastructure, help farmers create wealth"
581,7,7_food_farmers_agriculture_production,https://techcrunch.com/2022/03/16/cooks-venture-found-podcast-vc-backed-chicken/,"Most people don’t spend much time thinking about where their poultry comes from, much less what it was eating when it was alive but the feeding practices on large farms are harmful to the environment and have led to bland, unhealthy chickens. Today’s guest, Matthew Wadiak is a chef and founder and CEO of Cooks Venture, which is on a mission to create a new breed of broiler chicken that is more active, able to eat a varied diet and tastier. Through restorative agriculture and selective breeding, the Cooks Venture farm in Arkansas has bred chickens that are more heat resistant and can eat a variety of grains. As a chef and former co-founder of Blue Apron, Wadiak views it as his duty to try to change the food system for the better.Most people don’t spend much time thinking about where their poultry comes from, much less what it was eating when it was alive but the feeding practices on large farms are harmful to the environment and have led to bland, unhealthy chickens. Today’s guest, Matthew Wadiak is a chef and founder and CEO of Cooks Venture, which is on a mission to create a new breed of broiler chicken that is more active, able to eat a varied diet and tastier. Through restorative agriculture and selective breeding, the Cooks Venture farm in Arkansas has bred chickens that are more heat resistant and can eat a variety of grains. As a chef and former co-founder of Blue Apron, Wadiak views it as his duty to try to change the food system for the better.Watch the Cooks Venture videoWatch the Cooks Venture videoDon’t miss the next live episode with Shivani Siroya from Tala on 3/17 at 10 p.m. PDT/ 1 p.m. EDT.  RSVP:Don’t miss the next live episode with Shivani Siroya from Tala on 3/17 at 10 p.m. PDT/ 1 p.m. EDT.  RSVP:Take our listener surveyTake our listener surveyConnect with us:Connect with us:",The VC-backed chicken thatâs changing the way farmers breed and feed their livestock
122,7,7_food_farmers_agriculture_production,https://singularityhub.com/2022/10/26/from-pitless-cherries-to-softer-kale-this-startup-is-using-crispr-to-make-better-produce/,"Ninety percent of American adults don’t eat enough fruits and vegetables, opting for fast food and processed foods instead. Cost, flavor, and convenience are all factors in this imbalance, but as health statistics show, we should be working harder to reverse our dietary trends.A startup called PairWise is out to help change the way we eat by making fruits and vegetables more appealing. The company is zeroing in on traits that may deter people from consuming produce and tweaking those traits using CRISPR gene editing. Their hope is that the resulting products will not only pique consumers’ interest, but keep them healthy and keep them coming back. Tom Adams, PairWise cofounder and CEO, shared details about the company and its products in an interview.CRISPR’d ProduceCRISPR was first used to edit bacterial DNA in 2012. Since then, scientists have used the tool to edit the genomes of crops, animals, and even humans, and have begun testing it as a means of curing inherited conditions from blindness to muscular dystrophy and other genetic diseases.CRISPR is made up of a synthesized sequence of guide RNA that matches a target DNA sequence—that is, the portion of DNA to be altered—and a Cas enzyme. Once in a cell’s nucleus, the guide RNA links up with the target DNA sequence. The Cas enzyme cuts the DNA at that point, and the cell repairs the cut. The repairs can either knock out a gene, inactivating it, or insert a new sequence.Modifying a gene that encodes for a given trait either eliminates or alters that trait; in the case of fruits and vegetables, say, the bitterness of mustard greens or the seeds in blackberries. Given that the genomes of PairWise products are modified, some consumers may want to know whether the fruits and vegetables are classified as genetically modified organisms (GMOs).The short answer is no. The USDA doesn’t regulate gene-edited plants as long as their traits could have occurred through traditional breeding methods or a whim of nature. The CRISPR technique PairWise uses involves manipulating genes that exist naturally in a given species’ genome. “The changes PairWise has made in our greens are no different than what can be achieved through conventional breeding, contain no foreign DNA, and therefore are not considered GMOs,” Adams said.GMOs, on the other hand, can contain genes from other species, and wouldn’t come about naturally even after decades of traditional breeding. Bt crops, for example, are engineered to contain a natural form of pesticide derived from bacteria, which means they don’t need to be sprayed with chemical pesticides.Adams pointed out that the anti-GMO sentiment out there isn’t necessarily about resistance to the technology itself. “There’s a lot of reasons GMOs may be less popular, and one of them is that people didn’t feel there was transparency,” he said. “Most of the products that fall under the category GMO are things that get added to foods as ingredients, and nobody knew when they were getting it and when they weren’t, and it created this stigma.”He wants PairWise to take a more proactive and transparent approach. “We’re going to be very clear about the processes we’re using to create the products, and it’s your choice whether you like the benefits or you’re worried about the technology,” he said.Next-Gen ProduceThe first product PairWise will bring to market is a milder-tasting version of mustard greens. “Almost four years ago, we were searching for things we could do that were amenable to the technology but also were addressing a consumer need,” Adams said.The company’s market research found that people often ended up buying romaine lettuce even after saying they’d prefer kale or another green because of their greater nutritional value. “People want healthy salads, but they keep buying romaine because they’re used to the flavor,” Adams said. Ease of preparation is a factor too.Mustard greens, Adams told me, are a relative of kale, but they taste like horseradish when you bite into them. Two components come together that react and cause the horseradish flavor. PairWise used CRISPR-Cas12a to edit the green’s genome and remove one of those components. “It’s really just removing something that the plant doesn’t need for survival and doesn’t contribute to the nutritional benefits,” Adams said. The mustard greens have already been approved by the FDA and will start being sold in California and the Pacific Northwest in early 2023 under the brand Conscious Foods.The company’s not stopping there, though; they have multiple fruit and veggie improvement projects underway.One is a softer version of kale. If you’ve ever made a kale salad from the stuff that comes still on the stalk, you know it’s labor-intensive: there’s the washing, the de-stemming, the chopping, the massaging… (that’s right, massaging!). PairWise kale will retain all of the leafy green’s nutritional properties, but have a texture more like lettuce, making it easier to prepare and eat.Another is seedless berries, including blackberries and raspberries. Hate how those tiny seeds get stuck in your teeth, always in the hardest-to-reach places? CRISPR to the rescue.One of the company’s most intriguing projects, in my opinion, is pitless cherries.“I love cherries, but they’re a pain to eat,” Adams said. “Your fingers are all red when you’re done with a few of them, and if there’s not a trash can nearby you don’t know what to do with the pit.” I asked him how it’s possible to grow a cherry—or any other stone fruit, like plums, peaches, or apricots—without the pit, as it connects to the fruit’s stem and is its lifeline to the tree.“It’s easiest to think about it like a seedless grape,” he said. “It actually still has a seed in it, but the seed has lost its hard outer shell. There’s still that nutritious plant embryo that’s normally protected by the shell, we’re just making it so it’s all edible.” If they succeed, eating pitless cherries will be a different experience altogether; isn’t having to remove the pit the only thing keeping us from stuffing handfuls of the scrumptious fruit into our mouths at once?A Whole New PlantWhen it comes to cherries, it’s not just the end product PairWise is focusing on. Currently 90 percent of sweet cherries are grown in Washington state, where there’s little to no rain in the summer. The fruit is highly sensitive to changes in moisture and can only thrive in a dry climate; this specificity and the fact that the fruit needs to be shipped from the far northwest corner of the country pushes up its price. But what if cherries could grow in, say, Michigan, or Kansas, or Vermont?“We think we understand the genetics well enough that we could modify the architecture of the tree so that it’s more like a blueberry bush,” Adams said. “And then you can grow them in more environments and put them at less risk.”PairWise is also trying to alter the way blackberries grow. The moisture sensitivity has already been taken care of by nature, as has the bush-not-tree situation—but the bushes have thorns, and thorns make fruit hard to harvest. The company is working on cutting out the thorns.Here’s an idea: blueberry-sized cherries without a pit that grow on bushes without thorns in any climate. Seems like a product that would need a whole new name, and heck if I have any suggestions. (Not to mention, it probably wouldn’t occur naturally no matter how many generations you waited.)Getting HealthierBe it softer kale, pitless cherries, or thornless bushes, PairWise’s mission is to create a healthier world by taking away barriers to eating fresh produce. “We’re interested in anything that moves the needle on the fact that only 10 percent of Americans eat the recommended amounts of fruits and vegetables,” Adams said. “It really doesn’t change very much just by telling people they should eat more of them. Our idea is, you need to take down the barriers.”It’s certainly a noble aim. But how likely is engineered produce to play an instrumental role in changing consumers’ habits, or in attracting a previously-veggie-averse demographic? People who already buy and eat kale on a regular basis may opt for a smoother kale 2.0, but people who have never bought kale may not be so easily swayed by a newfangled version of the green.Adams believes there’s a consumer base out there who will benefit from products like PairWise’s. “There are people who are looking for a healthy lifestyle,” he said. “And they’re looking for something different in salads. We’re coming in with a new category: a nutritious green that still tastes good.”Physical traits of produce may be one barrier that deters people from buying them, but cost is equally important (if not more so). With years of research and development going into CRISPR’d produce, my assumption was that it won’t be affordable. Ten dollars for a bag of lettuce-like kale? No thanks.Not the case, according to Adams. “There’s a fairly wide range of pricing within the salad space. We expect to be in the top quartile of the pricing, but we’re not going to be above it,” he said. Production of PairWise greens, he added, is actually quite cost-competitive with other types of salad greens.Based on marketing activation events the company ran over the summer in Seattle, Austin, and Palo Alto, the outlook for their first product looks pretty rosy. They gave away bags of salad (which were clearly labeled as being gene-edited) consisting of red- and green-leaf mustard greens, and asked people to complete a short survey about it. Adams estimated that more than 6,000 people tried the salads, and over 90 percent responded that they were “very motivated” or “somewhat motivated” to buy the product.A New Green Revolution?Helping people make healthier dietary choices is just one benefit that CRISPR could bring to produce. Its possibilities are wide-ranging, as evidenced by PairWise’s work to create fruit trees that can grow in different climates and yield food that’s easier to harvest. It’s not unlike Norman Borlaug’s work back in the 1940s to create a high-yield wheat seed that was resistant to stem rust—a project that ended up saving millions of people from hunger and famine.The difference is that technology has taken over the painstaking, time-consuming steps Borlaug had to slog through, like pollinating and inspecting thousands of plants by hand (a hundred and ten thousand in just one growing season! Talk about labor-intensive).Adams sees PairWise’s work similarly and believes CRISPR holds all sorts of possibilities for a new frontier of engineered foods. “We’re doing the same thing as traditional plant breeders, but it’s just faster,” he said. “We could create a lot more resilience for the whole food system.”Image Credit: Anrita from Pixabay","From Pitless Cherries to Softer Kale, This Startup Is Using CRISPR to Make Better Produce"
576,7,7_food_farmers_agriculture_production,https://techcrunch.com/2022/03/14/supplant-series-b/,"Agtech startupAgtech startupIn a world where under-watering has far more damaging results than over-watering, a non-tech-enabled farmer may be tempted to keep the soil wetter than it needs to — wasting a tremendous amount of precious water in the process. By carefully measuring the plants, and pairing their status with weather and soil data, it can give very precise watering needs. The company just raised $27 million to continue fertilizing and watering its own growth trajectory.In a world where under-watering has far more damaging results than over-watering, a non-tech-enabled farmer may be tempted to keep the soil wetter than it needs to — wasting a tremendous amount of precious water in the process. By carefully measuring the plants, and pairing their status with weather and soil data, it can give very precise watering needs. The company just raised $27 million to continue fertilizing and watering its own growth trajectory.When the agricultural revolution happened 12,000 years ago, farmers started tracking how crops grew based on various weather patterns and broad-stroke variability in weather forecasting, etc. A lot of these patterns are being disrupted by climate change.When the agricultural revolution happened 12,000 years ago, farmers started tracking how crops grew based on various weather patterns and broad-stroke variability in weather forecasting, etc. A lot of these patterns are being disrupted by climate change.“The ability to make“The ability to makeThe company has two products: a software-only solution and a hardware-and-software product. The hardware product measures one plant per 25 acres or so, and uses this data to extrapolate the needs of all the crops in the field. The solution uses five different sensors — deep soil, shallow soil, trunk, leaf and fruit. Each of the sensors feeds data into an algorithm, which also takes into consideration weather patterns, weather forecasts, soil information and other proprietary data, to give advice about how to water crops strategically over the next 10 to 14 days.The company has two products: a software-only solution and a hardware-and-software product. The hardware product measures one plant per 25 acres or so, and uses this data to extrapolate the needs of all the crops in the field. The solution uses five different sensors — deep soil, shallow soil, trunk, leaf and fruit. Each of the sensors feeds data into an algorithm, which also takes into consideration weather patterns, weather forecasts, soil information and other proprietary data, to give advice about how to water crops strategically over the next 10 to 14 days.“We learned the specific patterns of the trunk and — later on the season — of the fruit itself through tweaking the irrigation recommendations that we’re giving. Our system learns the optimal patterns and irrigation regimes. It keeps the plant and fruit on the maximized growth pattern that the plant is capable of,” explains Ben Ner. “From there, it starts to get into specifics of their specific regimes to increase sugar levels. For wine grape growers, for example, you want to stress the plant at a certain time, three weeks from now, to encourage the plant to accumulate sugar. The more sugar in the grape, the better the wine. Basically, the end result is dramatic enhancements of yields. Our main goal is to increase and enhance produce, but the byproduct is that we save a lot of water.”“We learned the specific patterns of the trunk and — later on the season — of the fruit itself through tweaking the irrigation recommendations that we’re giving. Our system learns the optimal patterns and irrigation regimes. It keeps the plant and fruit on the maximized growth pattern that the plant is capable of,” explains Ben Ner. “From there, it starts to get into specifics of their specific regimes to increase sugar levels. For wine grape growers, for example, you want to stress the plant at a certain time, three weeks from now, to encourage the plant to accumulate sugar. The more sugar in the grape, the better the wine. Basically, the end result is dramatic enhancements of yields. Our main goal is to increase and enhance produce, but the byproduct is that we save a lot of water.”The company closed a $27 million round led byThe company closed a $27 million round led byIn addition to the hardware-based product, the company recently launched an API product, a sensor-less technology that has served 500,000 maize farmers in Kenya over the last season. SupPlant is making its technology available to these smallholder farmers by changing the basic concept of irrigation methods. The new technology is designed for the world’s 450 million small growers. In 2022, SupPlant’s aim is to get more than a million African and Indian smallholders on its platform.In addition to the hardware-based product, the company recently launched an API product, a sensor-less technology that has served 500,000 maize farmers in Kenya over the last season. SupPlant is making its technology available to these smallholder farmers by changing the basic concept of irrigation methods. The new technology is designed for the world’s 450 million small growers. In 2022, SupPlant’s aim is to get more than a million African and Indian smallholders on its platform.The SupPlant app can help farmers figure out how to respond to extreme weather events to help give plants as good a chance to thrive as possible.The SupPlant app can help farmers figure out how to respond to extreme weather events to help give plants as good a chance to thrive as possible.The SupPlant app can help farmers figure out how to respond to extreme weather events to help give plants as good a chance to thrive as possible.“After a couple of years on the market, and with tens of thousands of sensors deployed, we have millions of irrigation events, covering more than 33 crops, with over 200 varieties in any geography you can imagine, and in every climate condition you can imagine. So the fundamental value of the product today is the fact that we own the most unique database of irrigation on Earth. The hardware will be an enabler for that,” explains Ben Ner. Obviously, the hardware solution is more accurate than software alone, but as the company gathers more and more data, it is able to extrapolate how various crops will perform. “Our main markets — that is, where most of the sales teams are located — are Australia, Mexico, South Africa and Argentina. We are just on the verge after a super successful proof-of-concept, of signing 100% of the dates in UAE. That comprises 2.1 million trees. Our solution enables the UAE to save 70% of its water consumption. The amount that we are able to save per tree is the consumption of 10 people in the Emirates — one of the aridest places on Earth.”“After a couple of years on the market, and with tens of thousands of sensors deployed, we have millions of irrigation events, covering more than 33 crops, with over 200 varieties in any geography you can imagine, and in every climate condition you can imagine. So the fundamental value of the product today is the fact that we own the most unique database of irrigation on Earth. The hardware will be an enabler for that,” explains Ben Ner. Obviously, the hardware solution is more accurate than software alone, but as the company gathers more and more data, it is able to extrapolate how various crops will perform. “Our main markets — that is, where most of the sales teams are located — are Australia, Mexico, South Africa and Argentina. We are just on the verge after a super successful proof-of-concept, of signing 100% of the dates in UAE. That comprises 2.1 million trees. Our solution enables the UAE to save 70% of its water consumption. The amount that we are able to save per tree is the consumption of 10 people in the Emirates — one of the aridest places on Earth.”Update: A previous version of this article listed total funds raised at $45 million. That has been corrected to $46 million.Update: A previous version of this article listed total funds raised at $45 million. That has been corrected to $46 million.",SupPlant is an Internet of Trees solution dramatically reducing irrigation needs for thirsty crops
263,7,7_food_farmers_agriculture_production,https://techcrunch.com/2022/10/10/agriwebbs-software-seeks-to-boost-yields-lower-environmental-impacts-for-farmers-and-ranchers/,"AgriWebb is on a mission to help livestock producers feed the world efficiently, profitably and sustainably by providing its comprehensive, ground-truth database for beef production worldwide.The Australian startup, which builds a livestock management platform for ranchers and farmers, wants to digitize farm records and the meat production process from the cow to the consumer and drive the industry’s animal and environmental welfare transparency.The startup said today it has raised another $6.8 million of funding led by Germin8 Ventures and iSelect Fund. In total, AgriWebb has raised $27 million in Series B and about $29.3 million since its inception in 2014. It did not disclose its valuation when asked.Its app allows users to visualize their operations and give insights on animals and grazing, including the best grass location and which animals gain weight. On top of that, it lets ranchers improve their sustainable land management for better profits and leverage the on-farm data they’re recording to make more intelligent business decisions, according to the company.AgriWebb claims more than 16,000 farmers and ranchers globally are using its cloud-based platform and managing approximately 19 million animals on over 136 million acres of grazing land across the globe, including Australia, the U.S. and the U.K.The global beef market is estimated at $500 billion, but the pure farm management software market in its core geographies is estimated at around $3.5 billion, the company executive chairman Justin Webb told TechCrunch. AgriWebb’s key markets include Australia — where more than 15% of the national herd is managed using its platform — the U.K. and the U.S. Additionally, AgriWebb has partnerships in Brazil and South Africa, Webb said.Unlike most competitors who act as a point solution focused on one or two areas of farm management, AgriWebb’s platform brings together animal management, grazing management and team communication; task and compliance management; and daily record-keeping in one place, Webb explained. “Its grazing insights enable ranchers to maximize productivity, eliminate waste, and validate grazing and animal management decisions in a way that other record-keeping systems can’t touch,” Webb pointed out.Webb founded AgriWebb with John Fargher (chief revenue officer) and Kevin Baum (chief executive officer) in 2014. In Australia, the three founders discovered that farmers were not only interested in the advantages of technology but were also desperately cobbling their own solutions with scrappy spreadsheets and notebooks.“Livestock producers deserve better technology to help them maximize their business and consumers need more reliable provenance for the animal and environmental welfare of their food,” Webb said. “AgriWebb has always been about serving the farmers, and this round of funding doesn’t change our mission; it simply magnifies it.”The latest funding will be used for the international expansion of AgriWebb to ranchers and farmers in the U.S., the U.K., and Latin America, both directly and via partnerships. AgriWebb has secured customers in 28 of the 50 states since its U.S. launch in 2021 and plans to continue rapidly expanding. In addition, the latest funding will enable the company to establish its database.Apart from the funding, AgriWebb recently joined two project proposals to the USDA CSC program, one led by American Farmland Trust and the other by Farm Journal’s Trust in Food initiative, Webb said. Both aim to improve the U.S. beef supply chain’s climate footprint and scale regenerative agriculture practices, Webb continued. One project is focused on improving transparency in the beef supply chain and understanding the GHG impact of different practices; the second aims to scale the adoption of practices through payments for practice changes.“There’s a misconception that agriculture is at odds with climate, but the importance of sustainability and implementing sustainable practices is far from lost on farmers and producers,” Webb said. “In fact, the long-term sustainability and viability of their land are of utmost importance. Talk to any landholder and you’ll understand their long-term goal is to pass on their land in better condition to the next generation. Sustainable and regenerative practices can and do exist in tandem with productive and profitable farms, and we remain steadfast in our endeavor to support producers now and in the future through data that measures, manages and improves the sustainability of the food supply chain from farm to plate.”The company raised its first Series B from investors, including Telus Ventures, Grosvenor Food & AgTech and the Clean Energy Finance Corporation in January 2021.“AgriWebb fits with Germin8’s thesis to invest in the full-stack enterprise software companies within AgTech that bring essential enterprise value to farmers in alignment with practices that are sustainable,” said managing partner at Germin8 Ventures Michael Lavin. “There are very few software offerings capable of accelerating the regenerative agriculture practices our climate stands to benefit from, and even fewer that target livestock production rather than being at odds with it.”","AgriWebbâs software seeks to boost yields, lower environmental impacts for farmers and ranchers"
262,7,7_food_farmers_agriculture_production,https://techcrunch.com/2022/10/07/if-its-agtech-its-climate-change-how-the-crisis-is-shaping-investors-strategies/,"To state the (painfully) obvious: The fates of agriculture and climate change are inextricably linked.The weather dictates what grows where and when, but as the Earth warms beneath a wool blanket of excess carbon, agriculture is especially vulnerable in ways you might not expect.Record-setting heat and droughts fry grasses that farmers depend on to feed cattle, warmer temperatures are a boon for pests and fungi that harm crops, smoke from wildfires taint harvests, and extreme weather and rising seas make it more difficult to move everything (including food) around. The threats to food security and livelihoods go on and on.Undoubtedly, this has some deal-makers in tech salivating. As startups look for ways to adapt the global food system to the chaos of today, we reached out to seven agtech investors to get a better understanding of how the climate crisis has informed their strategies to date.“Climate challenges are not new to anybody operating in the broader food and agriculture space, so our approach is to invest in solutions that can help mitigate and adapt to climate change,” Yield Lab partner Camila Petignat told TechCrunch.Themes the firm looks at “include soil and water conservation, improved use of crop inputs, the shift from chemical to biological crop protection solutions and reduction of food waste,” Petignat said.“We could argue,” Petignat added, “that the increased awareness of carbon markets in recent years has triggered new opportunities at the intersection of agtech and fintech, a space that we are interested in.”“India is one of the most vulnerable countries to climate change,” Omnivore managing partner Jinesh Shah told TechCrunch. “Agriculture represents 20% of India’s GHG emissions, but the sector is also incredibly vulnerable to the impacts of climate change, which may begin to threaten Indian food security in the coming decade.” he said. Agriculture is responsible for about a quarter of global greenhouse gas emissions, per the EPA.Shah added that the firm’s strategy is to “invest in startups that align with one or more of our four key pillars — increasing smallholder profitability, enhancing smallholder resilience, improving agricultural sustainability and catalyzing climate action.” The investor went on to state that agtech in India must “evolve beyond digital technologies (farmer platforms and B2B marketplaces), and we look to agrifood life sciences for long-term solutions to climate change.”Read the full survey to learn where investors are looking to invest, what’s on their minds right now, the best way to pitch and contact them, and understand which emerging technologies have captured their attention.","If itâs agtech, itâs climate change: How the crisis is shaping investorsâ strategies"
261,7,7_food_farmers_agriculture_production,https://techcrunch.com/2022/10/07/flyfeed-angel-round/,"FlyFeed claims it signed more than $10 million worth of contracts and closed a $3 million round of investment to launch its first insect farm in Vietnam, in its push to produce low-cost, high-nutrient protein for human consumption.The company was founded less than a year ago and is targeting the three billion people who are facing food insecurity due to climate and global financial shifts. The production process uses organic leftovers as food for the black soldier fly (BSF) insects, which significantly reduces the final price of manufactured protein. As a bonus, it also helps reduce local food waste issues.“Five billion dollars invested in agtech in 2021 shows that the fragile global food chain, relying on exhaustible natural resources, faces new challenges and needs a change. In this case, insect proteins are an effective solution that can transform inedible resources into nutritious proteins. Supported by laboratory tests and approved by local authorities, we will open 10 farms in Asia and Africa by 2026 and contribute to solving the global food shortage crisis,” says Arseniy Olkhovskiy, CEO and founder of FlyFeed, in an interview with TechCrunch. “There are many things I find wrong with the world, but the fact that global hunger still exists is the most absurd. There are more than three billion people who cannot afford healthy food, and almost one billion live in food insecurity. The worst part is that this number continues to rise. Until we rethink our food chain, this won’t change because conservative agriculture isn’t designed to feed a growing population.”In the first instance, the insects will be used for animal feed and pet food, with future plans to include turning the insects into flour that can be used for human food in five years or so. The plan is for the first farm in Vietnam to produce 17.5+ thousand tons of insect products per year, including insect fat, protein flour and fertilizers, and process 40+ thousand tons of organic leftovers for BSF feed, gathered and processed for free by a partnership with local authorities.The $3 million angel round will be used to establish operations, further develop technological solutions and construct the first industrial-sized farm in Vietnam in 2023, producing affordable proteins, oils and fertilizers from insects.“With personal money invested and this seed round, we’ve been able to build an infrastructure that unlocks our scale. Our production technology was developed by our specialists and world-class engineering teams, we secured resources in Vietnam for construction and operations, we signed over $10 million in pre-contracts to verify demand and validate our product development strategy, and we built an A-class team and advisory board to support the company,” says Olkhovskiy. “FlyFeed’s goal is to feed 250 million people annually, providing nutritious and wholesome food to those who don’t have access to it or can’t afford it. This means 250 million more people can live healthier and happier lives and focus on what matters to them. And all of these while helping nature instead of harming it and providing other companies producing food with sustainable nutrients and fertilizers.”",FlyFeed flies in the face of the global food crisis
135,7,7_food_farmers_agriculture_production,https://techcrunch.com/2016/06/23/japans-e-commerce-leader-rakuten-gets-into-agriculture-tech/,"Rakuten may be cutting back its e-commerce business in Europe and Southeast Asia, but, at home in Japan, the internet giant is stepping into a new field — quite literally — after it invested in an agriculture tech company for the first.Weeks after withdrawing e-commerce sites from the UK, Spain and Austria, Rakuten is putting an undisclosed sum into seven-year-old Telefarm, which operates a platform that promotes organic farmers in Japan. Most notably, Telefarm connects consumers with organic farmers and their produce, while it also helps with processes such as storage, transportation, manufacturing, and even the hiring of farm workers.It’s an interesting move for Rakuten, which is best known for its online shopping empire in Japan, where it has also expanding into financial, a mobile service and more. Rakuten’s presence outside of Japan has been less successful. While it has invested in a range of U.S. companies including Lyft and Pinterest, and it has acquired firms like chat app Viber, video platform Viki, and U.S. coupon site Ebates, the company has withdrawn from less lucrative regions as part of a new strategy.Organic farming, or organic products, could be a part of that new focus in Japan. Rakuten said that this invest in Telefarm would lead to it launching new products in its domestic market.With the working population in the agricultural industry steadily declining, coupled with Japan’s aging population, labor shortages in the industry are becoming a serious problem, and the amount of deserted arable land is increasing every year. Farmers are facing serious challenges, with initial investment and unstable revenues posing obstacles for new farmers, and the securement of stable sales channels and a shortage of successors presenting challenges for established, small-scale farmers. Going forward, Rakuten will explore the development of new services in the agriculture field through the internet, while aiming to contribute to regional revitalization through the utilization of deserted arable land and support for new farmers.Beyond scaling back parts of its global business, Rakuten recently embraced drones — running its first trial on a golf course — and earlier this month it struck an agreement to bring a selection of premium Japanese goods to China via Kaola, the cross-border e-commerce service operated by Chinese internet giant NetEase.",Japanâs e-commerce leader Rakuten gets into agriculture tech
210,7,7_food_farmers_agriculture_production,https://techcrunch.com/2022/02/22/verdant-aims-to-be-the-robotic-king-of-carrot-weeders/,"Agtech is a massive industry aching to be disrupted by robotics. It’s a big category that’s going to require a number of solutions to various problems, though lately it’s been hit by a number of false starts. Abundant Robotics, for instance, went under, only to be resurrected by a new brand currently exploring the equity crowdfunding route. Meanwhile, strawberry-picking Traptic quite literally took itself out of the field, getting snapped up by Bowery for vertical farming applications.Founded in 2018, East Bay-based Verdant Robotics has raised $21.5 million to date, including an $11.5 million Series A, back in 2019. The firm appears to be casting its net fairly wide, though it’s starting specifically with carrot crops, offering up its system through a RaaS (robotics as a service) model to select farmers.The model seems to make the most sense for these systems — at least during the early stages. Likely most important to a company like Verdant is getting as many of its robotic systems as possible out to farms for real-world testing and data collection. Rental is a much easier way to do that, versus selling them outright — especially for an as of yet early technology, which likely requires technicians to implement (or at least monitor) these systems.The “autonomous farm-robot” does dual-duty — spraying and laser weeding — while building AI-based crop modeling, designed to give farmers a better picture of what their plants crave.“Farmers told us not to give them more data, but to figure out what to do with the mountains of data they already have, or better yet just go do it,” Verdant co-founder and CEO Gabe Sibley says. “They want a complete solution that takes action in real time and keeps farmers in control — all while improving profitability and automating dangerous, back-breaking field work.”The company says it logged “thousands” of hours over the course of 2021 and plans to fully commercialize a “precision multi-action machine for orchard” next year.",Verdant aims to be the (robotic) king of carrot weeders
165,7,7_food_farmers_agriculture_production,https://techcrunch.com/2019/03/07/vcs-have-growing-appetite-for-agrifood/,"Venture investors are pouring billions of dollars into feeding their hunger for food and agriculture startups. Whether that trend line is due to enthusiasm for the sector or just broader heavy investing in the VC space is much less clear.According to a recent report published by AgFunder – a VC and investing marketplace focused on the agriculture and food sectors – the “AgriFood” space is booming. Using data from Crunchbase and several other data partners, the organization published its “2018 AgriFood Tech Investing Report” this morning, finding that investment in AgriFood companies increased 43% year-over-year, reaching $16.9 billion in 2018.AgFunder classifies AgriFood tech as “the small but growing segment of the startup and venture capital universe that’s aiming to improve or disrupt the global food and agriculture industry.” Their definition is intentionally broad, encompassing everything from crop and livestock biotech, property management systems, and payments, to biomaterials and meat alternatives, all the way up to tech platforms for restaurants, grocers, deliveries and at-home cooks.While some of the AgriFood tech categories – such as delivery or restaurant software – have long been popular destinations for venture capital, we’re now seeing a more diverse array of startups innovating across the entire food supply chain. According to the report, expansion in AgriFood is fairly consistent across upstream (agricultural and farming) subsectors to downstream (more consumer-facing) subsectors, with each group growing roughly 44% and 42% year-over-year respectively.The data also shows growth occurring across almost all deal stages. AgriFood saw huge increases in the average deal size and total investment for late-stage companies in particular, as venture-backed startups have grown to global scale. And penetrating and attracting capital from international markets seems more feasible than ever. AgriFood investing, which traditionally has been largely US-centric, is rapidly becoming a global phenomenon, with more than half of total funding – and some of the largest rounds – now coming from companies and investors outside the US.",VCs have growing appetite for âAgriFoodâ
249,7,7_food_farmers_agriculture_production,https://techcrunch.com/2022/09/05/solar-foods-solein/,"Fermentation has a long, rich history in food production, from beer and wine to yogurt and cheese, leavened bread and coffee, miso and tempeh, sauerkraut and kimchi, to name just a few of the tasty things we can consume thanks to a chemical process thought to date back to the Neolithic period. But if this 2017-founded Finnish startup, Solar Foods, has its way, fermentation could have a very special place in the future of human food too.The industrial biotech startup is working on bringing a novel protein to market — one it says will offer a nutritious, sustainable alternative to animal-derived proteins. The product, a single-cell protein it’s branding Solein, is essentially an edible bacteria; a single-cell microbe grown using gas fermentation. Or, put another way, they’re harvesting edible calories from hydrogen-oxyidizing microbes.“Technically it’s like a brewery,” explains CEO and co-founder Dr. Pasi Vainikka in an interview with TechCrunch. “Like fermentation technologies are. It’s not that strange [a process] — there is this one difference, which is the feedstock.”The production of Solein requires just a handful of ‘ingredients’: Air, water and energy (electricity) — which means there’s no need for vast tracts of agricultural land to be given out to making this future foodstuff. It could be produced in factories located in remote areas or inside cities and urban centers.Nor indeed are other foods needed to feed it to create an adequate yield, as is the case with rearing livestock for human consumption. So the promise looks immense. (As Vainikka argues: “Land use and energy use are the two main problems of human kind — and the rest follows from these two.)Nutritionally speaking, Solein resembles some existing foodstuffs — sitting between dried meat, dried carrot or dried soy in terms of the blend of vitamins, amino acids, proteins (overall, it’s 65% protein), per Vainikka. “So it’s very familiar but it’s a bit [of a] new combination,” he suggests, adding: “The taste is very mild, very neutral.” (A mild taste may not sound especially scintillating for the tastebuds but it means it’s easy to include as an ingredient in a wide range of foods without the need for a strong flavor to be masked.)While Solar Foods has essentially discovered a new species through its fermentation process, the microbe itself obviously hasn’t just appeared on planet Earth — and is likely very ancient; perhaps even hundreds of millions of years old. So there’s a fascinating blend of old and new coming together in the startup’s bioreactor.Why is finding new forms of protein important? The problem Solar Foods is aiming to tackle is that the environmental costs of livestock-based meat production are indisputably massive — whether you’re talking unsustainable land and water use; climate-heating emissions and pollution; or animal welfare concerns. But what if you could produce billions of nutritious meals without the need to deforest huge swathes of land and slaughter masses of livestock to produce the food? What if humanity could feed itself and stop consuming the planet in the process?That’s the promise and the core differentiator that Solar Foods claims vs. animal-derived proteins.If you compare Solein to the growing gaggle of plant-based meat alternatives, they do still rely upon land being farmed to produce the necessary plants — whether soy or pea or oat, etc. — that form the basis of their products. Although they need far less land than meat production requires so the environment upside is still very real. But Solar Foods sees itself blending into this competitive mix — selling Solein to companies producing plant-based foods as another ingredient they can use to cook up nutritious, environmentally friendly meals.“Cereals, vegetables, fruits, herbs aren’t going anywhere,” says Vainikka, discussing how Solein might fit into an evolved food production system. “So if we go back to the original problem — 80% of all the problems that have to do with food, whether it’s loss of natural habitat or forest loss or whatever, has to do with the industrialized animal production … So actually Solein could solve 80% of the problem but 20% of the calories because mostly we are, on a calorie basis, eating carbohydrates.”And if you’re excited about the promise of lab-grown meat — which is also seeking to delink protein production from land use — Vainikka says the startup is supportive of such efforts since, once again, it’s spying potential customers as he says cultivated/lab-grown meat producers could use Solein to feed the cell cultures they’re using to grow slaughter-free steaks.So use cases for Solar Foods’ edible bacteria look broad, provided people are willing to eat it (or have it fed to something in their food chain). Conceivably it could even be used as a feedstock for livestock — although the startup’s messaging is focused on the need to transform a broken food system and enter “the era of sustainable food production,” as its website puts it.It is also working on developing a closed-loop system in which the sole byproduct of its production process — water containing bits of the Solein protein — would be continuously recycled back into production of more of the foodstuff. And if it can pull that off, the edible bacteria could potentially function as a life support system for humans on space missions where the timescales are too long for astronauts to rely on food supplies brought with them from Earth (such as, for example, a mission to Mars).“The specific thing that we think is different in what we’re doing — compared to anything else on the market today — is that we don’t use any agriculture in the foods,” Vainikka tells TechCrunch. “Electricity and carbon dioxide are the main ingredients — instead of sunlight and carbohydrates or oils. So that’s the fundamental point where the disconnection of food production from agriculture happens.“That’s our thing. And the reason to do that is once you can delink the connection between use of land and land-use impacts and food production then basically all the environmental benefits fall on your lap that there can be in relation to food production.”Down here on Earth, being able to unhitch food production from the vagaries of seasonal weather and other factors that can have major impacts on agricultural yields — such as pests, natural disasters, issues with supply chains specific to farming and so on — is another touted advantage for Solar Foods’ approach. “Security of supply … consistency and quality,” says Vainikka, checking off some of the added advantages he says the edible protein offers vs. traditional farming, i.e., on top of the massive heap of land-delinking-based environmental gains which could — for example — support a mass reforestation of farm land, promoting biodiversity and fighting global warming since trees suck up CO2.Europe’s energy crisis bitesSolein looks like a no-brainer on the environmental front. But one key component of its production — energy, i.e., electricity — is facing supply issues of its own in Europe at present in the wake of Russia’s invasion of Ukraine. (Russia being a major but unreliable supplier of gas to Europe.)Solar Foods’ long-term bet is on energy production costs being brought down (or, well, stabilized) by widespread access to cheap renewables — such as wind and hydro energy in the north of Europe and solar in the sunny south. Thing is, for now, the European energy markets are typically structured so that the wholesale price of energy is linked to the cost of the most expensive type of energy (fossil fuel derived) despite there already being a fair amount of renewable energy available which is far cheaper to produce. (Hence why if the price of gas goes up the wholesale price of energy rises, and the bill payer must pay more even if their energy supplier sources their energy from cheaper to produce renewable sources.)Since the Ukraine war started, Europe has been facing an exacerbated supply vs. demand issue. And over the past several months it’s been hard for Europeans to escape energy price spikes as their governments have sought to reduce reliance on Russian gas imports — shrinking energy supply options and helping keep war-spiked wholesale prices high.The coming winter looks very grim, with Russia recently electing to entirely shutter gas exports via its Nord Stream pipeline to Germany in what looks like an attempt to weaken Western support for the pro-Ukraine sanctions. So energy supply in Europe has become a weapon of economic war.It’s an incredibly volatile situation but one thing is clear: Europe’s ‘competitive’ marginal-cost-based energy markets are in desperate need of structural reform — to reflect the cheaper production costs of renewables and ensure consumers and businesses aren’t at the mercy of fossil fuel volatility and cripplingly high prices linked to Russian aggression.But, in the meanwhile, with electricity being a key component of Solar Foods’ process, the startup is having to manage what Vainikka — who has a background in energy economics that he says allows him to understand where the markets are headed — refers to with classic Nordic understatement as “turbulence.”He suggests Solar Foods may therefore need to wait out the current energy crisis before it’s able to scale commercial production of Solein in a way that’s economically viable — though it’s banking on Europe being able to find a way through to more stable electricity prices in the not too distant future. (In recent days, the Commission has said it will be coming with an emergency reform plan to curb energy prices — both in the short term and over the longer run, to ensure prices reflect cheaper renewables.)“At the moment we shouldn’t make electricity supply agreements for our factory. We can’t be on the market today to make those agreements,” confirms Vainikka. “Because of this [energy price volatility] — it’s a fact. The second [thing] is we are quite happy that we are not fermenting natural gas — we are fermenting electricity. So we have an opportunity to make a good deal after turbulence.”“We need to replace fossil fuels with electricity so we need a lot of new generation capacity which is also a problem in the market but we’re confident that this works,” he adds. “Unfortunately there is this turbulence now.”Solar Foods is pressing on regardless of the current energy crisis.It’s in the process of building its first factory — actually a demo facility, as a step on the road to future commercial scaling up of Solein production — at a cost of around €40 million, drawing on backing from a number of VC funds since 2017, over seed and Series A rounds, as well as raising debt financing (such as €15 million from Danske Bank Growth earlier this year).The demo facility at least won’t have major energy requirements to run. (Although he says it’s still holding off on signing an energy supply contract for now.)“We’ll manage the turbulence but of course it would be better for it not to continue too long,” says Vainikka. “We’re using this demo [facility] operated by one wind turbine to prove that this scales — but the real factories would be 100x larger in terms of energy use, 50x larger — and it would need rather 50 turbines to run a huge facility that will produce half a billion meals. Then you must get a good [energy supply] contract and if we were investing into that factory now it might be postponed because of the turbulence.”Good food and food for good?With the demo factory set to come on stream in 2023, Solar Foods’ hope is the first consumer product containing Solein will be on the market by the end of next year (or, failing that, in early 2024). Which global market will get the first commercial taste of the novel protein will depend on regulatory clearances.Solar Foods has applied for clearance in multiple jurisdictions but can’t predict whether regulators in Europe or the U.S. or Asia will be first with approval, given variances in this process. (But Vainikka says it’s possible the first clearance could happen this year.)What the first product for sale to consumers that contains Solein will be also isn’t yet clear.Vainikka suggests a few possibilities — such as that it could be added to existing foods like breakfast cereals or vegan meals for fortification purposes (owing to its vitamin and mineral content, such as iron and B vitamins); or as a main ingredient in plant-based meat replacement products, replacing stuff like pea protein. Or he says it could be used as an egg replacement in pasta or pastry production. Or as a principle ingredient in ice cream or yogurt (or even to make a spreadable faux cheese).“We leave the final formulation and product development for our customers so that we can empower them to renew categories,” he suggests. “And make having a food an act for good.”“Frankly as a company we think that it might be a good idea to focus on what we master — which is this conversion-fermentation; producing this ingredient and so that it would have the functionalities needed for food products,” he continues, expanding on Solar Foods’ decision to stay in its biotech lane. “There are so many, so huge, or so experienced or so old [food] companies on the market who have already access to the consumer, all the experience regarding textures, product development regarding all kinds of plant-based ingredients and so on. So when we introduce Solein into the market you would not only need to get everything right, what we are doing and mastering now, but also the final product — of course taste and texture is decisive.”“So that’s a heavy investment program that we’ve dived into,” he adds, emphasizing the still extensive range of requirements for developing a product that’s designed even to be an ingredient in processed foods that people eat.“Nutrition must be there … then second is safety, then functionality, of course — how it works and forms texture — and then scaling and production technology; who has it, how does it work, is it scalable, and how does the supply chain work — so who’s really the gatekeeper? So this we are in the middle of now … A lot will happen in the next 12-16 months.”While Solar Foods won’t be a food product maker itself it does have an R&D lab where it carries out culinary experiments with its product — and images on its website show a selection of demo foodstuffs, from chicken-style chunks served with pasta to soup, bread and a breakfast smoothies, all with a distinctive rich yellow hue.In its refined form — i.e., after it’s passed through Solar Foods’ electrolyzing and fermenting bioreactors and been dried — Solein takes the form of a yellow powder (the hue is down to betacarotene it naturally contains).The strong color makes it looks a bit like a custom blend of turmeric and cumin. But tastewise it’s nothing like that strong. Per Vainikka, one expert taster who sampled it suggested it was akin to dried carrot. But whether you’re a fan of carrots is beside the point; he emphasizes that the taste is mild enough that it can be easily masked in whatever food product it was being incorporated into — just without the added nutrients going anywhere.For example, in the sample case of adding Solein to pasta, Vainikka says it would — nutritionally speaking — be akin to eating, say, a plate of spaghetti bolognese with all the nourishment derived from an animal-based ingredient but without the need to have any minced meat on the plate. Which, well, might take some swallowing for those used to consuming traditional (and oftentimes culturally significant) recipes. (An Italian I described this meatless but nutritionally meat-like pasta dish to at a dinner party I attended recently was visibly shocked at the prospect and a second Italian she started to explain the concept to responded by suggesting we should focus on having fun eating the actual food on our plates instead of talking about, er, such high-concept stuff, so, well, there may be some acceptance humps in the short term.)But as plant-based faux meats advance in taste and texture it’s easy to envisage creative food producers being able to whip up something that has a meat-like taste and texture and — thanks to the addition of Solein — is also imbued with similar levels of protein, iron and vitamins as actual meat. And that could be a strong selling point for consumers, especially with the current food fad for high-protein eating.Other food ideas Solar Foods has been experimenting with in its labs are ‘cheese’ ball lollypops, mayonnaises and dressings, pancakes and plenty more besides.Vainikka says he hopes the first commercial food to contain the ingredient won’t be a burger — since there are so many meat-alternative patty options out there already. But he suggests it could be a “meat-like bite” — something akin to a nugget — such as might be be served in an Asian hot pot or similar. “Then yogurt, ice cream, soup, bakery pastry application is something that might go first,” he postulates.“You could imagine it could be a frozen food, fresh or even on the street kitchen of an Asian city,” he also suggests, saying the startup is keen to branch out and “appreciate different food cultures on the planet” — so it can “try to explain how Solein could be an ingredient in different kinds of dishes from the Asian hot pots to burger patties to soups or pastries or whatever.”Food is of course not only cultural but individual tastes can be hugely personal — and/or political. So once Solein leaves Solar Foods’ factories and arrives in customers’ commercial kitchens that’s where all these localizing product and branding challenges will really kick in — as buyers will have to work on figuring out how best to blend it in with other taste and cultural considerations or indeed make its presence stick out loudly (at least on the packet) where shouting about sustainability benefits might be the best way to reap big sales in their particular target market.One thing looks clear: The future of food won’t be dull — or even uniformly yellow hued. A full rainbow of possibilities for alternative eats are coming down the pipe — and the environmental challenges we face, as a species, demand we find the appetite to consume them.",Solar Foods wants to replace industrial animal farming with a high-tech protein harvest
211,7,7_food_farmers_agriculture_production,https://techcrunch.com/2022/03/01/source-seed-funding/,"Agtech startup Source.ag today announced it harvested a $10 million investment to make greenhouses smarter. The founders have set their eyes on a horizon where, driven by climate change and a rapid increase in global food demand as population continues to increase, more crops are being forced indoors to secure greater crop yields. You wouldn’t believe the amount of restraint it takes to not make a pun about seed funding in a piece about greenhouses. I have no such restraint, so let’s report on this, ahem, growth industry.The $10 million funding round was led by Acre Venture Partners, with participation from the E14 fund and food-focused venture firm Astanor ventures. The company also raised from industry insiders, including the international association of (mostly) salad growers Harvest House, tomato specialists Agrocare and bell pepper specialists Rainbow Growers.The company is developing software to empower greenhouses to get smarter. The company argues that greenhouse agriculture is a safer, more reliable and more climate-resilient mode of food production, producing up to 15 times higher yields, using a twentieth as much water as traditional agriculture. What Source adds to the mix is the ability to use data and AI to help greenhouses operate at even higher levels of efficiency and repeatability of high-yield harvests.“Climate change is driving substantial scarcity and strains in our global food supply. As this accelerates in the coming years, we must find ways to scale efficient growing solutions that lighten the footprint of agriculture,” said Lucas Mann, managing partner at Acre. “Greenhouse agriculture is a proven and viable solution, but without innovation, demand will be impossible to meet. We believe Source.ag can play a vital role in driving its global scalability.”The funding will be used to accelerate product development and expand commercial collaborations.“Greenhouses come in all shapes and forms — both more and less technically advanced. On the higher-tech side, want to have control over every dimension you can imagine, including humidity, irrigation and nutrition. Tomatoes, for example, don’t grow in soil. They go in substrate slabs. That means that these operations are arable land independent,” explains Rien Kamman, co-founder ad CEO at Source. “Because you have more control, you’ll have to make more decisions every day. Growers are making decisions on 60-70 parameters every day, which influences how this crop will grow for the rest of the season. You need to get the decision right every day. This might include what to feed the plant, plant-specific parameters, pruning, etc. It’s really a craft and this is why it’s still so hard. You need decades of experience to be doing well at it.”The complexity of the farming operation itself isn’t in doubt, and Source’s pitch is to take all of these growth parameters, combine that with historical crop yield data and market pricing etc., to create a better experience for the growers.“Our system is comprised of two aspects. One is a recommendation system that assesses the current state of the plant. It looks at forward-looking predictors like resource prices, weather, etc. And then gives very concrete recommendations to the grower. What should you do today and tomorrow, both on the plant (i.e. how should you trim it, prune it, etc.) and on the indoor climate around the plants to maximize sustainability and production. The second part is what happens when something doesn’t go to plan? This is where the algorithms come in,” Kamman says. “They collaborate with the different control systems to take that strategy and actually make sure to do so implemented in the most efficient way possible.”Indoor farming still requires a fair chunk of manual labor, especially for big-vine crops such as tomatoes, cucumbers, peppers, etc.; but Source suggests that it can be helpful there, too — by choosing how and where to prune or how many crops to leave to ripen on the vine, you can affect various aspects of the plants’ growth. The interesting thing is overlaying real-time pricing data here — by speeding up or slowing down ripening, I can imagine that could potentially time the harvest time around when your competitors have ripe products, potentially even trading lower yields for better prices, or working with temperatures and weather conditions to reduce production costs, for example.The company is running on a SaaS model, charging growers on a sliding scale depending on the amount of space they are using for growing.“Our belief is that agriculture is at an inflection point in history. [Farming] that brought us to where we are today won’t bring us to 10 billion people with a more changing extreme climate. This is a massive market — the need for climate-resilient food systems is going to increase. And then we haven’t even talked about what other traditional agriculture crops could be moved indoors in the next decades,” says Kamman. “I think that what unites our investors and our team is that we’re not so much looking at these advantages short term, but we can build knowledge that is scalable globally.”The company declined to share screenshots of its product, stating it was “competitively sensitive”.",Source makes greenhouses smarter to secure the future of food supply
171,7,7_food_farmers_agriculture_production,https://techcrunch.com/2019/06/24/scientists-discover-a-new-way-to-provide-plants-the-nutrients-they-need-to-thrive/,"Researchers at Carnegie Mellon University have discovered a new method for delivering key nutrients to plant roots – without having to ensure they’re present in the soil where the plants are growing.The landmark study greatly increases the efficiency of surface delivery of nutrients and pesticides to plants. Currently, when crops are sprayed with stuff that’s supposed to help them grow faster or better, the vast majority of that (up to 95 percent, according to CMU’s engineering blog) will just end up either as concentrated deposits in the surrounding soil, or dissolving into ground water. In both cases, accumulation over time can have negative knock-on effects, in addition to being terribly inefficient at their primary task.This method, described by researchers in detail in a new academic paper, would manage to improve efficiency to nearly 100% absorption of nutrients and pesticides delivered as nanoparticles (particles smaller than 50 nanometers across – a human hair is about 75,000 nanometers wide, for context) sprayed onto the leaves of plants, which then make their way through the plant’s internal vascular system all the way down into the root system.Using this method, agricultural professionals could also greatly improve delivery of plant antibiotics, making it easier and more cost-effective to treat plant diseases affecting crop yields. It would be cheaper to delivery all nutrients and pesticides, too, because the big bump in efficiency of uptake by the plants means you can use much less of anything you want to deliver to achieve your desired effect.This research could have huge impact in terms of addressing growing global food supply needs while making the most existing agricultural land footprint and decreasing the need for potentially damaging expansion of the same.",Scientists discover a new way to provide plants the nutrients they need to thrive
91,7,7_food_farmers_agriculture_production,https://news.nus.edu.sg/novel-technique-to-grow-meat-in-the-lab-using-magnetic-field/,"Scientist from the National University of Singapore (NUS) have found a novel way of growing cell-based meat by zapping animal cells with a magnet. This new technique simplifies the production process of cell-based meat by reducing reliance on animal products, and it is also greener, cleaner, safer and more cost-effective.Cultured meat is an alternative to animal farming with advantages such as reducing carbon footprint and the risk of transmitting diseases in animals. However, the current method of producing cultured meat involves using other animal products, which largely defeats the purpose, or drugs to stimulate the growth of the meat.To cultivate cell-based meat, animal cells are fed animal serum – usually foetal bovine serum (FBS), which is a mixture harvested from the blood of foetuses excised from pregnant cows slaughtered in the dairy or meat industries – to help them grow and proliferate. This is a critical, yet cruel and expensive, step in the current cell-based meat production process. Ironically, many of these molecules come from the muscles within the slaughtered animal, but scientists did not know how to stimulate their release in production scale bioreactors. Other methods to promote cell growth are using drugs or relying on genetic engineering.The complex production process for cell-based meat increases cost, limits the manufacturing scale and undermines the commercial viability of cell-based meat.To help address this challenge, a multidisciplinary research team led by Associate Professor Alfredo Franco-Obregón, who is from the NUS Institute for Health Innovation & Technology and the NUS Yong Loo Lin School of Medicine, came up with an unconventional method of using magnetic pulses to stimulate the growth of cell-based meat.",NUS scientists develop novel technique to grow meat in the lab using magnetic field
99,8,8_weight_study_tre_ends,https://norml.org/news/2022/10/20/survey-ibd-patients-report-fewer-er-visits-following-initiation-of-medical-cannabis/,"Bronx, NY: Patients with inflammatory bowel disease (IBD) report symptom mitigation and fewer emergency room visits following the use of cannabis products, according to data published in the Journal of Clinical Gastroenterology.Investigators affiliated with the Albert Einstein College of Medicine in New York City surveyed a cohort of 236 IBD patients registered in the state’s medical cannabis access program.Respondents “reported fewer emergency room visits in the 12 months after versus before MC [medical cannabis] use and less impact of symptoms on daily life.” Subjects in the study were most likely to consume THC-dominant products via vaporization. Minor adverse effects, specifically drowsiness, were reported among a minority (4.2 percent) of subjects.Authors concluded: “MC users with IBD perceive symptom benefits and report decreased emergency room visits without serious adverse effects. Further studies are needed to confirm these results with objective measures of healthcare utilization and disease activity.”Observational trials have previously documented that cannabis use is associated with “decreased inpatient health care utilization” in patients with irritable bowel syndrome and with fewer disease-related hospitalizations in patients with Crohn’s. In a randomized placebo-controlled trial involving 21 patients with refractory Chron’s disease, nearly half achieved disease remission following their use of herbal cannabis.Full text of the study, “Medical cannabis use patterns and adverse effects in inflammatory bowel disease,” appears in the Journal of Clinical Gastroenterology. Additional information on cannabis and IBD is available from NORML.Share this: TwitterFacebookLike this: Like Loading...",Survey: IBD Patients Report Fewer ER Visits Following Initiation of Medical Cannabis
4,8,8_weight_study_tre_ends,https://aacrjournals.org/clincancerres/article/doi/10.1158/1078-0432.CCR-22-2429/709888/Electronic-Nicotine-Delivery-Systems-An-Updated,"Combustible tobacco use has reached historic lows, demonstrating the importance of proven strategies to reduce smoking since publication of the 1964 Surgeon General's report. In contrast, the use of electronic nicotine delivery systems (ENDS), specifically e-cigarettes, has grown to alarming rates and threatens to hinder progress against tobacco use. A major concern is ENDS use by youth and adults who never previously used tobacco. While ENDS emit fewer carcinogens than combustible tobacco, preliminary evidence links ENDS use to DNA damage and inflammation, key steps in cancer development. Furthermore, high levels of nicotine can also increase addiction, raise blood pressure, interfere with brain development, and suppress the immune system. The magnitude of long-term health risks will remain unknown until longitudinal studies are completed. ENDS have been billed as a promising tool for combustible tobacco cessation, but further evidence is needed to assess their potential efficacy for adults who smoke. Of concern, epidemiological studies estimate that approximately 15% to 42% of adults who use ENDS have never used another tobacco product, and another 36% to 54% “dual use” both ENDS and combustible tobacco. This policy statement details advances in science related to ENDS and calls for urgent action to end predatory practices of the tobacco industry and protect public health. Importantly, we call for an immediate ban on all non-tobacco-flavored ENDS products that contain natural or synthetic nicotine to reduce ENDS use by youth and adults who never previously used tobacco. Concurrently, evidence-based treatments to promote smoking cessation and prevent smoking relapse to reduce cancer incidence and improve public health remain top priorities for our organizations. We also recognize there is an urgent need for research to understand the relationship between ENDS and tobacco-related disparities.The following sections outline updates since our previous statement related to the evidence of biological effects from ENDS that can contribute to cancer risk, use trends, effective tobacco cessation efforts, and ENDS regulations. The data support strong, urgent action to reduce ENDS use among youth and adults who never previously used tobacco. Because of the wide use of non-tobacco-flavored ENDS among these groups, we recommend an immediate ban on all non-tobacco-flavored ENDS products that contain natural or synthetic nicotine. However, if non-tobacco-flavored ENDS are reviewed and approved by FDA CDER to increase cessation efficacy, the AACR and ASCO would welcome these as cessation therapies at that time. At the same time, new tobacco regulations should be structured to avoid any increases in combustible tobacco use, including smoking initiation and relapse. The following sections describe the evidence by which we based our recommendations.Tobacco would likely not be the top public health issue without the highly addictive properties of nicotine when delivered rapidly. Every time someone consumes nicotine, the brain releases the neurotransmitter dopamine, which provides a sense of pleasure or satisfaction ( 12 ). Primarily due to the pharmacology of nicotine, over time, tobacco users become dependent on nicotine to feel pleasure and stave off withdrawal symptoms ( 13 ). This rewiring of brain circuitry is especially of concern for the developing brains of youth ( 14 ). Nicotine can also harm health by raising blood pressure ( 15 ) and suppressing immune function ( 16 ). Strong evidence from clinical trials examining very low nicotine cigarettes demonstrates that reducing nicotine to less addictive levels could effectively decrease smoking rates by reducing initiation and increasing cessation of cigarette use ( 17–21 ). In 2018, the FDA issued a proposed rule to lower the level of nicotine in cigarettes to nonaddictive or minimally addictive levels ( 22 ), but at the time of writing this rule has not advanced. While the present statement focuses on policies related to ENDS, additional regulations to reduce the addictiveness and appeal of combustible tobacco are also highly important.Carcinogens from combustible tobacco products are very harmful to health, contributing to nearly half a million deaths each year in the United States and more than 8 million deaths per year globally ( 6, 7 ). The process of burning creates a large amount of carcinogens, such as benzo[a]pyrene, that are inhaled in smoke from traditional cigarettes ( 8 ). The first ENDS were introduced to the U.S. market in 2006 as a way to deliver nicotine to users without burning tobacco ( 9 ). Instead of burning tobacco, ENDS use electricity to power a heating element that aerosolizes an e-liquid, containing a solvent (e.g., propylene glycol or glycerin); nicotine; flavors; and other additives. Some ENDS products can result in rapid delivery of a similar amount of nicotine as modern American cigarettes, which contribute to high addiction potentials ( 10, 11 ).In 2015, the American Association for Cancer Research (AACR) and the American Society of Clinical Oncology (ASCO) published a joint policy statement describing a rapidly growing epidemic of electronic nicotine delivery systems (ENDS), including e-cigarettes, and policies to address this trend ( 1 ). The 2015 statement sought to balance curtailing youth use while remaining optimistic that ENDS could be a less harmful alternative to combustible tobacco cigarettes for adult smokers. As detailed in the following sections, youth ENDS use has further increased since the 2015 statement while evidence remains insufficient to show ENDS are more effective than current smoking cessation strategies. Additionally, several major health authorities have determined that the current evidence base is lacking in supporting ENDS as tobacco cessation aids, including the U.S. Surgeon General ( 2 ); the National Academies of Science, Engineering, and Medicine (NASEM; ref. 3 ); the U.S. Preventive Services Task Force (USPSTF; ref. 4 ); and the National Comprehensive Cancer Network, a coalition of 31 leading cancer centers ( 5 ). At the time of this writing, no ENDS manufacturer has applied to the U.S. Food and Drug Administration (FDA) Center for Drug Evaluation and Research (CDER) for an Investigational New Drug (IND) application, a prerequisite to run a tobacco cessation clinical trial. The AACR and ASCO are publishing the present statement to detail advances in scientific understanding of the ENDS epidemic, strengthen recommendations to protect public health, promote evidence-based tobacco cessation across all groups, and highlight areas where more research is needed.The cancer-causing potential of ENDS is inferred from the currently available studies investigating the presence of carcinogens, human biomarkers of carcinogenesis, and animal and cell culture experiments. Carcinogens in ENDS can include four classes of chemicals, namely tobacco-specific nitrosamines; metals; volatile organic compounds; and polycyclic aromatic hydrocarbons. Table 1 highlights several recent reports comparing carcinogens and metabolites in urine or saliva samples from ENDS users and those who never used tobacco. The data show that at least 12 carcinogens are significantly elevated in ENDS users compared with nontobacco users, but that their levels were generally lower than the levels of carcinogens seen in smokers and dual users ( Table 1 ; refs. 23–26 ). Unfortunately, the data are limited by a small number of studies that compared ENDS users with nonusers, and each study reported a different set of carcinogens. Separate studies further characterized carcinogens in ENDS aerosols and found that the power and temperature of devices greatly influences the amount of toxic metals and volatile organic compounds emitted ( 27–30 ). Therefore, additional studies are needed for a more thorough and comprehensive understanding of the carcinogen load experienced by ENDS users. Nevertheless, the results of ENDS use investigated to date clearly indicate that vaping exposes the user to carcinogens and therefore likely increases long-term cancer risk, but for most carcinogens at levels far lower than from smoking combustible tobacco cigarettes.Furthermore, nicotine itself and ENDS extracts can inhibit DNA repair processes in cell cultures. The DNA Checkpoint is a critical cellular system that senses damage and prevents cells from making new DNA in order to prevent further damage and initiate DNA repair. Nishioka and colleagues found that nicotine overrides the DNA Checkpoint and allows cells to make DNA even when there is DNA damage ( 39 ). Base Excision Repair (BER) is a key repair mechanism for DNA that has been chemically altered; two studies found that ENDS extracts reduce the abundance of BER proteins, thus limiting the ability of cells to repair damage caused by ENDS ( 33, 34 ). It is possible inhibition of DNA repair from ENDS use could exacerbate DNA damage and related DNA mutations caused by smoking in people who dual use.Several reports have found that ENDS vapor or extracts cause DNA damage in cell culture either by directly changing the chemical structure of DNA or indirectly by increasing highly reactive oxygen-containing molecules ( 32–36 ). One of those reports found that potent antioxidant molecules prevented DNA damage in cell culture, confirming the contribution of reactive oxygen species ( 32 ). A limitation of some studies is that they use higher concentrations of ENDS vapor than experienced by ENDS users, but DNA damage was also found in studies that used lower concentrations. Chemical modification of DNA by ENDS extracts leads to broken DNA strands ( 35, 37 ), which must be repaired by cells, or they will die. Repairing broken DNA strands can cause mutations that predispose cells to become cancerous, depending on how the damage is repaired ( 38 ).In addition to DNA damage, ENDS vapor could also lead to cancer by promoting inflammation and cellular replication that expands mutations caused by prior carcinogen exposure. A core hallmark of cancer is uncontrolled cellular replication ( 40 ). Several constituents in ENDS vapor can cause inflammation, as demonstrated by increased pro-inflammatory cytokines such as IL6 and CXCL8 ( 41–46 ). Wang and colleagues found that nicotine signaling in mouse lungs was a significant contributor to inflammation, and that deleting the nicotine receptor in lung cells reduced inflammation, confirming nicotine directly causes inflammation ( 44 ). However, even use of ENDS that only contained propylene glycol and vegetable glycerin had moderate pro-inflammatory effects in human lungs ( 43 ). An additional study found that ENDS users had significantly elevated levels of IL6 and CXCL8 in the blood compared with never smokers ( 45 ). IL6 is well documented to induce cell signaling pathways that promote cellular replication and transform precancerous cells into cancerous cells ( 47–49 ). Singh and colleagues also found that ENDS users had elevated levels of growth signaling molecules commonly implicated in cancer progression compared with never tobacco users, including epidermal growth factor, vascular endothelial growth factor, and hepatocyte growth factor ( 45 ). These findings suggest that ENDS vapor can promote replication of precancerous cells and therefore promote cancer-predisposing DNA mutations.A growing body of evidence points toward a biologically plausible role for ENDS use in contributing to human carcinogenesis, based on the presence of carcinogens in ENDS aerosols; metabolites of carcinogens in human urine samples; inflammation markers in human lung swabs and blood samples; and cell culture and mouse experiments exhibiting DNA damage and inflammation. It is important to note that the evidence from biomarker studies tends to show lower carcinogen exposures in ENDS users compared with dual users and exclusive smokers of combustible tobacco, likely due to the absence of combustion-related carcinogens. Additionally, the lack of well-designed epidemiologic studies is a critical hurdle to definitively characterizing cancer risk. ENDS remain relatively new products, so it may take decades for enough exposure to occur that would enable studies with sufficient follow-up to fully characterize the associations between ENDS use and cancer. Even less is known about the harms of second-hand exposure to ENDS vapor. In contrast, the scientific evidence very clearly demonstrates smoking combustible tobacco increases the risk of being diagnosed with lung cancer by approximately 25-fold compared with never smoking ( 6 ), and is an established cause of at least 17 other human cancers ( 6, 50 ).While youth and adult use of combustible tobacco has decreased to historic lows (2), the epidemic of youth ENDS use threatens to diminish progress against nicotine addiction. The AACR and ASCO published our first ENDS statement in 2015 due to concerns regarding the almost 400% rise between 2012 and 2014 in ENDS use among U.S. high school students, according to the 2014 National Youth Tobacco Use Survey (NYTS; Fig. 1; ref. 51). The number of high school students who had used ENDS in the past 30 days increased by an additional 46% in 2020 compared with 2014 levels, to a total of 3.6 million youth (52). A separate national survey, Monitoring the Future (MTF), also found a dramatic 73% increase between 2015 and 2020 among 12th grade students who had vaped in the past 30 days (Fig. 1; ref. 53). This continued increase in the youth ENDS epidemic underscores the need for urgent action to save a generation of youth from life-long nicotine addiction.Numerous studies have clearly demonstrated that appealing flavors are key drivers of youth initiation of ENDS use, with the pharmacology of nicotine as the key driver of addiction to ENDS (61–68). The 2020 NYTS found that 82.9% of youth ENDS users used flavored products. Among high school ENDS users, 73% reported vaping fruit-flavored ENDS, 55.8% vaped mint, and 37% vaped menthol (percentages add to greater than 100 % due to use of multiple flavors by one person) (52). In comparison, the 2020 MTF found that only 2.9% of youth ENDS users vaped tobacco-flavored products (69). Youth who are offered fruit flavored ENDS by peers are 6.49-fold more likely to try ENDS compared with tobacco-flavored ENDS (61). In contrast, adults are 21-fold more likely to exclusively use tobacco-flavored ENDS compared with youth (63). Flavored ENDS follow a long history of the tobacco industry using flavors to attract youth towards nicotine by disguising the otherwise unpleasant taste of tobacco and purposefully altering perceptions of risk (61).In February 2020, the FDA implemented restrictions on pod- or cartridge-based ENDS product flavors, except for menthol and tobacco flavors (70). The policy lacked definitions of “mint” or “menthol,” thus allowing manufacturers to simply relabel products to avoid the flavor restriction (71). Open tank and single-use ENDS were also exempted from any flavor restrictions, which left thousands of appealing flavors on the market. Consequently, youth switched to exempted products. The 2020 NYTS found that disposable products were used by 2.4% of high school ENDS users in 2019 (52), but this increased 11-fold to 26.5% in 2020. The prevalence of flavored disposable ENDS also increased among middle schoolers, with a 5-fold increase in disposable product use between 2019 and 2020 (3.0% vs. 15.2%). Flavoring chemicals and other additives of ENDS have not been studied to determine the health risks associated with inhalation. The ability to mix flavors at the point of sale also increases the difficulty of regulators to gain a complete understanding of the health impact of these chemicals in real-world use.The use of ENDS among adults has also increased in recent years, particularly among young adults. According to the Behavioral Risk Factor Surveillance System (BRFSS, N = 1,156,411), the prevalence of ENDS use increased among U.S. adults from 4.5% in 2016 to 5.4% in 2018 (72), and was 15.0% among adults under the age of 24 years. These data correspond to almost 14 million adults using ENDS in 2018. A second study analyzed data from the Population Assessment of Tobacco and Health (PATH) study (N = 30,191), which is also representative of the population of U.S. adults, and found that 6.5% of U.S. residents used ENDS in 2018 (73). Concerningly, the BRFSS study found that 42% of adult ENDS users had never previously used another tobacco product (72), and the PATH study found 15% of adult ENDS users had never used another type of tobacco product (73). While the high variability between analyses necessitates further study, the data suggest ENDS are being used by millions of adults who never previously used tobacco. In addition, approximately 36% of ENDS users in the BRFSS study and 52% in the PATH study “dual use” ENDS and combustible tobacco. A separate nation-wide survey (N = 5,989) found that 27.7% of adults who smoked also dual used ENDS in 2018 (74). Notably, dual use rates were higher in adults who wanted to quit smoking within 6 months (33.1%), compared with 18.7% of those who did not plan to quit smoking. Similar to the general population, adult patients with cancer and survivors who use ENDS are more likely to be under the age of 50 years (75, 76), but patients with cancer who use ENDS are far more likely to be current or former smokers than never smokers. As presented in Table 1, dual users continue to be exposed to similarly high levels of carcinogens as exclusive users of combustible tobacco and the current evidence of the efficacy of dual using ENDS to help quit smoking remains unclear. The evidence is clear that any combustible smoking, even one cigarette per day, has significant negative health impacts (77).",Electronic Nicotine Delivery Systems: An Updated Policy Statement from the American Association for Cancer Research and the American Society of Clinical Oncology
375,8,8_weight_study_tre_ends,https://www.fau.eu/2022/10/04/news/research/pain-relief-without-side-effects-and-addiction/,"Doctoral candidate and co-lead author Philipp Seemann was one of the FAU researchers who have discovered substances that provide very effective pain relief, but without the addictive and sedative effects of drugs currently available. (Image: FAU/Stefan Löber)Better than opiates: Researchers at FAU use adrenaline receptors for highly-effective analgesicsNew substances that activate adrenalin receptors instead of opioid receptors have a similar pain relieving effect to opiates, but without the negative aspects such as respiratory depression and addiction. This is the result of research carried out by an international team of researchers led by the Chair of Pharmaceutical Chemistry at FAU. Their findings, which have now been published in the renowned scientific journal Science, are a milestone in the development of non-opioid pain relief.Opiates cause addiction, new substances do notThey are a blessing for patients suffering from severe pain, but they also have serious side effects: Opioids, and above all morphine, can cause nausea, dizziness and constipation and can also often cause slowed breathing that can even result in respiratory failure. In addition, opiates are addictive – a high percentage of the drug problem in the USA is caused by pain medication, for example.In order to tackle the unwanted medical and social effects of opioids, researchers all over the world are searching for alternative analgesics. Prof. Dr. Peter Gmeiner, Chair of Pharmaceutical Chemistry is one of these researchers. “We are focusing particularly on the molecular structures of the receptors that dock onto the pharmaceutical substances”, says Gmeiner. “It is only when we understand these on the atomic level that we can develop effective and safe active substances.” Collaborating with an international team of researchers, Prof. Gmeiner discovered an active substance in 2016 that bonds to known opioid receptors and that offers the same level of pain relief as morphine, even though it has no chemical similarity to opiates.New approach: Adrenaline receptors instead of opioid receptorsPeter Gmeiner is currently following a lead that seems very promising: “Many non-opioid receptors are involved in pain processing, but only a small number of these alternatives have as yet been validated for use in therapies”, he explains. Gmeiner and a team of researchers from Erlangen, China, Canada and the USA have now turned their attention to a new receptor that is responsible for binding adrenaline – the alpha 2A adrenergic receptor. There are already some analgesics that target this receptor such as brimonidine, clonidine and dexmedetomidine. Gmeiner: “Dexmedetomidine relieves pain, but has a strong sedative effect, which means its use is restricted to intensive care in hospital settings and is not suitable for broader patient groups.”The aim of the research consortium is to find a chemical compound that activates the receptor in the central nervous system without a sedative effect. In a virtual library of more than 300 million different and easily accessible molecules, the researchers looked for compounds that physically match the receptor but are not chemically related to known medication. After a series of complex virtual docking simulations, around 50 molecules were selected for synthesis and testing and two of these fulfilled the desired criteria. They had good bonding characteristics, activated only certain protein sub-types and thus a very selective set of cellular signal pathways, whereas dexmedetomidine responds to a significantly wider range of proteins.Pain relief without sedation in animal modelsBy further optimizing the identified molecules, for which extremely high-resolution cryo-electron microscopic imaging was used, the researchers were able to synthesize agonists that produced high concentrations in the brain and reduced the sensation of pain effectively in investigations with animal models. “Various tests confirmed that docking on the receptor was responsible for the analgesic effect,” explains Gmeiner. “We are particularly pleased about the fact that none of the new compounds caused sedation, even at considerably higher doses than those that would be required for pain relief.”The successful separation of analgesic properties and sedation is a milestone in the development of non-opioid pain medication, especially as the newly-identified agonists are comparatively easy to manufacture and administer orally to patients. However, Prof. Gmeiner has to dampen any hopes of rapid widespread use in human medicine: “We are currently still talking about basic research. The development of medication is subject to strict controls and in addition to significant amounts of funding, it takes a long time. However, these results still make us very optimistic.”Publication: https://doi.org/10.1126/science.abn7065Further information:Chair of Pharmaceutical ChemistryPhone: +49 9131 85 65547peter.gmeiner@fau.de",Pain relief without side effects and addiction
587,8,8_weight_study_tre_ends,https://www.eurekalert.org/news-releases/969839,"HERSHEY, Pa. — Lowering the amount of nicotine in cigarettes to non-addictive levels may reduce smoking without worsening mental health in smokers with mood or anxiety disorders, according toHERSHEY, Pa. — Lowering the amount of nicotine in cigarettes to non-addictive levels may reduce smoking without worsening mental health in smokers with mood or anxiety disorders, according toTobacco remains the leading preventable cause of premature death and disease in the United States. Recent proposals by the U.S. Food and Drug Administration and the New Zealand government seek to limit the amount of nicotine in cigarettes to minimally addictive levels. Prior research indicates that reducing nicotine content could help smokers quit, but there is little evidence to demonstrate if these policies could adversely affect smokers with current or prior affective disorders like depression and anxiety disorders — which affect anTobacco remains the leading preventable cause of premature death and disease in the United States. Recent proposals by the U.S. Food and Drug Administration and the New Zealand government seek to limit the amount of nicotine in cigarettes to minimally addictive levels. Prior research indicates that reducing nicotine content could help smokers quit, but there is little evidence to demonstrate if these policies could adversely affect smokers with current or prior affective disorders like depression and anxiety disorders — which affect anAccording toAccording toThe researchers studied 188 smokers with a history of or who had a current mood or anxiety disorder and had no plans to quit. Volunteer participants were randomly assigned to a group that received either research cigarettes containing the usual amount of nicotine (11.6 mg nicotine/cigarette) or a progressively reduced amount of nicotine for an additional 18-week period (the final amount was 0.2 mg nicotine/cigarette). At the beginning and conclusion of the study, the researchers measured levels of cotinine, a metabolite of nicotine, levels of harmful chemicals, cigarette dependence indexes and various mental health measures.The researchers studied 188 smokers with a history of or who had a current mood or anxiety disorder and had no plans to quit. Volunteer participants were randomly assigned to a group that received either research cigarettes containing the usual amount of nicotine (11.6 mg nicotine/cigarette) or a progressively reduced amount of nicotine for an additional 18-week period (the final amount was 0.2 mg nicotine/cigarette). At the beginning and conclusion of the study, the researchers measured levels of cotinine, a metabolite of nicotine, levels of harmful chemicals, cigarette dependence indexes and various mental health measures.The researchers observed no statistically significant differences in mental health measures between the two groups at the conclusion of the study. The team used theThe researchers observed no statistically significant differences in mental health measures between the two groups at the conclusion of the study. The team used the“These findings are important because we want to understand the effect these policies would have on smokers with anxiety or depressive disorders,” said Foulds, a“These findings are important because we want to understand the effect these policies would have on smokers with anxiety or depressive disorders,” said Foulds, aSimilar to what prior studies reported, Foulds and team found that groups in the reduced nicotine content group were absorbing lower amounts of nicotine and ingesting lower levels of harmful carcinogens such as the biomarker 4-(methylnitrosamino)-1-(3-pryidyl)-1-butanol), more commonly known as NNAL. That group also smoked fewer cigarettes and reported lower levels of nicotine addiction by the end of the randomized phase of the trial. The results were published in PLOS ONE today, Nov. 2.Similar to what prior studies reported, Foulds and team found that groups in the reduced nicotine content group were absorbing lower amounts of nicotine and ingesting lower levels of harmful carcinogens such as the biomarker 4-(methylnitrosamino)-1-(3-pryidyl)-1-butanol), more commonly known as NNAL. That group also smoked fewer cigarettes and reported lower levels of nicotine addiction by the end of the randomized phase of the trial. The results were published in PLOS ONE today, Nov. 2.Unique to this study, participants in both groups were also given the choice to “choose their treatment,” after the 18-week period. They could go back to using their own cigarettes, continue smoking the research cigarettes or attempt to quit. Of the 188 participants in the study, those randomized to reduced nicotine content cigarettes were more likely to have quit smoking 12 weeks later (18.1%), compared to those in the control (usual nicotine content) group (4.3%).Unique to this study, participants in both groups were also given the choice to “choose their treatment,” after the 18-week period. They could go back to using their own cigarettes, continue smoking the research cigarettes or attempt to quit. Of the 188 participants in the study, those randomized to reduced nicotine content cigarettes were more likely to have quit smoking 12 weeks later (18.1%), compared to those in the control (usual nicotine content) group (4.3%).“We believe this is the first randomized trial to find that smokers who used very low nicotine cigarettes were significantly more likely to have quit smoking (with biochemical verification), three months after the end of the trial,” Foulds said.“We believe this is the first randomized trial to find that smokers who used very low nicotine cigarettes were significantly more likely to have quit smoking (with biochemical verification), three months after the end of the trial,” Foulds said.“Our results suggest that these policies will likely result in reduced nicotine absorption from cigarettes without worsening the mental health of smokers with mood or anxiety disorders,” said Dr. Eden Evins, Cox Family Professor of Psychiatry at Harvard Medical School. “They also suggest that with proper support and resources, smokers with mood and anxiety disorders could quit successfully as a result of these policies.”“Our results suggest that these policies will likely result in reduced nicotine absorption from cigarettes without worsening the mental health of smokers with mood or anxiety disorders,” said Dr. Eden Evins, Cox Family Professor of Psychiatry at Harvard Medical School. “They also suggest that with proper support and resources, smokers with mood and anxiety disorders could quit successfully as a result of these policies.”For more information on nicotine, smoking and health studies at the Penn State Center for Research on Tobacco and Health, visitFor more information on nicotine, smoking and health studies at the Penn State Center for Research on Tobacco and Health, visitSusan VeldheerSusan VeldheerThis research was supported by the National Institutes of Health through the National Institute on Drug Abuse of the National Institutes of Health (award P50DA036107) and the National Center for Advancing Translational Sciences through Penn State Clinical and Translational Science Institute (award UL1 TR000127). The research was also supported by the Center for Tobacco Products of the U.S. Food and Drug Administration. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health or the Food and Drug Administration.This research was supported by the National Institutes of Health through the National Institute on Drug Abuse of the National Institutes of Health (award P50DA036107) and the National Center for Advancing Translational Sciences through Penn State Clinical and Translational Science Institute (award UL1 TR000127). The research was also supported by the Center for Tobacco Products of the U.S. Food and Drug Administration. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health or the Food and Drug Administration.","Reduced-nicotine cigarettes result in less smoking in anxious, depressed smokers"
101,8,8_weight_study_tre_ends,https://nypost.com/2022/10/28/uber-eats-threatens-to-suspend-customers-account-for-ordering-alcohol-too-often/,"Uber Eats is keeping an eye on how many alcohol orders you place – and one customer is not happy about being called out for having “a few”.An email sent to an unidentified Australian customer – who then shared it on Reddit – said: “Our systems indicate you’ve placed a few alcohol orders over the past few weeks.”It went on to say if the “ordering pattern” continued a hold may be put on the customer’s account or it could be raised for further review.“Experts recommend that no more than 10 standard drinks are consumed each week,” Uber advised.news.com.au understands Uber Eats began sending these alerts in August and does not specify maximum alcohol ordering limits to discourage users from over-ordering.The email shared a link to DrinkWise. Uber has had a partnership with the not-for-profit organization since 2016 when it launched a campaign against drunk driving and furthered its involvement in 2018 when Uber Eats first launched alcohol delivery in Melbourne, Australia.An Uber Eats delivery driver is required to scan a customer’s ID in the app to ensure they are over the age of 18 and check they are not visibly intoxicated before handing over an order.The driver is paid by Uber to return the alcohol to the store if a person is underage or visibly intoxicated.The Uber Eats customer posted the email to Reddit. @menotyoutoo/RedditThe unimpressed recipient of the email said they got an “alcohol order or two” a week.Some labeled it “lazy risk mitigation” and claimed Uber was only “looking after itself”.“These corporations jumping on these charitable causes to feign care is sickening,” one person criticized. “Alcoholism is a genuine issue but the way to solve it isn’t some vaguely threatening email from a delivery company.”Another added: “The charity partnership thing is an easy way of looking like they care and ‘fixing’ the problem.”“What would be better is ‘we noticed you’ve ordered a lot of alcohol, here’s some support services you can look into if you need it’. Not threatening them to close their account,” wrote a third.Some suggested restricting how much alcohol someone could buy a week for their health was ironic given there was no limit on junk food orders.“Yet order two dozen creme donuts, six triple cheeseburgers with extra cheese sauce, a kilo of fully-loaded cheese fries, four tubs of Ben & Jerry’s, six liters of Coke and you’re good to go,” one said.“But order 100 big macs, no problem sir, seems perfectly healthy,” another wrote.One user argued the effect of unhealthy foods was not comparable.“They both absolutely have an impact on the health system, but no one’s killing a group of kids with their 4WD because they’re driving under the influence of a triple cheeseburger,” they wrote.Start your day with all you need to know Morning Report delivers the latest news, videos, photos and more. Enter your email address Please provide a valid email address. By clicking above you agree to the Terms of Use and Privacy Policy. Thanks for signing up!Never miss a story. Check out more newslettersUber told news.com.au it was committed to ensuring responsible alcohol consumption.“This includes frequent communication to encourage moderation, educating consumers about the government guidelines, implementing ID and sobriety checks, and incorporating alcohol ordering limits,” the company said.“If a customer’s order exceeds the limits, they will be notified in the app and unable to check out. This is to ensure we can make safe, smart, and responsible decisions about alcohol consumption together.”A year ago it was revealed popular alcohol delivery service Jimmy Brings was being investigated over whether it breached liquor laws in relation to the June 2018 death of a man who reportedly spent $15,000 with the company over three years, including daily orders in the weeks leading up to his death.Almost 300 orders reportedly included several bottles of alcohol delivered almost daily, including two that were identical and placed within 10 minutes of each other, in the fortnight prior to the 49-year-old man’s death.Liquor & Gaming New South Wales, Australia confirmed to news.com.au on Friday it did not identify any breaches of the liquor laws that were in place at the time in 2018 and the investigation was closed.However, the New South Wales, Australia Government has since introduced new laws.“The new laws have made it mandatory for all delivery drivers to undertake the responsible service of alcohol training and to keep records of all refused deliveries,” a spokesman said.It is also now an offense to deliver to an intoxicated person with a maximum penalty of $7,000.New South Wales now has the strongest express alcohol delivery laws in the country.",Uber Eats threatens to suspend customerâs account for ordering alcohol too often
360,8,8_weight_study_tre_ends,https://www.eurekalert.org/news-releases/968532,"In the months after the advance federal Child Tax Credit cash payments ended in December 2021, low-income families with children struggled the most to afford enough foodContact:Jillian McKoy, jpmckoy@bu.eduMichael Saunders, msaunder@bu.eduMichelle Falinski , Michelle.Falinski@bmc.org##In the months after the advance federal Child Tax Credit cash payments ended in December 2021, low-income families with children struggled the most to afford enough food. Black, Latino, Indigenous, and immigrant families were also disproportionately impacted by the loss of the cash relief benefit.When Congress failed to renew advance Child Tax Credit (CTC) payments last year, public health experts feared that the loss of this monthly federal pandemic relief benefit would push millions of families and children in America back into poverty and hunger.Now, a new study led by researchers at Boston University School of Public Health (BUSPH) and Boston Medical Center (BMC) reveals that in the months after this policy ended, there was a substantial increase in the percentage of US households with children that could not afford enough food to eat in a seven-day period—a situation known as food insufficiency.Published in the journal JAMA Network Open, the study found that food insufficiency increased by approximately 25 percent among families with children from January 2022 to July 2022, after they stopped receiving monthly CTC payments on January 15, 2022. The monthly cash benefits were a cornerstone of the Biden administration’s American Rescue Plan, providing an estimated 92 percent of US households up to $3,000 per child ages 6 to 17 and up to $3,600 per child under age 6 from July 2021 to December 2021, with half of the credit amount distributed as advance monthly payments.The study is the first to measure the impact of the expired benefits on food insufficiency among households, and it follows the researchers’ previous study in JAMA Network Open, which showed that the CTC expansion reduced food insufficiency by 26 percent in 2021, findings that President Biden cited during the White House Conference on Hunger, Nutrition, and Health in late September.This increase in food insufficiency is an urgent problem, particularly among households with children, as poor nutrition uniquely affects the health and well-being of growing children, the researchers say.“This significant increase in food insufficiency among families with children is particularly concerning for child health equity, as child health, development, and educational outcomes are strongly linked to their family’s ability to afford enough food,” says study lead and corresponding author Allison Bovell-Ammon, director of policy and communications at Children’s HealthWatch, headquartered at BMC. “Even brief periods of deprivation during childhood can have lasting impacts on a child.”As Congress negotiates a year-end tax relief package, the researchers urge lawmakers to pass a fully refundable and inclusive advance CTC that ensures that all families with children across the US are able to afford enough food to keep their children healthy.“The six short months of these Child Tax Credit advanced payments clearly made a big difference for American families, a permanent expansion would be a game-changer for reducing child poverty for good,” says study senior author Paul Shafer, assistant professor of health law, policy & management at BUSPH. “There is more to do to make sure that very low-income families actually get the monthly payments, prompting efforts like GetCTC.org, but a permanent expansion allows resources and awareness to build around the policy in a way that short-term fixes don’t.”For the study, the researchers examined nationally representative census data on demographic characteristics, employment, social supports, and food insufficiency among nearly 600,000 household families, from July 2021 to July 2022 (the period before and after expiration of the CTC monthly payments).The findings also indicate that the expiration of the CTC exacerbates racial and economic inequities in consistent access to adequate and healthy food. The study showed that low-income households experienced the greatest increases in food insufficiency after the advance payments ended in January—particularly in the spring, after many families likely depleted the second half of their CTC credits issued in a lump sum payment after tax filings. The analysis showed that single-adult, non-Hispanic Black, and Hispanic households—groups that historically have faced greater hardships around food access—also experienced greater food insufficiency after losing their advance CTC payments.“Black, Latino, Indigenous, and immigrant families in the US consistently experience food insecurity—a broader measure that assesses quantity, quality, and variety of food —at higher rates than White families as a result of current and historical marginalization and systemic racism,” Bovell-Ammon says.The expanded CTC in 2021 reduced racial inequities by ensuring access to the credit for predominantly Black, Latino, and Indigenous children who were previously excluded from the full benefits of the CTC, she says.Immigrant families also experienced significant barriers to CTC access, says study coauthor Stephanie Ettinger de Cuba, research associate professor of health law, policy & management at BUSPH and executive director of Children’s HealthWatch. “These barriers were due in part to specific eligibility exclusions, but they also occurred even when immigrant families were eligible. Following the expiration of the payments at the end of 2021, the gains in racial equity were eroded, potentially further exacerbating racial and health inequities and increasing distrust.”This study was also coauthored by Nicole C. McCann and Martha Mulugeta, PhD students in Health Services Research at BUSPH; and Julia Raifman, assistant professor of health law, policy & management at BUSPH.**About Boston University School of Public HealthFounded in 1976, Boston University School of Public Health is one of the top five ranked private schools of public health in the world. It offers master's- and doctoral-level education in public health. The faculty in six departments conduct policy-changing public health research around the world, with the mission of improving the health of populations—especially the disadvantaged, underserved, and vulnerable—locally and globally.About Boston Medical CenterBoston Medical Center (BMC) is a private, not-for-profit, 514-bed, academic medical center that is the primary teaching affiliate of Boston University School of Medicine. It is the largest and busiest provider of trauma and emergency services in New England. BMC offers specialized care for complex health problems and is a leading research institution, receiving more than $110 million in sponsored research funding in fiscal year 2021. It is the 13th largest funding recipient in the U.S. from the National Institutes of Health among independent hospitals. WellSense Health Plan was founded in 1997 as Boston Medical Center HealthNet Plan, now one of the top ranked Medicaid MCOs in the country, as a non-profit managed care organization. Boston Medical Center and Boston University School of Medicine are partners in Boston HealthNet – 12 community health centers focused on providing exceptional health care to residents of Boston. For more information, please visit http://www.bmc.org.",US food insufficiency spiked by 25% after monthly Child Tax Credits expired
33,8,8_weight_study_tre_ends,https://edition.cnn.com/2022/11/01/health/drinking-deaths-us-study-wellness/index.html,"CNN —A beer, glass of wine or cocktail may feel so common place that you don’t even think about pouring another, but a new study suggested it may be important for everyone to be mindful of their alcohol use.An estimated 1 in 5 deaths of people ages 20 to 49 were attributable to excessive alcohol use in the United States, according to the study published Tuesday in JAMA Network Open. For people ages 20 to 64, drinking-related deaths accounted for 1 in 8, the study said.The percentage of deaths attributed to alcohol use varied state by state, but nationally it’s a leading cause of preventable death, said lead study author Dr. Marissa Esser, lwho leads the US Centers for Disease Control and Prevention’s alcohol program.Researchers took national and state mortality data from 2015 to 2019 and looked at deaths either fully or partially attributable to excessive drinking. Those causes of death included vehicle accidents, alcohol poisoning and other health impacts, such as liver disease, Esser said.The data showed that the deaths fully attributable to alcohol have risen in the past decade, Esser added.“I’m not surprised at the numbers,” said David Jernigan, a professor of health law, policy and management at Boston University. “This is a conservative estimate.”Jernigan was not involved in the study.Esser said there were deaths that alcohol likely contributed to that the study’s researchers could not include in their estimates. Some conditions may have had alcohol as a factor, but researchers were not able to verify for sure the role that drinking played. In other cases, they were not able to determine if someone who died of an illness used to drink excessively but then stopped, Esser added.And people often underreport how much they are drinking, Jernigan said.“It doesn’t get anywhere near the attention that it should,” he said. “The bottom line is (researchers) continue to show that excessive alcohol use is a big problem in the US.”How to know when it’s too muchFor health and safety, Jernigan said the goal for state and local government agencies should be encouraging almost everyone to drink less.“States and communities can prevent these premature deaths using evidence-based strategies to reduce the availability and accessibility of alcohol and increase its price,” Esser said.That can mean increasing taxes on alcohol or limiting where alcohol is sold, Esser added.On an individual level, Esser suggested that people could try to stop or limit alcohol consumption.The CDC defines moderate drinking as two drinks or less in a day for men or one drink or less in a day for women. Two-thirds of adults report drinking more than moderate amounts at least once a month, the organization added.The CDC also estimates that 1 in 6 adults binge drink — defined as consuming four or more drinks on an occasion for a woman or five or more drinks on an occasion for a man — with a quarter of those doing so at least weekly.Reducing drinking can have a similar effect as dieting – the more you tell yourself you can’t have it, the more you want it, said Natalie Mokari, a registered dietitian nutritionist in Charlotte, North Carolina.She recommends starting with one less drink than you would usually have at each occasion or breaking a daily habit by limiting drinking to certain days. You can also have a sparkling water in between drinks or make weaker cocktails than usual to reduce your alcohol consumption, she said previously.And if you are overcoming a social pressure to drink, remember that people may make you feel bad because they are uncomfortable about their own relationship with drinking, said Annie Grace, author of “This Naked Mind: Control Alcohol” in a prior article.It often helps to have a nonalcoholic drink in your hand at social events, so the offer to have a drink doesn’t even come up, said biological psychologist Aaron White, senior scientific adviser to the director of the National Institute on Alcohol Abuse and Alcoholism.Slow down your body’s alcohol intake by eating while drinking, alternate alcoholic and nonalcoholic drinks and planning alcohol-free days, Harvard Medical School in Boston suggests.A tool on the CDC website can help individuals evaluate their drinking and then come up with a plan to make healthier alcohol choices.If you need help or support immediately, the Substance Abuse and Mental Health Services Administration has a free, confidential National Helpline active 24/7/365 to provide information and treatment referrals to local treatment facilities, support groups and community-based organizations: 800-662-HELP (4357) and 800-487-4889 (TTY option).","1 in 5 deaths of US adults 20 to 49 is from excessive drinking, study shows"
286,8,8_weight_study_tre_ends,https://theveganherald.com/2022/10/study-plant-based-diets-improve-maternal-fetal-outcomes-in-ckd-pregnancies/,"Plant-based, moderately protein-restricted diets in pregnancy in patients with chronic kidney disease are associated with a lower risk of preterm delivery and small-for-gestational-age babies.This is according to a new study published in the journal Nutrients and epublished by the National Institute of Health.“Reducing protein intake in patients with chronic kidney disease (CKD) limits glomerular stress induced by hyperfiltration and can prevent the progression of kidney disease; data in pregnancy are limited”, states the study. “The aim of this study is to analyze the results obtained in CKD patients who followed a plant-based moderately protein-restricted diet during pregnancy in comparison with a propensity-score-matched cohort of CKD pregnancies on unrestricted diets.”A total of 52 CKD pregnancies followed up with a protein-restricted plant-based diet (Torino, Italy) were matched with a propensity score based on kidney function and proteinuria with CKD pregnancies with unrestricted protein intake (Cagliari Italy).“Outcomes included preterm (<37 weeks) and very preterm (<34 weeks) delivery and giving birth to a small-for-gestational-age baby”, states the study. “The median age in our cohort was 34 years, 63.46% of women were primiparous, and the median body mass index (BMI) was 23.15 kg/m2 with 13.46% of obese subjects.”No statistical differences were found between women on a plant-based diet and women who were not in terms of age, parity, BMI, obesity, CKD stage, timing of referral, or cause of CKD. No differences were found between the two groups regarding the week of delivery. However, “the combined negative outcome (birth before 37 completed gestational weeks or birth-weight centile <10) occurred less frequently in women following the diet than in women in the control group (61.54% versus 80.77%; p = 0.03).”The lower risk “was confirmed in a multivariable analysis adjusted for renal function and proteinuria (OR: 0.260 [Q1:0.093-Q3:0.724]; p = 0.010), in which the increase in proteinuria from the first to the last check-up before delivery was lower in patients on plant-based diets (median from 0.80 to 1.87 g/24 h; p: ns) than in controls (0.63 to 2.39 g/24 h p < 0.0001).”The study concludes by stating that “plant-based, moderately protein-restricted diets in pregnancy in patients with CKD are associated with a lower risk of preterm delivery and small-for-gestational-age babies; the effect may be mediated by better stabilization of proteinuria.”Below is the study’s abstract.Reducing protein intake in patients with chronic kidney disease (CKD) limits glomerular stress induced by hyperfiltration and can prevent the progression of kidney disease; data in pregnancy are limited. The aim of this study is to analyze the results obtained in CKD patients who followed a plant-based moderately protein-restricted diet during pregnancy in comparison with a propensity-score-matched cohort of CKD pregnancies on unrestricted diets. A total of 52 CKD pregnancies followed up with a protein-restricted plant-based diet (Torino, Italy) were matched with a propensity score based on kidney function and proteinuria with CKD pregnancies with unrestricted protein intake (Cagliari Italy). Outcomes included preterm (<37 weeks) and very preterm (<34 weeks) delivery and giving birth to a small-for-gestational-age baby. The median age in our cohort was 34 years, 63.46% of women were primiparous, and the median body mass index (BMI) was 23.15 kg/m2 with 13.46% of obese subjects. No statistical differences were found between women on a plant-based diet and women who were not in terms of age, parity, BMI, obesity, CKD stage, timing of referral, or cause of CKD. No differences were found between the two groups regarding the week of delivery. However, the combined negative outcome (birth before 37 completed gestational weeks or birth-weight centile <10) occurred less frequently in women following the diet than in women in the control group (61.54% versus 80.77%; p = 0.03). The lower risk was confirmed in a multivariable analysis adjusted for renal function and proteinuria (OR: 0.260 [Q1:0.093-Q3:0.724]; p = 0.010), in which the increase in proteinuria from the first to the last check-up before delivery was lower in patients on plant-based diets (median from 0.80 to 1.87 g/24 h; p: ns) than in controls (0.63 to 2.39 g/24 h p < 0.0001). Plant-based, moderately protein-restricted diets in pregnancy in patients with CKD are associated with a lower risk of preterm delivery and small-for-gestational-age babies; the effect may be mediated by better stabilization of proteinuria.",Study: Plant-Based Diets Improve Maternal-Fetal Outcomes in CKD Pregnancies
384,8,8_weight_study_tre_ends,https://www.gq-magazine.co.uk/lifestyle/article/weight-loss-drugs-tirzepatide/amp,"Earlier this summer, Forrest Smith got some promising news. The Denver-based petroleum engineer, who works for the National Park Service, had read reports on a new diabetes medication called tirzepatide. Clinical trials had confirmed a potent side effect: Tirzepatide users could shed up to 20 percent of their body weight. Smith told me he spent his childhood cast as “the fat kid in school,” and his adulthood locked in a cycle of losing pounds and regaining them. Though he is not diabetic, he was aware that some doctors were prescribing the drug for weight loss and, feeling like he had nothing to lose, sought one out for treatment. He took his first weekly injection in July, and says it was like “a switch was flipped overnight.” Food cravings disappeared. When watching skinny friends eat, he used to wonder, “How do you not eat that entire plate of cookies in front of you?” That all changed. “One cookie? Totally doable.”View moreHe now weighs 236 pounds, 24 pounds down from when he began the medication. Smith’s spouse and parents were so impressed with his progress that they decided to seek out tirzepatide, too. His young children have noticed that running around their garden now tires them out before their father. Since his first shot, Smith has been reaching deeper into his closet for clothes that will fit. “Hopefully,” he said, “I don't find parachute pants—I don’t have to go that far back.”Tirzepatide (marketed by Eli Lilly and Company as Mounjaro) first became available to the American public in May of 2022, when it was approved by the United States Food and Drug Administration (FDA) as a diabetes treatment. And while FDA approval for using the drug specifically for weight loss appears imminent, doctors have the authority to deviate from FDA mandates when prescribing drugs, and some have been writing scripts to treat obesity at their own discretion. “I’ve been very excited about these medicines,” said Dr. Melanie Jay, director of NYU Langone’s Comprehensive Program on Obesity. “[Obesity] has always been something that's under-treated.” It might be the trickle that precedes a torrent—tirzepatide is just one in a class of new extremely effective weight loss drugs that threaten to upend the way we think about and treat obesity.Many people are under the misconception that their weight can be completely controlled by diet and exerciseThat class of drugs is called incretins. Initially created to spur insulin production in diabetic patients, incretins often left participants in drug trials with two notable side effects: satiety and delayed gastric emptying. In other words, recipients feel full quicker, while food itself moves from stomach to intestine more slowly, which makes you feel even more full. The combination of those effects caused patients to eat less and consistently lose weight.To understand why these drugs could be so revolutionary, you need to understand how obesity works—which is often different from the way it’s talked about. Many people are under the misconception that their weight can be completely controlled by diet and exercise. (Look no further than the 18 seasons of The Biggest Loser for evidence.) But researchers and doctors are more inclined to think of the condition as just that: a chronic health condition. “Our brains regulate our appetite, and they regulate our metabolism,” said Jay. For the obese, those regulations are set to retain weight. A healthy lifestyle can help prevent obesity, but when a person who’s already overweight goes on a diet, their bodies increase appetite and decrease metabolism. The weight returns. (Exercise doesn’t seem to matter much at all.) These patients—a huge proportion of Americans—often aren’t well-served by the medical establishment. And as Matt Yglesias recently pointed out for Grid, the body-positivity movement has advocated for people to seek health at any size—an understandable reaction, but one that can obscure the real health costs of obesity.The effective treatment options for obesity that already exist are under-used. Bariatric surgery, for example, can be an effective way to address extra weight. But surgery is seen as a drastic option, and Americans tend to think poorly of weight loss surgery. Without a comorbidity like diabetes, a prospective candidate typically needs a BMI of at least 40 to undergo the surgery—and only a small sliver of those who qualify get the procedure. That’s one reason why obesity experts see so much promise in the new drugs.But there are big hurdles to widespread adoption, and not just questions of cost and approval that every drug faces on its way to the public. Weight loss drugs have a checkered past. “If you look back in the history of obesity, drugs that have been approved have then been taken off the market,” says Dr. Spencer Nadolsky, a physician who runs the obesity program for telehealth provider Weekend Health. Dangerous amphetamines were used as appetite suppressants, and more recent drugs, like Fen-Phen, a weight loss drug widely used in the 1990s, caused heart problems, leading to an FDA ban. Safer weight loss drugs began to reappear in the 2000s, but their efficacy was often mild.Incretins, on the other hand, are only continuing to get more effective. The newest of these medications cause around 20 percent weight loss, within the same range that bariatric surgery achieves. And while common side effects include nausea and other gastrointestinal distress, it's a much less disruptive medical intervention than surgery.One obesity expert at Harvard Medical School, Dr. Fatima Stanford, told me that some patients have reacted so strongly to one incretin, semaglutide, that they’ve avoided surgery completely. “They went from severe obesity with diabetes to no diabetes and no severe obesity—into a healthy weight range,” she said. “It’s effortless for them—we’re changing the way their brains see weight.” This has major quality of life implications. “When you have higher levels of obesity,” said Jay, “losing 15 or 20% of your body weight is huge, right? It's huge for resolving comorbidities and preventing diabetes, and all sorts of things.”One Washington woman in her 50s named Suzy, who asked to only be identified by her first name, has lost 26 pounds since starting tirzepitide. She has three siblings and two parents with type two diabetes. With the drug, she thinks she can avoid that disease. Another woman, Rachel McLaughlin, who started an oral incretin in 2021, said weight loss gave her the confidence to join an art class. “I don’t look like I’m carrying the weight of the world around,” she said.But remarkable advances in medical technology don’t mean much if they’re impossible to access. McLaughlin faced that setback when she lost her job earlier this year. Losing health insurance increased the cost of her prescription from $25 (£22) to more than $2,000 (£1,772) per month. Off the medication, she regained 15 of the 25 pounds she’d lost. Progress only resumed once she found a new job in June that restored her coverage.Like other diabetes medications, incretins are meant to be taken indefinitely. Dr. Mike Albert, co-founder of Accomplish Health, a telehealth company specializing in obesity, said the biggest influences over what he prescribes are cost and coverage. “That’s the limiter,” he said. “If a medicine is not covered on a health plan, or under the benefits of a health plan, or if it is cost prohibitive, it doesn't matter how good I think it will do for this person.”Medicare, which heavily influences how private insurance charts its own plans, doesn’t cover anti-obesity medications. (In an unfortunate twist, Medicare’s policy on obesity meds came in partial response to the Fen-Phen scandal.) A bill sitting in Congress called the Treat and Reduce Obesity Act would change that, but its fate remains unclear.Changing how both the public and health care system views obesity won't happen overnight—after all, it was only in 2013 that the American Medical Association acknowledged obesity as a chronic illness.“Most primary care doctors, unfortunately, still suffer from an obesity stigma and bias,” said Nadolsky. If you cornered a doctor at a dinner party, Stanford said, they’d likely be able to tell you how to treat an esoteric condition like Behcet’s disease. “Then if you ask them about obesity, which affects almost half the population, they would look like, I don't know what to do.”This all means that widespread adoption of incretins for weight loss still faces significant barriers. There is also the thorny (and still mostly theoretical) question of whether these drugs are appropriate for people who simply want to lose a few pounds. But for patients currently experiencing obesity and metabolic dysfunction, the possibilities a prescription brings are already too great to ignore. Suzy told me that she and her husband are planning to travel through Europe in retirement—unthinkable until she began a tirzepatide regimen. McLaughlin has travel plans too, but she's focusing on the beginning of the trip. “I cannot wait to get on a plane,” she said, “and take a seat comfortably.”",Is the world ready for extremely effective weight-loss drugs?
464,8,8_weight_study_tre_ends,https://www.psypost.org/2022/10/ultra-processed-food-consumption-linked-to-adverse-mental-health-symptoms-64041,"People who consume high amounts of ultra-processed foods report significantly more adverse mental health symptoms, according to new research published in Public Health Nutrition.Ultra-processed foods consist mostly of manufactured ingredients that have been extracted from foods and usually contain flavorings, colorings and other additives. Ultra-processed foods are often high in sugar, fat, and salt, and they frequently lack important nutrients like fiber and vitamins. A number of studies have found that ultra-processed foods can have negative consequences for physical health, but less is known about the link between these food substances and mental health outcomes.“I am a chronic disease epidemiologist and as such I am interested in a variety of different hypothesized disease causing exposures, and various health outcomes,” explained study author Eric Hecht, a physician and an adjunct professor at Florida Atlantic University and the University of Miami. “Ultra-processed food is of tremendous interest for a variety of health outcomes including obesity and inflammatory diseases.”“Other studies have also explored the relationship between diet and mental health, but few have examined the relationship between UPF consumption and mental health. Anecdotally, I have often wondered about a relationship between junk food and subsequent behavioral issues in kids and symptoms of anxiety and melancholy in adults. All of these ideas sort of led to this study.”For their study, the researchers examined mild depression, number of mental unhealthy days and number of anxious days in 10,359 adults 18 and older from the U.S. National Health and Nutrition Examination Survey, a series of nationally representative surveys that include both interviews and physical examinations. Importantly, the surveys collect information regarding diet behaviors and mental health.Hecht and his colleagues found evidence that the consumption of ultra-processed foods was associated with worse mental health outcomes. Individuals who consumed the most ultra-processed foods tended to have heightened symptoms of mild depression along with more “mentally unhealthy days” and “anxious days” over the past month compared with those who consumed the least amount.“We found a dose response relationship between UPF consumption and mental health symptoms,” Hecht told PsyPost. “Others have found a relationship between whole food consumption and improvement in mental health symptoms. The average American consumes 60% of their calories in the form of UPF. For many other health reasons, this is a bad idea. And now it appears that UPF consumption might be tied to worse mental health. I think in general the average person should look at how much packaged food they are consuming and make an effort to make the majority of their calorie consumption real, unprocessed food.”A substantial number of individuals who reported that ultra-processed foods accounted for less than 19% of their calorie intake per day had zero mentally unhealthy days and zero anxious days. “I was impressed that individuals who consume a diet with less UPF generally describe their last 30 days as being free of mental health symptoms,” Hecht said.The researchers controlled for potentially confounding variables, such as age, BMI, race/ethnicity, poverty status, smoking status, and physical activity level. But the study, like all research, includes some caveats.“Our study was cross-sectional so we can not be sure as to which came first, the UPF or the symptoms,” Hecht explained. “Reverse causation, meaning mental health symptoms might increase UPF consumption is a real possibility. Arguing against this however are longitudinal studies which found a temporal relationship between junk food consumption and mental health symptoms. In addition, experimental studies have found that reducing junk food improves mental health symptoms when compared to individuals who continue their poor diet.”“The link between UPF consumption and obesity, and the link between UPF consumption and inflammation also suggest pathways towards mental health symptoms since both extra weight gain and inflammation can lead to mental health symptoms as found in other studies,” the researcher added.The study, “Cross-sectional examination of ultra-processed food consumption and adverse mental health symptoms“, was authored by Eric M Hecht, Anna Rabil, Euridice Martinez Steele, Gary A. Abrams, Deanna Ware, David C. Landy and Charles H. Hennekens.",Ultra-processed food consumption linked to adverse mental health symptoms
447,8,8_weight_study_tre_ends,https://www.psypost.org/2022/10/cannabis-use-does-not-increase-actual-creativity-but-does-increase-how-creative-you-think-you-are-study-finds-64187,"A set of studies published in the Journal of Applied Psychology has failed to find evidence that cannabis has creativity-enhancing effects. But the researchers did find that cannabis elicited a sense of joviality, which in turn made cannabis users perceive their own ideas and the ideas of others as more creative.“Cannabis was legalized in Washington state a few years back. That got us talking about how cannabis is a topic which his generally ignored by the management and applied psychology research literature, with the exception of research which considers cannabis as harmful to work and health,” said study author Christopher Barnes (@chris24barnes), a Michael G. Foster Endowed Professor at the University of Washington.“We thought there might be some more nuance to the topic, and that the research literature should be expanded accordingly. A natural first step was to examine cannabis and creativity, given the common belief that they are linked.”For their study, the researchers recruited occasional cannabis users from Washington state. They ended up with a final sample of 191 participants, who were randomly assigned to one of two conditions. One group of participants were asked to begin the study within 15 minutes of using cannabis. The second group was instructed to only begin the study if they have not used cannabis in the past 12 hours.The participants first reported whether they were “happy” and “joyful” at the moment. They then completed the alternative uses task, a well-established measure of a type of creativity known as divergent thinking. In the task, the participants were asked to generate as many creative uses as they could for a brick in 4 minutes. Then, they provided a self-assessment of their creative output.Two research assistants and a separate sample of 430 individuals recruit via Prolific then viewed and rated the 2,141 ideas that had been generated. In both cases, the raters were blind to the experimental conditions.As expected, participants in the cannabis use condition were more likely to feel “happy” and “joyful” compared to those in the control condition. Those who had consumed cannabis also rated their own ideas as more creative — an effect that was associated with their improved mood. Unexpectedly, however, this state of joviality did not translate into increased creativity. That is, the independent raters found the ideas that had been generated by those in the cannabis condition to be just as creative as those in the control condition.“Cannabis probably won’t actually make you any more or less creative,” Barnes told PsyPost.In a second study, which included 140 participants, the researchers sought to replicate and extend their findings. The participants were again randomly divided into two conditions. But they also completed a measure of cognitive functioning known as the Sternberg memory scanning task. Instead of completing an alternative uses task, the participants were instructed to complete a work-focused creativity task.“Participants were instructed to imagine that they were working at a consulting firm and had been approached by a local music band, File Drawers, to help them generate ideas for increasing their revenues. They were told that their goal was to generate as many creative ideas as possible in 5 min,” the researchers explained.As in the previous study, the participants again provided a self-assessment of their creative output. In addition, they were asked to provide evaluations of other people’s ideas as well.The researchers found that cannabis use did not significantly impact cognitive functioning. However, participants in the cannabis condition tended to have more favorable evaluations of others’ creativity compared to those in the control condition. “Cannabis will make you think you are more creative, and make you think others are more creative as well,” Barnes said.The findings are in line with a previous study, published in the journal Psychopharmacology, which found no evidence that cannabis consumption boosted creativity ability. So why is there a widespread belief that cannabis improves creativity? The positive self-evaluations elicited by cannabis-induced joviality might be the culprit.“The gap between the effect of cannabis on self-evaluations of creativity versus actual creativity explains the common lay belief and why it is actually incorrect,” Barnes told PsyPost.It is also possible that “creative types” are more drawn to cannabis. A study published in 2017 found cannabis users tended to be more extraverted and open to experience. They also performed better than non-users on a test of convergent thinking — meaning the creative process of narrowing down potential solutions to find one correct answer. But their enhanced creativity was entirely explained by their heightened openness to experience.In addition, cannabis might increase creativity — just not the types of creativity that were tested in the current research. It is still possible that cannabis does increase creativity in specific contexts, such as musical and artistic production.“We have two studies which have consistent findings,” Barnes explained. “But this is still a new and developing science. We would not consider our findings to be the final word. Creativity at work across many different contexts is probably much more complex than the relatively simple creativity tasks we used in our 2 studies. So the effects of cannabis on creativity may very well be more complicated than what we found at this stage in the program of research.”“Cannabis has become legalized in many states, and probably will become legalized in many more,” the researcher added. “So many managers will either have to consider how cannabis influences their own work, or manage employees who use cannabis. Rather than ignore cannabis as a taboo topic, management and applied scholars should work to further enlighten the effects of cannabis on work. Future results are bound to be both interesting and important.”The study, “Cannabis use does not increase actual creativity but biases evaluations of creativity“, was authored by Yu Tse Heng, Christopher M. Barnes, and Kai Chi Yam.","Cannabis use does not increase actual creativity but does increase how creative you think you are, study finds"
290,8,8_weight_study_tre_ends,https://www.aan.com/PressRoom/Home/PressRelease/5021,"Press ReleaseEMBARGOED FOR RELEASE UNTIL 4 PM ET, October 12, 2022Does the Mediterranean Diet Really Decrease Your Risk of Dementia?MINNEAPOLIS – A number of studies have suggested that eating a healthy diet may reduce a person’s risk of dementia, but a new study has found that two diets including the Mediterranean diet are not linked to a reduced risk of dementia. The study is published in the October 12, 2022 online issue of Neurology®, the medical journal of the American Academy of Neurology. The Mediterranean diet includes a high intake of vegetables, legumes, fruits, fish and healthy fats such as olive oil, and a low intake of dairy products, meats and saturated fatty acids. “Previous studies on the effects of diet on dementia risk have had mixed results,” said study author Isabelle Glans, MD, of Lund University in Sweden. “While our study does not rule out a possible association between diet and dementia, we did not find a link in our study, which had a long follow-up period, included younger participants than some other studies and did not require people to remember what foods they had eaten regularly years before.” For the study, researchers identified 28,000 people from Sweden. Participants had an average age of 58 and did not have dementia at the start of the study. They were followed over a 20-year period. During the study, participants filled out a seven-day food diary, a detailed food frequency questionnaire and completed an interview. By the end of the study, 1,943 people, or 6.9%, were diagnosed with dementia, including Alzheimer’s disease and vascular dementia. Researchers examined how closely participants’ diets aligned with conventional dietary recommendations and the Mediterranean diet. After adjusting for age, gender, and education, researchers did not find a link between following either a conventional diet or the Mediterranean diet and a reduced risk of dementia. Glans noted that further research is needed to confirm the findings. Nils Peters, MD, of the University of Basel in Switzerland, who wrote an editorial accompanying the study, said, “Diet on its own may not have a strong enough effect on memory and thinking, but is likely one factor among others that influence the course of cognitive function. Dietary strategies will still potentially be needed along with other measures to control risk factors.” A limitation of the study was the risk of participants misreporting their own dietary and lifestyle habits. The study was funded by the Swedish Research Council, the Knut and Alice Wallenberg Foundation, the Marianne and Marcus Wallenberg Foundation, the Strategic Research Area MultiPark at Lund University, the Swedish Alzheimer Foundation, the Swedish Brain Foundation and other organizations. Learn more about dementia at BrainandLife.org, home of the American Academy of Neurology’s free patient and caregiver magazine focused on the intersection of neurologic disease and brain health. Follow Brain & Life® on Facebook, Twitter and Instagram. When posting to social media channels about this research, we encourage you to use the hashtags #Neurology and #AANscience.The American Academy of Neurology is the world's largest association of neurologists and neuroscience professionals, with 38,000 members. The AAN is dedicated to promoting the highest quality patient-centered neurologic care. A neurologist is a doctor with specialized training in diagnosing, treating and managing disorders of the brain and nervous system such as Alzheimer's disease, stroke, migraine, multiple sclerosis, concussion, Parkinson's disease and epilepsy.For more information about the American Academy of Neurology, visit AAN.com or find us on Facebook, Twitter, LinkedIn, Instagram and YouTube.",New window into brainâs computational function
44,8,8_weight_study_tre_ends,https://eurapa.biomedcentral.com/articles/10.1186/s11556-022-00304-1,"Participants: Recruitment, analyses, and ethicsCohorts from two studies were used in this paper as outlined in Fig. 1. The Oslo cohorts of PM women were recruited consecutively through an outpatient medical clinic. The women gave their verbal and written consent as described previously [12]. These cohorts represented healthy women (n = 18) or women with established primary osteoporosis (n = 17), (age, 55–80 years) without cardiovascular, endocrine, or neurological diseases. The osteoporotic women received anti-resorption medication (bisphosphonates), which was withdrawn 3 months prior to starting this study. The number of previous smokers and the level of physical activity were similar between the groups, as were other lifestyle factors and nutrition as previously described [12]. The serum and urine biomarkers of all participants were normal. Clinical evaluation: All participants underwent a clinical examination and completed detailed interview questionnaires on present and previous diseases, nutrition, and lifestyle factors (smoking, alcohol, physical activity). All participants received daily supplements of vitamin D3 (1000 IU) and calcium (1000 mg). The Ball State University (BSU) cohorts have been extensively described previously [9] but are summarized below and in Table 1. BSU participants were excluded based on similar criteria as the Oslo cohort: any acute or chronic illness, cardiac, pulmonary, liver, or kidney abnormalities, uncontrolled hypertension, insulin- or non-insulin-dependent diabetes, abnormal blood, or urine chemistries, arthritis, a history of neuromuscular problems, or if they smoked tobacco. From the BSU cohorts comprising 12 old donors (6 women, 6 men, average age 84 ± 3 years) or 15 young donors (7 women, 8 men, average age 24 ± 4 years), we used transcriptome data based on thigh muscle biopsies taken before the first and before the last exercise in their 12-week training period. The data are available from GEO: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE28422Fig. 1 Outline of study Full size imageTable 1 Demographic characteristics of muscle biopsy donors Full size tableTraining protocolsOslo Cohorts: The total duration of the intervention was 13 weeks for healthy and 15 weeks for the patients with primary osteoporosis because the first two weeks were used to familiarize them with the training protocol starting with lighter training loads. The training loads were gradually increased to ensure that the 13 weeks of training were conducted with optimal loading to improve muscle strength and muscle mass [12]. The training period was performed as traditional heavy-load strength training: three times per week with 1–3 sets involving all major muscle groups as detailed previously [12]. Briefly, the training protocol consisted of three exercises for the leg muscles (squat, leg press, and standing toe rise), and three exercises for the upper body muscles (chest press, seated rowing, and shoulder press). In addition, the participants performed self-selected exercises for the abdominal and lower back muscles at the end of each session. The strength-training regimen was a mix of linear periodization and daily undulating periodization. The participants started with 8–12 repetition maximum (RM) sets, and ended the 13-week protocol with 4–8 RM sets. In two sessions per week, the sets were run until failure (RM-sets); in the third session, performed between the two maximal sessions, sets were run with a load corresponding to 80–90% of the actual RM load. The total duration of training was about 60 min per session, and the participants exercised in groups of three with a personal instructor present.The BSU cohorts underwent a 12-week training regimen with progressive resistance training (PRT). The groups performed a smaller, leg focused selection of exercises, but included 36 training sessions (3 days/week) with three sets of 10 bilateral knee extensions at 70–75% of their 1 RM [9]. Thus, training of the vastus lateralis muscle group was similar in the Oslo and BSU studies, although the load used in the BSU cohort was somewhat lower.Questions may be asked if the transcription repertoire (the transcriptome) in the basal resting state is the same in mechanically loaded skeletal muscle (e.g., thigh) as in unloaded muscle (e.g., iliac/pelvic). If not, their responses to training could have been different, and the results would have been needed a further explanation. Thus, we compared if these muscles, serving vastly different functions (dynamic versus static), had the same transcriptional profile in the basal, rested state selecting muscle biopsies from pelvis of well characterized female donors (12 healthy and 12 with osteoporosis) with similar age and BMI and not part of a training study [13].Muscle biopsy collection and RNA purificationBiopsy collection and RNA purification from the Oslo and BSU cohorts used similar methods and technology, as previously described [9, 10]. In brief, thigh muscle biopsies were obtained under local anesthesia (xylocaine adrenalin, 10 mg/ml + 5 μg/ml; AstraZeneca, London, UK) from the mid portion of the vastus lateralis before and after the training period using a modified Bergström technique. The muscle samples were obtained at least 2 days after any training or testing, and the second biopsy was obtained approximately 3 cm distal to the previous site. The biopsies were taken from fasting participants of both studies in the morning (07–09 AM) to allow for similar levels of physical activity and dietary intake. A sample (10–20 mg) to be used for RNA extraction was immediately frozen in liquid nitrogen (applied to the cohort of osteoporotic women; muscle biopsies from all other participants were stored in RNAlater (Merck, Darmstadt, Germany) for 1 day at 4 °C before freezing. The samples were then stored at -80 °C until RNA purification using a RNeasy Mini Kit (Qiagen, Oslo, Norway) according to the manufacturer’s instructions.Microarray and data analysisThe same type of microarray analysis was performed on the BSU and Oslo cohorts employing Affymetrix HG-U133 Plus 2.0 or Affymetrix HuGene-1_0-st-v1 arrays (Thermo Fisher Scientific, Waltham, MA, USA). Robust microarray analysis (RMA) yielding normalized log2 transformed signal intensities was applied for normalization for both array types. (http://bip.weizmann.ac.il/toolbox/overview/Partek_Users_Guide.pdf). Gene transcripts with maximal signal values of < 5 (log2 values) across all arrays were removed to filter for low- and non-expressed genes. Differentially expressed transcripts before vs. after training were identified using two-way analysis of variance (ANOVA) as implemented in Partek Genomics Suite (Partek, St. Louis, MO, USA).Further bioinformatics analysis on thigh muscle biopsies was conducted on the significant genes to identify functional implications with Ingenuity Pathway Analysis (Ingenuity Systems, Redwood City, CA, USA). The microarray data generated from the Oslo and BSU cohorts have previously been validated using real-time qRT-PCR on selected genes [9, 11].Comparison of basal iliac and thigh muscle transcriptomesIn a previous study [13] we collected postmenopausal trans-iliac bone biopsies using a Bordier trephine [14]. Iliac muscle attached to the pelvic side of these bone biopsies were separated and snap frozen in liquid nitrogen (n = 24). Subsequent RNA isolation was performed as described for thigh muscle biopsies. However, Affymetrix HG-U133 Plus 2.0 arrays were used for transcriptome profiling, while HuGene-1_0-st-v1 arrays were used for transcriptome profiling of thigh muscle RNA. Use of different array types prevented us from normalizing the two datasets together. However, we conducted a Pearson correlation analysis of the Log 2 transformed signal values using the 17 652 transcripts present in both datasets (93.5% of all thigh muscle transcripts). To visualize the similarity the average signal values for the 17 652 transcripts were plotted against each other (Fig. S1). As a further measure of similarity/difference between the iliac and thigh muscle transcriptomes a paired T-test was performed on the datasets.Functional enrichment analysisFunctional enrichment analysis identifies trends in large scale biological datasets and determines whether some functions are enriched in our set of differentially expressed genes. We used Ingenuity Pathway Analysis (IPA) (Qiagen, Beverly, MA, USA) for this task. Fisher’s exact test was used by IPA to identify enriched gene sets with all genes on the Human Gene 1.0 ST Array as the background gene list. Further information is found in the Table 2 legend.Table 2 Over-represented diseases and functions Full size tablePrincipal component analysis (PCA)PCA is a frequently used dimensionality-reduction method with the aim to reduce the dimensionality of large data sets by transforming extra sized sets of variables into smaller ones that still contain most of the information. We used the PCA generator as implemented in the software Partek Genomics Suite (Partek, St. Louis, MO, USA).Volcano plotThe volcano plot was generated using the volcano plot generation module in Partek Genomics Suite 6.6 (Partek).Analyses of association between transcripts responsive to heavy-load exercise training with muscle eQTLs and loci for handgrip strength in womenWe identified SNPs associated with skeletal muscle expression for the genes listed in Table S1 using eQTL databases from the Genotype-Tissue Expression (GTEx) portal (https://gtexportal.org). Subsequently, significant SNPs were tested for association with low handgrip strength in females as summarized in a meta-analysis [11] adopting a nominal p-value < 0.01.",Heavy-load exercise in older adults activates vasculogenesis and has a stronger impact on muscle gene expression than in young adults
43,8,8_weight_study_tre_ends,https://ethz.ch/en/news-and-events/eth-news/news/2022/10/how-genetics-influences-our-body-weight-beyond-the-genes.html,"Heredity plays a role in how strongly we are predisposed to put on excess weight. In recent years, researchers have extensively examined which genes and gene variants play a role in this, and have identified roughly one hundred obesity susceptibility genes. However, genome-wide association studies have shown that less than half of all cases of hereditary obesity can be explained by these genes. The other half are the result of factors that, although part of our DNA, are not genes in the classical sense. Epigenetic information would be one example of such a factor.A group of researchers led by Professor Markus Stoffel from the Department of Biology have now identified a further non-classic genetic risk factor for hereditary obesity: an endogenous microRNA molecule known as microRNA-7.Like genes, the blueprints for microRNA molecules are part of our chromosomes. But while genes act as the building instructions for proteins, the information contained in microRNA is not translated into protein form. Instead, the microRNA molecules act in our cells in the form of RNA. “MicroRNA-7 is the first microRNA for which we’ve been able to demonstrate an association with obesity,” Stoffel says.Effect on mice and humansTogether with his team, Stoffel bred mice in which microRNA-7 was missing in certain nerve cells of the hypothalamus, the control center between the endocrine system and nervous system. These mice demonstrated a pathologically increased appetite and became obese.The ETH researchers were also able to demonstrate such a link in humans. Together with scientists from the University of Cambridge, they analysed genomic and medical data, including the anonymised data held in a British database relating to 500,000 people. This allowed Stoffel and his colleagues to show that people with genetic variations on their chromosomes near the blueprint for microRNA-7 are heavier and bigger than average. The consequence of these genetic variations is that the above-mentioned nerve cells of the people affected produce less microRNA-7.The scientists were also able to show that in these cells microRNA-7 affects a biochemical pathway known to be instrumental in maintaining the body’s energy balance, regulating appetite and controlling the production of growth hormones. MicroRNA acts there by regulating the production of proteins.For Stoffel, it was no surprise that this effect can be observed in both mice and humans. As he points out, microRNA-7 is a molecule that emerged very early in the evolutionary history of the animal kingdom and has survived to this day. It persists unaltered in very many animal species – from nematodes, to all vertebrates and human beings.Therapeutic potential“Up to now, it was unclear why genetic variations were only able to provide an explanation for less than half the causes of hereditary obesity,” Stoffel says. “Our study now shows that it’s not enough to look for the answer solely in the genes that encode information for proteins. The parts of DNA outside the genes also have to be examined, such as the regions containing the blueprints for microRNA.”In theory, at least, these new findings could also be used in medicine. There are already RNA-based drugs that use the mechanisms of action of microRNA molecules in the body. It may one day be possible to develop a treatment for people who are obese as a result of their hypothalamus producing insufficient amounts of microRNA-7. Treatment for the converse case would also be conceivable – for people with a predisposition to pathological low body weight microRNA-7 could be pharmacologically inhibited.But, Stoffel says, it is more likely that the still quite novel forms of RNA treatment will initially be used for neurodegenerative conditions such as Alzheimer’s disease. In the long term, as the safety of RNA therapy in the central nervous system is established, he believes it is possible that metabolic disorders such as obesity and unintentional weight loss may also be treated in this way.",How genetics influences our body weight beyond the genes
398,8,8_weight_study_tre_ends,https://www.kcl.ac.uk/news/snacking-on-almonds-boosts-gut-health,"A team of researchers from King’s investigated the impact of whole and ground almonds on the composition of gut microbes. The study, published today in the American Journal of Clinical Nutrition, is funded by the Almond Board of California.The gut microbiome consists of thousands of micro-organisms living in the gut. These play a vital role in digesting nutrients and can have a positive or negative influence on our health, including our digestive and immune systems. The mechanisms of how the gut microbiomes have an impact on human health is still being investigated, but evidence suggests eating specific types of food can positively influence the types of bacteria in our gut or what they do in our gut.Researchers recruited 87 healthy adults who were already eating less than the recommended amount of dietary fibre and who snacked on typical unhealthy snacks (e.g. chocolate, crisps). Participants were split into three groups: one group changed their snacks for 56 g of whole almonds a day, another for 56 g of ground almonds a day, and the control group ate energy-matched muffins as a control. The trial lasted four weeks.",Snacking on almonds boosts gut health
288,8,8_weight_study_tre_ends,https://twin-cities.umn.edu/news-events/individuals-experiencing-food-insecurity-likely-binge-eat-when-food-available,"Young adults experiencing food insecurity may be prone to binge eating in times when food is available, according to a new study from the University of Minnesota School of Public Health (SPH).SPH researchers found support for the “feast or famine” cycle hypothesis in food-insecure households. Findings from their study are consistent with the idea that fluctuating levels of food availability throughout the month may lead people to restrict their food intake out of necessity when food is scarce, and then compensate by overeating when food is more plentiful. Binge eating is a behavior linked with a number of adverse physical and mental health consequences, including type 2 diabetes and depression.The study, co-authored by Vivienne Hazzard, a post-doctoral associate in SPH, and published in Appetite, surveyed 75 young adults living in food-insecure households several times each day over a two-week period to investigate how momentary levels of food security related to binge-eating symptoms later in the day. The researchers found:Significantly more binge-eating symptoms occurred in the hours following instances of increased food security among young adults.This link only existed for young adults using food assistance programs, not for those who did not report using food assistance.The researchers suggest rethinking the timing of benefits distribution in food assistance programs. Hazzard noted that the way food assistance programs are currently structured may inadvertently exacerbate the “feast-or-famine” cycle.“Policymakers should consider how they could provide more stable and consistent access to adequate food,” said Hazzard. “While programs such as the Supplemental Nutrition Assistance Program (SNAP) and Women, Infants and Children (WIC) are critical to improving food security in the U.S., some have called for a restructuring of their benefit distribution schedules. Currently, these programs distribute benefits only once a month. Our study adds to a growing body of evidence that suggests more frequent distribution of benefits may be warranted.”Additional research in this area should test whether intervening to promote more stable food access can interrupt that cycle and reduce binge eating.This research was supported by the National Heart, Lung, and Blood Institute, the National Institute of Mental Health, the National Institute of Child Health and Human Development, the National Institute of Diabetes and Digestive and Kidney Diseases, and the National Institute of General Medical Science.-30-About the School of Public HealthThe University of Minnesota School of Public Health improves the health and wellbeing of populations and communities around the world by bringing innovative research, learning, and concrete actions to today’s biggest health challenges. We prepare some of the most influential leaders in the field, and partner with health departments, communities, and policymakers to advance health equity for all. Learn more at sph.umn.edu.",Individuals experiencing food insecurity likely to binge eat when food is available
482,8,8_weight_study_tre_ends,https://www.reuters.com/world/europe/germany-legalize-cannabis-use-recreational-purposes-2022-10-26/,"[1/4] A person holds a joint as activists gather to mark the annual world cannabis day and to protest for legalization of marijuana, in front of the Brandenburg Gate, in Berlin, Germany, April 20, 2022. REUTERS/Lisi Niesner/File PhotoBERLIN, Oct 26 (Reuters) - Germany set out plans on Wednesday to legalise cannabis, a move Chancellor Olaf Scholz's government said would make Germany one of the first countries in Europe to do so.Health Minister Karl Lauterbach presented a cornerstone paper on planned legislation to regulate the controlled distribution and consumption of cannabis for recreational purposes among adults.Acquiring and possessing 20 to 30 grams of recreational cannabis for personal consumption would also be made legal.The coalition government struck an agreement last year to introduce legislation during its four-year term to allow the controlled distribution of cannabis in licensed shops.Lauterbach did not give a timeline for the plan.Many countries of the region have already legalised cannabis for limited medicinal purposes, including Germany since 2017. Others have decriminalised its general use, while stopping short of making it legal.According to the paper, private self-cultivation would be permitted to a limited extent. Ongoing investigations and criminal proceedings connected to cases no longer illegal would be terminated.The government will also introduce a special consumption tax, and develop cannabis-related education and abuse prevention programmes.Legalising cannabis could bring Germany annual tax revenues and cost savings of about 4.7 billion euros ($4.7 billion) and create 27,000 new jobs, a survey found last year.Some 4 million people consumed cannabis in Germany last year, 25% of whom were between ages 18 and 24, Lauterbach said, adding the legalisation would squeeze out the cannabis black market.Germany will present the paper to the European Commission for pre-assessment and will only draft a law once the Commission approves the plan, the minister added.""If the EU Commission says no to Germany’s current approach, our government should seek alternative solutions. Not just say: Well, we tried our best,"" said Niklas Kouparanis, chief executive Bloomwell Group, one of Germany's largest cannabis firms.Berlin should have a plan B if the EU rejects the legalisation, Kouparanis said, adding that cannabis imports should be permitted as domestic cultivation will not be able to meet demand in the short term.The decision has already stirred a mix of reactions across Europe's biggest economy.Germany's pharmacists association warned of the health risks of legalising cannabis and said it would put pharmacies in medical conflict.Pharmacists are health care professionals, so ""a possible competitive situation with purely commercial providers is viewed particularly critically,"" Thomas Preis, head of the North Rhine Pharmacists' Association, told the Rheinische Post newspaper.The legalisation plan has not been welcomed by all federal states. Bavaria's health minister, for instance, warned that Germany should not become a drug tourism destination in Europe.But Germany's Greens said decades of prohibiting cannabis have only exacerbated the risks.""Because too-restrictive conditions for the legal market only promote the black market for particularly strong cannabis,"" lawmaker Kirsten Kappert-Gonther said on Wednesday.Lars Mueller, chief executive of German cannabis firm SynBiotic, said Wednesday's step was ""almost like winning the lottery"" for his company.""When the time comes, we will be able to offer franchise-like models for cannabis stores in addition to our own stores,"" Mueller said.($1 = 0.9995 euro)Reporting by Riham Alkousaa, editing by Miranda Murray and Bernadette BaumOur Standards: The Thomson Reuters Trust Principles.",Germany to legalize cannabis use for recreational purposes
316,8,8_weight_study_tre_ends,https://www.cell.com/cell-reports-medicine/fulltext/S2666-3791(22)00332-9,"Despite being an underdeveloped and less prosperous part of the country, the prevalence of abdominal obesity in the Shaanxi province in Northwestern China is close to the national average of 31.5%,and it can thus be regarded as a representative sample of China. In this region, regular eating habits include carbohydrate-rich staple foods, and an eating pattern of consuming three meals a day with late dinner and multiple snacks including a midnight snack is prevalent. In this study, we conducted a 3-month randomized clinical trial (RCT) to determine the effects of an LCD, TRE, and their combination on body weight, fat mass, and cardiometabolic outcomes in adults with MetS in Shaanxi, China. Furthermore, we allowed participants to choose freely between eTRE and lTRE, to keep their social eating pattern. We hypothesized that TRE effectively improves metabolic disease risk parameters without restricting carbohydrate consumption and that combination of TRE with LCD leads to additional metabolic benefits.Geographic variation in prevalence of adult obesity in China: results from the 2013-2014 national chronic disease and risk factor surveillance.Among dietary interventions, low-carbohydrate diet (LCD) seems more ideal to induce weight loss in overweight individuals compared with low-fat diet,as LCD generally exerts more rapid weight reduction with a greater loss in body fat and maintenance of lean mass.LCD restricts carbohydrate consumption to <26% of energy intake (or <130 g carbohydrate/day) and does not contain specific carbohydrates, such as starch and sugar, while containing healthy fats and a moderate protein content.The lower carbohydrate content of LCD is known to reduce insulin secretion, which promotes fat oxidation and lipolysis during negative energy balance.In addition to LCD, time-restricted eating (TRE) has become increasingly popular in recent years for inducing clinically significant weight reduction and ameliorating metabolic disorders.TRE is defined by intentionally restricting the times during the day when energy is consumed, confining the temporal window of food access to a specified number of hours each day, and fasting (water and tea without sugar or any artificial sweeteners are permitted) for the remainder of the day. Importantly, it is not necessary to monitor caloric intake in any way during the eating window. Chronic circadian disruption can aggravate the risk for components of MetS,and TRE maintains a robust daily cycle of eating and fasting to support circadian rhythm.One of the most popular regimens of TRE is 8-h TRE (also known as “the 16:8 diet”). There are specific regimens of TRE according to the timing of the eating window,including early TRE (eTRE) that involves eating earlier in the day and late TRE (lTRE) that skips breakfast. Although both LCD and TRE have been demonstrated to be metabolically beneficial,whether these 8-h TREs could exert a rapid weight reduction effect comparable to that of LCD in adults with MetS has not been assessed yet.Early time-restricted feeding improves insulin sensitivity, blood pressure, and oxidative stress even without weight loss in men with prediabetes.The relationship between working schedule patterns and the markers of the metabolic syndrome: comparison of shift workers with day workers.Health consequences of electric lighting practices in the modern world: a report on the National Toxicology Program's workshop on shift work at night, artificial light at night, and circadian disruption.Early time-restricted feeding improves insulin sensitivity, blood pressure, and oxidative stress even without weight loss in men with prediabetes.Effects of 4- and 6-h time-restricted feeding on weight and cardiometabolic health: a randomized controlled trial in adults with obesity.A lower-carbohydrate, higher-fat diet reduces abdominal and intermuscular fat and increases insulin sensitivity in adults at risk of type 2 diabetes.Dietary carbohydrate restriction as the first approach in diabetes management: critical review and evidence base.Dietary carbohydrates: a review of international recommendations and the methods used to derive them.A lower-carbohydrate, higher-fat diet reduces abdominal and intermuscular fat and increases insulin sensitivity in adults at risk of type 2 diabetes.Effects of diet macronutrient composition on body composition and fat distribution during weight maintenance and weight loss.A lower-carbohydrate, higher-fat diet reduces abdominal and intermuscular fat and increases insulin sensitivity in adults at risk of type 2 diabetes.Metabolic syndrome (MetS) is characterized by abdominal obesity, elevated blood pressure, and fasting blood glucose (FBG) as well as atherogenic dyslipidemia with high triglyceride (TG) and low high-density lipoprotein cholesterol (HDL-c) levels,and it remarkably increases the risk of type 2 diabetes mellitus (T2DM) and cardiovascular diseases (CVD).MetS has long been highly prevalent in Western countries and has steeply increased in the Chinese population over the past decades as well. Abdominal obesity is of central importance in the induction of metabolic dysfunctions including hypertension, hyperglycemia, atherogenic dyslipidemia, and release of proinflammatory cytokines by adipose tissue. Since 2004, the prevalence of general obesity in China has increased 3-fold, and abdominal obesity has increased by more than 50%; concomitantly, a rapid increase in the incidences of T2DM and CVD was also observed.Since even mild body weight reduction can ameliorate metabolic dysfunction,the first line of therapy for MetS comprises lifestyle interventions including aggressive dietary adjustment to reduce body weight.Nevertheless, long-term adherence to lifestyle intervention is always a challenge.The CardioMetabolic health alliance: working toward a new care model for the metabolic syndrome.Prevalence of diabetes recorded in mainland China using 2018 diagnostic criteria from the American Diabetes Association: national cross sectional study.Harmonizing the metabolic syndrome: a joint interim statement of the international diabetes federation task force on epidemiology and prevention; national heart, lung, and blood institute; American heart association; world heart federation; international atherosclerosis society; and international association for the study of obesity.The CardioMetabolic health alliance: working toward a new care model for the metabolic syndrome.We also analyzed acceptability and feasibility of the interventions. As shown in Table 2 , participants’ self-reported compliance with their meal-eating window during the 8-h TRE intervention was on average 65.9 ± 3.0 days out of the 3-month intervention period, which was significantly more to adherence to LCD (55.5 ± 3.5 days; p = 0.024). In addition, adherence to eTRE was substantially less (61.4 ± 4.0 days) compared with lTRE (74.9 ± 2.7 days; p = 0.031, Table S6 ). Nonetheless, at the end of our study, 46 out of 47 participants (98%) in the LCD group and 43 out of 44 participants (98%) in the TRE group who completed the intervention period reported to be willing to continue. In contrast, only 36 out of 44 participants (82%) in the combination group reported to be willing to continue with the intervention, which was significantly lower compared with LCD (p = 0.010) and TRE (p = 0.014).LCD, low-carbohydrate diet; TRE, time-restricted eating; Both, combination treatment. Differences between treatment arms (LCD, TRE, and Both) were tested by Chi-square test.No serious adverse events were observed. Approximately five adverse events were regarded as probably associated with the diet interventions, including constipation, dizziness, insomnia, dry mouth, and alopecia. The occurrence rate of adverse events was not significantly different among the three groups ( Table 3 ). Independent of the diet intervention, two participants reported the exacerbation of lumbar disc herniation and lithangiuria requiring surgery during the 3-month intervention, which caused withdrawal from the trial.Although none of the treatments had benefits on systolic blood pressure (SBP) ( Figure 3 I), diastolic blood pressure (DBP) was significantly reduced by combination treatment, but not by LCD and TRE alone ( Figure 3 J) after 3 months of intervention. Compared with changes of each group, no significant difference in DBP was observed among three treatments ( Table 2 ). When combined with LCD, eTRE significantly reduced DBP, whereas lTRE did not ( Table S3 ), albeit that eTRE did not induce a significantly different effect on DBP compared with lTRE. Both SBP and DBP strongly correlated to VFA but not SFA ( Figure S2 D).TRE with and without LCD, but not LCD alone, reduces diastolic blood pressureLCD did not cause any significant differences in plasma levels of TG, HDL-c, or the ratio between TG and HDL-c (TG/HDL-c ratio) after 3 months of intervention ( Figures 3 F, 3G, and 3H and Table 2 ). In marked contrast, TRE with and without LCD significantly reduced TG level and TG/HDL-c ratio, and the change in TG and TG/HDL-c ratio was significantly different between LCD and combination treatment (TG: −0.15 [1.20] mmol/L versus −0.51 [2.01] mmol/L, p = 0.011; TG/HDL-c: −0.02 [1.20] versus −0.59 [2.13], p = 0.003). While TRE did not affect low-density lipoprotein cholesterol (LDL-c) levels, LCD with and without TRE even significantly increased LDL-c levels ( Table 2 ). Moreover, we did not find any significant difference between eTRE and lTRE in the improvements of dyslipidemia ( Table S6 ). In line with the prominent contribution of VFA to glycemic control, TG/HDL-c ratio was only significantly correlated with VFA, not SFA ( Figure S2 C). Taken together, while LCD adversely affects LDL-c, TRE improves the lipoprotein profile.TRE with and without LCD, but not LCD alone, improves dyslipidemia(A–J) Change in (A) fasting blood glucose (FBG), (B) uric acid, (C) hemoglobin A1c (HbA1c), (D) fasting insulin, (E) homeostasis model assessment-IS (HOMA-IS), (F) triglycerides (TG), (G) high-density lipoprotein cholesterol (HDL-c), (H) triglycerides/high-density lipoprotein cholesterol (TG/HDL-c), (I) systolic blood pressure (SBP), and (J) diastolic blood pressure (DBP) among the low-carbohydrate diet (LCD), 8-h time-restricted eating (TRE), and combination treatment (Both) groups after 3 months of the intervention. Analyses were conducted using all participants (intention-to-treat), using a multiple imputation approach for other missing data. Each black data point represents an individual participant (LCD, n = 55; TRE, n = 55; Both, n = 52). Change from baseline is presented as mean ± standard error of the mean (SEM) for normally distributed variables or the median (interquartile range) for abnormal distribution. # p < 0.05, ## p < 0.01, ### p < 0.001: pairwise comparisons of change scores between the groups (e.g., TRE versus LCD, TRE versus Both, LCD versus Both) were evaluated by t test or Mann-Whitney U test. ∗p < 0.05, ∗∗p < 0.01, ∗∗∗p < 0.001: significant differences shown at x axis compared with baseline (paired t test or paired Wilcoxon test).We next compared the effects of LCD, TRE, and their combination on glycemic control. In line with findings on abdominal VFA, TRE with or without LCD, but not LCD alone, significantly improved FBG and UA ( Figures 3 A and 3B and Table 2 ). Only combination treatment significantly decreased hemoglobin A1c (HbA1c) ( Figure 3 C and Table 2 ). In contrast, compared with baseline, all three treatments clearly improved fasting insulin levels ( Figure 3 D), C-peptide, homeostasis model assessment of insulin resistance (HOMA-IR), homeostatic model assessment of insulin sensitivity (HOMA-IS) ( Figure 3 E), and quantitative insulin-sensitivity check index (QUICKI) ( Table 2 ). Notably, combination treatment caused more prominent changes on UA (combination: −51 ± 13 μmol/L, versus LCD: −17 ± 11 μmol/L, p = 0.039), HOMA-IR (combination: −2.16 [4.82], versus LCD: −1.15 [2.99], p = 0.049), HOMA-IS (combination: 0.10 [0.20], versus LCD: 0.03 [0.12], p = 0.042) and QUICKI (combination: 0.02 [0.03], versus LCD: 0.01 [0.02], p = 0.004) compared with LCD ( Figures 3 B and 3E and Table 2 ), indicating that the combination of LCD and TRE is most effective to combat cardiometabolic risk factors among the three interventions. Furthermore, we did not observe any significant differences in parameters related to glucose and insulin metabolism between eTRE and lTRE, whereas combined with LCD, eTRE displayed a further improvement of HOMA-IS ( Table S6 ). As shown in Figures S2 A and S2B, HOMA-IS and UA were significantly correlated with VFA but not with SFA.Both abdominal visceral fat area (VFA) and abdominal subcutaneous fat area (SFA) play important but distinct roles in metabolic function. Thus, we further dissected the changes in these two fat depots using bioelectrical impedance analysis. Although after 3 months of intervention all three treatments induced similar reduction of SFA ( Figure 2 H), VFA was only decreased by TRE (−13 ± 5 cm) and combination treatment (−10 ± 3 cm Figure 2 I and Table 2 ). Compared with LCD (6 ± 5 cm), VFA was significantly reduced by both TRE (p = 0.009) and combination treatment (p = 0.016). Furthermore, as shown in Table S6 , eTRE significantly reduced VFA and SFA, whereas lTRE did not, albeit that the change of VFA or SFA induced by eTRE and lTRE alone or combined with LCD did not differ.Abdominal fat is a pivotal risk factor and one of the drivers of the metabolic risk related to overweight and obesity. Waist-to-hip ratio (WHR), an indicator of abdominal obesity, is more closely correlated to MetS than body mass index (BMI).As compared with baseline, all three treatments induced a significant reduction of waist circumference, hip circumference, and body fat mass ( Figures 2 D, 2E, and 2F) after 3 months of intervention. Nevertheless, only TRE induced a more prominent reduction of WHR (−0.04 ± 0.01, Figure 2 G and Table 2 ) compared with LCD (−0.01 ± 0.01, p = 0.023) and combination (−0.01 ± 0.01, p = 0.033), suggesting that TRE more effectively alleviates abdominal obesity than LCD.The association of differing measures of overweight and obesity with prevalent atherosclerosis: the Dallas Heart Study.Obesity and the risk of myocardial infarction in 27, 000 participants from 52 countries: a case-control study.TRE, LCD, and their combination reduce subcutaneous fat, while only TRE with and without LCD reduces abdominal visceral fatLCD, low-carbohydrate diet; TRE, time-restricted eating; Both, combination treatment; BMI, body mass index; HOMA-IR, homeostasis model assessment insulin resistance; HOMA-IS, homeostatic model assessment of insulin sensitivity; QUICKI, quantitative insulin-sensitivity check index; LDL-c, low-density lipoprotein cholesterol; HDL-c, high-density lipoprotein cholesterol. All data were presented as mean ± standard error of the mean (SEM) for normally distributed variables or the median (interquartile range) for abnormal distribution. Change scores from baseline were represented by “Δ” in the table. Analyses were conducted using all participants (intention-to-treat), using a linear mixed model with randomized dietary intervention as factor to correct for the correlations of repeated measurements on changes in body weight, and using a multiple imputation approach for other missing data. After 3 months of intervention, pairwise comparisons of change scores between the groups (e.g., TRE vs. LCD, TRE vs. Both, LCD vs. Both) were evaluated by t test or Mann-Whitney U test. For weight a : significant differences compared with 1 month before (paired t test); for other parameters: significant differences compared with baseline (paired t test or paired Wilcoxon test).For (A) and (D)–(I), analyses were conducted using all participants (intention-to-treat) using a linear mixed model with randomized dietary intervention as factor to correct for the correlations of repeated measurements on changes in body weight and using a multiple imputation approach for other missing data. Each black data point represents an individual participant (LCD, n = 55; TRE, n = 55; Both, n = 52). Change from baseline is presented as mean ± standard error of the mean (SEM). # p < 0.05, ## p < 0.01, ### p < 0.001: pairwise comparisons of change scores between the groups (e.g., TRE versus LCD, TRE versus Both, LCD versus Both) were evaluated by t test or Mann-Whitney U test. ∗p < 0.05, ∗∗p < 0.01, ∗∗∗p < 0.001: significant differences shown at x axis compared with baseline (paired t test or paired Wilcoxon test). For (B), each column represents relative body weight change for each participant. For (C), change from baseline is presented as mean ± SEM, a p < 0.05, b p < 0.001: significant differences compared with 1 month before (paired t test).(C–I) Mean decrease in (C) body weight after 1, 2, and 3 months among three groups. Change in (D) waist circumstance, (E) hip circumstance, (F) body fat mass, (G) waist-to-hip ratio (WHR), (H) subcutaneous fat area (SFA), (I) visceral fat area (VFA) among three groups after 3 months of the intervention.(A and B) Body weight change (A), relative body weight change (B) for the low-carbohydrate diet (LCD), 8-h time-restricted eating (TRE), and combination treatment (Both) groups during the 3-month intervention period.As compared with baseline, after 3 months of intervention a significant reduction of body weight was observed in all three groups ( Figures 2 A and 2B ), and only combination treatment induced a further reduction of body weight at month 3 compared with month 2 ( Figure 2 C and Table 2 ). Moreover, as shown in Table 2 , combination treatment induced a higher reduction in body weight (−5.0 ± 0.4 kg) compared with either LCD (−2.2 ± 0.3 kg, p < 0.001) or TRE alone (−3.4 ± 0.4 kg, p = 0.004), and a significant difference in body weight reduction was also observed between LCD and TRE treatment (p = 0.013). Furthermore, both eTRE and lTRE alone or combined with LCD led to a sustained reduction of body weight as early as after 1 month, which persisted over 3 months ( Table S6 ).LCD, low-carbohydrate diet; TRE, time-restricted eating; Both, combination treatment; BMI, body mass index; HOMA-IR, homeostasis model assessment insulin resistance; HOMA-IS, homeostatic model assessment of insulin sensitivity; QUICKI, quantitative insulin-sensitivity check index; LDL-c, low-density lipoprotein cholesterol; HDL-c, high-density lipoprotein cholesterol. All data are presented as the mean ± standard error of the mean (SEM) for normal distribution or median (interquartile range) for abnormal distribution. Differences between treatment arms (LCD, TRE, and both) were tested by one-way ANOVA or Kruskal-Wallis H test.A total of 290 individuals were screened, and 77 were excluded because they did not meet one or more inclusion criteria. A total of 169 participants were randomized into the low-carbohydrate diet (LCD) group (n = 56), the 8-h time-restricted eating (TRE) group (n = 57), or the combination group (n = 56), and 162 participants received a diet intervention. During the 3 months of intervention, eight participants (LCD group, n = 1; TRE group, n = 6; combination group, n = 1) discontinued diet intervention due to lack of motivation or inability to stick to the diet. At the end of the 3-month trial, 47 participants (m/f 27/20) completed the LCD treatment, 44 participants (m/f 31/13) completed the TRE treatment (30 [m/f 20/10] completed early TRE, and 14 [11/3] completed late TRE), and 44 participants (m/f 31/13) completed the combination treatment (27 [m/f 18/9] completed early TRE, and 17 [13/4] completed late TRE).As illustrated in Figure 1 , 290 participants were screened for this study, and 121 were excluded because they did not meet the inclusion criteria, had scheduling conflicts, or declined to participate. A total of 169 participants were randomized to receive intervention with LCD (group A; n = 56), TRE (group B; n = 57), or their combination (group C; n = 56), and after dropout of seven individuals, 162 individuals finally participated in the study. All participants met three or more MetS criteria at enrollment, and a minority (n = 62) of participants were on medication. This trial started with a 2-week weight stabilization and was followed by a 3-month intervention. At the end of the 3-month trial, 47 participants completed LCD, 44 completed TRE, and 44 completed their combination intervention. The main reason for dropout was scheduling conflicts. Table 1 and Table S1 show the baseline characteristics of the participants (n = 162). In this trial, we allowed participants to choose freely between two meal-eating windows for participants: 8 a.m. to 4 p.m. (eTRE) and 12 p.m. to 8 p.m. (lTRE), and we compared effects of two meal-eating windows using an exploratory analysis. In the TRE group, 38 participants (m/f 23/15) chose eTRE, and 17 participants (m/f 12/5) chose lTRE. In the combination group, 32 participants (m/f 22/10) chose eTRE, and 20 participants (m/f 15/5) chose lTRE. Table S2 shows the baseline characteristics of the eTRE and lTRE subgroups within the TRE and combination groups. At baseline, there were no significant differences in primary outcomes (i.e., body weight and abdominal fat area) or any secondary outcomes (i.e., body composition, glycemic control, plasma lipids, uric acid [UA], and blood pressure) between groups and subgroups. Table S3 clearly shows that participants receiving LCD or combination intervention had decreased intake of food containing high carbohydrates, such as rice, wheat flour, and pastry. Table S4 and Figure S1 show physical activity and daily step counts of participants, respectively, and demonstrate that participants maintained their usual physical activity throughout the study. Furthermore, participants with or without more than 50% dietary log records during the first 2 weeks of the intervention period showed similar responses to every treatment on primary outcomes after 3 months of intervention ( Table S5 ).DiscussionTo our knowledge, this is the first clinical trial that directly compared the efficacy of weight loss and improvement of metabolic parameters of an LCD, 8-h TRE, and their combination in adults with MetS. We showed that although all three treatments significantly reduce body weight accompanied by a reduction in SFA, TRE yielded more benefits on abdominal visceral obesity and cardiometabolic outcomes and caused higher adherence to intervention compared with LCD. Moreover, both meal-eating windows of TRE (i.e., eTRE and lTRE) showed comparable beneficial effects on body weight, abdominal visceral fat, glucose metabolism, lipoprotein profile and blood pressure, as well as adherence. In addition, we observed that VFA, but not SFA, significantly correlated with several cardiometabolic parameters, including HOMA-IS, UA, the TG/HDL-c ratio, SBP, and DBP.29 Sato J.Kanazawa A.Makita S.Hatae C.Komiya K.Shimizu T.Ikeda F.Tamura Y.Ogihara T.Mita T.et al. A randomized controlled trial of 130 g/day low-carbohydrate diet in type 2 diabetes with poor glycemic control. 33 Schmidt S.Christensen M.B.Serifovski N.Damm-Frydenberg C.Jensen J.E.B.Fløyel T.Størling J.Ranjan A.Nørgaard K. Low versus high carbohydrate diet in type 1 diabetes: a 12-week randomized open-label crossover study. 34 Abbasi J. Interest in the ketogenic diet grows for weight loss and type 2 diabetes. 9 Samaha F.F.Iqbal N.Seshadri P.Chicano K.L.Daily D.A.McGrory J.Williams T.Williams M.Gracely E.J.Stern L. A low-carbohydrate as compared with a low-fat diet in severe obesity. , 33 Schmidt S.Christensen M.B.Serifovski N.Damm-Frydenberg C.Jensen J.E.B.Fløyel T.Størling J.Ranjan A.Nørgaard K. Low versus high carbohydrate diet in type 1 diabetes: a 12-week randomized open-label crossover study. , 35 Foster G.D.Wyatt H.R.Hill J.O.McGuckin B.G.Brill C.Mohammed B.S.Szapary P.O.Rader D.J.Edman J.S.Klein S. A randomized trial of a low-carbohydrate diet for obesity. , 36 Bazzano L.A.Hu T.Reynolds K.Yao L.Bunol C.Liu Y.Chen C.S.Klag M.J.Whelton P.K.He J. Effects of low-carbohydrate and low-fat diets: a randomized trial. 9 Samaha F.F.Iqbal N.Seshadri P.Chicano K.L.Daily D.A.McGrory J.Williams T.Williams M.Gracely E.J.Stern L. A low-carbohydrate as compared with a low-fat diet in severe obesity. 36 Bazzano L.A.Hu T.Reynolds K.Yao L.Bunol C.Liu Y.Chen C.S.Klag M.J.Whelton P.K.He J. Effects of low-carbohydrate and low-fat diets: a randomized trial. 37 Goldenberg J.Z.Day A.Brinkworth G.D.Sato J.Yamada S.Jönsson T.Beardsley J.Johnson J.A.Thabane L.Johnston B.C. Efficacy and safety of low and very low carbohydrate diets for type 2 diabetes remission: systematic review and meta-analysis of published and unpublished randomized trial data. 38 Liu D.Huang Y.Huang C.Yang S.Wei X.Zhang P.Guo D.Lin J.Xu B.Li C.et al. Calorie restriction with or without time-restricted eating in weight loss. 16 Wilkinson M.J.Manoogian E.N.C.Zadourian A.Lo H.Fakhouri S.Shoghi A.Wang X.Fleischer J.G.Navlakha S.Panda S.Taub P.R. Ten-hour time-restricted eating reduces weight, blood pressure, and atherogenic lipids in patients with metabolic syndrome. , 39 Gabel K.Hoddy K.K.Haggerty N.Song J.Kroeger C.M.Trepanowski J.F.Panda S.Varady K.A. Effects of 8-hour time restricted feeding on body weight and metabolic disease risk factors in obese adults: a pilot study. , 40 Gill S.Panda S. A smartphone App reveals erratic diurnal eating patterns in humans that can Be modulated for health benefits. 16 Wilkinson M.J.Manoogian E.N.C.Zadourian A.Lo H.Fakhouri S.Shoghi A.Wang X.Fleischer J.G.Navlakha S.Panda S.Taub P.R. Ten-hour time-restricted eating reduces weight, blood pressure, and atherogenic lipids in patients with metabolic syndrome. 39 Gabel K.Hoddy K.K.Haggerty N.Song J.Kroeger C.M.Trepanowski J.F.Panda S.Varady K.A. Effects of 8-hour time restricted feeding on body weight and metabolic disease risk factors in obese adults: a pilot study. 41 Williamson D.A.Bray G.A.Ryan D.H. Is 5% weight loss a satisfactory criterion to define clinically significant weight loss?. In this study, we have followed the ADA recommendation on the LCD, restricting subjects’ carbohydrate intake to <130 g/day and demonstrated a slight but significant reduction in body weight (−2.2 kg; −2.7%) in adults with MetS over the course of 3 months without apparent adverse effects. Similarly, a previous clinical trial showed that LCD treatment of T2DM patients, i.e., 130 g/day carbohydrates without other specific restrictions, caused 1.6 kg body weight loss over 6 monthsA 12-week randomized study also showed that LCD (<100 g carbohydrates/day) reduced body weight in type 1 diabetes subjects by 2.0 kg.In addition, the beneficial effects of very-low-carbohydrate ketogenic diets (VLCKD; < 50 g carbohydrates/day)on body weight reduction have been assessed.Samaha et al.showed that severely obese subjects with MetS significantly lost body weight (−5.8 kg) after 6 months on a VLCKD (<30 g carbohydrates/day). Another 1-year clinical trial showed that a VLCKD (<40 g carbohydrates/day) intervention resulted in significant weight loss (−3.5 kg) in obese individuals without T2DM or CVD.However, the efficacy and safety of VLCKD and adherence during long-term intervention are still under debate.Besides, we found participants from Northwestern China showed a 10.6-h baseline meal-eating window, which was calculated by participants’ self-report of average three meal times on 2-week recall. This baseline meal-eating window is comparable with the finding from a recent 1-year RCT study in Southern China,which was 10.4-h baseline eating window that was calculated by daily dietary log, food photograph, and eating time. We further demonstrated that an 8-h TRE significantly reduces body weight in adults with MetS (−4.0%), independent of timing of TRE. Several previous clinical trials have evaluated the weight-reduction efficiency of TRE in individuals with MetS.Wilkinson et al.found that a 10-h TRE led to an approximately 3% weight reduction and improvements in cardiovascular risk parameters in individuals with MetS. A recent trial showed that 8-h TRE decreased body weight of obese individuals by 2.6% after 3 monthsNonetheless, in our study only the combination of LCD and TRE produced clinically significant weight loss,i.e., a reduction of 5.8% from baseline over 3 months.42 Pou K.M.Massaro J.M.Hoffmann U.Vasan R.S.Maurovich-Horvat P.Larson M.G.Keaney Jr., J.F.Meigs J.B.Lipinska I.Kathiresan S.et al. Visceral and subcutaneous adipose tissue volumes are cross-sectionally related to markers of inflammation and oxidative stress: the Framingham Heart Study. , 43 Hajer G.R.van Haeften T.W.Visseren F.L.J. Adipose tissue dysfunction in obesity, diabetes, and vascular diseases. , 44 Tchernof A.Després J.P. Pathophysiology of human visceral obesity: an update. 9 Samaha F.F.Iqbal N.Seshadri P.Chicano K.L.Daily D.A.McGrory J.Williams T.Williams M.Gracely E.J.Stern L. A low-carbohydrate as compared with a low-fat diet in severe obesity. , 45 Hayes M.R.Miller C.K.Ulbrecht J.S.Mauger J.L.Parker-Klees L.Gutschall M.D.Mitchell D.C.Smiciklas-Wright H.Covasa M. A carbohydrate-restricted diet alters gut peptides and adiposity signals in men and women with metabolic syndrome. 46 Di Bonito P.Valerio G.Licenziati M.R.Campana G.Del Giudice E.M.Di Sessa A.Morandi A.Maffeis C.Chiesa C.Pacifico L.et al. Uric acid, impaired fasting glucose and impaired glucose tolerance in youth with overweight and obesity. , 47 Yu T.Y.Jee J.H.Bae J.C.Jin S.M.Baek J.H.Lee M.K.Kim J.H. Serum uric acid: a strong and independent predictor of metabolic syndrome after adjusting for body composition. , 48 Cheng D.Hu C.Du R.Qi H.Lin L.Wu X.Ma L.Peng K.Li M.Xu M.et al. Serum uric acid and risk of incident diabetes in middle-aged and elderly Chinese adults: prospective cohort study. 19 Sutton E.F.Beyl R.Early K.S.Cefalu W.T.Ravussin E.Peterson C.M. Early time-restricted feeding improves insulin sensitivity, blood pressure, and oxidative stress even without weight loss in men with prediabetes. 20 de Cabo R.Mattson M.P. Effects of intermittent fasting on health, aging, and disease. , 49 Mattson M.P.Moehl K.Ghena N.Schmaedick M.Cheng A. Intermittent metabolic switching, neuroplasticity and brain health. , 50 Huet C.Boudaba N.Guigas B.Viollet B.Foretz M. Glucose availability but not changes in pancreatic hormones sensitizes hepatic AMPK activity during nutritional transition in rodents. 17 Cienfuegos S.Gabel K.Kalam F.Ezpeleta M.Wiseman E.Pavlou V.Lin S.Oliveira M.L.Varady K.A. Effects of 4- and 6-h time-restricted feeding on weight and cardiometabolic health: a randomized controlled trial in adults with obesity. 19 Sutton E.F.Beyl R.Early K.S.Cefalu W.T.Ravussin E.Peterson C.M. Early time-restricted feeding improves insulin sensitivity, blood pressure, and oxidative stress even without weight loss in men with prediabetes. Our results showed that LCD decreased SFA without affecting VFA, while TRE and the combination treatment decreased SFA as well as VFA. Accumulating evidence indicates that visceral fat is crucially associated with many aspects of MetS, including hypertension, dyslipidemia, glucose intolerance, and insulin resistance, and it is more closely linked to inflammatory and oxidative stress biomarkers than subcutaneous fat.Our results suggest that compared with LCD, TRE might yield more benefits on cardiometabolic outcomes in adults with MetS. Indeed, in our study, LCD intervention did not significantly decrease FBG levels but prominently reduced insulin levels and ameliorated insulin sensitivity, which are consistent with previous trials,suggesting that LCD is more effective in lowering blood insulin levels and improving insulin sensitivity than in lowering blood glucose levels. This is likely explained by the fact that most studies were conducted with relatively healthy or overweight individuals but not individuals with T2DM, and not all participants in these studies had elevated FBG levels. In contrast, in our study, TRE intervention improved insulin levels as well as blood glucose levels, and furthermore, the combination of LCD and TRE significantly reduced fasting glucose, insulin, and HbA1c levels in MetS patients. In addition, compared with baseline, TRE with and without LCD reduced UA levels, while compared with changes among treatments, combination treatment caused more prominent reduction on UA. High UA is a strong and independent predictor of MetS and is associated with impaired fasting glucose and insulin resistance.A recent 6-h TRE trial in overweight individuals with prediabetes revealed an improvement in insulin sensitivity and β cell responsiveness but no reduction in FBG.Fasting might improve glycemic control as a result of metabolic switch from liver-derived glucose to adipose cell-derived ketones, occurring when switching from a fed to a fasted state, and it might induce ketoplasia, decrease fat accumulation, and increase insulin sensitivity.However, further studies need to be performed in participants with elevated FBG, prediabetes, or T2DM to better define the effects of LCD versus TRE on glucose regulation. Whether TRE with or without LCD has independent effects on visceral fat and metabolic outcomes or is simply an epiphenomenon of greater weight loss could not be addressed in this study. It is noted that a previous RCT indicated that TRE that induced mild body weight reduction (∼3%) without changing visceral fat mass was accompanied with improvements of insulin resistance and oxidative stress,while another study showed that although there were no effects on body weight reduction, TRE could still improve those cardiometabolic parameters in prediabetic men.19 Sutton E.F.Beyl R.Early K.S.Cefalu W.T.Ravussin E.Peterson C.M. Early time-restricted feeding improves insulin sensitivity, blood pressure, and oxidative stress even without weight loss in men with prediabetes. , 39 Gabel K.Hoddy K.K.Haggerty N.Song J.Kroeger C.M.Trepanowski J.F.Panda S.Varady K.A. Effects of 8-hour time restricted feeding on body weight and metabolic disease risk factors in obese adults: a pilot study. , 51 Bowen J.Brindal E.James-Martin G.Noakes M. Randomized trial of a high protein, partial meal replacement program with or without alternate day fasting: similar effects on weight loss, retention status, nutritional, metabolic, and behavioral outcomes. , 52 Moro T.Tinsley G.Bianco A.Marcolin G.Pacelli Q.F.Battaglia G.Palma A.Gentil P.Neri M.Paoli A. Effects of eight weeks of time-restricted feeding (16/8) on basal metabolism, maximal strength, body composition, inflammation, and cardiovascular risk factors in resistance-trained males. , 53 Harvie M.N.Pegington M.Mattson M.P.Frystyk J.Dillon B.Evans G.Cuzick J.Jebb S.A.Martin B.Cutler R.G.et al. The effects of intermittent or continuous energy restriction on weight loss and metabolic disease risk markers: a randomized trial in young overweight women. , 54 Wood R.J.Volek J.S.Liu Y.Shachter N.S.Contois J.H.Fernandez M.L. Carbohydrate restriction alters lipoprotein metabolism by modifying VLDL, LDL, and HDL subfraction distribution and size in overweight men. , 55 Volek J.S.Fernandez M.L.Feinman R.D.Phinney S.D. Dietary carbohydrate restriction induces a unique metabolic state positively affecting atherogenic dyslipidemia, fatty acid partitioning, and metabolic syndrome. 56 Westman E.C.Yancy Jr., W.S.Olsen M.K.Dudley T.Guyton J.R. Effect of a low-carbohydrate, ketogenic diet program compared to a low-fat diet on fasting lipoprotein subclasses. , 57 Bhanpuri N.H.Hallberg S.J.Williams P.T.McKenzie A.L.Ballard K.D.Campbell W.W.McCarter J.P.Phinney S.D.Volek J.S. Cardiovascular disease risk factor responses to a type 2 diabetes care model including nutritional ketosis induced by sustained carbohydrate restriction at 1 year: an open label, non-randomized, controlled study. 58 Trepanowski J.F.Kroeger C.M.Barnosky A.Klempel M.C.Bhutani S.Hoddy K.K.Gabel K.Freels S.Rigdon J.Rood J.et al. Effect of alternate-day fasting on weight loss, weight maintenance, and cardioprotection among metabolically healthy obese adults: a randomized clinical trial. 51 Bowen J.Brindal E.James-Martin G.Noakes M. Randomized trial of a high protein, partial meal replacement program with or without alternate day fasting: similar effects on weight loss, retention status, nutritional, metabolic, and behavioral outcomes. , 53 Harvie M.N.Pegington M.Mattson M.P.Frystyk J.Dillon B.Evans G.Cuzick J.Jebb S.A.Martin B.Cutler R.G.et al. The effects of intermittent or continuous energy restriction on weight loss and metabolic disease risk markers: a randomized trial in young overweight women. 19 Sutton E.F.Beyl R.Early K.S.Cefalu W.T.Ravussin E.Peterson C.M. Early time-restricted feeding improves insulin sensitivity, blood pressure, and oxidative stress even without weight loss in men with prediabetes. , 39 Gabel K.Hoddy K.K.Haggerty N.Song J.Kroeger C.M.Trepanowski J.F.Panda S.Varady K.A. Effects of 8-hour time restricted feeding on body weight and metabolic disease risk factors in obese adults: a pilot study. , 51 Bowen J.Brindal E.James-Martin G.Noakes M. Randomized trial of a high protein, partial meal replacement program with or without alternate day fasting: similar effects on weight loss, retention status, nutritional, metabolic, and behavioral outcomes. , 52 Moro T.Tinsley G.Bianco A.Marcolin G.Pacelli Q.F.Battaglia G.Palma A.Gentil P.Neri M.Paoli A. Effects of eight weeks of time-restricted feeding (16/8) on basal metabolism, maximal strength, body composition, inflammation, and cardiovascular risk factors in resistance-trained males. 59 Wang Y.Snel M.Jonker J.T.Hammer S.Lamb H.J.de Roos A.Meinders A.E.Pijl H.Romijn J.A.Smit J.W.A.et al. Prolonged caloric restriction in obese patients with type 2 diabetes mellitus decreases plasma CETP and increases apolipoprotein AI levels without improving the cholesterol efflux properties of HDL. 60 Kim J.Y.Kim S.M.Kim S.J.Lee E.Y.Kim J.R.Cho K.H. Consumption of policosanol enhances HDL functionality via CETP inhibition and reduces blood pressure and visceral fat in young and middle-aged subjects. The effects of LCD and TRE on dyslipidemia are highly variable between studies.We observed that LCD alone and combined with TRE adversely increased LDL-c after 3-month intervention, which is consistent with several studies showing that LCD increases cholesterol levels within the large LDL subfractions.In this study, while TRE treatment did not impact HDL-c, TRE and combination treatment, but not LCD, significantly reduced plasma TG levels. In fact, TRE was generally reported not to affect HDL-c, although one study reported a minor improvement.Yet, the effects of TRE on TG levels are still controversial. For instance, some TRE studies demonstrated a reduction in TG,whereas others showed no significant effects.In addition, we observed that TRE and combination treatment reduced the TG/HDL-c ratio, which could be partly due to reduced VFA by these treatments, but not by LCD alone. Indeed, VFA but not SFA strongly correlates with the TG/HDL-c ratio ( Figure S2 C). The TG/HDL-c ratio is a well-known predictor for CVD. A reduced TG/HDL-c ratio may be attributed to decreased cholesteryl ester transfer protein (CETP) activity, as CETP mediates the net transfer of CE from HDL to TG-rich lipoproteins in exchange for TG. Previous studies have demonstrated that weight loss induced by a very low calorie diet was correlated with reduced CETP concentration,and CETP inhibition was associated with the improvement of visceral fat.Therefore, TRE, with or without restricted carbohydrate consumption, could significantly improve visceral obesity and reduce TG level and TG/HDL-c ratio, as well as decrease CETP concentration.61 Zomer E.Gurusamy K.Leach R.Trimmer C.Lobstein T.Morris S.James W.P.T.Finer N. Interventions that cause weight loss and the impact on cardiovascular risk factors: a systematic review and meta-analysis. In addition, only combination intervention significantly decreased blood pressure in our study. Generally, moderate (5%–10%) weight loss caused by interventions is expected to lead to larger reductions in SBP of 5 mmHg and DBP of 3 mmHg than that of mild (0–5%) weight loss over 6–12 months as shown in a systematic review and meta-analysis.We observed that the mean reduction in DBP (5 mmHg) by combination intervention that produced a moderate weight loss of 5.8% was apparently higher and not accompanied by a reduction in SBP. It should be noted though that our study was not properly powered to observe a significant change in blood pressure, and larger studies are obviously needed to further address the effects of LCD and TRE on blood pressure in MetS patients.19 Sutton E.F.Beyl R.Early K.S.Cefalu W.T.Ravussin E.Peterson C.M. Early time-restricted feeding improves insulin sensitivity, blood pressure, and oxidative stress even without weight loss in men with prediabetes. 17 Cienfuegos S.Gabel K.Kalam F.Ezpeleta M.Wiseman E.Pavlou V.Lin S.Oliveira M.L.Varady K.A. Effects of 4- and 6-h time-restricted feeding on weight and cardiometabolic health: a randomized controlled trial in adults with obesity. 19 Sutton E.F.Beyl R.Early K.S.Cefalu W.T.Ravussin E.Peterson C.M. Early time-restricted feeding improves insulin sensitivity, blood pressure, and oxidative stress even without weight loss in men with prediabetes. 17 Cienfuegos S.Gabel K.Kalam F.Ezpeleta M.Wiseman E.Pavlou V.Lin S.Oliveira M.L.Varady K.A. Effects of 4- and 6-h time-restricted feeding on weight and cardiometabolic health: a randomized controlled trial in adults with obesity. 62 Lowe D.A.Wu N.Rohdin-Bibby L.Moore A.H.Kelly N.Liu Y.E.Philip E.Vittinghoff E.Heymsfield S.B.Olgin J.E.et al. Effects of time-restricted eating on weight loss and other metabolic parameters in women and men with overweight and obesity: the TREAT randomized clinical trial. , 63 Xie Z.Sun Y.Ye Y.Hu D.Zhang H.He Z.Zhao H.Yang H.Mao Y. Randomized controlled trial for time-restricted eating in healthy volunteers without obesity. 64 Morris C.J.Garcia J.I.Myers S.Yang J.N.Trienekens N.Scheer F.A.J.L. The human circadian system has a dominating role in causing the morning/evening difference in diet-induced thermogenesis. , 65 Poggiogalle E.Jamshed H.Peterson C.M. Circadian regulation of glucose, lipid, and energy metabolism in humans. , 66 Scheer F.A.J.L.Hilton M.F.Mantzoros C.S.Shea S.A. Adverse metabolic and cardiovascular consequences of circadian misalignment. 64 Morris C.J.Garcia J.I.Myers S.Yang J.N.Trienekens N.Scheer F.A.J.L. The human circadian system has a dominating role in causing the morning/evening difference in diet-induced thermogenesis. , 65 Poggiogalle E.Jamshed H.Peterson C.M. Circadian regulation of glucose, lipid, and energy metabolism in humans. , 66 Scheer F.A.J.L.Hilton M.F.Mantzoros C.S.Shea S.A. Adverse metabolic and cardiovascular consequences of circadian misalignment. , 67 Morris C.J.Yang J.N.Garcia J.I.Myers S.Bozzi I.Wang W.Buxton O.M.Shea S.A.Scheer F.A.J.L. Endogenous circadian system and circadian misalignment impact glucose tolerance via separate mechanisms in humans. 68 Ravussin E.Beyl R.A.Poggiogalle E.Hsia D.S.Peterson C.M. Early time-restricted feeding reduces appetite and increases fat oxidation but does not affect energy expenditure in humans. In this study, eTRE shows greater effects on reducing abdominal fat area (both SFA and VFA) than lTRE, while eTRE and lTRE showed comparable benefits on body weight, glycemic control, dyslipidemia, and blood pressure. Nevertheless, participants were not randomly assigned to eTRE or lTRE, and sample sizes were relatively small, so the comparison of eTRE and lTRE was exploratory. Previous studies showed that both eTREand lTREimproved multiple indicators of cardiovascular health. Sutton et al.conducted a 5-week study comparing eTRE (6-h eating window before 3:00 p.m.) with a control condition (conventional 12-h eating window) and found better glycemic control and improvement of blood pressure by eTRE without significant body weight changes. Furthermore, Cienfuegos et al.found that both 4- and 6-h lTRE caused mild body weight reduction (∼3%) over 2 months when compared with the control. However, several studies on lTRE demonstrated conflicting results regarding body weight.Moreover, the thermic effect of food, insulin sensitivity, and β cell function is better in early morning than nightbecause the body is optimized to ingesting food in early morning.Thus, an 8 a.m. to 4 p.m. eating window may be applied as a more effective intervention to improve insulin sensitivity. Besides, lipids were also affected by meal timing, which might be due to an increase of fat oxidation in eTRE.However, for participants who find it easier to skip breakfast than dinner, the latter being a more social meal in most cultures, a 12 p.m. to 8 p.m. eating window is an alternative. Thus, it is important to consider participants’ individual schedule and personal preference and allow them to choose the suitable TRE eating window in order to increase efficacy and adherence.Conclusions In conclusion, compared with baseline, all three treatments after a 3-month intervention reduce body weight and SFA, as well as some cardiometabolic outcomes, including fasting insulin, C-peptide, and insulin sensitivity index, but only TRE, with and without LCD, significantly reduces abdominal visceral fat, FBG, UA, TG, and TG/HDL-c ratio. More importantly, compared with changes of LCD, TRE and combination treatment further decrease body weight and VFA. Taken together, without changing physical activity, TRE with and without LCD significantly improves glycemic control, atherogenic dyslipidemia, and UA, thus largely improves metabolic disease risk, with TRE being superior over LCD with respect to reducing body weight and abdominal visceral obesity. Therefore, we anticipate that an 8-h TRE without and with LCD can serve as an effective intervention for MetS.",Time-restricted eating with or without low-carbohydrate diet reduces visceral fat and improves metabolic syndrome: A randomized trial
284,8,8_weight_study_tre_ends,https://themarijuanaherald.com/2022/10/study-legalizing-marijuana-has-no-impact-on-the-perceived-risk-of-marijuana-use-among-children/,"The state level legalization of recreational marijuana has no discernible impact on how children feel about the potential risks associated with marijuana use.This is according to a new peer-reviewed study being published in the upcoming issue of the journal Cannabis and Cannabinoid Research and epublished ahead of print by the National Institute of Health.“As more states pass recreational cannabis laws (RCLs) for adults, there is concern that increasing (and state-sanctioned) cannabis acceptance will result in a reduced perception of risk of harm from cannabis among children”, states the study’s abstract. “We aimed to discover whether children in states with RCLs had decreased perception of risk from cannabis compared with children in states with illicit cannabis.”For the study researchers “analyzed data from the multisite multistate Adolescent Brain and Cognitive Development Study to determine how the perception of cannabis harm among children (age at baseline: 9-10; N=10,395) changes over time in states with and without RCLs.”Using multilevel modeling, they “assessed survey responses from children longitudinally across 3 years, adjusting for state-, family-, and participant-level clustering and child-level factors, including demographics (sex, race, and socioeconomic status), religiosity, and trait impulsivity.”The study found that there “was no significant main effect of state RCLs on perceived risk of cannabis use, and no differences in change over time by state RCLs, even after controlling for demographic factors and other risk (e.g., impulsivity) and protective (e.g., religiosity) factors.”Researchers conclude by stating that “this analysis indicates that state-level RCLs are not associated with differential perception of cannabis risk among children, even after controlling for demographics, trait impulsivity, and religiosity. Future studies could assess how perception of risk from cannabis changes as children and adolescents continue to mature in states with and without RCLs.”More information on the study including a look at its full abstract can be found at the following link: https://pubmed.ncbi.nlm.nih.gov/36301559/",Study: Legalizing Marijuana Has No Impact on the Perceived Risk of Marijuana Use Among Children
400,8,8_weight_study_tre_ends,https://www.marijuanamoment.net/legal-marijuana-access-tied-to-lower-risk-of-lung-injuries-from-contaminated-vapes-study-indicates/,"A new study adds to the body of research indicating that access to legal and regulated marijuana markets served a “protective” purpose for people who vaped cannabis during a 2019 outbreak of lung injuries related to contaminated products.Researchers at Johns Hopkins, New York University and the American Heart Association reached that conclusion in a study published in the journal PLoS ONE this month. They analyzed the prevalence of marijuana vaping, cases of e-cigarette or vaping product-use-associated lung injury (EVALI) and cannabis laws in 13 states.While more people reported vaping marijuana in states that have legalized for medical or recreational use, that prevalence didn’t translate into a greater percentage of EVALI case burdens compared to non-legal states, according to data from the 2019 Behavioral Risk Factor Surveillance System (BRFSS) and the Centers for Disease Control and Prevention (CDC).The research article—which looked at data from four prohibition states, seven medical cannabis states and two states that have legalized marijuana for adult-use—found that “state-level cannabis vaping prevalence was not positively associated with EVALI caseload, even after accounting for state cannabis policies.”In fact, there was “an inverse association between state cannabis vaping prevalence and EVALI case burden.” That is, states that had higher levels of marijuana vaping, which tend to be the states that have some form of legal access, saw lower lung injury rates.“These findings, therefore, suggest that there may not be a direct, simple link between a state’s cannabis vaping prevalence and EVALI cases, but rather the relationship is likely more nuanced, supporting the CDC’s hypothesis that the EVALI outbreak is likely reflective of access to informal sources of THC-containing e-liquids,” the study authors said.“Although cannabis vaping prevalence was low in states with prohibitive cannabis laws, individuals from such states may more likely obtain cannabis from illegal sources, increasing their risk of using contaminated products and hence the higher prevalence of EVALI cases in such states,” it said.In states that had legalized medical or recreational marijuana, meanwhile, people were “likely to obtain cannabis from legal sources, reducing the risk of contamination.” The research article supports findings from previous studies demonstrating that “the presence of legal markets for cannabis may have been protective against EVALI,” according to the authors of the new paper.The EVALI crisis has dropped off significantly over the past three years as public education increased and regulators stepped up enforcement efforts against vaping manufacturers that included additives like vitamin-E-acetate into their THC cartridges that are believed to have caused the lung injuries.However, the researchers said “continued surveillance of cannabis vaping is warranted,” and “efforts to discourage black-market sales of contaminated products should be pursued to prevent future outbreaks.”Advocates have long maintained that cannabis legalization represents a key policy to mitigate the harms of illicit markets, and the new study seems to reinforce the public health benefits of regulations over prohibition.“In conclusion, state-level cannabis vaping prevalence was not positively associated with EVALI prevalence,” the article says. “This suggests that the EVALI outbreak may have not necessarily been a simple reflection of state-level cannabis vaping prevalence but rather due to the use of contaminated or illicitly-sourced vaping products, which are more likely in states with restrictive cannabis laws.”Relatedly, a study published in the journal Environmental Health Perspective last month determined that the lack of federal regulations, combined with a patchwork of state policies, “can confuse cannabis manufacturers and discourage compliance while subjecting cannabis users to a higher level of contaminant exposure in some jurisdictions.”Another recent study that analyzed National Poison Data System (NPDS) reports found that marijuana legalization minimizes the risk of people buying and using synthetic cannabinoid products that can lead to hospitalizations or deaths.Image courtesy of Lindsay Fox from Pixabay.","Legal Marijuana Access Tied To Lower Risk Of Lung Injuries From Contaminated Vapes, Study Indicates"
285,8,8_weight_study_tre_ends,https://theveganherald.com/2022/10/study-healthy-plant-based-diets-significantly-increase-sperm-density-and-motility/,"Plant-based diets can “significantly increase sperm density and motility in men”, and is “related to a lower risk of sperm volume deficiency”, according to a new study published in the peer-reviewed International Journal of Fertility and Sterility.“Infertility is a major clinical problem that affects people psychologically and medically”, states the study’s abstract. “For the past 40 years, studies have linked nearly 50% of childlessness to male infertility. It is worth noting that unlike other factors contributing to infertility, diet is a tunable factor and can be applied in counseling infertile men. ”The goal of this study “was to determine the relationship between plant diet index (PDI) and semen parameters in Iranian infertile men.”In this cross-sectional study, dietary intake was determined by a valid 168-item questionnaire (FFQ). In this study, “four dependent semen parameters, including total sperm motility (TSM), sperm concentration (SC), normal sperm morphology (NSM), and semen volume (SV) were measured.”Results of this study stated that “greater adherence to the healthful plant-based diet index (hPDI), can significantly increase sperm density and motility in men, as well as greater adherence to the PDI dietary pattern is related to a lower risk of sperm volume deficiency, and ultimately more adherence to the unhealthful plant-based diet index (uPDI), can reduce the risk of sperm motility.”Researchers conclude by stating that “In this study, for the first time, the relationship between PDI, hPDI, uPDI and male infertility was evaluated. Altogether, this study demonstrated that nutrition has an impact on semen quality and fertility of men.”Below is the full abstract for this study. You can find the study’s full text by clicking here.Background: Infertility is a major clinical problem that affects people psychologically and medically. For the past 40 years, studies have linked nearly 50% of childlessness to male infertility. It is worth noting that unlike other factors contributing to infertility, diet is a tunable factor and can be applied in counseling infertile men. The goal of this study was to determine the relationship between plant diet index (PDI) and semen parameters in Iranian infertile men. Materials and methods: In this cross-sectional study, dietary intake was determined by a valid 168-item questionnaire (FFQ). In this study, four dependent semen parameters, including total sperm motility (TSM), sperm concentration (SC), normal sperm morphology (NSM), and semen volume (SV) were measured. Results: Results of this study stated that greater adherence to the healthful plant-based diet index (hPDI), can significantly increase sperm density and motility in men, as well as greater adherence to the PDI dietary pattern is related to a lower risk of sperm volume deficiency, and ultimately more adherence to the unhealthful plant-based diet index (uPDI), can reduce the risk of sperm motility. Conclusion: In this study, for the first time, the relationship between PDI, hPDI, uPDI and male infertility was evaluated. Altogether, this study demonstrated that nutrition has an impact on semen quality and fertility of men.",Study: Healthy Plant-Based Diets âSignificantly Increase Sperm Density and Motilityâ
408,8,8_weight_study_tre_ends,https://www.nature.com/articles/s41380-022-01812-3,"Anderson RJ, Freedland KE, Clouse RE, Lustman PJ. The prevalence of comorbid depression in adults with diabetes: a meta-analysis. Diabetes Care. 2001;24:1069–78.Lee JH, Park SK, Ryoo JH, Oh CM, Mansur RB, Alfonsi JE, et al. The association between insulin resistance and depression in the Korean general population. J Affect Disord. 2017;208:553–9.Kan C, Silva N, Golden SH, Rajala U, Timonen M, Stahl D, et al. A systematic review and meta-analysis of the association between depression and insulin resistance. Diabetes Care. 2013;36:480–9.Phillips CM, Perry IJ. Depressive symptoms, anxiety, and well-being among metabolic health obese subtypes. Psychoneuroendocrinology 2015;62:47–53.Dutheil S, Ota KT, Wohleb ES, Rasmussen K, Duman RS. High-fat diet induced anxiety and anhedonia: impact on brain homeostasis and inflammation. Neuropsychopharmacology 2016;41:1874–87.Zemdegs J, Quesseveur G, Jarriault D, Pénicaud L, Fioramonti X, Guiard BP. High-fat diet-induced metabolic disorders impairs 5-HT function and anxiety-like behavior in mice. Br J Pharm. 2016;173:2095–110.Zemdegs J, Martin H, Pintana H, Bullich S, Manta S, Marqués MA, et al. Metformin promotes anxiolytic and antidepressant-like responses in insulin-resistant mice by decreasing circulating branched-chain amino acids. J Neurosci. 2019;39:5935–48.Hassan AM, Mancano G, Kashofer K, Fröhlich EE, Matak A, Mayerhofer R, et al. High-fat diet induces depression-like behaviour in mice associated with changes in microbiome, neuropeptide Y, and brain metabolome. Nutr Neurosci. 2019;22:877–93.Papazoglou IK, Jean A, Gertler A, Taouis M, Vacher CM. Hippocampal GSK3β as a molecular link between obesity and depression. Mol Neurobiol. 2015;52:363–74.Martin H, Bullich S, Guiard BP, Fioramonti X. The impact of insulin on the serotonergic system and consequences on diabetes-associated mood disorders. J Neuroendocrinol. 2021;33:e12928.Kleinridders A, Cai W, Cappellucci L, Ghazarian A, Collins WR, Vienberg SG, et al. Insulin resistance in brain alters dopamine turnover and causes behavioral disorders. Proc Natl Acad Sci. 2015;112:3463–8.Guiard BP, El Mansari M, Blier P. Prospect of a dopamine contribution in the next generation of antidepressant drugs: the triple reuptake inhibitors. Curr Drug Targets. 2009;10:1069–84.Labouèbe G, Liu S, Dias C, Zou H, Wong JCY, Karunakaran S, et al. Insulin induces long-term depression of VTA dopamine neurons via an endocannabinoid-mediated mechanism. Nat Neurosci. 2013;16:300–8.Könner AC, Hess S, Tovar S, Mesaros A, Sánchez-Lasheras C, Evers N, et al. Role for insulin signaling in catecholaminergic neurons in control of energy homeostasis. Cell Metab. 2011;13:720–8.Evans MC, Kumar NS, Inglis MA, Anderson GM. Leptin and insulin do not exert redundant control of metabolic or emotive function via dopamine neurons. Horm Behav. 2018;106:93–104.Yohn CN, Gergues MM, Samuels BA. The role of 5-HT receptors in depression. Mol Brain. 2017;10:28.Kiyasova V, Fernandez SP, Laine J, Stankovski L, Muzerelle A, Doly S, et al. A genetically defined morphologically and functionally unique subset of 5-HT neurons in the mouse raphe nuclei. J Neurosci J Soc Neurosci. 2011;31:2756–68.Hanson LR, Fine JM, Svitak AL, Faltesek KA, Intranasal administration of CNS therapeutics to awake mice. J Vis Exp. 2013;74:4440.Dhuria SV, Hanson LR, Frey WH. Intranasal delivery to the central nervous system: mechanisms and experimental considerations. J Pharm Sci. 2010;99:1654–73.Fan LW, Carter K, Bhatt A, Pang Y. Rapid transport of insulin to the brain following intranasal administration in rats. Neural Regen Res. 2019;14:1046–51.Dellu-Hagedorn F, Fitoussi A, De Deurwaerdère P. Correlative analysis of dopaminergic and serotonergic metabolism across the brain to study monoaminergic function and interaction. J Neurosci Methods. 2017;280:54–63.Puginier E, Bharatiya R, Chagraoui A, Manem J, Cho YH, Garret M, et al. Early neurochemical modifications of monoaminergic systems in the R6/1 mouse model of Huntington’s disease. Neurochem Int. 2019;128:186–95.Rainer Q, Nguyen HT, Quesseveur G, Gardier AM, David DJ, Guiard BP. Functional status of somatodendritic serotonin 1A autoreceptor after long-term treatment with fluoxetine in a mouse model of anxiety/depression based on repeated corticosterone administration. Mol Pharm. 2012;81:106–12.Qesseveur G, Petit AC, Nguyen HT, Dahan L, Colle R, Rotenberg S, et al. Genetic dysfunction of serotonin 2A receptor hampers response to antidepressant drugs: A translational approach. Neuropharmacology 2016;105:142–53.Guiard BP, Przybylski C, Guilloux JP, Seif I, Froger N, De Felipe C, et al. Blockade of substance P (neurokinin 1) receptors enhances extracellular serotonin when combined with a selective serotonin reuptake inhibitor: an in vivo microdialysis study in mice. J Neurochem. 2004;89:54–63.Ferreira de Sá DS, Römer S, Brückner AH, Issler T, Hauck A, Michael T. Effects of intranasal insulin as an enhancer of fear extinction: a randomized, double-blind, placebo-controlled experimental study. Neuropsychopharmacol Publ Am Coll Neuropsychopharmacol. 2020;45:753–60.Marks DR, Tucker K, Cavallin MA, Mast TG, Fadool DA. Awake intranasal insulin delivery modifies protein complexes and alters memory, anxiety, and olfactory behaviors. J Neurosci. 2009;29:6734–51.Schmid V, Kullmann S, Gfrörer W, Hund V, Hallschmid M, Lipp HP, et al. Safety of intranasal human insulin: A review. Diabetes Obes Metab. 2018;20:1563–77.Papazoglou I, Berthou F, Vicaire N, Rouch C, Markaki EM, Bailbe D, et al. Hypothalamic serotonin–insulin signaling cross-talk and alterations in a type 2 diabetic model. Mol Cell Endocrinol. 2012;350:136–44.Portal B, Delcourte S, Rovera R, Lejards C, Bullich S, Malnou CE, et al. Genetic and pharmacological inactivation of astroglial connexin 43 differentially influences the acute response of antidepressant and anxiolytic drugs. Acta Physiol Oxf Engl. 2020;229:e13440.MacKenzie RG, Trulson ME. Effects of insulin and streptozotocin-induced diabetes on brain tryptophan and serotonin metabolism in rats. J Neurochem. 1978;30:205–11.Orosco M, Nicolaidis S. Insulin and glucose-induced changes in feeding and medial hypothalamic monoamines revealed by microdialysis in rats. Brain Res Bull. 1994;33:289–97.Martín-Cora FJ, Fornal CA, Metzler CW, Jacobs BL. Insulin-induced hypoglycemia decreases single-unit activity of serotonergic medullary raphe neurons in freely moving cats: relationship to sympathetic and motor output: Medullary 5-HT neuronal responses to glucoregulatory challenges. Eur J Neurosci. 2002;16:722–34.Chaput Y, de Montigny C, Blier P. Effects of a selective 5-HT reuptake blocker, citalopram, on the sensitivity of 5-HT autoreceptors: electrophysiological studies in the rat brain. Naunyn Schmiedebergs Arch Pharm. 1986;333:342–8.Czachura JF, Rasmussen K. Effects of acute and chronic administration of fluoxetine on the activity of serotonergic neurons in the dorsal raphe nucleus of the rat. Naunyn Schmiedebergs Arch Pharm. 2000;362:266–75.Bosker FJ, Klompmakers A, Westenberg HGM. Postsynaptic 5-HT1A receptors mediate 5-hydroxytryptamine release in the amygdala through a feedback to the caudal linear raphe. Eur J Pharm. 1997;333:147–57.Martín-Ruiz R, Ugedo L. Electrophysiological evidence for postsynaptic 5-HT(1A) receptor control of dorsal raphe 5-HT neurones. Neuropharmacology 2001;41:72–8.Hajós M, Richards CD, Székely AD, Sharp T. An electrophysiological and neuroanatomical study of the medial prefrontal cortical projection to the midbrain raphe nuclei in the rat. Neuroscience 1998;87:95–108.Craven R, Grahame-Smith D, Newberry N. WAY-100635 and GR127935: effects on 5-hydroxytryptamine-containing neurones. Eur J Pharm. 1994;271:R1–3.Fletcher A, Forster EA, Bill DJ, Brown G, Cliffe IA, Hartley JE, et al. Electrophysiological, biochemical, neurohormonal and behavioural studies with WAY-100635, a potent, selective and silent 5-HT1A receptor antagonist. Behav Brain Res. 1996;73:337–53.Johnson DA, Gartside SE, Ingram CD. 5-HT1A receptor-mediated autoinhibition does not function at physiological firing rates: evidence from in vitro electrophysiological studies in the rat dorsal raphe nucleus. Neuropharmacology 2002;43:959–65.Deryabina IB, Andrianov VV, Muranova LN, Bogodvid TK, Gainutdinov KL, Effects of thryptophan hydroxylase blockade by P-Chlorophenylalanine on contextual memory reconsolidation after training of different intensity. Int J Mol Sci. 2020;21:2087.Johnson PL, Molosh A, Fitz SD, Arendt D, Deehan GA, Federici LM, et al. Pharmacological depletion of serotonin in the basolateral amygdala complex reduces anxiety and disrupts fear conditioning. Pharm Biochem Behav. 2015;138:174–9.Engin E, Smith KS, Gao Y, Nagy D, Foster RA, Tsvetkov E, et al. Modulation of anxiety and fear via distinct intrahippocampal circuits. eLife 2016;5:e14120.Wagle M, Zarei M, Lovett-Barron M, Poston KT, Xu J, Ramey V, et al. Brain-wide perception of the emotional valence of light is regulated by distinct hypothalamic neurons. Mol Psychiatry. 2022;1–17. Online ahead of print.Insel T, Cuthbert B, Garvey M, Heinssen R, Pine DS, Quinn K, et al. Research domain criteria (RDoC): toward a new classification framework for research on mental disorders. Am J Psychiatry. 2010;167:748–51.Sharma S, Fulton S. Diet-induced obesity promotes depressive-like behaviour that is associated with neural adaptations in brain reward circuitry. Int J Obes. 2013;37:382.Liu Z, Patil IY, Jiang T, Sancheti H, Walsh JP, Stiles BL, et al. High-fat diet induces hepatic insulin resistance and impairment of synaptic plasticity. PloS One. 2015;10:e0128274.Kothari V, Luo Y, Tornabene T, O’Neill AM, Greene MW, Geetha T, et al. High-fat diet induces brain insulin resistance and cognitive impairment in mice. Biochim Biophys Acta Mol Basis Dis. 2017;1863:499–508.Trivedi MH, Rush AJ, Wisniewski SR, Nierenberg AA, Warden D, Ritz L, et al. Evaluation of outcomes with citalopram for depression using measurement-based care in STAR*D: Implications for clinical practice. Am J Psychiatry. 2006;163:28–40.Blier P, Ward H. Toward optimal treatments for major depression. CNS Spectr. 2002;7:148–50. 153–4",Insulin modulates emotional behavior through a serotonin-dependent mechanism
404,8,8_weight_study_tre_ends,https://www.medicalnewstoday.com/articles/menopause-low-fat-plant-based-diet-may-improve-hot-flash-symptoms,"Share on Pinterest New research suggests that a low fat, plant-based diet rich in soy may help reduce hot flash symptoms and promote weight loss. Kelvin Murray/Getty Images A new study suggests that a low fat, plant-based diet rich in soy is as effective as hormone replacement therapy (HRT) for reducing hot flashes.The 12-week trial found that a plant-rich diet reduced moderate to severe hot flashes by 88%.The diet may have also helped women lose 8 pounds on average and improved their quality of life.Some experts say the study isn’t robust enough and that HRT is still the best option for reducing severe hot flashes.More research is needed to determine the impact of diet on reducing menopausal symptoms like hot flashes. A recent study reported that a plant-based, soy-rich diet could alleviate menopausal symptoms with the same effectiveness as hormone replacement therapy (HRT). The Women’s Study for the Alleviation of Vasomotor Symptoms (WAVS) trial found that this food-based approach reduced moderate to severe hot flashes by 88%. HRT has been shown to improve these symptoms by 70%–90%. Moreover, trial participants lost an average of 8 pounds during the 12-week study. Lead researcher Dr. Neal Barnard, president of the Physicians Committee for Responsible Medicine and adjunct professor at the George Washington University School of Medicine, said in a news release: “We do not fully understand yet why this combination works but it seems that these three elements are key— avoiding animal products, reducing fat, and adding a serving of soybeans. Our results mirror the diets of places in the world, like pre-Westernized Japan and modern-day Yucatán Peninsula, where a low fat, plant-based diet including soybeans is more prevalent and where postmenopausal women experience fewer symptoms.” These findings were recently published in Menopause: The Journal of the North American Menopause Society.Hot flash symptoms and nutrition Vasomotor symptoms include recurrent hot flashes, night sweats, and blood pressure fluctuations. Medical News Today discussed this study with Dr. G. Thomas Ruiz, the obstetrics and gynecology lead at MemorialCare Orange Coast Medical Center in Fountain Valley, CA. He was not involved in this research. Dr. Ruiz explained that vasomotor symptoms arise primarily from changes in the hypothalamic-pituitary-ovarian axis, which regulates sex hormone secretions. The hypothalamus, located in the center of the brain, helps control the internal thermostat. Fluctuating and decreasing hormone levels during menopause cause disruption in the hypothalamus. Hot flashes happen as the brain area is trying to reset its temperature, Dr. Ruiz shared.Effects of a low fat, plant-based diet on menopause symptoms Dr. Barnard and his research team recruited postmenopausal women ages 40 to 65 years for a parallel-design study in September 2020 and February 2021. Out of 1,662 respondents, 71 remained for the final data analysis. An intervention group followed a low fat, vegan diet with half a cup of cooked, nongenetically modified soybeans a day for 12 weeks. A control group made no dietary changes. Both groups took a vitamin B12 supplement daily and were asked not to take any other supplements, change medications, or exercise regimens. A mobile application recorded the frequency and severity of hot flashes. Participants completed the Menopause-Specific Quality of Life survey to record vasomotor, physical, psychosocial, and sexual symptoms. Why limit healthy fats? The study’s recommended diet was partially inspired by a traditional Japanese diet emphasizing plant-based foods, soy products, and low amounts of oils. MNT asked Dr. Barnard about limiting even “healthy” oils and fats, such as nuts and avocados: “The fat in nuts and avocados is healthier than the fat in dairy products and meat. The former are low in saturated fat, and the latter are loaded with it. But we often reduce fat of all kinds in our research studies. Oils and fats — ‘good’ or ‘bad’ — tend to interfere with weight loss [and] loss of excess weight seems to help with hot flashes.” “Oils and fats modify estrogen activity. During the study, we did find that those women who carefully avoided oily foods seemed to have faster benefits,” Dr. Barnard added.Certain foods reduce moderate to severe hot flashes The intervention diet may have resulted in significantly reduced menopausal symptoms. The total hot flash frequency in the fall cohort (September 2020) dropped by 78% in the intervention group and decreased by 39% in the control group. Moderate to severe hot flashes in the spring cohort (February 2021) intervention group fell by 88%, while the control saw a decrease of 34%. Women with seven or more hot flashes per day at the start of the study experienced a 93% reduction in symptoms in the intervention group. The control group had 36% fewer symptoms at the end of the study. The researchers found that reducing fat consumption and increasing fiber intake correlated with reduced severe hot flashes. The intervention group members lost an average of 7.93 pounds, while the control group participants lost one-half a pound on average.Limitations to menopause research Dr. Ruiz saw the cohort study design as a disadvantage. He argued that a double-blind study is “really the only way to come up with an observation that may be medically meaningful.” “A healthy diet will help you overall because it will make [you] feel better in general,” Dr. Ruiz said, adding that people who regularly eat healthily are also more likely to exercise as well. Dr. Ruiz added that the study’s small sample increased the possibility of a “huge placebo effect.” The study’s authors did admit that placebo effects could not be ruled out. It should also be noted that several of the study’s co-authors received compensation from the Physicians Committee for Responsible Medicine for their contributions. “When the results are very strong and consistent, a smaller sample can prove the effect,” Dr. Barnard said. “In this case, the 88% drop in moderate-to-severe hot flashes is enormous, and statistically, there is less than 1 [in] 1000 that this is due to chance. Our sample of 84 women was more than double the size needed to prove the effect.”Areas for future study Dr. Barnard hopes that future investigations will assess this study’s dietary approach for other conditions that cause hot flashes. For instance, he said that people with breast cancer or prostate cancer deal with hot flashes. To date, they have limited options for relief. “In both cases, a low fat vegan diet, plus soybeans, would be exactly the diet they should have clinically,” Dr. Barnard said.","Menopause: Low fat, plant-based diet may improve hot flash symptoms by 88%"
403,8,8_weight_study_tre_ends,https://www.mdpi.com/2072-6643/14/20/4294/htm,"Dietary fats may also have an effect on inflammation, which has a central role in the onset of many chronic pathologies. The first mechanism by which fats can affect inflammatory status is promotion of the translocation of microbial endotoxins, especially lipopolysaccharide (LPS), from the gut into the bloodstream [ 15 ]. LPS is a toxic cell-wall component of all our gut microbiome Gram-negative bacteria. The signaling of LPS is mediated through TLR4 receptors and leads to the stimulation of NF-kB, which in turn determines the secretions of many proinflammatory cytokines, such as IL-1, TNF-α, IL-6 and IL-8. SFAs seem to greatly stimulate inflammatory response because they are also structural components of LPS.Although dietary SFAs are generally considered harmful to global health, these recent findings are quite controversial [ 9 ]. A possible explanation for these controversial results may derive from the type of SFA consumed in the diet. The absorption of short-chain fatty acids (SCFAs, two to six carbons) and medium-chain fatty acids (MCFAs, 8 to 12 carbons) occurs directly via portal circulation, while long-chain fatty acids (LCFAs, 14 to 20 or more carbons) are packaged into micelles and circulate via lymphatic-forming chylomicrons. While meat products are typically rich in LCFAs (i.e., palmitic and stearic acids), SCFAs (i.e., propionic acid and butyric acid) are produced when dietary fiber is fermented in the colon and are naturally present in milk and whole dairy products [ 10 ]. Scientific evidence from observational studies shows that higher intake of dairy products [ 11 ] and whole grains (rich in fiber) [ 12 ] are associated with a lower risk of CVD, while higher consumption of meat (red or processed) [ 13 ] is associated with higher risk. The evidence further suggests that the effects of SFA on lipid markers depend on the number of carbon atoms in the chain; a higher intake of lauric acid increases HDL cholesterol and reduces the TC to HDL ratio, while stearic, myristic and palmitic acids may raise LDL cholesterol [ 14 ].Although calorie excess is the major determinant of weight gain, dietary fats have also been blamed for being the culprit in the dramatic increase in obesity and its associated diseases over the past half century. Over recent decades, dietary fat consumption has been discouraged but, in spite of the reduction in total daily individual fat intake by 10% and the increase in consumption of low-fat food, obesity rates are dramatically growing [ 5 ]. Furthermore, indistinctly cutting total fat intake from the diet inevitably leads to an increase in the consumption of highly processed grains and simple sugars while lowering the intake of liposoluble vitamins and unsaturated fats, especially from nuts, vegetable oils and fatty fish, which are particularly valuable for health [ 6 ].Noncommunicable diseases (NCDs), such as cardio-metabolic disorders and cancer, are currently the main causes of global mortality, representing 71% of all deaths in the world [ 1 ]. Recent evidence shows that the major risk factor for these conditions is chronic subclinical low-grade inflammation [ 2 ], which is usually determined by physical inactivity, tobacco, pollution, sleep alterations, dysbiosis, and infections [ 3 ]. Among the triggers for inflammation, poor diet and excessive weight also play important roles [ 4 ]. The growth in the volume and number of adipocytes due to an excessive caloric intake leads to an increase in monocyte adhesion and recruitment to adipose tissue. Macrophages in adipocytes show many pro-inflammatory receptors, such as tumor necrosis factor receptors (TNFRs), Toll-like receptors (TLRs) and interleukin-1 receptor (IL-1R), as well as high activation of the nuclear factor-kB (NF-kB) transcription factors for pro-inflammatory cytokines [ 4 ]. Moreover, a condition of chronic low-grade inflammation may impair insulin sensitivity; insulin resistance worsens the inflammatory state, increasing abdominal obesity, intrahepatic fat stocking, vascular inflammation and endothelial dysfunction, which leads to an increase in cardiovascular risk [ 3 ].Continuous variables are expressed as means and standard deviations (SDs), while categorical variables are expressed as frequencies of occurrence and percentages. Individuals were grouped by quartiles of total fat intake and distributions of background characteristics were compared between groups. Differences were tested with the chi-squared test for categorical variables, ANOVA for continuous variables distributed normally and the Kruskall–Wallis test for variables distributed non-normally. Energy-adjusted and multivariate logistic regression models were used to test the association between fat consumption and cardio-metabolic outcomes; the multivariate model adjusted for all other background characteristics (physical activity, educational status, occupational status, smoking status, alcohol consumption, menopausal status) was performed to test whether the observed associations were independent from the aforementioned variables. All reported p-values were based on two-sided tests and compared to a significance level of 5%. SPSS 17 (SPSS Inc., Chicago, IL, USA) software was used for all the statistical calculations.The nutritional assessment was conducted using two food frequency questionnaires (FFQs; a long and a short version) previously validated for the inhabitants of Sicily, south Italy [ 21 22 ]. The determination of the food ingested, the calories introduced and, especially, macro- and micro-nutrient intake were achieved through comparison with the food composition tables of the Research Center for Foods and Nutrition. The mean daily intake of each food was calculated in g or ml by considering the portion sizes provided in the FFQs and then converted to 24 h intake. Then, the content of total and specific fatty acids in each food was searched in the food composition tables of the Research Center for Foods and Nutrition and an estimation of their daily intake was calculated by multiplying the content of total and individual fatty acid molecules by the daily consumption of each food.Anthropometric measurements were obtained following standard protocols [ 20 ]. Individuals were grouped according to body mass index (BMI) cut-offs as under/normal weight (BMI < 25 kg/m), overweight (from BMI 25 to 29.9 kg/m) and obese (BMI ≥ 30 kg/m). Arterial blood pressure was measured in sitting position and after at least 5 min of rest at the end of the physical examination. Due to the possibility of differences in blood pressure measurement, the measurements were taken three times from the right arm, relaxed and well-supported by a table, with an angle of 45° from the trunk. The mean of the last two measurements was considered for inclusion in the database. Patients were considered hypertensive when average systolic/diastolic blood pressure levels were equal to or more than 140/90 mmHg, in accordance with the European Society of Cardiology (ESC)/European Society of Hypertension (ESH) guidelines, or when participants had been previously diagnosed with hypertension. Patients were considered dyslipidemic or diabetic if diagnosed by a physician with hypercholesterolemia/hypertriglyceridemia or diabetes, respectively. Previous diagnosis of diseases was collected from the medical records of the referred general practitioner.Data were collected with a presence-assisted interview through tablet computers. All subjects were supplied with a paper copy of the questionnaire in order to see each response option. However, the final answers were recorded instantly by the interviewer. The demographic information, such as age at recruitment, gender, highest educational level achieved, occupation (the most relevant job during the year before the investigation) or last occupation before retirement and marital status, were collected. Occupational status was classified as: (i) unemployed, (ii) low (unskilled workers), (iii) medium (partially skilled workers) and (iv) high (skilled workers). Educational status was categorized as: (i) low (primary/secondary), (ii) medium (high school) and (iii) high (university). The International Physical Activity Questionnaire (IPAQ) was used to examine motor activity [ 19 ], and it included a panel of questionnaires (five domains) investigating the time spent being physically active in the last week. In accordance with the IPAQ, physical activity level was classified as: (i) low, (ii) moderate and (iii) high. Smoking status was categorized as: (i) non-smoker, (ii) ex-smoker and (iii) current smoker, and alcohol consumption was classified as: (i) none, (ii) moderate drinker (0.1–12 g/d) and (iii) regular drinker (>12 g/d).The Mediterranean Healthy Eating, Aging and Lifestyle (MEAL) study is an observational study aimed at examining the relationship between lifestyle and dietary habits in the Mediterranean region and NCDs. The original cohort included a sample of 2044 randomized adults (≥18 years old) from the main districts of Catania, a city in the south of Italy. The recruitment and data collection were carried out between 2014 and 2015. Details of the project protocol are published elsewhere. All subjects involved in the study were advised about the objective of the project and provided written informed consent. All the study methods were carried out following the Declaration of Helsinki (1989). The study protocol was examined and accepted by the relevant ethical committee.The association between total and classes of dietary fats and metabolic outcomes is shown in Table 2 . Multivariate-adjusted analysis revealed a significant inverse association between total dietary fats and hypertension (for Q4, odds ratio (OR) = 0.57, 95% CI: 0.35, 0.91). Moreover, individuals reporting moderate consumption of total fats were less likely to have diabetes (for Q3, 0.27, 95% CI: 0.12, 0.61). Among single classes of dietary fats, individuals in the highest quartile of SFA intake were less likely to have hypertension (OR = 0.55, 95% CI: 0.34, 0.89). Moreover, subjects reporting moderate intake of MUFA were less likely to have hypertension and diabetes (OR = 0.61, 95% CI: 0.42, 0.88 and OR = 0.47, 95% CI: 0.22, 0.97, respectively). No associations were found between PUFA and any of the investigated outcomes.4. DiscussionIn the current study, we investigated the relationship between dietary fat subtype intake and cardio-metabolic risk factors in a cohort of Mediterranean adults. Interestingly, total SFA consumption was not detrimentally associated with any cardio-metabolic outcomes; conversely, individuals who had higher intake of total SFA were less likely to have hypertension and those who specifically consumed more SCSFA–MCSFA were less likely to have dyslipidemia and diabetes. Although SFAs have been assumed to be the main nutritional risk factor for cardio-metabolic diseases, recent studies have provided new controversial and interesting evidence suggesting that the SFA–CVD relationship may not be as strong as initially thought. A recent systematic review by the Cochrane group reported that a reduction in the intake of SFAs induced a 17% lowering of the risk of cardiovascular disease and that the beneficial effects increase when SFAs are replaced with PUFAs or starchy food [ 23 ]. However, cutting SFAs had a null effect on the other CVD end-points investigated and, furthermore, the putative adverse effect of SFAs on CVD events became non-significant when the analysis included only clinical trials that had successfully reduced SFA intake while removing those that were not successful [ 24 ]. The Prospective Urban Rural Epidemiological (PURE) study [ 25 ] conducted on 135,000 subjects without CVD demonstrated that increased consumption of total fats was linked with lower mortality and had a null association with CVD. Moreover, subjects in the highest quintile of SFA intake had a lower risk of stroke. Furthermore, a recent dose–response meta-analysis of cohort studies [ 8 ] highlighted that total fat, SFA, MUFA, and PUFA intake were not associated with CVD risk. Moreover, a meta-analysis of epidemiological studies conducted by SiriTarino [ 26 ] found no evidence that SFAs are associated with an increased risk of CHD. In addition, De Souza [ 27 ] and colleagues found no association between SFA intake and all-cause mortality, CHD, CHD mortality, ischemic stroke or type-2 diabetes among healthy subjects. A possible explanation for these controversial results on cardio-metabolic health may be the chain length of the fatty acids mainly consumed in diets [ 28 ]. SCFA may have a favorable impact on metabolism through activation of G protein-coupled receptors in endocrine and colon epithelial cells that, in turn, release anorectic hormones, such as glucagon-like peptide 1 (GLP) and peptide YY (PYY), which may contribute to reducing food intake and protecting individuals against obesity and diabetes, as shown in our results. Specifically, butyric acid (4:0) may have a beneficial effect on cardiovascular risk via inhibition of the NF-kB pathway and pro-inflammatory cytokines [ 29 ]. MCSFAs, such as caproic acid (6:0) and caprylic acid (8:0), have been shown in a cellular model to reduce the activity of fatty acid synthase (FAS), a primary enzyme of de novo lipogenesis that may contribute to the development of obesity and non-alcoholic fatty liver disease (NAFLD) [ 30 ]. Moreover, MCSFA may prevent endotoxic lipopolysaccharide (LPS)-mediated inflammation and lesions, which are linked to metabolic syndrome [ 31 ]. Caprylic acid (C8:0) and capric acid (C10:0) may reduce intestinal bile acid reabsorption with a simultaneous increase in excretion, which, in turn, lower TC and LDL-C [ 32 ].In our cohort, myristic acid (14:0) intake was not associated with dyslipidemia and diabetes, while we found a positive association with hypertension. Although this molecule has been reported to play a role in post-translational protein changes and pathways that regulate several metabolic processes [ 33 ], data from the literature on its actual effect on metabolic health are not univocal: some studies reported potential beneficial effects from increasing HDL-C, such as reducing triglycerides levels, improving long-chain omega-3 levels in plasma phospholipids [ 34 ] and obesity-associated insulin resistance [ 35 ] and increasing LDL-C and apoB levels [ 36 ]. Among LCSFAs, in our study, stearic acid (18:0) was associated with a lower risk of diabetes and hypertension. Palmitic acid (16:0) and stearic acid (18:0) have been shown to increase cardiovascular risk through worsening of the lipid profile and alteration of inflammatory response [ 37 38 ], although the effects of stearic acid are less widely agreed on [ 39 ]. Moreover, stearic acid undergoes conversion to MUFA oleic acid (18:1) through the hepatic enzyme stearoyl–CoA–desaturase, which may in part explain the extensive detrimental effects.n -3 and n -6 PUFAs could have contrasting roles in human health. Indeed, n -6 PUFAs, such as linoleic acid (LA, 18:2), were thought to have pro-inflammatory effects because they can be converted into arachidonic acid (AA, 20:4) with consequent production of pro-inflammatory eicosanoids and, at the same time, reduce the conversion of n -3 PUFA alpha-linolenic acid (ALA, 18:3) into eicosapentaenoic acid (EPA, 20:5) and/or docosahexaenoic acid (DHA, 22:6) by competing for the same enzymes [ n -6 PUFAs, but not n -3 PUFAs, may improve homeostasis model assessment—insulin resistance (HOMA-IR), lowering insulin in healthy subjects [ Despite the new nutritional scenario regarding the role of dietary fat in cardio-metabolic health, the American Heart Association recently remarked on the importance of limiting SFA and substituting it with PUFA to reduce cardiovascular risk [ 40 ]. The American Dietary Guidelines Advisory Committee showed strong evidence that replacing SFAs with PUFAs reduces the risk of CHD events and CVD mortality, but there is a paucity of evidence to establish whether this substitution affects the risk of stroke or heart failure [ 41 ]. Data from previous epidemiological studies [ 42 ] and clinical trials [ 43 ] have shown that replacing SFA with PUFA and MUFA reduces the risk of combined CVD events, mainly lowering LDL-C and reducing inflammation. However, a recent umbrella review showed that replacing SFA with PUFA does not convincingly reduce cardiovascular events or mortality, probably due to the original invalidity of the diet–heart hypothesis and numerous research biases [ 44 ]. In our study, neither total PUFA intake nor the individual PUFAs were associated with dyslipidemia. PUFAs have always been generally considered “healthy” fats. They are implicated in vascular function, cell membranes, the nervous system and inflammation, being precursors of eicosanoids and beneficial lipid mediators, such as resolvins, docosatrienes and protectins. Previous evidence suggested that-3 and-6 PUFAs could have contrasting roles in human health. Indeed,-6 PUFAs, such as linoleic acid (LA, 18:2), were thought to have pro-inflammatory effects because they can be converted into arachidonic acid (AA, 20:4) with consequent production of pro-inflammatory eicosanoids and, at the same time, reduce the conversion of-3 PUFA alpha-linolenic acid (ALA, 18:3) into eicosapentaenoic acid (EPA, 20:5) and/or docosahexaenoic acid (DHA, 22:6) by competing for the same enzymes [ 45 ]. This hypothesis has lost its strength based on the latest results from clinical studies on humans demonstrating that LA intake has little influence on conversion into AA, while, conversely, it may be converted into nitrosylated LA and 13-hydroxyoctadecadienoic acid, which show anti-inflammatory effects [ 46 ]. Interestingly, in our study, subjects reporting a higher intake of LA were less likely to be hypertensive in a linear way. The blood pressure-lowering effects of LA are probably due to changes in vasodilator prostaglandin metabolism [ 47 ]. Moreover, we found that the intake of EPA and DHA was inversely associated with hypertension. A recent dose–response meta-analysis showed that EPA and DHA may reduce the risk of CHD by lowering high blood pressure among people already diagnosed with hypertension [ 48 ]. These fatty acids may exert cardioprotective effects, mainly by reducing ventricular fibrillation, heart rate and platelet aggregation through various mechanisms, including improved synthesis of eicosanoids [ 49 50 ] and inhibition of the NF-kB pathway by acting on PPAR-gamma and certain G-protein-coupled receptors in macrophages and adipocytes, thus reducing the production of inflammatory cytokines [ 51 ]. Finally, PUFA may also influence the risk of type-2 diabetes by reducing insulin resistance, activating PPAR-alpha and suppressing sterol regulatory binding protein-1c (SREBP-1c) [ 52 ]. A recent meta-analysis of RCTs showed that substituting SFAs with PUFAs improved glycemia and insulin resistance [ 53 ]. However, in line with our results, not all subtypes of PUFA appear to have the same effects on type-2 diabetes risk. The latest data show that-6 PUFAs, but not-3 PUFAs, may improve homeostasis model assessment—insulin resistance (HOMA-IR), lowering insulin in healthy subjects [ 54 ]. Moreover, a recent study reported that high levels of LA, but not AR, are associated with a lower risk of becoming diabetic [ 55 ]. In our study, only subjects with the highest intake of LA were less likely to have diabetes while the association with other PUFA was null.In addition to PUFAs, MUFAs have also been considered for replacing dietary SFAs, as highlighted by the latest American dietary guidelines, although evidence is weak. Although substitution of SFAs with MUFAs has been associated with a decrease in metabolic syndrome [ 56 ], findings from a meta-analysis of observational studies showed null results on risk of CVD [ 8 42 ]. Our results showed that individuals with higher intake of oleic acid (18:1) were less likely to have hypertension and diabetes, while no association was found for dyslipidemia. Typical sources of MUFAs in the Mediterranean region are nuts, which have been demonstrated to have potential benefits for metabolic disorders [ 57 ]. MUFAs may have anti-inflammatory and antioxidant properties, lowering endoplasmic reticulum stress, inhibiting NF-kB transcription factor and acting through AMP-activated protein kinase (AMPK) phosphorylation, and may also reduce the polarization of M1 macrophages to M2 macrophages [ 51 ]. However, recent published studies report a beneficial effect of extra virgin olive oil and, probably, its polyphenols rather than the simple intake of MUFAs [ 58 ]. When MUFAs were isocalorically replaced with a non-lipidic component, such as carbohydrates, observed effects on blood lipids were rather small or negligible, and a recent review even highlighted the potentially negative effects of olive oil and oleic acid if introduced in large quantities [ 59 ]. These findings are in line with those of our study concerning gadoleic acid (20:1), a long-chain MUFA that appeared to be detrimentally associated with each investigated outcome in our study. The Ludwigshafen Risk and Cardiovascular Health Study showed an inverse association of gadoleic acid with LDL-C, HDL-C and eGFR but direct correlations with markers of inflammation and endothelial activation, as well as heart failure [ 60 ]. In contrast, a randomized controlled trial found that the supplementation of LCMUFAs (gadoleic acid and cetoleic acid) improved endothelial function by lowering trimethylamine-N-oxide levels, IL-6 and TNF-α, possibly due to improved gut microbiota profile [ 61 ]. The debate on the effects of LCMUFAs (more than 18 carbons) remains inconclusive as research is still scarce.The results of the current study should be considered in light of several limitations. First, the observational nature of this investigation did not make it possible to define a causal relation between variables but only an association. The cross-sectional design of the study may have limited the interpretation of the results, as they may have suffered from revere causation. Moreover, although we performed multivariate-adjusted logistic regression analyses, residual vulnerability to type-1 errors still exists and should be taken into account. Another limitation concerns the dietary assessment method: although there is a univocal and perfect approach to collecting dietary data (with integrated use of multiple 24 h recalls and dietary diaries being highly desirable), FFQs are known to potentially under- or overestimate food intake due to recall bias, portion size miscalculation and social desirability bias. Finally, cases were confirmed by medical visit, but potential undiagnosed patients may have led to inclusion of false-negative cases in the study sample. These findings require further investigation in studies that are better designed, more controlled and use a prospective approach.",Dietary Fats and Cardio-Metabolic Outcomes in a Cohort of Italian Adults
123,9,9_nasa_asteroid_space_dart,https://spacenews.com/china-seeks-new-partners-for-lunar-and-deep-space-exploration/,"China's partner Russia not mentioned during space Congress in ParisPARIS — China is looking to build partnerships for its upcoming missions to the moon and deep ventures into the solar system, while omitting mention of main partner Russia.Chinese space officials presented a range of opportunities for international cooperation in the country’s plans during a session at the International Astronautical Congress (IAC) in Paris, Sept. 21.Wang Qiong of the Lunar Exploration and Space Engineering Center under the China National Space Administration (CNSA) stated that China was open to proposals for its Chang’e-7 lunar south pole landing and orbiting mission—with a coinciding call announced by CNSA—and later Chang’e-8 in-situ resource utilization test mission.Chang’e-6 already features participation from Sweden and ESA in the form of a negative ion detector, an Italian retroreflector, a French radon instrument and a Pakistani CubeSat, named ICUBE-Q, Wang stated.The UAE will also have a small rover with a mass of around 10 kilograms on board the mission.In deep space, China is working on Tianwen-2, a near-Earth asteroid sampling mission which will also visit a main belt comet, launching around 2025. The Tianwen-3 Mars sample return and Tianwen-4 mission to Jupiter and Uranus are still at preliminary stages and open to collaboration. The Tianwen-4 mission will include a solar-powered Jupiter orbiter and a smaller, radioisotope-powered spacecraft to make a flyby of Uranus.Currently China is inviting proposals for payloads to join its own, already planned and approved Chang’e lunar missions due to launch before the end of the decade. This has characterized much of China’s cooperation, with the main exception of collaborative projects with Europe.The International Lunar Research Station, a megaproject envisioning the establishment of a permanent robotic and later human-occupied moon base in the 2030s, will however be open to a much wider scope and depth of involvement. This will allow countries, agencies, companies and other entities to join in at the planning and other stages to form a coordinated set of infrastructure on the moon.The elephant in the room was however not mentioned. The ILRS roadmap was presented as a joint project by nominally equal partners China and Russia in June 2021 in St. Petersburg during another International Astronautical Federation (IAF) event. There was no Russian presence at IAC due to the country’s invasion of Ukraine.The project had generally been referred to as a joint China-Russia program until after the invasion. Wang’s presentation stated instead that the ILRS was conceived in 2014 and selected as an “ongoing program of international major scientific project in China” in 2020.The only visible representation of potential Russian came in a slide listing future Chinese Chang’e and Russia Luna missions, alongside graphics of the Chinese Long March 9 super heavy-lift rocket and a large Russian launch vehicle. The slide was taken straight from ILRS handbook released to coincide with the St. Petersburg event in 2021, and Russia nor its missions were not explicitly named.It is hard to say if the lack of representation of Russian involvement reflects a change in Beijing’s thinking or a sensitivity to the current geopolitical context. But China appears to face a dilemma for its grandest space ambitions so far.“Be it in space or elsewhere, China has a very realistic view of Russia and partnering with Moscow has never been Beijing’s most preferred outcome, for the two countries are not natural partners,” Marco Aliberti, a senior research fellow at the European Space Policy Institute (ESPI) in Vienna, told SpaceNews.“This uneasiness is well reflected in the very nature of their cooperation initiatives, including most notably their joint ILRS, which still remains little more than a coordination mechanism rather than a bold undertaking sharing a common goal.”“In moving forward, however, Beijing now seems to be increasingly confronted with a difficult dilemma: turn the relationship into a real partnership or drop it altogether.”Aliberti says China has been eager to build a credible alternative to the US-led Artemis, not only from a programmatic but also a normative perspective. But potential gains from partnering with Russia, previously including tapping into technological knowhow, are evaporating.“Beyond a few launcher programmes, with questionable success, military satellites and heritage human spaceflight experience, Russia has not been able to offer novel and innovative efforts to the international community in the recent past and I believe this will be exacerbated even more by the continuing sanctions and overall isolation of the country,” says Tomas Hrozensky, also of ESPI.Given Russia’s current standing in the world, a partnership “may prevent new, and possibly more auspicious, partners” such as European countries, working with China, Aliberti notes.He adds that what may result is a continuation of an ambiguous stance that will “officially celebrate the importance of cooperation with Russia while in parallel pursuing opportunities that better serve its national interests.”",China seeks new partners for lunar and deep space exploration
179,9,9_nasa_asteroid_space_dart,https://techcrunch.com/2019/10/24/nasa-administrator-jim-bridenstine-explains-how-startups-can-help-with-artemis-moon-missions/,"At this week’s International Astronautical Congress, where the space industry, international space agencies and researchers from around the world convene to discuss the state of space technology and business, I asked NASA Administrator Jim Bridenstine about what role he sees for startups in contributing to his agency’s ambitious Artemis program. Artemis (named after Apollo’s twin sister, one of the gods of Greek mythology) is NASA’s mission to return human beings to the surface of the Moon — this time to stay — and to use that as a staging ground for further exploration to Mars and beyond.Bridenstine, fielding the question during a press Q+A about Artemis, said the program is incredibly welcoming of contributions from startups large and small, and that it sees a number of different areas where contributions from younger space companies can have a big impact.“When we talk about entrepreneurs, there are big entrepreneurs and there are small entrepreneurs, but know this: What we’re building in the [Lunar] Gateway is open architecture, and we want to go with commercial partners,” Bridenstine said. “So there are in fact, a number of companies here [at IAC], big companies that have said they want to go to the Moon, they want to go sustainably, they want to be part of Artemis, and the Gateway is available to them.”The Lunar Gateway is a station NASA intends to put in orbit around the Moon to act as a staging ground for its vehicles, a key step to ensure the process of landing things on the Moon once they reach lunar orbit is more easily accomplished. Bridenstine pointed out that in the Broad Agency Agreement (BAA) that NASA originally put out for the Artemis program, it went further still and said that it welcomed proposals from private space companies that involve going directly to the Moon, bypassing the Gateway entirely.Actually getting to the Moon has been taken on by some of the deeper-pocketed and more well-established entrepreneurs among the so-called “New Space” companies, including SpaceX. But Artemis participation goes well beyond the high-priced task of building vehicles capable of getting from Earth to lunar orbit, according to Bridenstine.“We’re going to need cargo on the surface of the Moon,” he said, noting that the Space Launch System (SLS) and Orion crew capsule Artemis will use to take humans to the Moon in 2024 will lean on advance payloads to better ensure mission success. “[W]hen we talk about aggregating a lander at the gateway — when we talk about, maybe even putting hardware on the surface the Moon, including science hardware, like the Viper neutron spectrometer, an IR spectrometer helping us understand the regolith and the water ice, what’s there on the surface of the Moon, where it is and in what quantities […] we’re going to need those science instruments delivered to the surface of the Moon.”Indeed, there are companies poised to deliver cargo via lunar landers in advance of, or in time with, NASA’s 2024 target for a human landing, including Astrobotic’s Peregrine Moon lander, which is looking to launch in 2021, and Blue Origin’s Blue Moon lander. Both these landers, and the payloads they carry, could include startup-designed equipment and systems to pave the way for sustainable human occupation of our large natural satellite. In fact, Bridenstine suggested some potential payloads that could be even more wild than advance data-gathering hardware.“Maybe even — again it depends on budgets, and I’m not promising anything between now and 2024 — but maybe even an inflatable habitat on the surface of the Moon so that when our astronauts get there they have a place to go, and they can stay for longer periods of time,” he said. “Is that in the realm of possibility? Absolutely.”Bridenstine continued that the agency is already working with many smaller, entrepreneurial businesses, and intends to continue exploring partnerships with more. There’s a clear and growing need for lunar cargo from NASA, in increasing volumes, the Administrator pointed out.“On top of SLS and Orion we need additional capability, there are opportunities there for all kinds of commercial companies entrepreneurs,” he said. “We also have small business investment and research that NASA is involved in, and we’re on-ramping small businesses all the time. In fact, right now we have the Commercial Lunar Payload Services [CLPS] program underway. We have nine companies that have signed up […] two of them now have task orders to deliver to the Moon in 2021 […] We’re on-ramping, not only those nine companies, but we want to on-ramp additional companies, and maybe even bigger companies for larger landing opportunities because, like I said, we’re going to have a lot more needs in the future for cargo on the surface of the Moon.”",NASA Administrator Jim Bridenstine explains how startups can help with Artemis Moon missions
477,9,9_nasa_asteroid_space_dart,https://www.reuters.com/lifestyle/science/nasas-asteroid-deflecting-dart-spacecraft-nears-planned-impact-with-its-target-2022-09-26/,"Sept 26 (Reuters) - NASA's DART spacecraft successfully slammed into a distant asteroid at hypersonic speed on Monday in the world's first test of a planetary defense system, designed to prevent a potential doomsday meteorite collision with Earth.Humanity's first attempt to alter the motion of an asteroid or any celestial body played out in a NASA webcast from the mission operations center outside Washington, D.C., 10 months after DART was launched.The livestream showed images taken by DART's camera as the cube-shaped ""impactor"" vehicle, no bigger than a vending machine with two rectangular solar arrays, streaked into the asteroid Dimorphos, about the size of a football stadium, at 7:14 p.m. EDT (2314 GMT) some 6.8 million miles (11 million km) from Earth.The $330 million mission, some seven years in development, was devised to determine if a spacecraft is capable of changing the trajectory of an asteroid through sheer kinetic force, nudging it off course just enough to keep Earth out of harm's way.Whether the experiment succeeded beyond accomplishing its intended impact will not be known until further ground-based telescope observations of the asteroid next month. But NASA officials hailed the immediate outcome of Monday's test, saying the spacecraft achieved its purpose.""NASA works for the benefit of humanity, so for us it’s the ultimate fulfillment of our mission to do something like this - a technology demonstration that, who knows, some day could save our home,"" NASA Deputy Administrator Pam Melroy, a retired astronaut, said minutes after the impact.DART, launched by a SpaceX rocket in November 2021, made most of its voyage under the guidance of NASA's flight directors, with control handed over to an autonomous on-board navigation system in the final hours of the journey.Monday evening's bullseye impact was monitored in near real time from the mission operations center at the Johns Hopkins University Applied Physics Laboratory in Laurel, Maryland.Cheers erupted from the control room as second-by-second images of the target asteroid, captured by DART's onboard camera, grew larger and ultimately filled the TV screen of NASA's live webcast just before the signal was lost, confirming the spacecraft had crashed into Dimorphos.DART's celestial target was an oblong asteroid ""moonlet"" about 560 feet (170 meters) in diameter that orbits a parent asteroid five times larger called Didymos as part of a binary pair with the same name, the Greek word for twin.Neither object presents any actual threat to Earth, and NASA scientists said their DART test could not create a new hazard by mistake.Dimorphos and Didymos are both tiny compared with the cataclysmic Chicxulub asteroid that struck Earth some 66 million years ago, wiping out about three-quarters of the world's plant and animal species including the dinosaurs.[1/4] The last complete image of asteroid moonlet Dimorphos, taken by the DRACO imager on NASA's DART mission 12 kilometers from the asteroid and 2 seconds before impact, showing a patch of the asteroid that is 31 meters across, released September 26, 2022. NASA/Johns Hopkins APL/Handout via REUTERS 1 2 3 4Smaller asteroids are far more common and present a greater theoretical concern in the near term, making the Didymos pair suitable test subjects for their size, according to NASA scientists and planetary defense experts. A Dimorphos-sized asteroid, while not capable of posing a planet-wide threat, could level a major city with a direct hit.Also, the two asteroids' relative proximity to Earth and dual configuration make them ideal for the first proof-of-concept mission of DART, short for Double Asteroid Redirection Test.ROBOTIC SUICIDE MISSIONThe mission represented a rare instance in which a NASA spacecraft had to crash to succeed. DART flew directly into Dimorphos at 15,000 miles per hour (24,000 kph), creating the force scientists hope will be enough to shift its orbital track closer to the parent asteroid.APL engineers said the spacecraft was presumably smashed to bits and left a small impact crater in the boulder-strewn surface of the asteroid.The DART team said it expects to shorten the orbital path of Dimorphos by 10 minutes but would consider at least 73 seconds a success, proving the exercise as a viable technique to deflect an asteroid on a collision course with Earth - if one were ever discovered.A nudge to an asteroid millions of miles away years in advance could be sufficient to safely reroute it.Earlier calculations of the starting location and orbital period of Dimorphos were made during a six-day observation period in July and will be compared with post-impact measurements made in October to determine whether the asteroid budged and by how much.Monday's test also was observed by a camera mounted on a briefcase-sized mini-spacecraft released from DART days in advance, as well as by ground-based observatories and the Hubble and Webb space telescopes, but images from those were not immediately available.DART is the latest of several NASA missions in recent years to explore and interact with asteroids, primordial rocky remnants from the solar system's formation more than 4.5 billion years ago.Last year, NASA launched a probe on a voyage to the Trojan asteroid clusters orbiting near Jupiter, while the grab-and-go spacecraft OSIRIS-REx is on its way back to Earth with a sample collected in October 2020 from the asteroid Bennu.The Dimorphos moonlet is one of the smallest astronomical objects to receive a permanent name and is one of 27,500 known near-Earth asteroids of all sizes tracked by NASA. Although none are known to pose a foreseeable hazard to humankind, NASA estimates that many more asteroids remain undetected in the near-Earth vicinity.(This story corrects name in paragraph 6 to Pam from Palm)Reporting by Steve Gorman in Los Angeles; Additional reporting by Joey Roulette in Los Angeles; Editing by Sandra Maler and Stephen CoatesOur Standards: The Thomson Reuters Trust Principles.",NASA's DART spacecraft hits target asteroid in first planetary defense test
178,9,9_nasa_asteroid_space_dart,https://techcrunch.com/2019/10/15/nasas-new-artemis-spacesuits-make-it-easier-for-astronauts-of-all-sizes-to-move-on-the-moon/,"NASA revealed new spacesuits, specifically created for the Artemis generation of missions, which aim to get the first American woman and the next American man to the surface of the Moon by 2024. The new design’s toppling feature is greater mobility and flexibility, in basically every respect. NASA unveiled both a full suit designed for use in extra-vehicular activities on the surface of the Moon, and a flight suit for use while in transit to lunar orbit.Guided by NASA Administrator Jim Bridenstine, the agency first demonstrated the suit that astronauts will use on the surface of the Moon (and, with modifications, eventually on Mars). It’s called the “xEMU” variant, and it looks a lot like what you might think of when you imagine “space suit” in your mind. But it’s quite different in many respects from what astronauts used to visit the surface of the Moon during the Apollo program.It allows you to actually moonwalk, for instance: The original suit used for Moon-based activities actually only offered enough range of motion for Neil Armstrong and Buzz Aldrin to be able to essentially “bunny hop” on the lunar surface, in Bridenstine’s own words. This new design allows them to move around much more dynamically, including actually walking, and offers plenty of range of motion for their arms. Combined with new gloves that actually allow astronauts to freely move their fingers, they can do things like pick up rocks off the lunar surface with relative ease.The new spacesuit design is also designed to work with virtually everyone who could want to become an astronaut, with inclusive sizing that can accommodate anyone from the “first percentile female to the 99th percentile male,” according to Kristine Davis, an Advanced Space Suit Engineer at NASA and the person who demonstrated the xEMU variant of the suit onstage at the event on Tuesday.“We want every person who dreams of going into space to be able to say to themselves, that yes, they have that opportunity,” Bridenstine added, regarding the suit’s inclusive design.Because NASA is also looking to ensure that this time when they return to the Moon, they do so sustainably (meaning with the intent of eventually setting up shop and staying), they also designed this suit with a much higher range of temperature variances to ensure it can serve at both the North and South poles of the Moon, as well as around the equatorial region. This xEMU suit is designed to survive temperature ranges from between -250 and +250 degrees Fahrenheit.NASA also talked about how this is an improvement from the spacesuit currently used on the International Space Station. For one, this one has usable legs, where the legs on the ISS suits are essentially just for protection since you aren’t using them in zero and microgravity. The bearing designs used to connect the arms here also mean you have greater range of motion for reaching and grabbing, as mentioned.The other suit, called the “Orion Crew Survival Suit,” is a much lighter suit that’s designed to be worn during take-off and landing. It’ll generally be depressurized when in use, but can provide protection in case of accidental depressurization. It was demonstrated by Dustin Gohmert, project manager on the Orion crew suit, who explained that it also has thermal protection and radiation protection, though not to the level of the xEMU.The larger xEMU suit is also intentionally designed to be upgradeable, somewhat like a PC motherboard, and it’s designed so that it can be upgraded and worked on in space by the astronauts to adopt new and improved technologies as they become available, rather than having to be round-tripped back to Earth for updates.Bridenstine re-iterated that NASA is also working with commercial partners on sourcing the production of the Artemis suits, as the agency announced earlier this month. It’s also looking to these companies to provide advice and input about what to do in terms of future evolutions and upgrades for the technology used in the suit.Overall, Bridenstine was obviously eager to talk about commercialization, and NASA’s eagerness to work with commercial partners on the Artemis program, and on space in general.“What NASA has already done is invested in commercial resupply of the International Space Station […] We have invested now in commercial crew. And in the first part of next year, we’re going to once again launch American astronauts on American rockets from American soil for the first time since the retirement of the Space Shuttles in 2011,” he said. “That’s going to be a really positive development for our country, but it’s going to be commercial […] And of course, we want to see a lot of robust commercial habitats in low-Earth orbit as well. Ultimately, what that enables us to do is then take the resources that the taxpayers give us, and go to the Moon and on to Mars, always keeping an eye on commercialization even there. The goal here is to expand humanity further into space than ever before.”",NASAâs new Artemis spacesuits make it easier for astronauts of all sizes to move on the Moon
177,9,9_nasa_asteroid_space_dart,https://techcrunch.com/2019/10/04/nasa-calls-for-input-on-moon-spacesuits-and-plans-to-source-them-commercially-in-future/,"NASA issues a new formal request for info from industry specifically around spacesuits. The agency is hoping to gather information in order to help it figure out a future path for acquisition of spacesuit production and services from external industry sources.That doesn’t mean it’s outsourcing its spacesuit design and production immediately – NASA will build and certify its own spacesuits for use in the first Artemis missions, including Artemis III which is the one that’ll see the next American man and the first American woman take their trip to the lunar surface. But for Artemis missions after that, of which there are currently five more proposed (Artemis 4 through 8), four of which will have crew on board.NASA has of course already worked with private industry, as well as academic institutions and researchers, on the technologies that will go into its own space suits. And the agency fully expects that the current exploration suit will form the basis of any future designs. It is however looking to fully transition their prouduction and testing to industry partners, and will additionally expect those partners to “facilitate the evolution of the suits” and also suggest improvements on the existing suit design.On top of the suits, NASA is looking for input on tools and support hardware to be used with the suits, during extra-vehicular activities, or in making sure the suits work well with the vehicles that’ll be transporting them, as well as the lunar gateway that will act as the staging ground between Earth and the Moon’s surface.Finally, NASA also would like to hear from companies about how to better commercialize spacesuits and spacewalks – making them available to customers outside of the agency itself, as well.This isn’t surprising given how many signals NASA has been giving lately that it’s interesting in partnering with industry more deeply across both Artemis, future Mars exploration, and the ISS (and its potential commercial successor). The full RFI issued by NASA is available here, in case you’re interested in spinning up a spacesuit startup.",NASA calls for input on Moon spacesuits and plans to source them commercially in future
63,9,9_nasa_asteroid_space_dart,https://interestingengineering.com/innovation/amazon-spacex-help-launching-starlink-rival,"Amazon signed that agreement, totaling 83 Kuiper launches, with United Launch Alliance (ULA), European firm Arianespace, and Jeff Bezos' Blue Origin.There's one important caveat, though. Those launches partially rely on rockets that have yet to reach orbit, including Blue Origin's New Glenn and the Ariane 6. In a webcast with The Washington Post, Amazon senior VP of devices and services Dave Limp touched on this concern, and he refused to rule out asking the company's rival SpaceX for help with its launches.Amazon is playing catch-up with StarlinkAmazon's two prototype satellites are set to launch aboard the new Vulcan Centaur rocket early next year, as per an October press statement. The Project Kuiper mega-constellation is eventually expected to total 3,236 satellites in low Earth orbit, bringing high-speed internet anywhere in the world, much like SpaceX's Starlink. SpaceX currently has more than 3,000 satellites in orbit, and it aims to eventually send roughly 30,000 more up to the skies.That's a lot of catching up to do, and Amazon may even need to turn to its rival for help, Limp conceded during his recent webcast interview with The Washington Post. ""You'd be crazy not to, given their track record,"" Limp said after he was asked whether Amazon might turn to SpaceX to launch its Kuiper satellites.",Amazon may have to turn to SpaceX for help launching its Starlink rival service
566,9,9_nasa_asteroid_space_dart,https://www.zdnet.com/article/nasa-develops-a-tiny-high-powered-laser-to-find-water-on-the-moon/,"Image: NASA/Michael GiuntoPrevious technologies have allowed NASA to confirm that there are small amounts of hydration across the Moon. However, these technologies have not been able to identify where it came from, how much water there is or even if it is, in fact, water. Now, utilizing Goddard technology, engineer Dr. Berhanu Bulcha has developed an instrument to definitively locate and identify water sources on the Moon, according to a NASA post.""Other missions found hydration on the Moon, but that could indicate hydroxyl or water. If it's water, where did it come from? Is it indigenous to the formation of the Moon, or did it arrive later by comet impacts? How much water is there? We need to answer these questions because water is critical for survival and can be used to make fuel for further exploration,"" said Dr. Bulcha.SEE: NASA enters contract for computing processor that will change space explorationNASA prioritizes identifying water and other resources because it is crucial to exploring the Moon and other objects in the solar system.Dr. Bulcha said that an instrument known as a heterodyne spectrometer could zoom in on particular frequencies to locate and identify the hydration on the Moon. A stable, high-power terahertz laser that is needed for the spectrometer was prototyped in collaboration with Longwave Photonics through NASA's Small Business Innovation Research (SBIR) program.""The problem with existing laser technology is that no materials have the right properties to produce a terahertz wave,"" said Dr. Bulcha.SEE: NASA is blazing an inspirational trail. We need to make sure everyone can follow itHydrogen-containing compounds such as water emit photons in the terahertz frequency range, says NASA. However, traditional lasers fall short in the portion of the spectrum known as the terahertz gap. In order to fill the gap, Dr. Bulch's team is developing quantum cascade lasers.To bypass issues with the quantum cascade laser beams spreading out in a large angle, the team, using innovative technology supported by Goddard's Internal Research and Development (IRAD) funding, integrated the laser on a waveguide with a thin optical antenna to tighten the beam.Dr. Bulch hopes to have a flight-ready laser for NASA's Artemis program. However, the use of this laser can go far beyond the Artemis mission. ""It could also power a handheld device for use by future explorers on the Moon, Mars, and beyond,"" said NASA.","NASA's new tiny, high-powered laser could find water on the Moon"
66,9,9_nasa_asteroid_space_dart,https://interestingengineering.com/innovation/nasa-successfully-tests-robot-balloon-meant-to-one-day-explore-venus,"The aerobotThe aerial robotic balloon prototype, also known as the aerobot, could one day “take to the Venusian skies,” NASA said on its website, after the balloon completed two test flights in Nevada without any issues.The high pressure, extreme heat and gases on Venus’ surface makes it difficult to prepare any object to be sent there. The lack of hospitability on the planet can hinder even well-prepared spacecraft in a few hours. However, robotic exploration seems to be an option that could work in exploring the planet. A few miles above Venus, there is an area that would allow for an aerobot to move and operate safely.The concept and prototype balloonThe idea NASA came up with includes “a balloon with a Venus orbiter, with the two working together to study Earth’s sister planet. While the orbiter would remain far above the atmosphere, taking science measurements and serving as a communication relay, an aerial robotic balloon, or aerobot, about 40 feet (12 meters) in diameter would travel into it.”To test the concept, a research team from NASA’s Jet Propulsion Laboratory in Southern California and the Near Space Corporation — a high altitude/near space platforms and flight service provider — in Tillamook, Oregon, carried out the two successful flights of the prototype balloon that is one-third the size of one needed to explore Venus. The scientists and engineers wanted to test the balloon’s materials for the first time, allowing the team to assess the possibility of creating a full size aerobot that could explore Venus.",NASA successfully tests robot balloon meant to one day explore Venus
346,9,9_nasa_asteroid_space_dart,https://www.engadget.com/nasa-successfully-smacked-its-dart-impactor-spacecraft-into-an-asteroid-231706710.html,"After nearly a year in transit, NASA's experimental Double Asteroid Redirection Test (DART) mission, which sought to answer the questions, ""Could you potentially shove a asteroid off its planet-killing trajectory by hitting it with a specially designed satellite? How about several?"" has successfully collided with the Dimorphos asteroid. Results and data from the collision are still coming in but NASA ground control confirms that the DART impact vehicle has intercepted the target asteroid. Yes, granted, Dimorphos is roughly the size of an American football stadium but space is both very large and very dark, and both asteroid and spacecraft were moving quite fast at the time.NASA""It's been a successful completion of the first part of the world's first planetary defense test,"" NASA Administrator Bill Nelson said after the impact. ""I believe it's going to teach us how one day to protect our own planet from an incoming asteroid. We are showing that planetary defense is a global endeavor and it is very possible to save our planet.""NASA launched the DART mission in November, 2021 in an effort to explore the use of defensive satellites as a means of planetary defense against Near Earth Objects. The vending machine-sized DART impactor vehicle was travelling at roughly 14,000 MPH when it fatally crossed Dimorphos' path nearly 68 million miles away from Earth.Whether future iterations of a planetary defense system brimming with satellites willing to go all June Bug vs Chrysler Windshield against true planet-killer asteroids remains to be seen. Dimorphos itself is the smaller of a pair of gravitationally-entangled asteroids — its parent rock is more than five times as large — but both are dwarfed by the space rock that hit Earth 66 million years ago, wiping out 75 percent of multicellular life on the planet while gouging out the Gulf of Mexico.NASAThe DART team will likely be poring over the data generated by both the impactor and cameras released before the spacecraft made its final approach for days to come. However the team will consider shortening the orbital track of Dimorphos around Didymos by 10 minutes an ideal outcome, though any change of at least 73 seconds will still be hailed as a rousing success. The team will have to observe Dimorphos' orbit for half a day to confirm their success, as the moonlet needs nearly 12 hours to complete an circuit around Didymos.ATLAS observations of the DART spacecraft impact at Didymos! pic.twitter.com/26IKwB9VSo — ATLAS Project (@fallingstarIfA) September 27, 2022Update 9/27/2022 2:29 AM ET: The NASA funded ATLAS (Asteroid Terrestrial-impact Last Alert System) managed to record video of the impact (above). While fuzzy, it's still pretty cool.",NASA successfully smacked its DART spacecraft into an asteroid
577,9,9_nasa_asteroid_space_dart,https://thehill.com/opinion/technology/3647216-china-has-returned-helium-3-from-the-moon-opening-door-to-future-technology/,"The Chinese Chang’e 5 mission hasThe crystal mineral was exceedingly tiny, about one-tenth the size of a human hair. The new mineral is of immense interest to lunar geologists. The helium-3 that it contains has the potential to change the world.Scientists have known the lunar surface contains deposits of helium-3 since the Apollo program. The main advantage of helium-3 fusion over fusion using tritium and deuterium, isotopes of hydrogen, is that it doesn’t create radioactive neutrons. Its main disadvantage is that achieving a controlled fusion reaction with helium-3 is far more difficult than using more conventional fuels.According to NASAChina, perhaps in partnership with Russia, still plans crewed lunar landings sometime in the 2030s.In the meantime, NASA’s twice delayed Artemis 1 missionTwo robotic space missions,NASA still plans to send Artemis 2 and a crew of four astronauts, one of them from Canada, around the moon in 2024. The next year (or perhaps the year following), Artemis 3 will land the first astronauts on the lunar surface since the mission of Apollo 17 in 1972.Many reasons exist for returning to the moon:Of course, the problem remains of getting the technology of helium-3 fusion working. Helium-3 fusion may not become a reality before the middle of this century because of the technological obstacles involved. Some changes in American space and energy policy might hasten the advent of helium-3 fusion, however.The United States should start testing mining operations on the moon’s surface, particularly extracting helium-3 from lunar soil. Then helium-3 could be transported to Earth and provided to research laboratories so they can continue research and development of what promises to be a solution to both energy scarcity and climate change.The country that controls the source of energy that keeps technological civilization running will control the Earth. If China becomes that country, considering its human rights record and imperial foreign policy, history will take a dark turn. Therefore, the United States and the countries that have signed the Artemis Accords must acquire control of lunar helium-3 and develop the technology to use it as a source of fusion energy. Thus, the Artemis program will ensure the continuance of prosperity and human freedom on the Earth.Mark R. Whittington is the author of space exploration studies “","China has returned helium-3 from the moon, opening door to future technology"
174,9,9_nasa_asteroid_space_dart,https://techcrunch.com/2019/09/11/relativity-space-signs-its-the-satellite-transportation-company-momentus-as-its-first-customer/,"Relativity Space, the startup developing manufacturing technologies for entirely 3D printed rockets and space equipment, has signed its latest paying customer, the orbital transportation startup, Momentus.Relativity’s Terran 1 rocket will carry Momentus’ small and medium-sized satellite payloads on its rocket and Momentus will then move those satellites into geosynchronous orbit using its own in-space shuttle technology.The deal between Momentus and Relativity covers the first Terran 1 launch scheduled for 2021, with the option for five additional Relativity launches, according to a statement from the company.Carrying Momentus’ payloads enables the company to include more diverse ranges of orbits for Terran 1’s initial launch, including geostationary transfer orbit, Lunar and deep space orbits, lower inclinations and phasing multiple spacecraft in low Earth orbit, the company said.The tie-up links two of Y Combinator’s space-focused alumni, with Momentus graduating in 2018 and Relativity launching from the accelerator in 2016.In July, Momentus closed on a $25 million round of funding to move its business from simply providing a thruster for existing small-sats to becoming a full-service provider of orbital transportation services for payloads. The company’s key innovation was the development of a water-based plasma propulsion system for low-cost transportation in space. That’s what powers the company’s Vigoride orbital shuttle.Meanwhile, Relativity Space is barreling ahead with its own technology development.With the goal of building a rocket that goes from raw materials to launch-ready in less than 60 days with a payload capacity of up to 1250 kilograms, the company is planning its first test launch in 2020 with a commercial payload ready for 2021.So far the company has performed 200 engine tests to date across 14 different serial numbers and begun conducting turbo pump testing as well. Testing has also begun on the company’s initial avionics hardware, according to company co-founder Tim Ellis.Relativity has also started printing and stress testing some second stage structures and is beginning to print its larger primary stage structures now.“With Momentus’ innovations in sustainable in-space ‘last mile’ solutions, we look forward to working together to expand Terran 1’s flexibility and offering beyond LEO, offering small and medium satellite launch opportunities with industry-defining lead time, flexibility, and cost,” Ellis said in a statement. “This partnership will enable us to build the space economy faster, and accelerate the future of humanity in space.”The company has dramatically expanded its production, testing and launch facilities to include 280,000 square feet of operations on facilities at Cape Canaveral in Florida and the NASA Stennis Space Center in Mississippi.Relativity also has customer agreements with Telesat, to support their low Earth orbit constellation; the Thai satellite and space technology company, mu Space; and Spaceflight Industries to launch their smallsat ride-shares.",Relativity Space signs the satellite transportation company Momentus as a new customer
150,9,9_nasa_asteroid_space_dart,https://techcrunch.com/2017/04/17/engage-mister-sulu/,"At this moment there are more than 500,000 pieces of space debris hurtling around the Earth. Traveling at speeds of up to 40,000 kilometers per hour, a satellite collision with these fragments is enough to damage and even destroy our communications networks.Accion Systems is building the technology to prevent that. The company, which spun out of MIT in 2014, manufactures ion engines that enable satellites to maneuver in space and avoid these collisions.In an interview for Flux, I sat down with Natalya Bailey, the co-founder and CEO of Accion Systems. She reveals how the company is able to meet the needs of the burgeoning small satellite industry and why legacy manufacturers can’t keep up, how she’s handled the jump from academia to business and what she’s learned from Bill Swanson of Raytheon about managing a team.Bailey also discusses how the Apollo mission helped push computing forward, why space exploration is critical for our survival and how to get more women into STEM fields.An excerpt of the conversation is published below.AMLG: Today I’m excited to welcome Natalia Bailey, founder and CEO of Accion Systems. Accion was founded three years ago — they build liquid ion systems that power the electric propulsion in satellites. It’s a technology that could change the dynamics in space by enabling satellites to maneuver and reposition, extending their lifetime, which is pretty huge, and could be used for a lot of missions including (one day) interplanetary exploration. Welcome Natalya, great to have you here.NB: Thanks for having me on the show.AMLG: Let’s jump in. I’d love to hear how you came to found Accion — how did you get where you are now?NB: My path to founding Accion started with my interest in studying space propulsion and doing research in the field. That brought me to MIT for my PhD in aeronautics and astronautics.Actually prior to starting there, I was doing my masters at Duke and tried to start a rocket company, with different technology but similar ideas, and that company kind of imploded. I thought that would be my last go at entrepreneurship and startups, so I went to do my PhD and thought I would end up as a professor or at a research lab. And I started working on this new type of ion engine. We ended up getting a lot of interest from industry. The timing was phenomenal — as we were testing proof of concept of this technology the whole industry was getting more interested in smaller satellites. There was this huge technology gap as far as propulsion goes and we were working on exactly the thing that could fill that gap. We had some of the big aerospace companies, the Lockheeds and the Boeings of the world, coming to MIT and trying to license the technology or buy systems from MIT. My research advisor remembered that I had tried to start a company once before so he knew I had entrepreneurial leanings and suggested I try again.It was in 2012 that my labmate and I decided to spin out, so we formed a placeholder company to grab the IP. Then I defended my PhD and he dropped out, and we officially spun out in 2014 and hit the ground running.AMLG: Were you surprised by all the interest coming from these big companies — the Lockheeds, the Boeings? Did you have to ward them off? Was it tempting to license out all the core tech, or did you feel a huge relief that you knew you’d hit on the right opportunity?NB: Looking back it feels like a pretty clear sign of product market fit. I think it was Marc Andreessen or Ben Horowitz who said, “If the market really wants a technology it will pull it out of a company.” So even if the team is inexperienced and moving slowly and there are other challenges, if the market really wants something, that strong force can make it happen. We also still live under the fantastical notion that no one out there can necessarily manufacture these systems better than we can, because they’re non-traditional for aerospace. There aren’t established manufacturing plants or processes at a Lockheed that could make these technologies better than we can. So that was a neat position to be in.AMLG: It seems crazy that they don’t have this capability. Is it just too niche for them to allocate resources? What’s the advantage of being a startup doing this?NB: The technology itself is very different, and the manufacturing. We leverage MEMs fabrication techniques — the same manufacturing lines that are used to make Intel computer processors — we use those to make our thruster chips. A Lockheed, who wouldn’t traditionally be making computer processors, doesn’t have those capabilities set up.AMLG: So you found this specific niche that they can’t tackle. For listeners that don’t know what a propulsion system on a satellite does, can you explain why it’s so important?NB: The main application is for onboard propulsion systems, that’s what we’re working on. That’s different from launching from the surface of the Earth. We deal with when the satellite’s already in space. If you look at the progression of a mission, a satellite is launched into orbit on a big bulky rocket that’s not very precise, so first it will have to reposition itself to get where it actually intended to go.Then over the lifetime of the mission, there are always small disturbances pulling the satellite off of its intended track. It has to correct for those, like gravity and atmospheric drag and other perturbations.You also have to budget for collision avoidance. If NASA notifies you that you’re on track to collide with something, you’re responsible for moving out of the way. Then at the end of a lifetime of a satellite you’re also responsible for de-orbiting it. You can’t leave a satellite in orbit indefinitely — that’s called space junk and it’s a hot-button topic for us. So that’s one of the most typical missions, but as we start going beyond low-earth orbit, propulsion systems are also useful for transferring orbits, for reaching the moon and Mars and other interplanetary exploration missions.Can you talk about the concrete applications for the first set of satellites that will be using your technology?NB: Ultimately the two main applications are imaging or earth observation and communications. Those are the two main applications commercially that we’ll be addressing. Our initial batch of customers happen to be folks that can tolerate higher risk. We haven’t been launching propulsion systems into space for the past 10 years like some of the more established players would like for their suppliers. So our initial batch of customers are trying to do things like take a small satellite from low-earth orbit to lunar orbit, or trying to demonstrate other novel technology or mission components that haven’t been proven before. So we’re focusing on those higher-risk-tolerance customers right now.AMLG: You mentioned interplanetary and Mars — a bunch of people in the industry are complaining that SpaceX’s focus on Mars colonization has pulled attention away from regular rocket launches and supporting the ecosystem of near-earth satellites, and those near-earth satellites are probably most of your customer base. What are your thoughts on that?NB: Two thoughts on that. First, it’s analogous to Apollo in the sense that when you have an incredible, audacious goal that you set out to achieve, the spin-off technologies and other challenges you have to solve along the way are actually extremely beneficial in other areas, not just space. Looking at Apollo, you could even argue that some of the returns on that investment were computers in general, and everything else that came out of that program.AMLG: What did we get specifically out of Apollo in terms of computer advances?NB: The personal computer was just starting to be thought of around that time, and in a lot of the early Apollo missions everything was done by hand calculations, then later they started to input the punch hole cards into a computer on board. It was right at that transition point to computing, and a lot of money was being funneled into developing it for Apollo. So fans of Apollo say that computers came about because of that program. I think, in general, setting big hairy goals for an entire nation to achieve usually ends in a lot of cool spin-off technologies.The other thing about Mars colonization is, if you think about humankind as a species, if we’re around 300-400 years from now, it’s probably not because we stayed put on Earth. For our long-term preservation, it’s worth starting to explore other planets in the solar system, and hopefully eventually beyond that.AMLG: Where did this interest in exploring other planets come from. You grew up in Oregon right? What was that like and how did you get interested in space?NB: Growing up in Oregon was great. It was very outdoorsy, I don’t really remember spending any time indoors. I would spend nights outside on my trampoline. In Oregon there’s not a lot of light pollution so I could look at the stars. I would think about aliens and then notice these very solid specks of light moving across the sky and I realized they weren’t airplanes. So I tried to figure out what they could be and realized I was watching the space station. Mostly I wanted to study aliens but my family is quite practical so I decided to combine that with math, which I loved, and ended up in aerospace engineering. I haven’t looked back since.AMLG: Were you always interested in science? Were your parents interested in science?NB: Yeah. My dad was a biologist. I was always gathering bugs and studying them and keeping them in tanks in my bedroom until my parents found them. In school I started excelling in math and found that it came easily to me. That combined with the science and alien bit led me to engineering.AMLG: When you talk about aliens I think of one of my favorite books by Carl Sagan — Contact. I don’t know if you ever watched the movie or read the book, but I picture you like Ellie in that film. She’s this brilliant scientist and stumbles across something big.NB: I’ve definitely seen it. I’m currently making my way through Carl Sagan’s original Cosmos again.AMLG: I love the original Cosmos. I’m a huge Carl Sagan fan, I love his voice, he’s so inspiring to listen to. Talking about books, I know you’re an avid reader. Did any books in particular influence you or your path to building Accion?NB: Well I’m a gigantic Harry Potter fan and a lot of things around Accion are named after various aspects of Harry Potter, including the name Accion itself.AMLG: Is that the Accio spell? The beckoning spell?NB: Yes exactly. My co-founder and I were g-chatting late one night on a weekend and looking through a glossary of Harry Potter spells trying to name the company. Accio, the summoning spell, if you add an “N” to the end of it, it becomes a concatenation between “accelerate” and “ion,” which is what we do. That’s the official story of how we named the company, but really it was from the glossary of spells.[To continue reading, a full transcript of the conversation can be found on Medium.]",Accelerating the future of space technology
476,9,9_nasa_asteroid_space_dart,https://www.reuters.com/lifestyle/science/nasa-says-dart-mission-succeeded-altering-asteroids-trajectory-2022-10-11/,"Oct 11 (Reuters) - The spacecraft NASA deliberately crashed into an asteroid last month succeeded in nudging the rocky moonlet from its natural path into a faster orbit, marking the first time humanity has altered the motion of a celestial body, the U.S. space agency announced on Tuesday.The $330 million proof-of-concept mission, which was seven years in development, also represented the world's first test of a planetary defense system designed to prevent a potential doomsday meteorite collision with Earth.Findings of telescope observations unveiled at a NASA news briefing in Washington confirmed the suicide test flight of the DART spacecraft on Sept. 26 achieved its primary objective: changing the direction of an asteroid through sheer kinetic force.Astronomical measurements over the past two weeks showed the target asteroid was bumped slightly closer to the larger parent asteroid it orbits and that its orbital period was shortened by 32 minutes, NASA scientists said.""This is a watershed moment for planetary defense and a watershed moment for humanity,"" NASA chief Bill Nelson told reporters in announcing the results. ""It felt like a movie plot, but this was not Hollywood.""Last month's impact, 6.8 million miles (10.9 million km) from Earth, was monitored in real time from the mission operations center at the Johns Hopkins University Applied Physics Laboratory (APL) in Laurel, Maryland, where the spacecraft was designed and built for NASA.DART's celestial target was an egg-shaped asteroid named Dimorphos, roughly the size of a football stadium, that was orbiting a parent asteroid about five times bigger called Didymos once every 11 hours, 55 minutes.The test flight concluded with the DART impactor vehicle, no bigger than a refrigerator, slamming directly into Dimorphos at about 14,000 miles per hour (22,531 kph).Comparison of pre- and post-impact measurements of the Dimorphos-Didymos pair as one eclipses the other shows the orbital period was shortened to 11 hours, 23 minutes, with the smaller object bumped tens of meters closer to its parent.POSSIBLE WOBBLETom Statler, DART program scientist for NASA, said the collision also left Dimorphos ""wobbling a bit,"" but additional observations would be necessary to confirm that.[1/5] View of debris blasted from the surface of Dimorphos 285 hours after the asteroid was intentionally impacted by NASA's DART spacecraft on September 26, in this imagery from NASA's Hubble Space Telescope taken October 8, 2022. NASA/ESA/STScI/Hubble /Handout via REUTERS 1 2 3 4 5The outcome ""demonstrated we are capable of deflecting a potentially hazardous asteroid of this size,"" if it were discovered well enough in advance, said Lori Glaze, director of NASA's planetary science division. ""The key is early detection.""Neither of the two asteroids involved, nor DART itself, short for Double Asteroid Redirection Test, posed any actual threat to Earth, NASA scientists said.But Nancy Chabot, DART's coordination lead at APL, said Dimorphos ""is a size of asteroid that is a priority for planetary defense.""A Dimorphos-sized asteroid, while not capable of posing a planet-wide threat, could level a major city with a direct hit.Scientists had predicted the DART impact would shorten Dimorphos' orbital path by at least 10 minutes but would have considered a change as small as 73 seconds a success. So the actual change of more than a half hour, with a margin of uncertainty plus or minus two minutes, exceeded expectations.The relatively loose composition of rubble that Dimorphos appears to consist of may be a factor in how much the asteroid was budged by DART's blow.The impact blasted tons of rocky material from the asteroid's surface into space, visible in telescope images as a large debris plume, producing a recoil effect that added to the force exerted on Dimorphos from the collision itself, NASA said.Launched by a SpaceX rocket in November 2021, DART made most of its voyage under the guidance of flight directors on the ground, with control handed over to the craft's autonomous on-board navigation system in the final hours of the journey.Dimorphos and Didymos are both tiny compared with the cataclysmic Chicxulub asteroid that struck Earth some 66 million years ago, wiping out about three-quarters of the world's plant and animal species including the dinosaurs.Smaller asteroids are far more common and present a greater theoretical concern in the near term, making the Didymos pair suitable test subjects for their size, according to NASA scientists and planetary defense experts.Also, the two asteroids' relative proximity to Earth and dual configuration made them ideal for the DART mission.The Dimorphos moonlet is one of the smallest astronomical objects to receive a permanent name and is one of 27,500 known near-Earth asteroids of all sizes tracked by NASA. Although none are known to pose a foreseeable hazard to humankind, NASA estimates that many more asteroids remain undetected in the near-Earth vicinity.Reporting by Steve Gorman in Los Angeles; editing by Jonathan Oatis, Sandra Maler and Chris ReeseOur Standards: The Thomson Reuters Trust Principles.",Asteroid's path altered in NASA's first test of planetary defense system
499,9,9_nasa_asteroid_space_dart,https://www.supercluster.com/editorial/detection-of-venus-phosphine-survives-heavy-scrutiny,"While the debate and data still churn, the unexpected finding of phosphine offered enough intrigue and wonder to change the tide of opinions and interest in our sister planet. Venus has been considered the forgotten planet, as it has been 30 years since NASA has sent a space mission to Venus, but that is about to change. At least six spacecraft are scheduled to visit Venus in the next ten years, including two NASA missions that could launch as early as 2028: DAVINCI will explore Venus' atmosphere and VERITAS will use radar to map Venus’ surface. India’s Shukrayaan-1 is scheduled to launch as soon as 2024 and will include an instrument that will be able to detect phosphine. A private company, Rocket Lab is hoping to send their own spacecraft to hunt for the source of phosphine in Venus’ atmosphere.",Itâs been two years since researchers made a stunning announcement.
495,9,9_nasa_asteroid_space_dart,https://www.space.com/dart-asteroid-mission-on-track-for-impact,"NASA is just days away from slamming a spacecraft into an asteroid 7 million miles (11 million kilometers) from Earth.The agency's long-awaited Double Asteroid Redirection Test ( DART ) mission will impact with the asteroid moonlet Dimorphos on Monday (Sept. 26), if all goes according to plan. The DART mission launched on Nov. 23, 2021 on top of a SpaceX Falcon 9 rocket and is now hurtling through deep space toward the binary near-Earth asteroid (65803) Didymos and its moonlet Dimorphos.The mission, which is managed by the John Hopkins University Applied Physics Laboratory (JHUAPL), is humanity's first attempt to determine if we could alter the course of an asteroid, a feat that might one day be required to save human civilization. While changing the orbit of an asteroid 7 million miles away sounds daunting, DART team members from NASA and JHUAPL said during a media briefing on Thursday (Sept. 22) that they are confident that the years of planning that have gone into the mission will lead to success.Related: NASA's DART asteroid-impact mission will be a key test of planetary defenseTraveling at speeds of 4.1 miles per second (6.6 km/s), or 14,760 mph (23,760 kph), the DART spacecraft will impact the 560-foot-wide (170 meters) Dimorphos, a moonlet that orbits the other member of its binary system, the 2,600-foot-wide (780 m) asteroid Didymos.Doing so, NASA believes, will shift Dimorphos' orbital period enough to alter its gravitational effects on the larger Didymos, changing the trajectory of the pair.DART will crash into Dimorphos, causing a change to the moonlet's orbit. (Image credit: NASA/Johns Hopkins APL)Katherine Calvin, chief scientist and senior climate advisor at NASA, said that while DART will be a key test of this ""kinetic impactor"" planetary defense strategy, the mission will also produce valuable science that will allow astronomers to peer back into the deep history of the solar system.""We're looking at asteroids to make sure that we don't find ourselves in their path. We also study asteroids to learn more about the formation and history of our solar system. Every time we see an asteroid, we're catching a glimpse of a fossil of the early solar system,"" Calvin said.""These remnants capture a time when planets like Earth were forming,"" she added. ""Asteroids and other small bodies also delivered water, other ingredients of life to Earth as it was maturing. We're studying these to learn more about the history of our solar system.""Lindley Johnson, planetary defense officer at NASA, said that DART marks a turning point in the history of the human species.""This is an exciting time, not only for the agency, but for space history and the history of humankind,"" Johnson said during Thursday's briefing. ""It's quite frankly the first time that we are able to demonstrate that we have not only the knowledge of the hazards posed by these asteroids and comets that are left over from the formation of the solar system, but also have the technology that we could deflect one from a course inbound to impact the Earth. So this demonstration is extremely important to our future.""That sentiment was echoed by Tom Statler, a DART program scientist at NASA. ""The first test is a test of our ability to build an autonomously guided spacecraft that will actually achieve the kinetic impact on the asteroid. The second test is a test of how the actual asteroid responds to the kinetic impact,"" Statler said. ""Because, at the end of the day, the real question is: How effectively did we move the asteroid, and can this technique of kinetic impact be used in the future if we ever needed to?""Read more: DART asteroid mission: NASA's first planetary defense spacecraftThe asteroid Didymos and its moonlet Dimorphos are shown in a composite image taken by DART's DRACO instrument on July 27, 2022. (Image credit: NASA JPL DART Navigation Team)The outcome of the DART mission on Monday (Sept. 26) will certainly help answer that question, and many of the DART team members shared their confidence in the mission during the briefing. Edward Reynolds, DART project manager at JHUAPL, said the spacecraft is ready to smash itself to pieces on the surface of Dimorphos when the time comes.""What we can say at this point is that all subsystems on the spacecraft are green, they're healthy, they're performing very well. We have plenty of propellant and we have plenty of power,"" Reynolds said. ""We've been doing a bunch of rehearsals, and some of the rehearsals are very nominal.""""At this point, I can say that the team is ready,"" Reynolds added. ""The ground systems are ready, and the spacecraft is healthy and on track for an impact on Monday.""Engineers on the DART team are watching the spacecraft's trajectory carefully over the coming days leading up to the impact, which should occur at 7:14 p.m. EDT (2314 GMT) on Monday (Sept. 26). Elena Adams, DART mission systems engineer at JHUAPL, said that the team is still making sure the impactor spacecraft is on course.""Over the next couple of days, we're actually still performing some trajectory correction maneuvers to make sure that we are on the right path to hit the asteroid,"" Adams said. ""We rehearsed a lot. But as we go through the cruise phase, we update parameters in the spacecraft to make sure that we can actually hit the asteroid. And so in the last couple of days, we'll update those parameters; we'll do checks like streaming images back to Earth.""""So in the next few days, we'll take more images of the Didymos system, we'll do trajectory correction maneuvers, and then at 24 hours prior to impact, it's all hands on deck,"" she added.Adams said the team has 21 contingencies in place in case DART's Small-body Maneuvering Autonomous Real Time Navigation (Smart Nav) system determines that the spacecraft is off course. ""We've planned for all the things, and we're ready to intervene. And we have been rehearsing this for quite some time.""NASA's Double Asteroid Redirection Test, or DART, is moved into a shipping container for its trip to the Vandenberg Space Force Base in California for a launch on Nov. 24, 2021. (Image credit: NASA/Johns Hopkins APL/Ed Whitman)The 21st contingency the team has planned for is DART's survival. In the event that DART misses Dimorphos, Adams says the team will immediately begin processing the data the spacecraft collected and plan for a possible impact with other objects.""We're going to sit down back into our seats and we're going to start preserving all the data on board if it misses. And we'll have time with our Deep Space Network right afterwards to be able to actually get all that data down,"" Adams said. ""And then we'll start conserving propellant and we'll start looking for [other] objects to come back to.""In response to a question from Space.com concerning any flight testing the team has conducted, Adams mentioned a recent set of images the DART spacecraft's DRACO camera took of Jupiter and its four big Galilean moons . The DART team captured the images in order to ""fool"" the DART spacecraft's SMART Nav system so that its tracking capabilities could be tested.""We actually watched Europa exit from behind Jupiter. And we fooled our Smart Nav that Jupiter was Didymos and Europa was Dimorphos, and we actually watched the separation happen,"" Adams said.That's important, she added, ""because in the last four hours during our terminal phase, when the spacecraft is completely autonomous, we're going to watch Dimorphos emerge from behind Didymos. So, we already trained the system to do this in flight. So we're looking forward to it. I think we can do it.""Statler reiterated that confidence, adding that, while this type of mission was once the stuff of fantasy, the DART team believes we now have the tools and the knowledge to carry out a successful planetary defense mission.""We're moving an asteroid. We are changing the motion of a natural celestial body in space,"" Statler said. ""Humanity has never done that before. And this is the stuff of science fiction books, and really corny episodes of 'Star Trek' from when I was a kid. And now it's real. And that's kind of astonishing that we are actually doing that and what that bodes for the future: What we can do, as well as our discussions of what humanity should do.""It opens up an amazing frontier,"" he added. ""It's very exciting.""","DART asteroid-smashing mission 'on track for an impact' Monday, NASA says"
537,9,9_nasa_asteroid_space_dart,https://www.theverge.com/2022/8/30/23329998/nasa-artemis-rocket-launch-second-attempt-date-time,"Clear your plans on Saturday: NASA says we’re going to have a rocket launch.The space agency moved the date for the next Artemis I rocket launch attempt to Saturday, September 3rd, after determining that the initial plan for Friday was going to run into bad weather.There was a 60 percent chance that the launch would have been delayed for weather on Friday, officials said during a media briefing. The two-hour launch window opens at 2:17PM.This will be NASA’s second attempt this week at launching its massive next-generation rockets. The first attempted launch on Monday was scrubbed after one of the four RS-25 engines failed to reach the appropriate temperature to allow for liftoff.This will be NASA’s second attempt this week at launching its massive next-generation rocketsThe Artemis I mission is comprised of the 322-foot tall Space Launch System (SLS) rocket, with the Orion crew capsule at the top. If the launch is successful, the 39-day mission will see SLS carry the uncrewed Orion to an altitude of just under 4,000 kilometers before the two craft separate and the core stage of the rocket falls back to Earth.Orion will continue onward to the Moon, which it will orbit for six days before returning to Earth. The capsule is scheduled to splash down in the ocean on October 11th.",NASA moves next Artemis I rocket launch attempt to September 3rd
505,9,9_nasa_asteroid_space_dart,https://www.technologyreview.com/2022/10/21/1062001/spacex-starlink-signals-reverse-engineered-gps/,"In a non peer-reviewed paper published on arXiv, Humphreys claims to have provided the most complete characterization of Starlink’s signals to date. This information, he says, is the first step toward developing a new global navigation technology that would operate independently of GPS or its European, Russian, and Chinese equivalents.“The Starlink system signal is a closely guarded secret,” says Humphreys. “Even in our early discussions, when SpaceX was being more cooperative, they didn’t reveal any of the signal structure to us. We had to start from scratch, building basically a little radio telescope to eavesdrop on their signals.”To get the project started, UT Austin acquired a Starlink terminal and used it to stream high-definition tennis videos of Rafael Nadal from YouTube. This provided a constant source of Starlink signals that a separate nearby antenna could listen in on.Humphreys quickly realized that Starlink relies on a technology called orthogonal frequency-division multiplexing (OFDM). OFDM is an efficient method of encoding digital transmissions, originally developed at Bell Labs in the 1960s and now used in Wi-Fi and 5G. “OFDM is all the rage,” says Mark Psiaki, a GPS expert and aerospace professor at Virginia Tech. “It’s a way to pack the most bits per second into a given bandwidth.”The UT Austin researchers did not try to break Starlink’s encryption or access any user data coming down from satellites. Instead, they sought out synchronization sequences—predictable, repeating signals beamed down by the satellites in orbit to help receivers coordinate with them. Not only did Humphreys find such sequences, but “we were pleasantly surprised to find that they [had] more synchronization sequences than is strictly required,” he says.Each sequence also contains clues to the satellite’s distance and velocity. With the Starlink satellites transmitting about four sequences every millisecond “that’s just wonderful for dual use of their system for positioning,” says Humphreys.If the terrestrial receiver has a good idea of the satellites’ movements—which SpaceX shares online to reduce the risk of orbital collisions—it can use the sequences’ regularity to work out which satellite they came from, and then calculate the distance to that satellite. By repeating this process for multiple satellites, a receiver can locate itself to within about 30 meters, says Humphreys.",Starlink signals can be reverse-engineered to work like GPSâwhether SpaceX likes it or not
32,9,9_nasa_asteroid_space_dart,https://edition.cnn.com/2022/09/19/world/nasa-artemis-1-cryo-test-preview-scn/index.html,"Sign up for CNN’s Wonder Theory science newsletter. Explore the universe with news on fascinating discoveries, scientific advancements and more.CNN —The Artemis I mega moon rocket is gearing up for another test Wednesday before its next launch attempt to journey around the moon and back.The Artemis I cryogenic demonstration test began with fueling at 7:30 a.m. ET Wednesday.Since the second scrubbed launch attempt of the uncrewed Artemis I mission on September 3, engineers have replaced two seals on an interface for the liquid hydrogen fuel line between the rocket and mobile launcher, according to NASA officials. These seals were associated with a large hydrogen leak that led to the scrub of the launch attempt.Engineers found an indentation on the seal on an 8-inch (20-centimeter) quick disconnect line for hydrogen, said Mike Sarafin, Artemis mission manager, at a Monday NASA press conference.The team did not recover any piece of debris, but the dent was clear and pointed to a problem that contributed to the hydrogen leak, Sarafin said.The indentation on the seal was under 0.01 inch (0.3 millimeter), but it allowed pressurized gas to leak through, something that can be very dangerous given the flammability of hydrogen when it meets air. The team believes the dent was associated with the leak, but the results of the test could confirm it. They have since replaced the seal.On September 3, the large hydrogen leak was between two and three times the accepted limit, Sarafin said.Testing ‘kinder’ proceduresThe purpose of the cryogenic demonstration is to test the seals and use updated, “kinder and gentler” loading procedures of the supercold propellant, which is what the rocket would experience on launch day.Unlike the wet dress rehearsals, the previous tests of Artemis I that simulated every stage leading up to launch, the cryo test focuses on a very specific aspect in the countdown: loading supercold liquid oxygen and liquid hydrogen into the rocket’s core stage and upper stage.The Orion spacecraft and rocket boosters remained unpowered during the test, and the team does not intend to go into terminal count, or the final 10 minutes that occur in the countdown before launch, said Jeremy Parsons, deputy manager for NASA’s Exploration Ground Systems Program at Kennedy Space Center.The kinder and gentler loading procedure is to minimize pressure spikes and thermal spikes witnessed during prior launch attempts.“It’s going to be a very slow, steady ramp,” Parsons said. “So (we’re) really just trying to slowly introduce some of those thermal differences and reduce thermal and pressure shock.”Liquid oxygen is relatively dense, about the density of water, and it is pumped into the rocket. Meanwhile, hydrogen is very light, so it is moved using pressure rather than being pumped, said Tom Whitmeyer, deputy associate administrator for NASA’s common exploration systems development.The new loading operations will use a slower rate of pressure with more gradual temperature changes, Whitmeyer said.The call to stations for the test, when all of the teams associated with the mission arrive at their consoles and report they’re ready, begins today at 3 p.m. ET. The mission team anticipates receiving a “go” to begin loading the rocket with propellant around 7 a.m. ET on Wednesday. If all goes well, the team expects the test to be completed by 3 p.m. ET that day, Parsons said.The test will also include an engine bleed, which chills the engines for launch. The mission team scrubbed the first Artemis I launch attempt on August 29 largely due to an issue with a faulty sensor that occurred during this bleed.So far, the forecast looks promising for the test. The Artemis team is receiving daily briefings about Hurricane Fiona in case it has any impact on whether or not the rocket stack needs to be rolled back into the Vehicle Assembly Building, a process that can take three days.Preparing for launchIf the cryo test goes well, the next launch attempt could take place on Tuesday, September 27, with a 70-minute window that opens at 11:37 a.m. ET. The mission managers will meet to discuss the test results on September 25 to assess the potential launch date.If Artemis I launches on September 27, it would go on a 39-day mission and return to Earth on November 5. Another backup launch date is possible on October 2. While these launch dates are recommended by NASA, the team ultimately depends on a decision by the US Space Force, which would need to issue a waiver for the launch.The US Space Force, an arm of the military, still oversees all rocket launches from the United States’ East Coast, including NASA’s Florida launch site, and that area is known as the Eastern Range.The officials at the range are tasked with making sure there’s no risk to people or property with any launch attempt.The Artemis team continues to have “productive and collaborative” discussions with the Eastern Range, NASA officials said, and NASA is sharing additional detailed information requested by the Space Force for review.The team is taking things one step at a time and wants to get through the test before other decisions are made, Whitmeyer said.“We’re going to go when we’re ready,” Sarafin said. “But in terms of the reward of flying this flight, we have said from the outset that this is the first in an increasingly complex series of missions, and it is a purposeful stress test of the rocket.”The inaugural mission of the Artemis program will kick off a phase of NASA space exploration that intends to land diverse astronaut crews at previously unexplored regions of the moon – on the Artemis II and Artemis III missions, slated for 2024 and 2025, respectively – and eventually deliver crewed missions to Mars.",NASAâs Artemis I mega moon rocket prepares for prelaunch test
323,9,9_nasa_asteroid_space_dart,https://www.cnbc.com/2022/10/12/japanese-moon-company-ispace-launching-cargo-mission-in-november.html?utm_term=Autofeed&utm_medium=Social&utm_content=Main&utm_source=Twitter#Echobox=1665585387,"Japanese lunar exploration company ispace announced plans on Wednesday to launch its first cargo mission next month, racing to be the first of several private ventures to deliver payloads to the moon's surface.The private company aims to launch its ""Mission 1"" lunar lander during a window between Nov. 9 and Nov. 15, riding on one of SpaceX's Falcon 9 rockets from Cape Canaveral, Florida.Mission 1 will carry a variety of payloads for both companies and governments, including a pair of rovers. The company completed testing of its spacecraft in September, and is about to transport the lander to Florida.Alongside ispace in the burgeoning lunar cargo marketplace are the likes of U.S. companies Astrobotic and Intuitive Machines, both of which plan to launch missions to the moon's surface next year.Born out of Google's Lunar XPRIZE competition last decade, ispace aims to provide a wide variety of lunar-focused services, ranging from transportation of cargo to selling data to space agencies.It now has more than 200 employees across its offices in Japan, Luxembourg and the U.S. To date ispace has raised more than $200 million in funding.",Japanese lunar company ispace aims to launch first cargo mission to the moon next month
345,9,9_nasa_asteroid_space_dart,https://www.engadget.com/nasa-fixed-glitch-voyager-1-120545004.html,"Back in May, NASA reported that the Voyager 1 space probe was sending back jumbled or inaccurate telemetry data. The probe itself seemed to be in good shape, with a signal that's still strong enough to beam back information, and nothing was triggering its fault protection systems that would put it in ""safe mode."" According to NASA, the Voyager team has not only figured the problem out since then — it has also solved the issue.Turns out we're getting jumbled data here on Earth, because the probe's attitude articulation and control system (AACS) has been sending back information through an onboard computer that had stopped working years ago. The computer was corrupting the data before it even went out. Voyager project manager Suzanne Dodd said that when her team suspected that this was the issue, they implemented a low-risk fix: They commanded the AACS to send its data through the probe's working computer again.While the engineers have fixed the glitch, they've yet to figure out why the AACS started routing information through the old computer in the first place. They believe it was triggered by a faulty command by another onboard computer, which was itself triggered by an underlying issue with the spacecraft. Voyager's engineers will keep looking for the problem's root case, NASA said, but they don't think it will have a huge effect on the spacecraft's operations.Turn on browser notifications to receive breaking news alerts from Engadget You can disable notifications at any time in your settings menu. Not now Turned on Turn onVoyager 1 has been operational for almost 45 years and had reached interstellar space in 2012. NASA expects it to continue being able to run at least one science instrument until 2025, after which it will keep drifting away from our solar system until it loses contact with NASA's Deep Space Network.",NASA fixed the glitch that caused Voyager 1 to send back jumbled data
223,10,10_climate_ozone_emissions_world,https://techcrunch.com/2022/07/28/heres-how-manchin-and-schumers-surprise-725-page-bill-could-boost-climate-tech/,"Senator Joe Manchin, a Democrat from West Virginia, pulled a fast one yesterday. After months of opposing major climate legislation, citing inflation risks, he suddenly changed his tune. Last night, climate legislation was back on the table, and the odds of it passing had gone up considerably.While not as large, in terms of dollars, as President Joe Biden’s Build Back Better plan, the Inflation Reduction Act of 2022 still stands as one of the most significant pieces of climate legislation proposed in the U.S. It would put $369 billion toward clean energy, electric vehicles and a range of other climate programs.Senate Democrats say that the bill could cut emissions by 40% (most likely below 2005 levels, a benchmark the Biden administration has used in the past). That’s not enough to keep warming to 1.5 degrees Celsius, the globally agreed upon level that would limit most catastrophic outcomes, but it’s close. According to Climate Action Tracker, the bill would bring the U.S. from “insufficient” to “almost sufficient,” increasing the chances that the world can keep global average temperatures from rising more than 2 degrees Celsius.The bill, at 725 pages, goes into great detail outlining how the hundreds of billions of dollars should be spent. I’ve skimmed through the entire text, reading the most relevant pages and picking out the key points. If the bill is signed into law, here’s what will be driving climate policy in the U.S. for the foreseeable future.",Hereâs how Manchin and Schumerâs surprise 725-page bill could boost climate tech
264,10,10_climate_ozone_emissions_world,https://techcrunch.com/2022/10/10/fears-of-climate-tech-underinvestment-are-probably-overblown/,"There’s been a lot of hand-wringing over whether the world will get its act together enough to prevent catastrophic warming. There’s certainly a case to be made there — we’ve spent the last several decades kicking the can down the road at every opportunity.Well, here we are again, with the can again before us and the end of the road fast approaching.Lucky for me, I tend to be an optimist. I still think we’re in for a world of pain, and we’ll probably have to rely on some exotic technologies like fusion power and direct air capture to pull ourselves back from the brink. But in my opinion, when the chips are down, humanity tends to pull through.If we use computing and software as a guide, we should expect to see a nearly fivefold increase in the capital committed in the next 30 years.That is why I think many of the gloom-and-doom scenarios regarding climate tech investments tend to be overly bearish. Take the International Energy Agency’s (IEA) forecasts, which for years habitually underestimated the growth of solar power. The agency has since added better models to its toolkit, but it and others still make predictions that go on to be proven overly pessimistic.In reality, renewable energy and other climate tech is likely to follow an adoption curve that’s similar to other industries. It might even follow an accelerated version given how broad and deep the impacts and benefits of climate tech are likely to be — and the very real prospect of Armageddon if we do nothing.To see how climate tech stands to outperform today’s forecasts, you only have to look as far back as 1970, when the computing revolution was beginning.Exponential trendsThe overarching trend of investment in the computing and telecommunications space over 50 years has been exponential. But that simplistic analysis papers over the significant growth that happened in the early years. It also fails to pick up on key technological advances that sparked wider adoption.",Fears of climate tech underinvestment are probably overblown
573,10,10_climate_ozone_emissions_world,https://thehill.com/policy/energy-environment/3669504-supreme-court-to-hear-case-that-could-have-massive-impact-on-us-water-quality/,"The Supreme Court on Monday will hear arguments of a case between Idaho landowners and the Environmental Protection Agency (EPA), a dispute that could redefine the scope of the country’s clean water regulations.The first case of the justices’ new term, landing just ahead of the Clean Water Act’s 50th anniversary, will feature arguments about wetlands and when they can or cannot be regulated by the federal government.Although technical in nature, the legal dispute could have broad implications for the country’s water quality if the 6-3 conservative majority court uses the case to narrow the EPA’s regulatory reach.“If that’s what the Supreme Court should decide, we’re basically rolling back the clock 50 years,” said Rep. Peter DeFazio (D-Ore.), who chairs a House panel on water resources and the environment. “That would remove 50 percent of our critical wetlands, and 70 percent of our rivers and streams from federal protection.”The case began in 2007 when Michael and Chantell Sackett were told they needed a federal permit to build a home on land they owned because it contained wetlands, prompting the Sacketts to sue.A federal court, siding with the U.S. government, ruled that the wetlands on the Sacketts’ property contained a “significant nexus” with other regulated waters, meaning the couple would need authorization to build there.The Sacketts are now urging the Supreme Court to discard the “significant nexus” threshold. Instead, their petition favors a separate test from former Justice Antonin Scalia that called for the waters to have a “continuous surface water connection” — a higher threshold that would apply to fewer wetlands.The stakes of this case, however, go far beyond one property dispute. It attracted briefs from environmental groups, which argue that it would hamper the government’s ability to keep people safe from pollution, as well as industries like farming, mining, construction and oil and gas, which support the deregulatory effort.“This is a very, very, big deal for the Clean Water Act. It will determine, likely, whether the Clean Water Act can protect half of the water bodies in the country, and if it can’t, meeting the water quality goals of the law that we all count on will be virtually impossible,” said Jon Devine, who leads the Natural Resources Defense Council’s federal water policy team.The case appears to mirror regulatory differences between the Trump administration’s efforts to limit regulations to just wetlands with continuous surface water connections to other regulated waters, and the Obama administration’s regulations, which applied the significant nexus test.A total of 51 percent of the country’s wetlands would not be protected under the Trump-era rule,The Biden administrationAnd while the Sacketts’ petition to the court appears to be in support of the continuous surface water test,They say that a wetland should be “inseparably bound up” with another regulated water and also subject to Congress’s authority over interstate waters.Damien Schiff, a lawyer representing the Sacketts said that abiding by the Clean Water Act can be significantly burdensome, both in the application process itself and in the requirements to mitigate environmental damage.“Whenever the Clean Water Act applies, it does add a significant financial burden, not just because there is a lot of costs involved in the application process … but also just simply the cost of compensatory mitigation,” said Schiff, a senior attorney at the Pacific Legal Foundation.He said that the Army Corps of Engineers “might very well issue a permit, but typically not only is the permit issued for a much smaller project than was originally requested but it’s always accompanied by a pretty significant compensatory mitigation obligation and that can run into the hundreds of thousands if not millions of dollars.”A looser test would be expected to apply to fewer wetlands, allowing individuals and corporations to act there without EPA oversight.Under the current system, many polluters are also not necessarily blocked from carrying out activities in regulated waters.Instead, they may need to either apply for a permit that contains stipulations that they follow environmental safeguards, or follow existing stipulations in a “general permit” that gives a blanket waiver to certain activities.But environmentalists say that while pollution still occurs when permits are in place, the stipulations they offer are important for averting the worst damages.“This case is not about prohibiting construction or development, it’s about what safeguards are in place when someone does so,” Devine said.Environmentalists say that this pollution may end up in America’s drinking water and also harm fish that people catch for consumption. And while many public water systems are treated to prevent pollution, some people get their water from private wells, which may not get the same level of treatment.“Drinking water does have standards, but that doesn’t mean that all those standards are perfect and the pollution that comes in through those sources means more cost of treatment. It means that people who live on wells or in areas where the water treatment systems aren’t as big or fancy or as expensive are going to suffer,” said Sam Sankar, senior vice president of programs at Earthjustice.Sankar said his organization has 18 tribes as clients, and many of them will face “direct impacts.”The court began its work Monday after an epochal term in which the six Republican-appointed justices advanced an aggressive conservative legal agenda.The case will be the first that is heard by Ketanji Brown Jackson in her tenure as a Supreme Court justice.Although overshadowed by the court’s overruling of Roe v. Wade, last term saw the court vote 6-3 to pare back federal agency power in West Virginia v. EPA, a case thatCourt watchers believe the conservative majority court will continue its rightward trajectory this term.“There’s no reason to think this coming term, or any term in the foreseeable future, will be any different,” Irv Gornstein, executive director of Georgetown Law’s Supreme Court Institute said recently. “On things that matter most, get ready for a lot of 6-3s.”",Supreme Court to hear case that could have massive impact on US water quality
96,10,10_climate_ozone_emissions_world,https://news.sky.com/story/un-chief-antonio-guterres-tells-rich-countries-to-impose-tax-on-fossil-fuel-firms-feasting-on-windfall-profits-12701867,"The leader of the United Nations has urged all rich countries to impose a windfall tax on fossil fuel companies.The industry is ""feasting on hundreds of billions of dollars in subsidies and windfall profits while household budgets shrink and our planet burns"", Antonio Guterres told world leaders in New York.Record profits enjoyed by fossil fuel firms at a time of high energy costs and a cost of living crisis have prompted calls in many countries for leaders to impose a one-off tax on the extra income.Money raised should be used to help people struggling with rising food and energy bills, as well as to compensate countries suffering the most severe effects of climate change, the secretary-general told the United Nations General Assembly, which is expected to be dominated by discussions of Russia's invasion of Ukraine.Sparks fly over damage from climate changeIn spite of demanding ""polluters must pay"", Mr Guterres cannot mandate action from developed countries, many of which are grappling with extreme weather, high food and energy prices and the Ukraine war.But Antony Froggatt, from international affairs think tank Chatham House, said the statement ""is an important signal"" and highlights the ""unequal nature of the current crisis, with some countries, companies and citizens benefiting hugely"".But Mr Guterres has previously urged an end to funding for more oil and gas exploration and production, ""which has not stopped these taking place"", Mr Froggatt added.The European Union plans to raise about €140bn (£121bn) by imposing windfall taxes on energy companies' ""abnormally high profits"", a move that could put pressure on Prime Minister Liz Truss to do more in the ""mini budget"" on Friday.AdvertisementMs Truss decided against extending the UK's windfall tax for North Sea extractors - set at 25% but which can largely be avoided if companies reinvest the money in new extraction. Instead she has pledged to freeze energy bills at an average of £2,500 a year, which will be paid for by borrowing.The UN chief told world leaders that nations are ""gridlocked in colossal global dysfunction"" and are not ready or willing to tackle the major challenges that threaten the future of humanity and the fate of the planet.He warned of ""cascading"" climate, energy and cost of living crises that are ""feeding on each other, compounding inequalities, creating devastating hardship, delaying the energy transition and threatening global financial meltdown"".Please use Chrome browser for a more accessible video player 3:03 'I am against a windfall tax'""Social unrest is inevitable - with conflict not far behind,"" he said.Mr Froggatt said that ""adapting to and preparing for further environment and resource shocks will be a, if not the challenge"", in future.Mr Guterres also said that it was time to hold to account fossil fuel companies ""and their enablers"", including financial and PR firms that are ""raking in billions to shield the fossil fuel industry from scrutiny"".""Fossil fuel interests need to spend less time averting a PR disaster and more time averting a planetary one,"" Mr Guterres said.Scientists agree that pollution must be reduced by 45% by 2030 to have any hope of reaching net zero by 2050, and stave off the worst of climate breakdown. But emissions, which mostly come from burning oil, gas and coal, are on course to rise by 16% by 2030, compared with 2010 levels.Ipek Gençsü, from global affairs think tank ODI, said windfall taxes are unpopular with fossil fuel companies as well as from some economic perspectives, which argue they disincentivise private investment by ""changing the rules of the game"".But we cannot have conversations about windfall taxes ""without understanding the bigger picture, and the fact that fossil fuel companies and fossil fuel emissions are the biggest contributor to the climate crisis,"" she said.Watch the Daily Climate Show at 3.30pm Monday to Friday, and The Climate Show with Tom Heap on Saturday and Sunday at 3.30pm and 7.30pm.All on Sky News, on the Sky News website and app, on YouTube and Twitter.The show investigates how global warming is changing our landscape and highlights solutions to the crisis.",UN chief Antonio Guterres urges rich countries to impose tax on fossil fuel firms 'feasting' on windfall profits
430,10,10_climate_ozone_emissions_world,https://www.npr.org/2022/10/14/1128858953/animal-populations-are-shrinking-drastically,"Animal populations shrank an average of 69% over the last half-century, a report saysEnlarge this image toggle caption Michael Dantas/AFP via Getty Images Michael Dantas/AFP via Getty ImagesGlobal animal populations are declining, and we've got limited time to try to fix it.That's the upshot of a new report from the World Wildlife Fund and the Zoological Society of London, which analyzed years of data on thousands of wildlife populations across the world and found a downward trend in the Earth's biodiversity.According to the Living Planet Index, a metric that's been in existence for five decades, animal populations across the world shrunk by an average of 69% between 1970 and 2018.Not all animal populations dwindled, and some parts of the world saw more drastic changes than others. But experts say the steep loss of biodiversity is a stark and worrying sign of what's to come for the natural world.""The message is clear and the lights are flashing red,"" said WWF International Director General Marco Lambertini.According to the report's authors, the main cause of biodiversity loss is land-use changes driven by human activity, such as infrastructure development, energy production and deforestation.Climate change may become the leading cause of biodiversity lossBut the report suggests that climate change — which is already unleashing wide-ranging effects on plant and animal species globally — could become the leading cause of biodiversity loss if rising temperatures aren't limited to 1.5°C.Lambertini said the intertwined crises of biodiversity loss and climate change are already responsible for a raft of problems for humans, including death and displacement from extreme weather, a lack of access to food and water and a spike in the spread of zoonotic diseases.He said world leaders gathering at the U.N. Biodiversity Conference in Montreal in December should take major steps to reverse environmental damage.""This is the last chance we will get. By the end of this decade we will know whether this plan was enough or not; the fight for people and nature will have been won or lost,"" Lambertini said. ""The signs are not good. Discussions so far are locked in old-world thinking and entrenched positions, with no sign of the bold action needed to achieve a nature-positive future.""But the dire news comes with signs of hope: Though there is no panacea, experts say there are feasible solutions to the loss of biodiversity.Solutions range from the conservation of mangroves to a cross-border barter system in Africa to the removal of migration barriers for freshwater fish, the report said.Human habits have to changeWWF chief scientist Rebecca Shaw told NPR that humans have the opportunity to change how they do things to benefit nature.""We don't have to continue the patterns of development the way we have now. Food production, unsustainable diets and food waste are really driving that habitat destruction. And we have an opportunity to change the way we produce, the — what we eat and how we consume food and what we waste when we consume our food,"" Shaw said. ""Little things that we can do every day can change the direction of these population declines.""The report calculated the average change in the ""relative abundance"" of 31,821 wildlife populations representing 5,230 species.Latin America and the Caribbean saw a whopping 94% average population loss and Africa saw a 66% decline, while North America experienced only a 20% drop and Europe and central Asia saw its wildlife populations diminish by 18%.The WWF said the disparity could be due to the fact that much of the development in North America and Europe occurred before 1970, when the data on biodiversity loss started.","Animal populations shrank an average of 69% over the last half-century, a report says"
243,10,10_climate_ozone_emissions_world,https://techcrunch.com/2022/08/16/winners-losers-abound-as-inflation-reduction-act-becomes-law/,"President Joe Biden signed the climate-and-energy-focused Inflation Reduction Act into law today, a turn of events that just a few months ago seemed impossible. The move will undoubtedly bolster the United States’ stance in the next round of climate negotiations. And by the end of the decade, it’s expected that the law will reduce the country’s emissions by 40% below 2005 levels. That’s enough to put the country within spitting distance of reductions that could limit warming to 1.5 degrees Celsius.As with any legislation, there are winners and losers. In the new law, climate tech is undoubtedly a winner, with provisions that will bolster renewable power, net-zero buildings and zero-emissions transportation.But the details matter, and some sectors got a better deal than others. Here’s a rundown of which companies are likely to benefit and which didn’t get what they expected.The winnersAt or near the top of the list are renewable-energy developers. Before the Inflation Reduction Act passed, tax credits on solar and wind were going to expire at the end of 2024. Now, they’re a little sweeter and are extended through 2032. For developers like Terabase, which recently raised a $44 million round led by Breakthrough Energy Ventures, and Arcadia, which recently closed a $200 million Series E, that’ll be a boon.","Winners, losers abound as Inflation Reduction Act becomes law"
240,10,10_climate_ozone_emissions_world,https://techcrunch.com/2022/08/15/oil-and-gas-didnt-benefit-from-investor-largesse-in-recent-years-but-renewables-did/,"Oil and gas didn’t benefit from investor largesse in recent years — but renewables didWith the climate-and-energy-focused Inflation Reduction Act expected to be signed by President Joe Biden this week, The Wall Street Journal asked Dealogic to analyze the amount of money being loaned to “green” companies and to oil and gas companies. Investors, WSJ concludes, aren’t ready to give up on fossil fuels.But the data suggests that they’re starting to pull back already.Fossil fuel financing has been more or less steady since 2015, when the WSJ/Dealogic data series begins. For oil and gas companies, that should be a worrying trend given overall low rates and the amount of money that’s been sloshing around the market the past few years.Investment-grade bond issuance surged in 2020 before dropping to still-elevated levels in 2021. Yet fossil fuel investment didn’t follow the trend, dipping slightly instead of rising along with the market.Bonds and loans for renewable projects and companies did the opposite, ticking steadily upward from 2015 on. In 2021, they more than doubled the previous year, matching the amount invested in fossil fuels for the first time.This year, renewable companies remain neck-and-neck with oil and gas companies.",Oil and gas didnât benefit from investor largesse in recent years â but renewables did
557,10,10_climate_ozone_emissions_world,https://www.vox.com/future-perfect/23362175/un-human-development-report-ord-existential-security,"Since 1990, the United Nations Development Programme has been tasked with releasing reports every few years on the state of the world. The 2021/2022 report — released earlier this month, and the first one since the Covid-19 pandemic began — is titled “Uncertain Times, Unsettled Lives.” And unsurprisingly, it makes for stressful reading.“The war in Ukraine reverberates throughout the world,” the report opens, “causing immense human suffering, including a cost-of-living crisis. Climate and ecological disasters threaten the world daily. It is seductively easy to discount crises as one-offs, natural to hope for a return to normal. But dousing the latest fire or booting the latest demagogue will be an unwinnable game of whack-a-mole unless we come to terms with the fact that the world is fundamentally changing. There is no going back.”Those words ring true. Only a few years ago, we lived in a world where experts had long warned that a pandemic was coming and it could be devastating — now, we live in a world that a pandemic has clearly devastated. Only a year ago, there hadn’t been a large land war in Europe since World War II, and some experts optimistically assumed that two countries with McDonald’s in them would never go to war.Now, not only is Russia occupying stretches of Ukraine, but the destruction of Russia’s army in the fighting there has kicked off other regional instability, most notably with Azerbaijan attacking Armenia earlier this month. Fears of the use of nuclear weapons in wartime, quiet since the Cold War, are back as people worry about whether Putin could turn to tactical nukes if faced with a total defeat in Ukraine.Help inform the future of Vox We want to get to know you better — and learn what your needs are. Take Vox’s survey here.Of course, all of those situations are possible — even likely — to resolve without catastrophe. The worst rarely happens. But it’s hard to avoid a feeling that we’re just rolling the dice, hoping that we somehow won’t eventually hit on an unlucky number. Every pandemic, every minor war between nuclear-armed powers, every new and uncontrolled technology, may pose only a small chance of escalating to a catastrophic-scale event. But if we take that risk every year without taking precautions, humanity’s lifespan may be limited.Why “existential security” is the opposite of “existential risk”Toby Ord, senior research fellow at Oxford’s Future of Humanity Institute and the author of the existential risk book The Precipice: Existential Risk and the Future of Humanity, explores this question in an essay in the latest UNDP report. He calls it the problem of “existential security”: the challenge not just of preventing each individual prospective catastrophe, but of building a world that stops rolling the dice on possible extinction.“To survive,” he writes in the report, “we need to achieve two things. We must first bring the current level of existential risk down — putting out the fires we already face from the threats of nuclear war and climate change. But we cannot always be fighting fires. A defining feature of existential risk is that there are no second chances — a single existential catastrophe would be our permanent undoing. So we must also create the equivalent of fire brigades and fire safety codes — making institutional changes to ensure that existential risk (including that from new technologies and developments) stays low forever.”He illustrates the point with this fairly terrifying graph:The idea is this: Say we go through a situation where a dictator threatens to use nuclear war, or where tensions between two nuclear powers seem to be hitting the breaking point. Maybe most of the time the situation is defused, as indeed was the case during the many, many Cold War close calls. But if this situation recurs every few decades, then the probability we’ll defuse every single prospective nuclear war will get steadily lower. The odds that humanity will still be around in 200 years eventually become quite low, just as the odds that you can keep winning at craps drop with every roll.“Existential security” is the state where we are mostly not facing risks in any given year, or decade, or ideally even century, that have a substantial chance of annihilating civilization. For existential security from nuclear risk, for instance, perhaps we reduce nuclear arsenals to the point where even a full nuclear exchange would not pose a risk of collapsing civilization, something the world made significant progress on as countries slashed nuclear arsenal levels after the Cold War. For existential security from pandemics, we could develop PPE that is comfortable to wear and provides approximately total protection against disease, plus a worldwide system to detect diseases early — ensuring that any catastrophic pandemic would be possible to nip in the bud and protect people from.The ideal, though, would be existential security from everything — not just from the knowns, but the unknowns. For example, one big worry among experts including Ord is that once we build highly capable artificial intelligences, AI will dramatically hasten the development of new technologies that imperil the world while — because of how modern AI systems are designed — it’ll be incredibly difficult to tell what it’s doing or why.So an ideal approach to managing existential risk doesn’t just fight today’s threats but makes policies that will prevent threats from arising in the future too.That sounds great. As longtermists have argued recently, existential risks pose a particularly devastating threat because they could destroy not just the present, but a future where hundreds of billions more people could one day live. But how do we bring it about?Ord proposes “an institution aimed at existential security.” He points out that preventing the end of the world is exactly the sort of thing that’s supposed to be within the purview of the United Nations — after all, “the risks that could destroy us transcend national boundaries,” he writes. The problem, Ord observes, is that to prevent existential risk, an institution would have to have broad ability to intervene in the world. No country wants any other country to be allowed to pursue an incredibly dangerous research program, but at the same time, no country wants to give other countries purview over their own research programs. Only a supranational authority — something like the International Atomic Energy Agency, but with a far broader remit — could potentially overcome those more narrow national concerns.Often, the hard part in securing humanity’s future isn’t figuring out what needs to be done but actually doing it. With climate change, the problem and the risks were well understood for a long time before the world took action to shift away from greenhouse gases. Experts warned about the risks of pandemics before Covid-19 struck, but they largely weren’t listened to — and institutions that the US thought were ready, like the CDC, turned out to fall on their face during a real crisis. Today, there are expert warnings about artificial intelligence, but other experts assure us there’ll be no problem and we don’t need to try to solve it.Writing reports only helps if people read them; building an international institute for existential security only works if there’s a way to transform the study of existential risks into serious, coordinated action to make sure we don’t face them. “There is not sufficient buy-in at the moment,” Ord acknowledges, but “this may change over years or decades as people slowly face up to the gravity of the threats facing humanity.”Ord doesn’t speculate on what might bring that change about, but personally, I’m pessimistic. Anything that changed the international order enough to support international institutions with real authority with respect to existential risk would likely have to be a devastating catastrophe in its own right. It seems unlikely we’ll make it to the path of “existential security” without taking some serious risks — which hopefully we survive to learn from.",How to stop rolling the dice on the destruction of human civilization
508,10,10_climate_ozone_emissions_world,https://www.theatlantic.com/science/archive/2022/10/inflation-reduction-act-climate-economy/671659/?utm_source=feed,"Sign up for The Weekly Planet, Robinson Meyer’s newsletter about living through climate change, here.Late last month, analysts at the investment bank Credit Suisse published a research note about America’s new climate law that went nearly unnoticed. The Inflation Reduction Act, the bank argued, is even more important than has been recognized so far: The IRA will “will have a profound effect across industries in the next decade and beyond” and could ultimately shape the direction of the American economy, the bank said. The report shows how even after the bonanza of climate-bill coverage earlier this year, we’re still only beginning to understand how the law works and what it might mean for the economy.The report made a few broad points in particular that are worth attending to: First, the IRA might spend twice as much as Congress thinks. Many of the IRA’s most important provisions, such as its incentives for electric vehicles and zero-carbon electricity, are “uncapped” tax credits. That means that as long as you meet their terms, the government will award them: There’s no budget or limit written into the law that restricts how much the government can spend. The widely cited figure for how much the IRA will spend to fight climate change—$374 billion—is in large part determined by the Congressional Budget Office’s estimate of how much those tax credits will get used.But that estimate is wrong, the bank claims. In fact, so many people and businesses will use those tax credits that the IRA’s total spending is likely to be more than $800 billion, double what the CBO projects. And because federal spending tends to catalyze private investment, that could send total climate spending across the economy to roughly $1.7 trillion over the next 10 years. That’s significantly more money flowing into green-energy industries than the CBO projected, though it’s unclear if that additional money will lead to more carbon reductions than earlier analyses have projected.Second, the U.S. is “poised to become the world’s leading energy provider,” according to the bank. America is already the world’s largest producer of oil and natural gas. The IRA could further enhance its advantage in all forms of energy production, giving it a “competitive advantage in low-cost clean electricity and hydrogen production, infrastructure, geologic storage, and human capital,” the report states. By 2029, U.S. solar and wind could be the cheapest in the world at less than $5 per megawatt-hour, the bank projects; it will also become competitive in hydrogen, carbon capture and storage, and wind turbines. (The law will help America’s battery industry, but the bank doesn’t see the U.S. becoming the world’s biggest battery producer, given that China already has such a dominant advantage.)Perhaps rosiest of all was the bank’s view of major risks to the IRA. The bill passed with not even a single Republican vote, but the bank concludes that the GOP is relatively unlikely to repeal the law, even if they take the White House in 2024. That’s because it would hurt their own voters most: “Republican-leaning states are likely to see the most investment, job, and economic benefits from the IRA,” the report claims. Instead, the IRA is most likely to stumble because America still struggles with building out its energy infrastructure: The country might not be able to get government approval to permit enough power lines, green infrastructure, and carbon-injection wells for the law to matter, the bank said. This risk is all the more heightened now that Senator Joe Manchin’s permitting-reform bill—which, for all its flaws, would have clearly allowed for more renewable transmission construction—has failed. Powerful business groups are also lobbying to revise the most transmission-friendly sections from that bill if Congress revisits it.The Credit Suisse report is truly remarkable. What stuck with me most was this declaration: For big corporations, the IRA “definitively changes the narrative from risk mitigation to opportunity capture.” In other words, companies should no longer worry that they might be unprepared for future climate regulation, such as a carbon tax. They should be scared of missing out on the economic growth that the energy transition (and the IRA) will bring about.If the bill’s passage wasn’t signal enough, the report shows that climate change as a political issue—and frankly environmental protection more broadly—has arrived to a wholly new place. For decades, the country’s biggest climate advocates have tried to reduce the harm that the economy causes to the environment. Now they find themselves tasked with the biggest story in the economy itself.Perhaps most strange, even if the United States slips into recession in the next year, the IRA will only become more important. Historically, economists and businesses have treated helping the environment as a product of prosperity—if the economy is good, then companies can afford to do the right thing. But the IRA’s programs and incentives will keep flowing no matter the macro environment, which makes betting on clean energy one of the most certain economic trends of the next few years. Clean energy is now the safe, smart, government-backed bet for conservative investors. It’s really a shocking reversal of the past 40 years. It is such a change that it hasn’t yet been metabolized by the world of people involved in the issue.So inspired by the vigor of Credit Suisse’s forecast, let me venture a few predictions of my own. The number of Americans working in a climate-relevant industry is going to explode. It is going to undergo what you might call a techification. I was a nerd and a dreamer in high school in the late aughts, which meant I paid attention to the start-ups of that era—such as Twitter, Facebook, and Flickr—in their early years. I remember that fateful moment around 2010 when the valence of the industry switched—it was right around when The Social Network came out—and working in tech went from being a career choice for dorky optimists to the default career track for many ambitious college students. A similar switch is coming for companies working on climate change: The opportunity will be too large, the money too persuasive, the problems too intriguing.Finally, those of us who have long worked in climate change—and here I include myself, who started covering this topic in 2015—should have some excitement and even humility about this deluge of new talent. Even setting its arduous politics aside, managing climate change is a legitimately difficult technical and cultural problem—it’s going to require as many attentive and enthusiastic brains as possible, and the path to decarbonizing always required an infusion of new workers, investment, and good will. If you don’t yet work in the industry, but have always cared about climate change as an issue, well, this is your moment to get involved. These companies are going to need engineers, yes, but also programmers, accountants, marketers, HR staff, general counsels—there is space for everyone now.The fight against climate change is going to change more in the next four years than it has in the past 40. The great story of our lives is just beginning. Welcome aboard.",The Climate Economy Is About to Explode
515,10,10_climate_ozone_emissions_world,https://www.theguardian.com/environment/2022/oct/27/world-close-to-irreversible-climate-breakdown-warn-major-studies,"The climate crisis has reached a “really bleak moment”, one of the world’s leading climate scientists has said, after a slew of major reports laid bare how close the planet is to catastrophe.Collective action is needed by the world’s nations more now than at any point since the second world war to avoid climate tipping points, Prof Johan Rockström said, but geopolitical tensions are at a high.He said the world was coming “very, very close to irreversible changes … time is really running out very, very fast”.Emissions must fall by about half by 2030 to meet the internationally agreed target of 1.5C of heating but are still rising, the reports showed – at a time when oil giants are making astronomical amounts of money.On Thursday, Shell and TotalEnergies both doubled their quarterly profits to about $10bn. Oil and gas giants have enjoyed soaring profits as post-Covid demand jumps and after Russia’s invasion of Ukraine. The sector is expected to amass $4tn in 2022, strengthening calls for heavy windfall taxes to address the cost of living crisis and fund the clean energy transition.All three of the key UN agencies have produced damning reports in the last two days. The UN environment agency’s report found there was “no credible pathway to 1.5C in place” and that “woefully inadequate” progress on cutting carbon emissions means the only way to limit the worst impacts of the climate crisis is a “rapid transformation of societies”.Current pledges for action by 2030, even if delivered in full, would mean a rise in global heating of about 2.5C, a level that would condemn the world to catastrophic climate breakdown, according to the UN’s climate agency. Only a handful of countries have ramped up their plans in the last year, despite having promised to do so at the Cop26 UN climate summit in Glasgow last November.The UN’s meteorological agency reported that all the main heating gases hit record highs in 2021, with an alarming surge in emissions of methane, a potent greenhouse gas.Separately, the IEA’s world energy report offered a glimmer of progress, that CO2 from fossil fuels could peak by 2025 as high energy prices push nations towards clean energy, though it warned that it would not be enough to avoid severe climate impacts.Rockström, director of the Potsdam Institute for Climate Impact Research in Germany, said: “It’s a really bleak moment, not only because of the reports showing that emissions are still rising, so we’re not delivering on either the Paris or Glasgow climate agreements, but we also have so much scientific evidence that we are very, very close to irreversible changes – we’re coming closer to tipping points.”Research by Rockström and colleagues, published in September, found five dangerous climate tipping points may already have been passed due to the global heating caused by humanity to date, including the collapse of Greenland’s ice cap, with another five possible with 1.5C of heating.“Furthermore, the world is unfortunately in a geopolitically unstable state,” said Rockström. “So when we need collective action at the global level, probably more than ever since the second world war, to keep the planet stable, we have an all-time low in terms of our ability to collectively act together.”“Time is really running out very, very fast,” he said. “I must say, in my professional life as a climate scientist, this is a low point. The window for 1.5C is shutting as I speak, so it’s really tough.”His remarks came after the UN secretary general, António Guterres, said on Wednesday that climate action was “falling pitifully short”. “We are headed for a global catastrophe [and] for economy-destroying levels of global heating.”He added: “Droughts, floods, storms and wildfires are devastating lives and livelihoods across the globe [and] getting worse by the day. We need climate action on all fronts and we need it now.” He said the G20 nations, responsible for 80% of emissions, must lead the way.Inger Andersen, head of the UN environment programme (UNEP), told the Guardian that the energy crisis must be used to speed up delivery of a low-carbon economy: “We are in danger of missing the opportunity and a crisis is a terrible thing to waste.”Prof Corinne Le Quéré, at the University of East Anglia, UK, said: “It is fundamental to avoid cascading risks that responses to existing crises are made in a way that limits climate change to the lowest possible level.”Further reports published in the last two days said the health of the world’s people is at the mercy of a global addiction to fossil fuels, with increasing heat deaths, hunger and infectious disease as the climate crisis intensifies.Sign up to Down to Earth Free weekly newsletter The planet's most important stories. Get all the week's environment news - the good, the bad and the essential Privacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy . We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply.In the UK, the government was accused of a “severe dereliction of duty” in leaving critical UK infrastructure at risk to climate impacts. The new prime minister, Rishi Sunak, will not attend COP27, his spokesman said on Wednesday.High gas and oil prices delivered huge profits to Shell and TotalEnergies on Wednesday, which booked $9.5bn and $9.9bn respectively. Shell said it would not pay any UK-imposed windfall tax this year as the profits were being offset against investment in North Sea fields.The fossil fuel industry as a whole amassed $4tn in 2022, according to another new report from International Energy Agency (IEA), a sum that could otherwise transform climate action.The IEA report said: “Net income for the world’s oil and gas producers is set to double in 2022 to an unprecedented $4tn, a huge $2tn windfall.” The oil and gas sector has gained an average of $1tn a year in unearned profits for the last 50 years.The IEA said clean energy investment would have to be at least $4tn a year by 2030 to hit net zero emissions by mid-century. “If the global oil and gas industry were to invest this [$2tn] additional income in low‐emissions fuels, such as hydrogen and biofuels, it would fund all of the investment needed in these fuels for the remainder of this decade.”Prof Myles Allen, at the University of Oxford, said: “The combined profits, taxes and royalties generated by the oil and gas industry over the past few months would be enough to capture every single molecule of CO2 produced by their activities and reinject it back underground. So why are we only talking about transforming society and not about obliging a highly profitable industry to clean up the mess caused by the products it sells?”“The situation is serious and bleak,” said Prof Simon Lewis, at University College London. “Shell has made £26bn profit this year, carbon emissions are back at pre-pandemic levels, while 53,000 people died of heat stress in Europe in the summer, and floods have displaced millions from Nigeria to Pakistan. The solution is to do everything we can to defeat the fossil fuel industry – they stand between us all and a prosperous future.”Rockström was pessimistic about any breakthrough in the speed of climate action at the Cop27 climate summit, which he said would be dominated by nations such as Pakistan demanding funding to rebuild their countries after climate disasters. Rich, high-emitting nations have long rejected such claims, fearing unlimited liabilities.“This is a necessary discussion,” Rockström said. “But it leads to a deeper rift between the global north and the south. And that’s exactly what we do not need now.”'Like something you watch in a movie': climate crisis intensifies with catastrophic floods – videoBut he said progress could be made within a few years: “The Ukraine war is the nail in the coffin for the fossil-fuel-driven advanced economies. In the short term, it costs us a lot and we lose speed on climate action.” But in the longer term, he said, the energy and food crises add national security to the planetary and health reasons for climate action.Prof Michael Mann, at the University of Pennsylvania in the US, said it was important to note that progress was being made: “More work clearly needs to be done if warming is to be kept below 1.5C, but nobody foresaw the major policy progress in recent months in both Australia and the US. It is estimated that the US legislation will lower national emissions by 40% this decade. With US leadership, we can expect other major emitters to now come to the table at Cop27.”Climate experts agree that every action that limits global heating reduces the suffering endured by people from climate impacts. “The 1.5C target is now near impossible, but every fraction of a degree will equate to massive avoided damages for generations to come,” said Prof Dave Reay, at the University of Edinburgh, UK.Röckstrom said: “Despite the fact that the situation is depressing and very challenging, I would strongly advise everyone to act in business or policy or society or science. The deeper we fall into the dark abyss of risk, the more we have to make efforts to climb out of that hole. It’s not as if we don’t know what to do – it’s rather that we’re not doing what is necessary.”","World close to âirreversibleâ climate breakdown, warn major studies"
232,10,10_climate_ozone_emissions_world,https://techcrunch.com/2022/08/08/that-big-climate-bill-might-actually-make-a-difference/,"With the Senate passing the Inflation Reduction Act of 2022 last night, and House passage later this week all but assured, it’s likely that the U.S. will be taking significant — though not comprehensive — congressional action on climate change.The bill is expected to trim U.S. carbon emissions to 40% below 2005 levels by the end of the decade. That’s short of President Joe Biden’s target of 50%, and not quite enough to help put the world on the preferred path of warming no more than 1.5 degrees Celsius. But it’s still a major step, one that could restore confidence in global climate agreements.It’ll also give a big boost to climate tech, a sector that’s been red hot and seemingly immune to cooling sentiment.The new version, which passed after negotiations with Senator Kyrsten Sinema, a Democrat from Arizona, has a few changes. The corporate minimum tax reportedly has been tweaked to be more lenient on manufacturers, and the changes to tax on carried interest are out, though it’s not clear whether investors were all that concerned with them anyway. It’s been replaced with a 1% excise tax on stock buybacks that goes into effect next year. Sinema also successfully lobbied for $4 billion for Western states to fight the megadrought they’re currently experiencing.The rest of the massive bill, which we’ve covered in detail, remains largely the same. That means enticements to get people to buy EVs and heat pumps; carrots for companies to set up domestic supply chains for batteries, solar panels and wind turbines; and $20 billion to help agriculture overhaul itself with an eye toward trimming emissions.But will the bill be enough? Among realists, there’s largely agreement that the Inflation Reduction Act is better than nothing. It may not be perfect, but there’s still time to improve on it, right?",That big climate bill might actually make a difference
324,10,10_climate_ozone_emissions_world,https://www.cnbc.com/2022/10/12/new-zealand-plans-to-tax-emissions-from-livestock-burps-and-dung.html,"Cattle photographed in New Zealand. Agriculture plays a major role in New Zealand's economy, especially when it comes to exports. David Clapp | Stone | Getty ImagesNew Zealand plans to tax agricultural emissions — including those related to the burps, urine and dung from livestock like cows and sheep — in a move its government hopes will help the country meet climate change goals. The aim is for the ""agricultural emissions-pricing system"" to come into force in 2025. A consultation looking at how levies are set, transition assistance and sequestration — which the document defines as ""the process of removing carbon dioxide from the atmosphere"" — was launched this week, and will run until Nov. 18. The government said revenue from the levy would be ""recycled back into [the] agriculture sector through new technology, research and incentive payments to farmers."" The idea of introducing such a system by the middle of this decade was contained within an emissions reduction plan published in May 2022, as well as a recommendation published in June by the He Waka Eke Noa – Primary Sector Climate Action Partnership.In a statement Tuesday, New Zealand's Prime Minister Jacinda Ardern backed the plans. ""This is an important step forward in New Zealand's transition to a low emissions future and delivers on our promise to price agriculture emissions from 2025,"" she said. ""No other country in the world has yet developed a system for pricing and reducing agricultural emissions, so our farmers are set to benefit from being first movers,"" Ardern went on to say. Agriculture plays a major role in New Zealand's economy, including exports, but it accounts for a considerable chunk of the country's emissions. In the consultation document, authorities said greenhouse gas emissions from agriculture — carbon dioxide, nitrous oxide and methane — were responsible for more than half of New Zealand's gross emissions. According to the document, carbon dioxide stems from urea, while nitrous oxide comes from livestock dung and urine. Methane is emitted through belching and, to a lesser extent, gas.",New Zealand plans to tax emissions from livestock burps and dung
213,10,10_climate_ozone_emissions_world,https://techcrunch.com/2022/07/08/this-startup-hopes-to-get-us-to-net-zero-via-its-platform-to-construct-wooden-buildings/,"This startup hopes to get us to net zero via its platform to construct wooden buildingsOne of the stunning facts that’s emerged over the last few years — especially as VCs and startups have turned their attention toward the climate crisis — is that our cities produce an enormous amount of CO 2 : In fact, buildings are responsible for around 40% of global CO 2 emissions. But of course, the problem is that cities are unlikely to stop building, and growiing.Some estimates say that if global urban growth continues at its current pace, then we’d build a New York City every month for the next 40 years. So if we could reduce this amount or transition, this growth to “net zero” (or better), we’d would do to a lot alleviate the impending, and disastrous, affects of climate change. This is why we are seeing so many new climate funds appear, which are concentrating on the built environment.A large part of this problem is that concrete and steel are just not sustainable materials, unlike (say) timber.Now, “011h“, based out of Barcelona, thinks it might have the answer.Currently, the building processes that use manual labor and usustainalble materials don’t pass muster, so if you can standardize and digitize the building process to make it repeatable and scalable (says 011h) while shifting sustainable materials — like mass timber — you can allow architects, builders, developers and investors to make net-zero buildings faster, cheaper and more sustainable.It all sounds lovely in theory, but in fact 011h says it has already completed such a project with Renta Corporación, a publicly traded developer, where the “embodied carbon” of the building was reduced by more than 90% compared to conventional methods, while construction timelines were reduced by 35%. This has led to three more major projects being commissioned.No doubt partly as a result of this, 011h has now raised a significant €25 million in Series A funding. The funding round was led by Redalpine, accompanied by Seaya Andromeda and Breega, with the participation of Aldea Ventures, among others. Previous investors also joined the round, including Giuseppe Zocco, Foundamental and A/O Proptech, which accelerates 011h’s ambition to create a sustainably built world.The funding will be used to further develop 011h’s platform, building system and team, initially focusing on Spain, then internationally.Lucas Carné, co-founder and co-CEO of 011h, said the impact of reducing the carbon footprint of building can’t be iunderestimated: “If the embodied carbon of every building were reduced by more than 90%, this would reduce 10% of annual global CO 2 emissions, this is equivalent to three gigatonnes of CO 2 every year. In real terms, this is almost 2x more carbon than completely transitioning from petrol to electric vehicles; and is equivalent to eliminating 1 billion domestic gas boilers,” he told me.Harald Nieder, general partner of Redalpine, added in a statement: “At Redalpine, we believe that there are massive opportunities around sustainability. In fact, the opportunities are such that we are not looking for marginal improvements of the status quo. We are looking for teams that are aiming to have a real impact, worthy of the global challenges we are facing. Construction is both one of the most unsustainable industries and one of the least digital and the 011h vision is exactly what we were looking to support.”",This startup hopes to get us to net zero via its platform to construct wooden buildings
545,10,10_climate_ozone_emissions_world,https://www.transportenvironment.org/discover/carmakers-lifetime-emissions-50-higher-than-reported/,"Carmakers’ global emissions are on average 50% higher than what they report with Hyundai-Kia and BMW underreporting emissions by as much as 115% and 80% respectively, a new Transport & Environment (T&E) report shows. With obligatory scope 3 (lifetime) emissions disclosure set to be introduced in 2023, asset managers with exposure to carbon intensive carmakers face a ‘ticking carbon bomb’, says T&E.In 2023, the EU will introduce a requirement that financial institutions disclose their scope 3 emissions (indirect emissions)[1]. The new requirement will hit asset managers with exposure to carmakers hard. Unlike manufacturers of furniture or mobile phones, the vast majority (98%) of a car company’s emissions come under scope 3 – primarily the use of the cars [2].But, as T&E’s analysis shows, carmakers’ already high scope 3 emissions are likely far larger than officially reported.Luca Bonaccorsi, director of sustainable finance at T&E, said: “For green investing to be effective, we need accurate data. Carmakers are trying to pull the wool over investors eyes by underreporting the lifetime emissions of their cars. This makes a mockery of carmakers’ green claims.”Carmakers base their total reported emissions on a number of factors such as the average size of the vehicles, where the cars are driven and the lifespan of vehicles. Carmakers on the whole have used selective data to reach a lower figure. Toyota, for example, bases the lifetime average emissions of its vehicles on a scarcely believable 100,000 kilometres.This makes car companies, from an investment perspective, almost as carbon intensive as the oil industry. At today’s prices, €1 million invested in an average of oil giants Exxon Mobil, BP and Shell finances around 5,000 tonnes of CO2 equivalent (CO2e). The same amount finances more than 4,500 tCO2e in the car sector[3]. In some cases the carbon intensity of carmakers is significantly higher: nearly 10,000 tonnes if invested in Renault-Nissan-Mitsubishi and 7,000 tonnes for Honda according to carmakers’ reporting [4].Luca Bonaccorsi added: “According to official disclosures, a euro invested in a car company finances virtually the same amount of carbon as a euro in an oil company. This should be a wake up call for the financial industry. Asset managers wanting to avoid a ticking carbon bomb will have to start ditching carmakers that continue to sell polluting cars.”By the end of 2022, Morningstar, a US-based financial firm, estimates that some 50% of all new financial products sold will be environmental, social and governance (ESG) based. However, ESG ratings fail to capture the companies’ true climate impact. Despite CO2 emissions being the most important environmental indicators, they represent less than 1% of the ESG rating for S&P and MSCI, two of the world’s leading ESG indexes. T&E calls on the EU to regulate and harmonise the methodology for ESG ratings to ensure consistent and transparent reporting of data.ENDSNotes to editor[1] Car companies’ true carbon intensity will be revealed in 2023 thanks to mandatory Scope 3 disclosure (both in the Sustainable Finance Disclosure Regulation and in the Corporate Sustainability Reporting Directive).[2] Greenhouse gas emissions are categorised into three groups (Scopes) by the most widely-used international accounting tool, the Greenhouse Gas (GHG) Protocol. Scope 1 covers direct emissions from owned or controlled sources. Scope 2 covers indirect emissions from the generation of purchased electricity, steam, heating and cooling consumed by the reporting company. Scope 3 includes all other indirect emissions that occur in a company’s value chain.[3] This is based on the official scope 3 disclosures of carmakers and oil companies, not T&E’s estimates. Oil companies are likely also significantly underreporting their scope 3 emissions. However, we have no evidence to believe that the under-reporting is higher in any specific sector. On the contrary, in another analysis, evidence suggests the error must be rather homogeneous across sectors and is somewhat filtered out by the use of ratios.[4] The chart does not compare companies’ total emissions but the amount of emissions equivalent to a specific financial investment. This ratio is influenced by the appetite of the market for a specific company, so the higher the evaluation of the company, the lower the carbon intensity ratio. A company’s evaluation is affected by a large number of variables.",Carmakersâ global emissions 50% higher than reported
556,10,10_climate_ozone_emissions_world,https://www.vox.com/future-perfect/22686105/future-of-life-ozone-hole-environmental-crisis,"In 1985, atmospheric scientists in Antarctica noticed something troubling. For decades, they’d been measuring the thickness of the ozone layer in the upper atmosphere, the layer of gas that deflects much of the sun’s radiation. Starting in the 1970s, it had started plummeting. By the mid-1980s, they observed that it was on track to be wiped out in the next few decades.Their discovery was cause for worldwide alarm and unprecedented action. In short order, the international community marshaled its resources — scientific, economic, diplomatic — to mount a campaign to ban the chemical that caused the damage, chlorofluorocarbons (CFCs), and to restore the ozone layer.Fast-forward to today: The ozone is on the path to recovery, if not fully restored. New data released on October 26 by NASA indicates that the annual ozone hole over the Antarctic reached an average area of 8.9 million square miles over the past year. That’s slightly smaller than last year, and continues a trend toward overall shrinking over the past several years. “Over time, steady progress is being made, and the hole is getting smaller,” Paul Newman, chief scientist for earth sciences at NASA’s Goddard Space Flight Center, said in a statement. “The elimination of ozone-depleting substances through the Montreal Protocol is shrinking the hole.”That progress hasn’t been without setbacks — the hole grew in 2020, following a 2019 when it was unusually small. Researchers have also raised suspicions that the rate at which atmospheric CFCs are falling suggests not all signatories to a treaty banning new production of CFCs are abiding by the agreement. And there have been unintended consequences in phasing out CFCs with a different chemical that has hurt our fight against climate change (more on this below).But the damage we wrought last century has been reversed. Even with the complications and caveats, the world’s response to the ozone crisis should be seen as an instructive, even inspiring, success story — one that can perhaps inform our response to the climate crisis.That’s the thrust of the 2021 Future of Life Award from the Future of Life Institute, a nonprofit that studies how to reduce risks to our world. The award, handed out last month, went to three people who played a significant role in our triumph over the depletion of the ozone layer: atmospheric chemist Susan Solomon, geophysicist Joseph Farman, and Environmental Protection Agency official Stephen Andersen.The award, which comes with a $50,000 prize for each recipient, is given to unsung heroes who made our world safer from existential or global catastrophic risks. In 2020, the institute gave its award to William Foege and Viktor Zhdanov, who played key roles in the smallpox eradication fight. The previous year, it went to Matthew Meselson for his work on the Biological Weapons Convention.The FLI award harks back to a crisis that unnerved — and galvanized — humanity in the 1980s and ’90s. The ozone layer reduces how much radiation makes it to the surface of the Earth. Without it, sunshine would be significantly deadlier to life on the planet. The primary culprit for its thinning, researchers discovered, was CFCs, a chemical compound that was present in everything from aerosol cans to refrigerators to solvents. As CFCs degrade in the upper atmosphere, they can break down ozone.“Projections suggested that the ozone layer would collapse by 2050,” the Future of Life Institute’s Georgiana Gilgallon told me. “We’d have collapsing ecosystems, agriculture, genetic defects.” The sudden plunge in atmospheric ozone heralded a coming disaster.But the world responded. With consumer boycotts, political action, a major international treaty called the Montreal Protocol, and a huge investment in new technologies to replace CFCs in all their commercial and industrial uses, new CFC production was brought effectively to a halt over the 1990s and early 2000s. It took a while to phase out existing devices that used CFCs, but CFC emissions have been steadily falling since the protocol went into effect.“We see this as potentially the first instance in which humanity recognized and addressed a global catastrophic risk,” Gilgallon told me. There is still much to be done and some new problems to contend with, but measurements from the present day make it clear that the process of healing the ozone layer is well underway.The ozone “hole,” explainedOzone is a molecule made up of three oxygen atoms. (The oxygen we breathe is made up of just two.) There’s not much ozone floating around in the layer of atmosphere that we breathe — a good thing, since it’s actually a lung irritant and linked with respiratory disease.But there’s a lot of it in the stratosphere (comparatively speaking, at least; it’s still only a tiny fraction of the overall air). It’s that layer of ozone that absorbs ultraviolet (UV) radiation, especially the specific wavelengths called UV-B.UV-B radiation is what causes sunburns, and in high concentrations it causes more problems than that. It can lead to many kinds of cancer by damaging our DNA; most plants and animals also suffer when growing in a high-UV-radiation environment.In the 1970s, researchers noticed that the ozone layer had started thinning, especially around the poles. (With the ozone layer constituting only about three in a million atoms in the stratosphere in the first place, “hole” is technically a misnomer — the “ozone hole” was really just an area where ozone levels had dropped by more than 30 percent in a decade.)By the time the thinning of the ozone layer was measured, researchers Mario Molina and Sherry Rowland had already established the probable cause: CFCs.CFCs were everywhere, and as far as everyone knew, they were the perfect chemical: nonreactive, cheap, and highly effective in a wide variety of manufacturing applications. They were building up in the atmosphere, but it was thought that since they were nonreactive, it couldn’t be a problem.Molina and Rowland realized that that assumption was wrong. There’s a (possibly apocryphal) story of Rowland’s wife asking him how his work was going, and Rowland responding, “Well, the work is fantastic, but I think the Earth is ending.”The problem was that CFCs break down in the upper atmosphere. And the chlorine in CFCs was actually reactive, binding with ozone to make oxygen and chlorine monoxide.Molina and Rowland’s 1974 paper in Nature laying out the problem prompted discussion and debate, and environmental activists started pushing for change. But it didn’t move governments to coordinated international action. At the time, the exact implications of Molina and Rowland’s theory were hotly contested. Many researchers believed that ozone depletion would be a problem only on a time scale of centuries. There were some early worrying measurements that were dismissed as flukes.What the measurements in the Antarctic taken a decade later showed definitively is that it was happening much, much faster than that. “Sometime around the late ’70s, it started dropping like a rock — [there] was more ozone depletion than Molina and Rowland had ever imagined,” Solomon said.From diagnosis to global actionThe fight in the 1980s against the depletion of the ozone layer had several stages that might seem familiar to those trying to unite the world to combat other problems.First, there was the challenge of determining that there was in fact a threat and that CFCs were the cause. The initial work there was done by Molina and Rowland. But from the 1985 measurements taken by Joseph Farman — a geophysicist at the British Antarctic Survey — and his colleagues, it looked like the ozone layer was vanishing much faster than their models predicted.Susan Solomon was the lead researcher on the team that figured out how the chlorine from CFCs was breaking down so much ozone. In 1986 and 1987, she led the National Ozone Expedition to Antarctica to collect the evidence that would confirm her theory. Scientists had originally thought that, while chlorine would interact with ozone, the process was naturally limited — after all, there weren’t that many atoms of chlorine loose.Solomon and her team claimed that the process by which chlorine broke down ozone actually wasn’t as limited as initially thought and that the ozone breakdown could quickly spiral out of control: The chlorine monoxide that formed from chlorine’s interaction with ozone would then break down, releasing the chlorine atom to go break down more ozone.“You can destroy hundreds of thousands of ozone molecules with one chlorine atom from a CFC molecule in the timescale that this stuff is in the stratosphere,” Solomon said.The next stage of the fight was then convincing the world to do something about the problem. In 1986, UN negotiations began on a treaty to ban substances that reacted with ozone in the upper atmosphere, mainly CFCs. Stephen Andersen, at the time an official in the US Environmental Protection Agency, was a major figure in the negotiations. “He really made it happen,” the Future of Life Institute program director David Nicholson says.The Montreal Protocol on Substances that Deplete the Ozone Layer was agreed upon and opened for signature in 1987. It went into force in 1989. Countries gradually began phasing out CFCs. Andersen’s team, Nicholson says, “systematically identified hundreds of solutions for phasing out CFCs from hundreds of industry sectors,” making it possible to shift manufacturing processes worldwide to chemicals that weren’t ozone-depleting.Those chemicals in some cases have presented their own problems. For refrigerants, the world shifted to hydrofluorocarbons (HFCs), which endanger the ozone layer much less. Like the CFCs they replaced, though, HFCs are potent greenhouse gases — thousands of times more effective than carbon dioxide at trapping heat in our atmosphere. Twenty years ago, HFCs were an environmental step forward, allowing us to phase out CFCs. Today, policymakers and scientists are trying to phase out HFCs as well. Human ingenuity can solve our problems, but it can also create new ones as it does.But in terms of the primary goal — healing the ozone layer — the worldwide effort was a huge success. CFC consumption declined from over 800,000 metric tons in the 1980s to an estimated 156 metric tons in 2014. Experts estimate that by 2050, the ozone layer will be back to the state it was in 1980.And keeping the ozone intact buys us time in the fight against climate change. Yes, HFCs are a potent greenhouse gas. But CFCs contributed to global warming as well: They were powerful greenhouse gases in their own right, and by destroying the ozone layer, they contributed to warming by allowing more energy to reach the planet’s surface. One study found that ozone-depleting chemicals drove half of Arctic warming in the 20th century.With that said, HFCs are still a big climate problem. In recent years, governments have been working to extend the hugely successful Montreal Protocol to phase them out too. It’s fair to say that, in some ways, the global fight against the ozone crisis was a complicated story, one that continues to be written.But in other ways, it does offer some bracing clarity. The sheer speed with which the world went to work and enacted a global treaty to address a pressing environmental problem is, to contemporary eyes, downright bewildering. To a public accustomed to decades-long stalemates over climate policy, hearing how countries quickly lined up to sign an accord to save the planet may feel almost like a rebuke of our failures.In many ways, the international community of the 1980s had an easier problem. CFCs were industrially useful, but there were substitutes; cost-effective substitutes for fossil fuels are coming into production now, decades into the climate crisis, but they certainly didn’t exist when we first started addressing it.Politicians were more united in addressing the ozone layer than they’ve proven in addressing climate change. The Senate ratified the Montreal Protocol 83–0. Margaret Thatcher, not generally known for her friendliness to regulation, was a leader in the push for the Montreal Protocol and the effort to enable compliance by poor countries.By contrast, politicians today (especially in the US) are fiercely divided over the proper government role in ending climate change, and the public is divided along partisan lines as well.The picture we’re left with by the fight to heal the ozone layer is that specific individuals played a huge role in changing humanity’s trajectory but they did that mostly by enabling public activism, international diplomacy, and collective action. In the fight to improve the world, we can’t do without individuals and we can’t do without coordination mechanisms. But we should keep in mind how much we can do when we have both.Update, October 27, 2022, 10:15 am ET: This story was originally published in October 2021 and has been updated to reflect new data about the size of the ozone hole.",The shrinking ozone hole shows that the world can actually solve an environmental crisis
137,11,11_legal_lawyers_atrium_law,https://techcrunch.com/2016/08/23/logikcull-raises-10m-to-let-lawyers-analyze-documents-at-the-speed-of-a-thousand-interns/,"As entertaining and interesting as made-for-TV lawsuits (like the O.J trial) are, they always leave out one key element. The hundreds of hours of research that goes on behind the scenes to prepare for an important trial.Before any big trial a lawyer (and their associates and interns) has to find, organize and examine thousands of documents. The process is officially called Discovery, and is a period when both sides gather and request all the information they think they will need for the trial.This information can look like anything from email databases to Powerpoints, and while some of the process has become computerized in recent years (aptly termed eDiscovery), it’s still remained pretty manual and inefficient.For example, eDiscovery may mean a team of associates combing through hundreds of pages of email correspondence on a screen, instead of printing it all out like lawyers used to do. A win for the environment yes, but still very, very time consuming and not really taking advantage of technology.But Logikcull is a software company trying to change this, and just closed $10M in Series A funding from OpenView Ventures and Storm Ventures to help.The company officially calls itself “cloud-based legal intelligence”, but is essentially Dropbox for the legal world.Lawyers can bulk-upload all the messy information they need to examine (even if it’s an entire hard drive of different types of files) and Logikcull will organize all the different file types into one searchable database.Need an example? Imagine uploading 1,000 pages of old contracts, then using a search engine to find the exact ones that are relevant to the case. Or uploading a 20 GB email database of tens of thousands of messages and narrowing it down to only the messages between people relevant in your case on a certain date from a certain device – you get the point.[gallery ids=""1373234,1373235,1373236,1373237""]Logikcull also uses OCR so you can upload old scanned documents that weren’t previously searchable.While originally designed for (and still mostly used by) lawyers, these tools obviously have applications in other industries. A company’s HR department could use it to sort through the thousands of documents typically involved in an internal investigation. Or a city could use it to quickly find documents related to specific FOIA requests, a process that is still sometimes done by hand. Letting a government employee quickly find the specific documents that were requested can turn a one month turnaround time for FOIA requests into just a few days.Essentially, the platform can be used for any task that requires you to organize and search a crazy amount of documents.The company charges per user, and a small law firm can expect to pay $15k-$30k per year. Expensive yes, but not if the alternative is paying hundreds of extra hours in legal fees at $600 per hour.Logikcull says they are signing up new clients at a rate that has grown revenues 3x year-over-year, which is impressive considering how expensive the service. But that’s the thing about legal tech – the alternative is so crazy expensive that startups can charge an arm and a leg as long as they are providing a solution that actually save time.",Logikcull raises $10M to let lawyers analyze documents at the speed of a thousand interns
189,11,11_legal_lawyers_atrium_law,https://techcrunch.com/2020/10/28/priori-series-a/,"Priori Legal, a startup rethinking the way that large corporations hire outside counsel, has raised $6.3 million in Series A funding.Founded by CEO Basha Rubin and CPO Mirra Levitt (who met while classmates at Yale Law School), Priori launched as a legal marketplace for small and medium businesses before finding its current model in 2016.Rubin explained that although Fortune 500 companies have their own in-house legal teams, they still spend an average of $150 million a year on outside legal counsel. And finding that counsel can be an arduous process — a consumer goods company, for example, might need to hire lawyers in all 50 states.So by creating a marketplace of vetted lawyers (it says it only accepts 10% of applicants), by running a bidding process for the work and by streamlining the billing and on-boarding process, the startup can save companies an average of 60% of the money they spend on outside counsel and reduce the search time by 80%.“We don’t get involved in the substance of the lawyer-client relationship,” Levitt added. “We are not a law firm, we don’t do any of the legal work. Our innovation is focused entirely on the process of rapidly identifying the right talent and, once the matter is up and running, making billing seamless.”There are currently more than 1,500 lawyers in the marketplace, representing all 50 states in the U.S., as well as 47 countries and 700 practice proficiencies. Levitt said that while the first lawyers to join the platform were usually independent or worked at small firms that might not previously had access to these kinds of clients, there are now larger firms signing up as well.And Rubin said interest in Priori has only grown during the pandemic and the resulting economic downturn. Companies are trying to do “more with less,” and “part of our value proposition is fundamentally cost savings.” For example, she noted that client spending on the platform has increased 200% in the last year.“We began to see so much inbound demand that we would log onto Slack at 11pm and the entire team would be working,” she said. “We have a truly extraordinary team, but a) that’s not sustainable from a human perspective, and b) we saw an opportunity to really grow dramatically if we could throw resources at it.”The Series A comes from Hearst Corporation (also a Priori customer), Great Oaks Venture Capital, Jambhala, Tim Steinert (former general counsel of Alibaba Group), Mindset Ventures, Bridge Venture Fund and Orrick’s Legal Technology Fund.In addition to growing the team, Rubin said that the new funding will allow Priori to expand its network of lawyers, especially internationally.“From a product perspective, we’re really building out our use of data throughout the platform,” Levitt said, adding that the company plans to use machine learning to improve attorney vetting, matchmaking, bidding, project scoping and more.",Priori raises $6.3M to help large companies hire outside legal help
572,11,11_legal_lawyers_atrium_law,https://techcrunch.com/2022/03/01/zero-systems-gets-12m-series-a-to-bring-automation-to-professional-services/,"Zero SystemsZero SystemsThey came up with an AI-driven system that can identify work product on a lawyer’s work systems like their inbox or messaging apps, classify it according to client and project and then move the different kinds of information into adjacent systems such as a document management system or a time management system. The idea was to eliminate a lot of the dreary manual tasks that are part of every lawyer’s day.They came up with an AI-driven system that can identify work product on a lawyer’s work systems like their inbox or messaging apps, classify it according to client and project and then move the different kinds of information into adjacent systems such as a document management system or a time management system. The idea was to eliminate a lot of the dreary manual tasks that are part of every lawyer’s day.“We focused on the high value processes where a cognitive component was required to mimic the decision making process of a human user,” company co-founder and CEO Alex Babin explained.“We focused on the high value processes where a cognitive component was required to mimic the decision making process of a human user,” company co-founder and CEO Alex Babin explained.For lawyers, that starts with governance and properly filing content as it relates to the client and project they’re working on, and moving it into a document management system or client management system automatically. Next, it looks at time management and tracking the lawyer’s time in an automated way and finally it includes a security component to help keep all of that information secure.For lawyers, that starts with governance and properly filing content as it relates to the client and project they’re working on, and moving it into a document management system or client management system automatically. Next, it looks at time management and tracking the lawyer’s time in an automated way and finally it includes a security component to help keep all of that information secure.The system largely targets unstructured data like documents, emails, messages, files and so forth where they live on a laptop, smartphone or other device with the goal of organizing information automatically. It’s worth noting that the solution is installed in the customers’ facilities, rather than in the cloud, says Gevorg Karapetyan, the startup’s CTO and co-founder.The system largely targets unstructured data like documents, emails, messages, files and so forth where they live on a laptop, smartphone or other device with the goal of organizing information automatically. It’s worth noting that the solution is installed in the customers’ facilities, rather than in the cloud, says Gevorg Karapetyan, the startup’s CTO and co-founder.This is partly for security reasons and to meet the requirements of their customers, but also because the data gets processed at the point of ingestion on the edge device the professional is using. “So basically, we bring machine learning and data processing to where the data is, not the other way around. We don’t see that as a limitation, but as a feature in our use case,” he said.This is partly for security reasons and to meet the requirements of their customers, but also because the data gets processed at the point of ingestion on the edge device the professional is using. “So basically, we bring machine learning and data processing to where the data is, not the other way around. We don’t see that as a limitation, but as a feature in our use case,” he said.Over time, they realized the solution would also work for financial services and consulting professionals, who used similar types of systems that would work well with the classification system they had created.Over time, they realized the solution would also work for financial services and consulting professionals, who used similar types of systems that would work well with the classification system they had created.The company launched the product two and a half years ago. It is making headway with the AmLaw 100, the largest law firms in the United States, with 11 customers online using the product and another 10 piloting it. They went from around 25 employees at the beginning of last year to almost 80 by the end of the year, more than tripling head count.The company launched the product two and a half years ago. It is making headway with the AmLaw 100, the largest law firms in the United States, with 11 customers online using the product and another 10 piloting it. They went from around 25 employees at the beginning of last year to almost 80 by the end of the year, more than tripling head count.Babin says that as they build the company and add people, they are focused on building a diverse company, not only because it’s the right thing to do, but because clients expect it. He says part of that is looking outside of tech for people with skills which might be applicable to their mission.Babin says that as they build the company and add people, they are focused on building a diverse company, not only because it’s the right thing to do, but because clients expect it. He says part of that is looking outside of tech for people with skills which might be applicable to their mission.“We bring in people from other industries who want to go into tech, and give them an opportunity to learn a job, which I believe is very, very important. So that’s how we address things because there’s so much we can learn from different cultures, different different backgrounds,” he said.“We bring in people from other industries who want to go into tech, and give them an opportunity to learn a job, which I believe is very, very important. So that’s how we address things because there’s so much we can learn from different cultures, different different backgrounds,” he said.As Zero has gained traction, it required more capital, and today announced a $12 million Series A to help keep building out the platform. Today’s round was led by Streamlined Ventures with participation from 468 Capital, AltaIR Capital, PBJ Capital, Gutbrain Ventures, s16vc, AiSprouts VC, Paul Grewal and others.As Zero has gained traction, it required more capital, and today announced a $12 million Series A to help keep building out the platform. Today’s round was led by Streamlined Ventures with participation from 468 Capital, AltaIR Capital, PBJ Capital, Gutbrain Ventures, s16vc, AiSprouts VC, Paul Grewal and others.",Zero Systems gets $12M Series A to bring automation to professional services
198,11,11_legal_lawyers_atrium_law,https://techcrunch.com/2021/03/31/leeway-is-a-contract-workflow-service-for-your-legal-team/,"Meet Leeway, a French startup that is building an end-to-end software-as-a-service solution for your contracts. Leeway lets you centralize all your contracts in a single repository, go through multiple negotiation steps and trigger a DocuSign event for the signature.The company raised a $4.2 million seed round from HenQ, Kima Ventures as well as several business angels, such as the founders of Algolia, Eventbrite, Spendesk, MeilleursAgents, Livestorm and Luko.If you’re working for the legal department of your company, you’re probably working with multiple tools. Chances are you’re using Microsoft Word to write a contract, a cloud service to store and share the contract with your teammates and business partners, and an e-signature and archival service.Leeway is optimizing this workflow at every step. First, you can store all your contracts on Leeway. In addition to making it easier to find a contract later down the road, you can get reminders when a contract is about to expire so you can renew a contract.Second, you can edit your contract from Leeway directly. For instance, a manager can review a contract and write changes in Leeway’s interface. The employee can then start a revision and save a new version of the contract.After that, you can send the contract from the same interface. Administrators can set up approval workflows so that several people need to approve a contract before it is signed. As everything is centralized, you can get an overview of all your contracts that are currently in the pipeline.Up next, Leeway is thinking about integrating conditional clauses within the product. Usually, big companies have several versions of the same clause — very favorable, favorable, not so favorable, etc. When a client is negotiating, Leeway customers could switch the clause from very favorable to favorable for instance.Right now, around 30 companies are using Leeway to manage their contracts. Clients include Voodoo, Evaneos, Ifop and Fitness Park. “We have a very specific customer base — the legal department of companies with 100 to 500 employees,” co-founder and CEO Antoine Fabre told me.It doesn’t mean that smaller and bigger companies shouldn’t be using Leeway. But companies with fewer than 100 employees don’t necessarily have a full-fledged legal department. The sales team or the finance department could act as the legal-ish team. But Leeway still has a lot of room to grow.",Leeway is a contract workflow service for your legal team
161,11,11_legal_lawyers_atrium_law,https://techcrunch.com/2018/08/17/klarity-uses-ai-to-strip-drudgery-from-contract-review/,"Klarity, a member of the Y Combinator 2018 Summer class, wants to automate much of the contract review process by applying artificial intelligence, specifically natural language processing.Company co-founder and CEO Andrew Antos has experienced the pain of contract reviews first hand. After graduating from Harvard Law, he landed a job spending 16 hours a day reviewing contract language, a process he called mind-numbing. He figured there had to be a way to put technology to bear on the problem and Klarity was born.“A lot of companies are employing internal or external lawyers because their customers, vendors or suppliers are sending them a contract to sign,” Antos explained They have to get somebody to read it, understand it and figure out whether it’s something that they can sign or if it requires specific changes.You may think that this kind of work would be difficult to automate, but Antos said that contracts have fairly standard language and most companies use ‘playbooks.’ “Think of the playbook as a checklist for NDAs, sales agreements and vendor agreements — what they are looking for and specific preferences on what they agree to or what needs to be changed,” Antos explained.Klarity is a subscription cloud service that checks contracts in Microsoft Word documents using NLP. It makes suggestions when it sees something that doesn’t match up with the playbook checklist. The product then generates a document, and a human lawyer reviews and signs off on the suggested changes, reducing the review time from an hour or more to 10 or 15 minutes.They launched the first iteration of the product last year and have 14 companies using it with 4 paying customers so far including one of the world’s largest private equity funds. These companies signed on because they have to process huge numbers of contracts. Klarity is helping them save time and money, while applying their preferences in a consistent fashion, something that a human reviewer can have trouble doing.He acknowledges the solution could be taking away work from human lawyers, something they think about quite a bit. Ultimately though, they believe that contract reviewing is so tedious, it is freeing up lawyers for work that requires a greater level of intellectual rigor and creativity.Antos met his co-founder and CTO, Nischal Nadhamuni, at an MIT entrepreneurship class in 2016 and the two became fast friends. In fact, he says that they pretty much decided to start a company the first day. “We spent 3 hours walking around Cambridge and decided to work together to solve this real problem people are having.”They applied to Y Combinator two other times before being accepted in this summer’s cohort. The third time was the charm. He says the primary value of being in YC is the community and friendships they have formed and the help they have had in refining their approach.“It’s like having a constant mirror that helps you realize any mistakes or any suboptimal things in your business on a high speed basis,” he said.",Klarity uses AI to strip drudgery from contract review
154,11,11_legal_lawyers_atrium_law,https://techcrunch.com/2017/09/14/twitch-co-founder-justin-kan-launches-tech-enabled-law-firm-for-startups/,"Justin Kan, co-founder of startups like Twitch.tv and Exec, is pulling the curtains off his new tech platform for law firms, Legal Technology Services. The first law firm to use LTS is Atrium, co-founded by Augie Rakow and BeBe Chueh. Both are launching today to bring a full-stacked technology-enabled law firm to startups.What makes Atrium different from traditional law firms, Kan told me, is its technology and upfront pricing. With most law firms, it’s not always clear to the customer how much they’re going to have to pay.Atrium, which has 30 startup customers focused on everything from cryptography to autonomous cars to medical tech, offers two products. One is Atrium Counsel, which offers ongoing services with fixed-rate, upfront pricing. It sort of functions as preventative legal services, Kan told me. The other is Atrium Financings, a fixed-fee service for startups to navigate the legal intricacies of their financing rounds from start to finish.Since June, Atrium has advised its customers on $94 million in financing. Its customers include Protocol Labs, cannabis startup Meadow and health care startup Notable Labs. Atrium’s legal team can also help startups with joint ventures, mergers and acquisitions, ICOs, litigation and more.Behind the scenes, doing all the technical work at Atrium, is LTS, co-founded by Kan and Chris Smoak. It provides the technical backbone to Atrium with its suite of tools, like document creation and e-signing, and project management workflows.“It does everything except give advice,” Kan said.Atrium was LTS’s first client. For Atrium’s Series A financing round, the company executed the process all through LTS. Down the road, LTS plans to sell its tools to other firms. Although technology plays a large role in Atrium’s offering, the firm still needs real-life lawyers to provide legal advice to startups.“We’re not trying to do the Uber of lawyers” or “lawyer AI,” Kan said.So far, Atrium has eight lawyers on board, including two of its co-founders, Rakow and Chueh. Rakow was formerly a partner at Orrick, and Chueh previously worked as a litigator, and co-founded Attorney Fee, which sold to LegalZoom. Atrium’s goal is to bring on over 50 lawyers.Kan and I are chatting on stage at Disrupt SF next week, so be sure to tune in to learn more about the startup and how it’s going to compete against the likes of Orrick and Wilson Sonsini.[gallery ids=""1273179,1545034,1545032,1545033""]",Twitch co-founder Justin Kan unveils tech platform for law firms
182,11,11_legal_lawyers_atrium_law,https://techcrunch.com/2020/01/13/atrium-layoffs/,"Seventy-five-million-dollar-funded legal services startup Atrium doesn’t want to be the next company to implode as the tech industry tightens its belt and businesses chase margins instead of growth via unsustainable economics. That’s why Atrium is laying off most of its in-house lawyers.Now, Atrium will focus on its software for startups navigating fundraising, hiring and collaborating with lawyers. Atrium plans to ramp up its startup advising services. And it’s also doubling down on its year-old network of professional service providers that help clients navigate day-to-day legal work. Atrium’s laid-off attorneys will be offered spots as preferred providers in that network if they start their own firm or join another.“It’s a natural evolution for us to create a sustainable model,” Atrium co-founder and CEO Justin Kan tells TechCrunch. “We’ve made the tough decision to restructure the company to accommodate growth into new business services through our existing professional services network,” Kan wrote on Atrium’s blog. He wouldn’t give exact figures, but confirmed that more than 10 but less than 50 staffers are impacted by the change, with Atrium having a headcount of 150 as of June.The change could make Atrium more efficient by keeping fewer expensive lawyers on staff. However, it could weaken its $500 per month Atrium membership that included some services from its in-house lawyers that might be more complicated for clients to get through its professional network. Atrium will also now have to prove the its client-lawyer collaboration software can survive in the market with firms paying for it rather than it being bundled with its in-house lawyers’ services.“We’re making these changes to move Atrium to a sustainable model that provides high-quality services to our clients. We’re doing it proactively because we see the writing on the wall that it’s important to have a sustainable business,” Kan says. “That’s what we’re doing now. We don’t anticipate any disruption of services to clients. We’re still here.”Founded in 2017, Atrium promised to merge software with human lawyers to provide quicker and cheaper legal services. Its technology can help automatically generate fundraising contracts, hiring offers and cap tables for startups while using machine learning to recommend procedures and clauses based on anonymized data from its clients. It also serves like a Dropbox for legal, organizing all of a startup’s documents to ensure everything’s properly signed and teams are working off the latest versions without digging through email.The $500 per month Atrium membership offered this technology plus limited access to an in-house startup lawyer for consultation, plus access to guide books and events. Clients could pay extra if they needed special help such as with finalizing an acquisition deal, or access to its Fundraising Concierge service for aid with developing a pitch and lining up investor meetings.Kan tells me Atrium still has some in-house lawyers on staff, which will help it honor all its existing membership contracts and power its new emphasis on advising services. He wouldn’t say if Atrium is paid any equity for advising, or just cash. The membership plan may change for future clients, so lawyer services are provided through its professional network instead.“What we noticed was that Atrium has done a really good job of building a brand with startups. Often what they wanted from attorneys was…advice on ‘how to set my company up,’ ‘how to set my sales and marketing team up,’ ‘how to get great terms in my fundraising process,’ ” so Atrium is pursuing advising, Kan tells me. “As we sat down to look at what’s working and what’s not working, our focus has been to help founders with their super-hero story, connect them with the right providers and advisors, and then helping quarterback everything you need with our in-house specialists.”LawSites first reported Saturday that Atrium was laying off in-house lawyers. A source tells TechCrunch that Atrium’s lawyers only found out a week ago about the changes, and they’ve been trying to pitch Atrium clients on working with them when they leave. One Atrium client said they weren’t surprised by the changes because they got so much legal advice for just $500 per month, which they suspected meant Atrium was losing money on the lawyers’ time as it was so much less expensive than competitors. They also said these cheap legal services rather than the software platform were the main draw of Atrium, and they’re unsure if the tech on its own is valuable enough.One concern is Atrium might not learn as quickly about which services to translate into software if it doesn’t have as many lawyers in-house. But Kan believes third-party lawyers might be more clear and direct about what they need from legal technology. “I feel like having a true market for the software you’re building is better than having an internal market,” he says. “We get feedback from the outside firms we work with. I think in some ways that’s the most valuable feedback. I think there’s a lot of false signals that can happen when you’re the both the employer and the supplier.”It was critical for Atrium to correct course before getting any bigger, given the fundraising problems hitting late-stage startups with poor economics in the wake of the WeWork debacle and SoftBank’s troubles. Atrium had raised a $10.5 million Series A in 2017 led by General Catalyst alongside Kleiner, Founders Fund, Initialized and Kindred Ventures. Then in September 2018, it scored a huge $65 million Series B led by Andreessen Horowitz.Raising even bigger rounds might have been impossible if Atrium was offering consultations with lawyers at far below market rate. Now it might be in a better position to attract funding. But the question is whether clients will stick with Atrium if they get less access to a lawyer for the same price, and whether the collaboration platform is useful enough for outside law firms to pay for.Kan had gone through tough pivots in the past. He had strapped a camera to his head to create content for his live-streaming startup Justin.tv, but wisely recentered on the 3% of users letting people watch them play video games. Justin.tv became Twitch and eventually sold to Amazon for $970 million. His on-demand personal assistant startup Exec had to switch to just cleaning in 2013 before shutting down due to rotten economics.Rather than deny the inevitable and wait until the last minute, with Atrium Kan tried to make the hard decision early.","Atrium lays off lawyers, explains pivot to legal tech"
175,11,11_legal_lawyers_atrium_law,https://techcrunch.com/2019/09/17/ironclad-raises-50m-series-c-round-for-its-digital-contracting-platform/,"Ironclad, a startup that makes it easier for legal teams to manage their contracts workflow, today announced that it has raised a $50 million Series C round led by Y Combinator Continuity, with participation from Emergence Capital, as well as existing investors including Accel and Sequoia Capital. This round brings Ironclad’s total funding to $83 million, according to Crunchbase.In addition to the new funding, Ironclad, which was part of Y Combinator’s Summer 2015 class, also today announced the launch of its Workflow Designer. This tool allows teams to easily create their own custom workflows based on their individual business processes and timelines. Setting up those workflows looks be a pretty straightforward process. After tagging the existing contract, teams can then set up their processes based on what’s in a specific document. If a contract is over a specific value, for example, they can add a payment clause, or set up an approval process based on that value.Workflow Designer complements the service’s existing tools for managing the contract life cycle and collaborating on legal documents.The company says it will use the new funding to expand into new geographies and expand its product.“This round and our continued momentum highlights how big the opportunity is to streamline contracting for every type of company in the world,” said Jason Boehmig, co-founder and CEO of Ironclad. “Our newest investors bring a depth of later-stage company experience and a vision for what cloud companies will look like in the future. Our new funding will fuel continued product innovations, like our new Workflow Designer, which is accelerating contracting time by 85% for our customers.”",Ironclad raises $50M Series C round for its digital contracting platform
136,11,11_legal_lawyers_atrium_law,https://techcrunch.com/2016/07/19/legalist-is-making-it-easier-for-lawyers-to-find-state-court-records/,"Imagine a lawyer with a client who lives in one county and works in another. Or even a lawyer who litigates in multiple states. Both common occurrences, but situations that make it very hard to keep track of legal documents. Essentially, it should be easy to keep track of court records from multiple counties and states – but it’s not.In fact, it’s pretty awful. Most are hosted online, but each county could have different databases and even different databases providers, making it a huge hassle to constantly search for court records and updates. For example, Ohio has 88 counties, and you have to search each one separately for legal records. It’s such a mess that some lawyers have found it easier to have employees just drive from county to county tracking down records in person.Enter Legalist – a startup launching in Y Combinator’s Summer ’16 batch. Founded by Eva Shang and Christian Haigh, two current Harvard undergrads, the startup is trying to become a Google for state court records.They are doing this by scraping these databases and aggregating the documents into one main searchable database. This takes a while – most counties and states have records going back to 1989. For example, the startup is currently scraping 10 different states – a process that is providing them with 400,000 new documents a day.Besides searchable records, the startup also offers email updates for cases. This means that the site will scrape databases each day for updates to flagged cases and automatically email lawyers with the new documents so they don’t have to manually check every day for case updates.So far the site is live for users in Massachusetts, Ohio, and Maryland – with more to come soon. These three states have provided the databases with documents for over 7 million cases and 110,000 different lawyers.The service is also free for any licensed attorney registered with their state’s bar association. However the startup plans on charging for additional features in the future. These include an option to see cases sorted by outcome based on a certain judge – this will help lawyers choose the best litigation strategy in a specific case.Another future paid feature is “predicted timeline”, which uses their millions of archived cases to provide an estimate on how long a certain case will take. The startup says that lawyers find this feature especially helpful because the first question a client often asks their lawyer is how long the entire legal process will take.For now, the startup is just focused on state and county records. This is because the vast majority of court cases happen on the state level. Out of an approximately 95 million cases filed each year nationwide, only about 1 million happen in federal court. Plus, federal court records are already organized in a central database called PACER. So while Legalist eventually plans on adding federal records to their database, it isn’t an immediate need.",Legalist is making it easier for lawyers to find state court records
196,11,11_legal_lawyers_atrium_law,https://techcrunch.com/2021/03/11/legl-gets-7m-to-help-law-firms-upgrade-to-digital-workflows/,"B2B SaaS startup Legl has bagged $7M in Series A funding led by Octopus Ventures for its platform for law firms which offers tools to streamline core business processes such as customer onboarding, due diligence and payments.Existing investors Backed, Samaipata and First Round Capital, and angels including Carlos Gonzalez-Cadenas (ex CPO and COO GoCardless), Al Giles (ex CRO of legal business Axiom) and Hayden Brown (CEO of Upwork), also participated in the round.The UK startup was founded just over a year ago by Julia Salasky, a lawyer by background, who previously founded the public interest legal crowdsourcing campaign platform, CrowdJustice.Legl says it’s now working with around 100 UK-based law firms, including around a dozen of the top 200. The Series A will be used to expand Legl’s team and grow its UK user base as well as for further development of the product.Salasky tells TechCrunch she spotted the opportunity to build a platform to help law firms digitize their business processes through the experience of working with law firms at CrowdJustice.“There have been a few big shifts toward digital [in the legal sector], which we were able to first spot through our work with hundreds of law firms at CrowdJustice. One is the growing expectation of clients, both individuals and businesses, that they should have a good, digital experience as they do with the other service providers they interact with,” she says, discussing the opportunity she saw to help law firms digitise business processes.“The second is risk. Law firms are rightly risk averse, and doing client-facing processes in a manual, fragmented way – like doing compliance checks via email or taking payments over the phone (yes this is a real thing) – actually increases risk.“And the third is Covid. A lot of manual or face-to-face operational processes simply don’t work when remote-first is the norm.”While law firms can have a bit of a reputation for being ‘disinterested’ in making efficiency gains, given the business model of billing clients by the hour, Salasky emphasizes that Legl doesn’t intrude on the billable hour — crediting that as one of the reasons the SaaS has seen such “huge uptake” in short order.“We’re removing the time-consuming, admin-heavy work that lawyers can’t bill for,” she says. “The outcome is that lawyers can focus on what they’re best at — doing legal work.”Another driver for law firms to improve their back office processes is customer expectations, she says. “We do also see that there’s a big move in the industry toward better client experience, which from my perspective is a big change that’s emerged over the last year and a half or so, I think as a result of digital client experiences becoming the norm in other industries.”Asked about the flagship features of Legl, Salasky highlights “no-code workflows” — aka configurable workflows that let non-techie users replicate what they do now “for any client, in any practice area” but without the manual faff.“For example what might have taken multiple people a lot of emails and tooling to do, like a complex onboarding process, we replace with DIY workflows,” she explains. “We’re also using the trends from those interactions to surface really key client insights so that firms can start to understand their client base better.”The upside for Legl’s law firm clients is efficiency and revenue, per Salasky.“We see that there are both efficiency gains — firms are citing transaction times speeding up by 1-2 weeks — but also revenue gains, as firms can onboard more clients faster, reduce drop-off in their onboarding funnel, and improve cash flow,” she adds.Asked about training/retraining requirements for firms that opt to move workflows to Legl’s SaaS, Salasky says they have focused on making the tools super simple to use to avoid an arduous learning curve.“I’ve always been conscious that it’s hard to retrain law firms and we didn’t want to create friction in using our product – that would be ironic and counterproductive for a productivity focused platform,” she says. “So instead we have focused hard on consumerizing our tech so that it’s both super easy to use, and also that it replicates the workflows that law firms have now.”“I don’t think this necessarily replaces back office roles — it just makes it easier for firms to allocate higher value work to people doing manual, bitty operational tasks. Which is often done by back office staff, and often done by lawyers themselves, by the way,” she adds.As regards competitive landscape, Salasky acknowledges a “few point solutions” – name checking the likes of ThirdFort and SmartSearch in the compliance space as an example – but says that Legl goes “way beyond a point solution into business operations more generally”.“Our view is that firms don’t want to plug a tool into a manual, fragmented process – they want the tool to replace that process. And to be able to do that and deliver a great client experience and surface great client insights, is unique in the market,” she adds.While the SaaS is UK-only for now Legl is already getting enquiries from markets further afield and planning to start international expansion. Salasky says interest is coming from English speaking markets where some of its UK clients have international offices, noting: “That’s where we’re planning to start.”Commenting on the Series A in a statement, Zoe Chambers, early stage investor at Octopus Ventures, added: “It’s rare to find a founder and team with such insider knowledge to tackle a big industry that has started to adopt technology quickly. Covid has accelerated the move to digital in the legal industry, and Julia and team, with deep expertise across legal, SaaS and fintech, are in prime position to win the market.”Early Stage is the premier ‘how-to’ event for startup entrepreneurs and investors. You’ll hear first-hand how some of the most successful founders and VCs build their businesses, raise money and manage their portfolios. We’ll cover every aspect of company-building: Fundraising, recruiting, sales, product market fit, PR, marketing and brand building. Each session also has audience participation built-in – there’s ample time included for audience questions and discussion. Use code “TCARTICLE at checkout to get 20 percent off tickets right here.",Legl gets $7M to help law firms upgrade to digital workflows
589,11,11_legal_lawyers_atrium_law,https://techcrunch.com/2022/03/16/legal-tech-startups-bringing-law-order-to-fragmented-industry/,"It’s long known that the legal profession has not embraced technology as quickly as other industries.It’s long known that the legal profession has not embraced technology as quickly as other industries.As a result, there are a number of legal tech startups eager to not only help lawyers, but automate some of the processes bogged down by pen and paper. Here, we take a look at two companies that recently secured funding, Justpoint and New Era ADR, to see their approaches.As a result, there are a number of legal tech startups eager to not only help lawyers, but automate some of the processes bogged down by pen and paper. Here, we take a look at two companies that recently secured funding, Justpoint and New Era ADR, to see their approaches.JustpointJustpointVictor Bornstein, founder and CEO ofVictor Bornstein, founder and CEO ofPersonal injury lawyers rely heavily on ads and easy-to-memorize 800 numbers to attract clients, but Justpoint believes that using data is a better tool.Personal injury lawyers rely heavily on ads and easy-to-memorize 800 numbers to attract clients, but Justpoint believes that using data is a better tool.Here’s why: The Boulder, Colorado-based company has collected over 300,000 historical claims and uses data extraction models to plug into a law firm to provide a score on how good the firm is at winning cases, like sexual assault, medical malpractice and product liability.Here’s why: The Boulder, Colorado-based company has collected over 300,000 historical claims and uses data extraction models to plug into a law firm to provide a score on how good the firm is at winning cases, like sexual assault, medical malpractice and product liability.That’s one side. The second is equipping the firm with information on whether a certain claim is worth the law firm’s time to take, mainly because of the time involved in diving into a case, plus the fact that firms often put up their own money initially to file lawsuits and obtain expert witnesses. Justpoint also brings medical expertise in-house to process the data and train the model.That’s one side. The second is equipping the firm with information on whether a certain claim is worth the law firm’s time to take, mainly because of the time involved in diving into a case, plus the fact that firms often put up their own money initially to file lawsuits and obtain expert witnesses. Justpoint also brings medical expertise in-house to process the data and train the model.“Lawyers have an incentive,” Bornstein said. “A claim could receive $2 million, but if they settle quickly, it will save a lot of effort, though they will receive much less. We’ve looked at how to make claims more efficient so lawyers can take a claim to the end instead of settling.”“Lawyers have an incentive,” Bornstein said. “A claim could receive $2 million, but if they settle quickly, it will save a lot of effort, though they will receive much less. We’ve looked at how to make claims more efficient so lawyers can take a claim to the end instead of settling.”The company recently raised $6.9 million in a seed extension co-led by Divergent Capital and Charge Ventures. Additional investments came from Crossbeam Venture Partners, Honeystone Ventures, Interplay.vc, Weekend Fund, Turing co-founder Vijay Krishnan, Mainstreet co-founder Jackson Moses and Stonks founder Ali Moiz. It brings the total amount raised to $7.9 million.The company recently raised $6.9 million in a seed extension co-led by Divergent Capital and Charge Ventures. Additional investments came from Crossbeam Venture Partners, Honeystone Ventures, Interplay.vc, Weekend Fund, Turing co-founder Vijay Krishnan, Mainstreet co-founder Jackson Moses and Stonks founder Ali Moiz. It brings the total amount raised to $7.9 million.Justpoint makes money when the lawyer wins their case, which explains the company’s incentive to send claims worth spending the lawyer’s time on, Bornstein said.Justpoint makes money when the lawyer wins their case, which explains the company’s incentive to send claims worth spending the lawyer’s time on, Bornstein said.“That puts a lot of work on us validating the claims,” he added. “It’s also why we are seeing an uptick in legal technology. Many firms are not interested in using technology, but this allows us to do the work for them. The way we see it is in 10 years, the legal tech space will bloom in a way we have not seen.”“That puts a lot of work on us validating the claims,” he added. “It’s also why we are seeing an uptick in legal technology. Many firms are not interested in using technology, but this allows us to do the work for them. The way we see it is in 10 years, the legal tech space will bloom in a way we have not seen.”New Era ADRNew Era ADROn the dispute resolution side,On the dispute resolution side,Co-founder Rich Lee explained that legal disputes often take 18 to 24 months and hundreds of thousands of dollars to resolve. New Era is building a digital and virtual tool that cuts down on both the time and cost of resolving disputes by up to 90%. The company highlights risks so that companies and their law firms can reduce unnecessary litigation gamesmanship.Co-founder Rich Lee explained that legal disputes often take 18 to 24 months and hundreds of thousands of dollars to resolve. New Era is building a digital and virtual tool that cuts down on both the time and cost of resolving disputes by up to 90%. The company highlights risks so that companies and their law firms can reduce unnecessary litigation gamesmanship.“We are taking the temperature down, reducing acrimony and refocusing litigation back on story-telling,” Lee added. “The procedures in court systems and current arbitration systems don’t lend themselves to fast, efficient resolutions, so we rewrote them.”“We are taking the temperature down, reducing acrimony and refocusing litigation back on story-telling,” Lee added. “The procedures in court systems and current arbitration systems don’t lend themselves to fast, efficient resolutions, so we rewrote them.”New Era manages all of the case intake, payments and scheduling and facilitates virtual meetings with arbitrators so that clients can get binding resolutions in as little as 60 days.New Era manages all of the case intake, payments and scheduling and facilitates virtual meetings with arbitrators so that clients can get binding resolutions in as little as 60 days.The Chicago-based company recently raised $4.6 million in seed funding led by Nextview Ventures, with participation from Jump Capital. The company’s original pre-seed investors, Motivate Ventures and Alumni Ventures, also participated in this round along with a group of individual investors, including David Kalt, Sean Chou, Pete Kadens and Lon Chow. This latest round gives New Era total funding of $6.3 million.The Chicago-based company recently raised $4.6 million in seed funding led by Nextview Ventures, with participation from Jump Capital. The company’s original pre-seed investors, Motivate Ventures and Alumni Ventures, also participated in this round along with a group of individual investors, including David Kalt, Sean Chou, Pete Kadens and Lon Chow. This latest round gives New Era total funding of $6.3 million.New Era charges a flat fee per case, and in less than a year, was named as the dispute resolution platform in over 50 million contracts. So far in 2022, the company has already surpassed its 2021 revenue. Lee said the goal is to triple that in the next year.New Era charges a flat fee per case, and in less than a year, was named as the dispute resolution platform in over 50 million contracts. So far in 2022, the company has already surpassed its 2021 revenue. Lee said the goal is to triple that in the next year.Continued investment in legal techContinued investment in legal techJustpoint and New Era are among friends in raising capital to bring the legal industry into the digital age, with many of them also leveraging AI.Justpoint and New Era are among friends in raising capital to bring the legal industry into the digital age, with many of them also leveraging AI.Earlier this month,Earlier this month,“There’s been growing enthusiasm for legal tech for a while now,” Zack Hutto, director of advisory within Gartner’s legal and compliance practice, told TechCrunch. “Corporate law spending is up 50% and we are projecting budgets will make a three-fold increase by 2025.”“There’s been growing enthusiasm for legal tech for a while now,” Zack Hutto, director of advisory within Gartner’s legal and compliance practice, told TechCrunch. “Corporate law spending is up 50% and we are projecting budgets will make a three-fold increase by 2025.”He cited a SeptemberHe cited a SeptemberThat was not something Hutto was surprised by, saying it was proof of all of the demand, which resulted in VCs wanting to grab a piece of the pie.That was not something Hutto was surprised by, saying it was proof of all of the demand, which resulted in VCs wanting to grab a piece of the pie.He feels like the rise has been dramatic because it started from a small base. Corporate legal departments are spending millions of dollars, but are not using technology as much as you might expect. The legal profession was most insulated from technology and digital transformation, so the trend of startups coming in was bound to happen, though there is still some skepticism of how transformative those tools will be, Hutto added.He feels like the rise has been dramatic because it started from a small base. Corporate legal departments are spending millions of dollars, but are not using technology as much as you might expect. The legal profession was most insulated from technology and digital transformation, so the trend of startups coming in was bound to happen, though there is still some skepticism of how transformative those tools will be, Hutto added.“PDF invoices do not give you the kind of insight to make better decisions around that spend,” he said. “One-third of departments were using those in 2010, and fast-forward to the last couple of years, and that number has increased to around half of organizations using e-billing technology, but you still have to marvel at the fact that there is a large, unpenetrated market there.”“PDF invoices do not give you the kind of insight to make better decisions around that spend,” he said. “One-third of departments were using those in 2010, and fast-forward to the last couple of years, and that number has increased to around half of organizations using e-billing technology, but you still have to marvel at the fact that there is a large, unpenetrated market there.”","Legal tech startups bringing law, order to fragmented industry"
204,11,11_legal_lawyers_atrium_law,https://techcrunch.com/2021/10/17/dispute-resolution-platform-immediation-gets-3-6m-aud-to-expand-in-the-u-s/,"The pandemic forced the legal profession to cobble together remote work strategies, often through a combination of video conferencing and emails. Founded in Melbourne, Immediation provides a tailor-made solution with digital courtrooms and mediation tools. It has been adopted by Australian federal courts and New Zealand government agencies and is now expanding in the United States and European markets after raising $3.6 million AUD (about $2.7 million USD). Investors include Thorney Investment Group and its founder and chair, Alex Waislitz.Founded in 2017 and launched in 2019, Immediation’s users include the Federal Court of Australia, the Victorial Civil and Administrative Tribunal (VCAT), and New Zealand agencies like the Ministry of Justice, Sport New Zealand and Domain Name Commission NZ. The startup says over the past 12 months, its revenue has increased 6x year over year and its user growth has jumped 2,000%. Immediation currently has about 40 employees in five countries, and a panel of more than 100 mediators and arbitrators. Its new funding brings Immediation’s total raised to $10 million AUD.In addition to Australia and New Zealand, Immediation also has users in Southeast Asian markets and will spend the next 12 months focused on growing in the U.S. and European markets.Before starting Immediation as a mediation platform (it also now supports law firms, tribunals and resolution bodies), founder and managing director Laura Keily worked for two decades as a corporate lawyer and barrister. She told TechCrunch in an email that she wanted to create an online mediation platform because “I saw firsthand that people were locked out of being able to access justice effectively. The legal system is complex, lengthy and expensive. It’s an old system regimented by ancient rules and processes, which are not scalable and are inefficient.”Before using Immediation, many of its clients only had the option of face-to-face meetings in a mediation center or courtroom. Immediation launched its platform publicly in September 2019, a few months before the pandemic hit.“The onset of COVID-19 was a turning point,” Keily said. “As industries were forced to move online overnight, our team pivoted quickly to address the immediate concerns of the legal industry and provide a blueprint for a seamless transition online.”In 2020, Immediation saw a 2,200% increase in users across more than 2020, including a 500-person hearing over five days for the first ever Willem C. Vis International Arbitration Moot, a moot competition attended by hundreds of law schools.Keily said Immediation was created by lawyers to replicate physical courtrooms, mediation suites, legal client floors and dispute resolution environments. Its tools include the ability to record hearings, share and manage documents, co-draft and execute contracts, enable confidential communication between lawyers and clients during proceedings and set up secure, private rooms for different parties. Judicial officers and mediators retain control participants in a private room, so they can move or remove them as necessary.Maintaining lawyer-client confidentiality is essential. Immediation built secure chat and party rooms so “client-lawyer teams can communicate in complete confidence with their own team, even when proceedings are in full flight, knowing that no one else, by design, can see those messages or enter the party room,” said Keily.Immediation also announced today that it has appointed Christine Christian, the chair of Auctus Investment Group and Tamara Credit Partners, as its new chair, and Rachael Neumann and Greg Wildisen to its board. It added Afterpay chair Elena Rubin and Rampersand VC founding partner Jim Cassidy to its advisory board.",Dispute resolution platform Immediation raises $3.6M AUD to expand in the US
148,11,11_legal_lawyers_atrium_law,https://techcrunch.com/2017/03/08/cognitiv-is-using-ai-for-contract-analysis-and-tracking/,"Another legal tech startup coming out of the UK: Cognitiv+ is applying artificial intelligence to automate contract analysis and management, offering businesses a way to automate staying on top of legal risks, obligations and changing regulatory landscapes.Co-founder Vasilis Tsolis might therefore be forgiven for viewing Brexit as a sizable opportunity for his startup — though he more tactfully describes it as a “legislative challenge that we can help out with”.“There’s going to be a lot of changes in legislation, there’s going to be a lot of changes in regulation, and you really need to know what’s going to happen to your contracts and if you need to do any changes on your legal documents or not. So it’s going to be a huge challenge,” he says of Brexit.“I think this is going to happen more and more often,” he adds, pointing to another incoming EU regulation that will be upping businesses’ compliance needs in the near future: aka the GDPR, coming into force (including in the UK) in May 2018.“Because you see legislation changing so fast and it’s getting so much bigger that actually it’s impossible to monitor and impossible to read it. Who can read half million of pages?”“This is about day-to-day contract management but we think that compliance is going to be more and more strict, and it’s going to be much more difficult — there are so many new regulations, about Slavery Act, about GDPR, MiFid II and so many other compliances that all this accumulated risk analysis from your contracts we think it’s not possible to be viable for humans anymore — you need to bring the robots,” he adds.Cognitiv+’s data-parsing tool is not being designed to interpret legislation but rather to “monitor it in a structured fashion”, combining that tracking with analysis of a company’s own contracts with a view to flagging compliance risks and requirements.The overarching thesis is that contract analysis space/legal process outsourcing has yet to be disrupted by technology. Tsolis has both an engineering and a legal background, as you’d hope give the nature of the startup. Other co-founder, Achilleas Michos’, background is in computer science.“When you do contract analysis and compliance and when you do regulatory analysis there’s a lot of repetition, and the majority of the people spend a lot of time — perhaps the majority of the time — looking for basic stuff… Legal but also admin assignments. So the majority of those tasks can be accelerated by automation,” argues Tsolis.Cognitiv+ is using what he describes as “a number of AI technologies” to perform the contract analysis at near real-time speeds, leaning on open source algorithms for the core tech. But he describes the IP as “the process and all the stages we take for analyzing a contract, the training” — so, in other words, the legal expertise needed to get a proper handle on compliance.“We use machine learning, we use NLP [natural language processing], we use neural networks… We aim to be a risk management tool; we identify as much as possible that the machine can do,” he tells TechCrunch. “When it comes to [analyzing an area such as] limit of liability if the contract is not very well drafted then obviously we cannot help you on this one — but we can help you for the vast majority of the contracts that you have on your library.”Presumably the tech might also be able to flag up a badly drafted contract.Contracts are uploaded to the system for analysis and tracking, with examples of the sorts of critical information Cognitiv+ can extract including the parties of the contract; the limit of liability; renewal and termination information; and jurisdiction.Users are delivered intel on an ongoing basis via reports, dashboards and notifications. The tool is generally being designed for use by in-house lawyers, commercial staff, procurement, financial and compliance departments.The current industry focus for the team is procurement in the financial sector, but next year it plans to expand to target the insurance, real estate and engineering industries too. They’re currently also only tracking UK-related compliance, but are intending to add EU and US in the “coming months”.Given the financial services focus, they’re also looking at how the tech could be used to help combat financial crime, according to Tsolis.The early stage startup has been bootstrapping since being founded in late 2015, and has just gone through London’s Winton Labs accelerator for data-focused businesses. It’s also in the midst of closing a seed round, and is running pilots of v1 of its contract-parsing platform with a “small number” of UK companies.Tsolis says the first version is a fairly generic analytical tool, but a more vertical-specific v2 is coming in September — for financial and procurement users — and another version planned for March 2018 will target the other three target sectors. The aim is to begin revenue generating this year, via a SaaS business model.Could the tech also be applied for drafting contracts in future, not just analyzing them? UK startup Juro, for example, already offers both contract authoring and management, though it looks to have a bit of a different focus (on marketplaces and sales contracts).“The legal world is a long lived world for centuries now, and it’s a very traditional sector. And actually I think you need to disrupt it step by step,” says Tsolis, emphasizing the need to”bring together all the stakeholders” to ensure buy-in.“It’s not just the lawyers — a lot of people read contracts, like the financial people, commercial, procurement, compliance, so you need to bring all the stakeholders together to ensure that people understand what the machine does. And move on to new ways of interacting with the machine and NLP and new technologies.”",Cognitiv+ is using AI for contract analysis and tracking
163,11,11_legal_lawyers_atrium_law,https://techcrunch.com/2018/09/05/mccarthyfinch-ai-services-platform-automates-tedious-legal-tasks/,"McCarthyFinch sounds a bit like a law firm — and with good reason. The startup has developed an AI as a Service platform aimed at the legal profession. This week, it’s competing in the 2018 TechCrunch Disrupt Battlefield in San Francisco.The company began life as a project at a leading New Zealand law firm, MinterEllisonRuddWatts. They wanted to look at how they could take advantage of AI to automate legal processes to make them more efficient, cost-effective and faster, according to company president Richard DeFrancisco.“They were working on leveraging technology to become the law firm of the future, and they realized there were some pretty tremendous gaps,” he explained. They found a bunch of Ph.Ds working on artificial intelligence who worked with more than 30 lawyers over time to address those gaps by leveraging AI technology.That internal project was spun out as a startup last year, emerging as an AI platform with 18 services. MinterEllison, along with New Zealand VC Goat Ventures, gave the fledgling company US$2.5 million in pre-seed money to get started.The company looked at automating a lot of labor-intensive tasks related to legal document review and discovery such as document tagging. “Lawyers spend a lot of time tagging things with regards to what’s relevant and not relevant, and it’s not a good use of their time. We can go through millions of documents very quickly,” DeFrancisco said. He claims they can lower the time it takes to tag a set of documents in a lawsuit from weeks to minutes.[gallery ids=""1705643,1705649,1705647,1705648,1700421,1700422,1700423,1700424,1700425,1700426,1700427,1700428,1700429""]He says that one of their key differentiators is their use of natural language processing (NLP), which he says allows the company to understand language and nuance to interpret documents with a high level of accuracy, even when there are small data sets. Instead of requiring thousands of documents to train their models, which he says law firms don’t have time to do, they can begin to understand the gist of a case in as little as two or three documents with 90 percent accuracy, based on their tests.They don’t actually want to sell their platform directly to law firms. Instead, they hope to market their artificial intelligence skills as a service to other software vendors with a legal bent who are looking to get smarter without building their own AI from scratch.“What we are doing is going to technology service providers and talking to them about using our solution. We have restful APIs to integrate into their technology and do a Powered By-model,” DeFrancisco explained.The startup currently has 10 trials going on. While he couldn’t name them, he did say that they include the largest law firm in Europe, largest global provider of legal information and the fastest growing SaaS company in history. They are also working on agreements with large systems integrators including Deloitte and Accenture to act as resellers of their solution.While they are based in New Zealand, they plan to open a U.S. office in the Los Angeles area shortly after Disrupt. The engineering team will remain in New Zealand, and DeFrancisco will build the rest of the company in the U.S as it seeks to expand its reach. They also plan to start raising their next round of funding.",McCarthyFinch AI services platform automates tedious legal tasks
157,11,11_legal_lawyers_atrium_law,https://techcrunch.com/2018/04/12/helpself-uses-simple-ai-to-help-those-in-legal-trouble/,"HelpSelf is a AI-assisted legal app that helps you deal with simple issues. Need protection against debt collectors? Need an expungement? Want to deal with domestic violence? This robot can help.The project is an “automated legal technology company” that automates simple legal procedures. They currently work in the above areas but are moving into housing, family law, certain immigration tasks, and employment law, said Dorna Moini, co-founder of the project.“We self-funded from the start and are completely bootstrapped. We are making a profit through licensing fees for our document automation platform,” she said. “We use this document automation platform to create all of our new products and license it to lawyers to fund the tools we create. We just brought on another engineer and may be looking for funding in the next few months so we can expand more quickly.”Moini has a background in trial litigation and worked for BigLaw and Sheppard Mullin. She also worked on civil rights issues in Africa including drafting legislation. Co-founder Michael Joseph has a background in engineering and information security.The company sells its services to consumers and other lawyers.“We built this all on our Document Automation Bot, which is available to any lawyer who wants to create similar ‘Turbo-Tax-like’ workflows, either just to streamline their internal work or to contribute to the library of legal tools available to the public,” she said.“Honestly, there aren’t enough people in this field, especially those creating doc automation tools for access to justice. Apps like DoNotPay have gotten a lot of press in this area for their parking ticket app. Our services are more extensive and we provide lawyers with the tools to create their own version of DoNotPay for any area of law.”The pair see their niche is vitally important. Because they focus on issues that other services ignore, they can solve real problems and get real justice for people. Competitors, said Moini, “serve small businesses and higher net worth individuals with needs like wills, trusts, and employment agreements.”“I started HelpSelf because I saw the disparity between the technology available to my legal clients at my law firm and that available to my pro bono clients,” she said. “At the same time, the Trump administration had proposed cutting funding to legal aid to zero from about $400 million. I worked with domestic violence victims and asylum applicants, and set out to build tools that would streamline the process, allowing one lawyer to serve more clients pro bono and allowing individuals to take control of their own legal needs through tech.”Photo by Ian Roseboro on Unsplash",HelpSelf uses simple AI to help those in legal trouble
151,12,12_women_health_pill_cycles,https://techcrunch.com/2017/05/17/clue-now-advises-women-who-forgot-to-take-their-contraceptive-pill/,"Clue now advises women on what to do if they forgot to take their contraceptive pillClue, the app for tracking your period and all things related, surveyed a fraction of its 5 million worldwide users and found out that in some countries a lot of young women didn’t know what to do if they didn’t take their contraceptive pill on time.In Russia, for instance, you can buy the pill without a prescription and Clue found one out of four women there were given zero advice to go along with even the basics about their period. Not taking the pill on time can start a woman’s cycle and could potentially lead to an unwanted pregnancy. Young women not aware of that point could find themselves in an awful situation.Clue’s younger user base now relies on the app for information on topics they are too uncomfortable asking an adult or medical provider, says founder Ida Tin. And in that, she saw an opportunity in education. Now women who forgot to take their pill or took it the wrong way can get advice from the app on what to do next.Clue already helped women track when they were taking their pill as well as whether it was taken late, on time, missed, or double dosed. However, Tin says the app now offers medically validated advice, based on the type of pill you take. The new feature also helps users track the side effects of a combined pill or progestogen-only pill, and could prove useful for those thinking of changing to another type of pill but are too embarrassed to ask a professional about it.Users on the pill will now be able to track categories relevant to the type of pill they take and will see empty dots for days they didn’t take the pill. The feature showing a user’s fertile window will also go away if they opt to instead track contraception.“Clue’s ultimate aim is to help people better understand their bodies by providing them with scientifically accurate, easy to understand information. The Clue app is not a form of contraception, and with the pill being the most commonly used form of birth control, we thought it hugely important to offer our users a feature that not only helps them track their usage, but that also offers immediate and unbiased advice on what to do if a pill is missed or taken late,” Tin said.You can read the rest of the findings in Clue’s worldwide survey of 90,000 women here.",Clue now advises women on what to do if they forgot to take their contraceptive pill
156,12,12_women_health_pill_cycles,https://techcrunch.com/2018/02/21/cognoas-ai-platform-for-autism-diagnosis-gets-first-fda-stamp/,"Cognoa has gained regulatory recognition for its machine learning software as a class II diagnostic medical device for autism — meaning the digital health startup is now positioned to submit an application for full FDA clearance.It’s a first but important regulatory step for a business that was founded back in 2014, and plays in a still nascent digital health space where untested ‘wellness’ apps are far more plentiful than medical technologies with robust data to prove out the efficacy of their interventions.Discussions with the FDA started in early 2017, says Cognoa CEO Brent Vaughan, adding that it’s hoping to gain full FDA clearance this year.He says the ultimate goal for the US startup is to become a standard part of domestic health insurance-covered medical provision — and for that FDA clearance is essential to opening the doors.We first covered the Cognoa at launch in 2014 and the following year when it was still being careful to describe its technology as a screening rather than a diagnostic system.It’s since gathered enough data to be confident in using the ‘D’ word — having run a pilot with 250,000 parents, offering free screening for their children so it could gather more data to refine its machine learning models.“We were lucky that we had investors,” says Vaughan. “There’s not a huge business model in providing free screening services to kids, right, because we were certainly never going to sell ads. That wasn’t the goal.“It took a little patience but in the process of providing free screening and at least showing parents how to navigate their way to the front of a line as more of an information service we were able to build the data models to support a development of a diagnostic device actually a couple of years sooner than we originally thought we would. So it ultimately paid off for us.”Cognoa has raised $20.4M to date. Its main investor is the Chinese private investment group Morningside. Vaughan tells TechCrunch it’ll likely be looking to raise another round by the end of this year.It has also conducted multiple studies over the last 2.5 years across the US, including blinded control trials and side-by-side comparisons of its different versions — working with children’s hospitals and secondary care centers. It now bills its technology as a “pediatric behavioral health diagnostics and digital therapeutics platform”.The initial machine learning model, which was targeted at screening for autism, was based on the work of Stanford pediatrics and psychiatry professor Dennis Wall. The model itself was built by combining and structuring existing datasets of behavioral observations on about 10,000 children.Though, as noted above, Cognoa has continued to refine its autism model with structured contributions from parents participating in the pilot and inputting data via its app. (Aka: If an AI service is free, you’re the training data.)[gallery ids=""1600569,1600568,1600566,1600567""]“In our last study we were able to come through with a sensitivity of greater than 90 per cent,” Vaughan tells TechCrunch. “In our first algorithm… targeting autism, we would find it over 90 per cent of the time — and when we said it was autism it was correct well over 80 per cent of the time.“What we see when we look in the data, and that we’re quite interested by, is when we say it’s autism or it looks like autism and it wasn’t… we were able to show [the FDA] that they were often very similarly related conditions.”Vaughan says a lot of the team’s early work focused on figuring out how to create a product that enables non-healthcare professionals (i.e. parents) to capture robust data in a reproducible way. “One of the… questions that came up quite early, even from early potential investors and clinicians, was can you actually get parents to give you the information on which you could base a clinical diagnostic decision? Can you get them to do this reproducibly without a clinician being in a room?… So we certainly had to address that.I remember sitting down with one venture capitalist who looked at me and said, you know what — you’re never going to find 5,000 parents that are going to do this.“I remember sitting down with one venture capitalist who looked at me and said, you know what — you’re never going to find 5,000 parents that are going to do this. And that are going to be able to do this reproducibly,” he continues. “Within a couple of years we were up over a quarter of a million parents that had actually done it — and we learned a lot about how to reproducibly collect information on which you can build a clinical diagnosis but collecting it outside of the clinical setting. Parents providing us information in their living room in the evening. So that was certainly one major step for us. And in doing that we showed that the unmet need was much, much bigger than we originally had estimated.”As well as aiming to support earlier diagnosis than parents might be able to get if they had to wait for specialist appointments for their child to be monitored in person, Cognoa’s platform provides guidance on actions (it calls them “activities”) parents can take themselves to help manage their children’s condition. Which in turn provides more opportunities for response data to be fed back so its models can keep learning and refining recommendations.While the first focus is autism, with the aim of trying to shrink intervention times to improve long term outcomes for children — given what Vaughan describes as a “well-documented” link between earlier intervention and better autism outcomes — the intent is to address other behavioral conditions too, in time, such as ADHD.“For us we see this — even the autism clearance that we’re looking forward to in the future — that’s just a step down the path of being able to be the platform that can diagnose an entire spectrum of these developmental conditions,” he says.Interestingly, Vaughan concedes that the learning element of AI-based technologies can cause unintended problems in healthcare service provision, saying some clinicians it talked to early on raised concerns that by widening access to autism screening the startup risked making an existing diagnosis bottleneck worse by increasing demand for specialist services without there being a parallel increase in resource to avoid creating even more of a backlog.Which is exactly the kind of serious, knock-on consequence that’s possible when unproven ‘disruptive’ technologies change existing dynamics and bring new pressures to bear on a critical and sensitive industry like healthcare. It also seems especially true of AI technologies which need to be fed with lots of data before they can learn to become really useful.So how to conduct responsible training of machine learning models presents something of an existential challenge for AI and healthcare startup initiatives — and one which has already opened up operational pitfalls for some very well resourced tech giants.“Back in 2014 and 2015 we were really starting down the path of let’s just prove that we can triage these kids and find them earlier. And a lot of people embraced that, but there was certainly some that were pretty thoughtful who said if you guys find the kids earlier and the problem in the system is that kids that are identified and referred to specialists for appointments are currently waiting between one and three years to get a diagnosis, aren’t you just going to be making the problem worse?” he says.“So then we had to sit down and say listen, step one is being able to show that we can just screen these kids. But longer term we think we can really aid in getting a faster diagnosis. But we were very careful to not say, publicly, that we thought that we could diagnose these kids because we thought it would just be too controversial. And the idea of using an AI-based platform, the idea of collecting information primarily from the parent, from the caregiver and from the child, that was pretty controversial.”Another change that’s being driven by AI-based software targeting the healthcare industry is to regulatory regimes — with regulators like the FDA needing to come up with new systems and processes for assessing and managing software designed to get better over time.“The FDA is struggling with how to regulate AI-based software because the idea of the FDA is they look at a version of a product and that product once cleared by the FDA does not change — and the idea of AI and machine learning, which is what our product is based on, is that it’s learning and it gets better,” says Vaughan, talking about its discussions with the FDA. “And so understanding with the FDA how we were going to control and document that learning — those were some of the discussions where we walked in with ideas but not very clear understanding what the outcome would be.”While he believes the FDA will likely take a case-by-case approach to the challenge of regulating AI platforms, he suggests companies will probably have to operate using a versioning system — whereby they restrict ongoing machine learning to the research lab, releasing a next version of a model into the wild only once the step change in their software has also gained regulatory approval.“It’s the algorithm part of the device that [the FDA] feel the strongest about in terms of how they regulate it,” he says. “And keep in mind this is evolving, and their thinking might also evolve on this, but for us they look at the algorithm part and we can certainly, in our software, lock down a current version of the algorithm. And we can allow that to not change in the production version of the product — and at the same time we can have a research arm that’s continuing to evolve. And you could start to think about versioning coming out in the future.”“So I think it’ll be a little bit more of a stair-step approach,” he adds. “With periodic reviews by the FDA. And I think that they’re in parallel trying to think of a way to streamline that approach going forward because of the flexibility that these products have. So I think it’ll be a little bit of a hybrid between continuous machine learning which seems quite difficult and the old style, which was quite waterfall.”This post was amended to correct Cognoa’s total funding figure after it told us CrunchBase‘s listing for it was out of date",Cognoa's AI platform for autism diagnosis gets first FDA stamp
184,12,12_women_health_pill_cycles,https://techcrunch.com/2020/02/05/louise-samet-interview/,"Early-stage European VC firm Blossom Capital is fresh from closing a $185M fund— a big jump up on its prior close. The firm makes just a handful of investments per year, mostly at the Series A stage, working very closely with founders in its portfolio, a strategy it refers to as “high conviction” investing.One of its chosen few is Inne, a Berlin-based femtech startup that’s building a novel, hormone-tracking subscription product for fertility-tracking and “natural” contraception. The aim is to offer a high-tech alternative to taking hormones to prevent pregnancy or using an established barrier method (such as a condom).Inne came out of stealth last fall to announce $8.8M in funding, giving us the first glimpse of the medical device it’s been working on since 2017. This test-at-home hormone tracker is slated to launch in select markets in Scandinavia this year, but at a scale akin to a limited beta. The startup said it would be iterating the product based on feedback from the first usersWe chatted with Blossom Capital partner Louise Samet, who led the fund’s investment in Inne, to get the inside track on that deal and further understand how the fund thinks about femtech and the key challenges and opportunities she sees for founders building products targeted to women.This interview has been edited for length and clarity.TechCrunch: How does the fund approach femtech?Samet: The way we define femtech is products that are built for women. But I think the definition of the term is not necessarily important. The more important thing is the problem the founders are focusing on solving.",Blossom Capitalâs Louise Samet talks hormone tracking and femtech bets
162,12,12_women_health_pill_cycles,https://techcrunch.com/2018/08/29/contraception-app-natural-cycles-facebook-ad-banned-for-being-misleading/,"Natural Cycles, a Swedish startup which touts its body temperature-based algorithmic method for tracking individual fertility as an effective alternative to hormonal birth control, has been wrapped by the UK advertising regulator which today upheld three complaints that an advert the company ran last year via Facebook’s platform was misleading.The regulator has banned Natural Cycles from running the advert again, and warned it against exaggerating the efficacy of its product.The ad had stated that “Natural Cycles is a highly accurate, certified, contraceptive app that adapts to every woman’s unique menstrual cycle. Sign up to get to know your body and prevent pregnancies naturally”, and in a video below the text it had also stated: “Natural Cycles officially offers a new, clinically tested alternative to birth control methods”.The company has leaned heavily on social media marketing to target its ‘digital contraception’ app at young women.“We told Natural Cycles Nordic AB Sweden not to state or imply that the app was a highly accurate method of contraception and to take care not to exaggerate the efficacy of the app in preventing pregnancies,” said the Advertising Standards Authority (ASA) handing down its decision.While Natural Cycles gained EU certification for its app as a contraceptive in February 2017, and most recently FDA clearance for marketing the app as a contraception in the US (with the regulator granting its De Novo classification request this month), those regulatory clearances come with plenty of caveats about the complexity of the product.The FDA, for example, warns that: “Users must be aware that even with consistent use of the device, there is still a possibility of unintended pregnancy.”At the same time, Natural Cycles has yet to back up the efficacy claims it makes for the product with the scientific ‘gold standard’ of a randomized control trial. So users wanting to be able to compare the product’s efficacy against other more tried and tested birth control methods (such as the pill or condoms) are not able to do so.No birth control method (barring abstention) is 100% effective of course but, as we’ve reported previously, Natural Cycles’ aggressive marketing and PR has lacked nuance and attempted to downplay concerns about the complexity of its system and the chance of failure even though the product’s performance is impacted by multiple individual factors — from illness, to irregular periods. Which risks being irresponsible.In the ruling, the ASA flags up the relative complexity of Natural Cycles’ system vs more established forms of contraception — pointing out that:The Natural Cycles app required considerably more user input than most forms of contraception, with the need to take and input body temperature measurements several times a week, recording when intercourse had taken place, supplemented with LH measurements, abstention or alternative methods of contraception during the fertile period.The company also remains under investigation in Sweden by the medical regulator after a local hospital reported a number of unwanted pregnancies among users of the app. A spokesperson for the Medical Products Agency told us that it has finalized its investigation and plans to publish the findings next week.Despite all that, Natural Cycles’ website bills its product as “effective contraception”, claiming the app is “93% effective under typical use” and making the further (and confusingly worded) claim that: “With using the app perfectly, i.e. if you never have unprotected intercourse on red days, Natural Cycles is 99% effective, which means 1 woman out of 100 get pregnant during one year of use.”Perfect use of the app actually means a woman would accurately perform daily measurement of her body temperature without fail or fault, and before she’s even sat up in bed, at least several times a week, correctly inputting the data. Forgetting to do so once because — say — you got up to go to the toilet or were otherwise interrupted before taking or inputting a reading could constitute imperfect use.The BBC spoke to a women who says she made the decision to use the app after seeing that 99% effective claim in Natural Cycles’ marketing on Instagram — and subsequently fell pregnant while using it. “I was sort of sucked into this “99% effective” [claim],” she told the broadcaster. “You know “even more effective than the pill”… What could possibly go wrong?”In its ruling, the regulator said it investigated two issues related to the advert run by Natural Cycles on Facebook on July 20, 2017, and both issues were upheld.The complaints were that Natural Cycles’ advert included misleading and unsubstantiated claims — specifically that the product was: 1. “Highly accurate contraceptive app”; and 2. “Clinically tested alternative to birth control methods”.Natural Cycles told the ASA that the latter claim is in fact a quote from a Business Insider article which it “considered to be correct” and had thus reproduced in its marketing.After taking expert evidence, and reviewing three published papers on accumulated data obtained from the app, the regulator deemed the combination of the two claims to be misleading.It writes:We considered that in isolation, the claim “clinically tested alternative to birth control methods” was unlikely to mislead. However, when presented alongside the accompanying claim “Highly accurate contraceptive app”, it further contributed to the impression that the app was a precise and reliable method of preventing pregnancies which could be used in place of other established birth control methods, including those which were highly reliable in preventing unwanted pregnancies. Because the evidence did not demonstrate that in typical-use it was “highly accurate” and because it was significantly less effective than the most reliable birth control methods, we considered that in the context of the ad the claim was likely to mislead.The ASA also found the advert to have breached rules for substantiation and exaggeration of marketing messages in the Medicines, medical devices, health-related products and beauty products category, as well as being misleading.At the time of writing Natural Cycles had not responded to requests for comment. Update: A spokeswoman has now emailed us the following statement in response to the ASA ruling:We respect the outcome of the investigation by the UK Advertising Standards Authority (ASA) into one Facebook advertisement, which ran for approximately 4 weeks in mid-2017. The investigation was initiated nearly 12 months ago and the advertisement was removed as soon as we were notified of the complaint. This investigation triggered an internal review of all our advertisements and the way that we communicate more broadly, to ensure our message is clear and provides women with the information they need to determine if Natural Cycles is right for them. As part of these efforts, every advertisement now undergoes a strict approval process by a dedicated taskforce to ensure that it gives an accurate overall impression to the viewer. We actively seek feedback from Natural Cycles users to help us improve the quality of our communications and, moving forwards, we plan to work even more closely with HCPs, women and our user community to test and refine our marketing approach. Natural Cycles has been independently evaluated and cleared by regulators in Europe and the US based on clinical evidence demonstrating its effectiveness as a method of contraception.This report was updated with comment from the Swedish medical products regulator, and with comment from Natural Cycles",Contraception app Natural Cyclesâ Facebook ad banned for being misleading
197,12,12_women_health_pill_cycles,https://techcrunch.com/2021/03/26/vibrant-raises-7-5m-for-a-drug-free-mechanical-pill-to-treat-constipation/,"Vibrant, a medical technology company that’s developed a disposable vibrating pill to treat chronic constipation, today announced its Series E for $7.5 million. The company is based in Tel Aviv and is lead by Lior Ben-Tsur, a startup veteran. Since its founding in 2007, the company has raised a total of $25 million. This round is being led by Unorthodox Ventures, with participation by Sequoia.Vibrant, which is going through its third and final round of Food and Drug Administration (FDA) testing, plans to launch in the U.S. in the next year. The capsules are about the size of a multi-vitamin, Ben-Tsur said.“Patients are used to taking drugs day in and day out, so this wouldn’t be a different experience in that regard, but this pill doesn’t have any medication,” Ben-Tsur said. While Ben-Tsur is not a founder, he was brought on about 10 years ago to serve as the company’s CEO.According to a study published in the American Gastroenterological Association, about 16% of American adults suffer from constipation, and the number jumps to 33.5% in adults between the ages of 60-101. Also, constipation is 1.5 times more common in women than in men.The most common way to treat constipation is through the use of over-the-counter or prescription drugs, most of which target the nerves in the colon, which in turn prompt a bowel movement. The Vibrant Capsule, however, “once swallowed, kickstarts the natural impulses of your intestinal wall to contract, relax and get things moving again — without the use of chemicals,” the company said in a statement.In addition to being medication-free, the value of Vibrant over laxatives, according to the company, is that the bowel movements are more controlled, whereas laxatives can cause unexpected diarrhea and long-term side effects. Also, while laxatives are meant to be taken on a daily basis, the disposable capsule can be used anywhere from 2-5 times per week. The capsules connect to an app that automatically records when you take a pill, and upon having a bowel movement, the person notes it in the app, which then sends a monthly report to the patient’s doctor, allowing them to monitor and adjust the treatment protocol as necessary.In a 2019 human trial organized by Vibrant, 250 patients were enrolled in a double-blind study (Vibrant Capsule = 133, placebo = 117). The results showed that those who took the Vibrant Capsule were more likely to experience a bowel movement within three hours. The trial details and the results were published in the journal of Neurogastroenterology and Motility.Several years ago a group of doctors and engineers performed a test in a live pig’s colon, and accidentally pinched the side of the colon wall. As a result, they noticed that the pig promptly had a bowel movement. The test was actually about something totally unrelated to constipation, and the results were a random discovery. To replicate the effects, the team created a vibrating belt that when worn for about three hours, would also cause a bowel movement.“The problem is no one wants to shake for three hours to have a bowel movement,” said Ben-Tsur. With this information in hand, the group set out to develop a treatment for constipation in humans that would produce similar results but where the vibrations couldn’t be felt. There were other mechanical capsules already on the market, such as the Smart Pill, a mechanical diagnostic capsule that reports on generalized motility through the entire digestive tract and aids doctors in diagnosing motility disorders, so the team knew that people could safely swallow and excrete capsules.According to Ben-Tsur, there hasn’t been any development in the treatment of constipation in the last 20 years — the treatment protocol has continued to focus on medication. When he learned about the market size, the lack of innovation in the space and the potential, he was convinced that he wanted to lead Vibrant.Vibrant plans on using this round of funding to take the capsule to market in the U.S. — its first market. The company is currently speaking with healthcare providers and insurance companies so that the capsule will be covered by insurance starting at the time of launch. The Smart Pill, while only used once as a diagnostic test, is still not covered and costs, on average, about $1,400 out of pocket. Ben-Tsur and his team aim to offer a product that is accessible. “From day one we were on a mission to build something that wouldn’t be more expensive than existing drugs,” he said.Early Stage is the premier “how-to” event for startup entrepreneurs and investors. You’ll hear firsthand how some of the most successful founders and VCs build their businesses, raise money and manage their portfolios. We’ll cover every aspect of company building: Fundraising, recruiting, sales, product-market fit, PR, marketing and brand building. Each session also has audience participation built-in — there’s ample time included for audience questions and discussion. Use code “TCARTICLE” at checkout to get 20% off tickets right here.",Vibrant raises $7.5M for a drug-free mechanical pill to treat constipation
152,12,12_women_health_pill_cycles,https://techcrunch.com/2017/06/05/birth-control-app-nurx-now-delivers-to-the-contraceptive-deserts-of-texas/,"About half the counties in Texas don’t have the number of public clinics required to meet the contraceptive needs of the population. So Nurx, an at-home birth control delivery app, decided to give women in the state the option to get birth control whenever they want and without ever needing to step into a clinic or even physically see a doctor.Starting today, those in the Lone Star State will be able to tap the Nurx app and get contraceptives delivered straight to their door.While Texas isn’t the only state with a giant “contraceptive desert,” or an area without at least 1 clinic to every 1,000 women in need of publicly funded contraception, it is certainly the biggest area of land in the United States not meeting these needs.And with Trumpcare looming, and Trump’s recent “Religious Freedom” order, which allows businesses to deny birth control coverage based on religious reasons, many women could lose access to their publicly funded birth control pills and even more publicly funded clinics could go under, leaving a large and vulnerable population wide open to other, possibly dangerous methods of preventing birth.While there are plenty of birth control delivery services out on the market, such as Maven, The Pill Club, Lemonaid and BirthControlBuzz, I had a hard time finding any that delivered in Texas (get at me if you do). That’s not to say they won’t at some point, as each of them could easily open up shop in this area, but it does seem Nurx, which is not a free birth control delivery service, but does provide the pills at a reasonable cost, may have discovered a goldmine of people in need, for the time being.For instance, a little more than half of all pregnancies in Texas were unplanned in 2015, costing taxpayers $2.9 billion that year. However, according to a Guttmacher Institute report, the total gross public savings from preventing unintended pregnancies would have been $2.14 billion if women and couples could be empowered to prevent them. Couple that with the teen birth rate in Texas, which sharply declined by 56 percent over the last two decades, thanks in large part to contraceptives, according to the National Campaign to Prevent Teen and Unplanned Pregnancy.Couple that with an additional estimate of more than 19 million women living in these “contraceptive deserts” nationwide and it’s easy to see adding these types of services could save money at the state level by removing middlemen and increasing access, as well as provide a lucrative area for Nurx and other birth control delivery apps to tap.",Birth control app Nurx now delivers to the âcontraceptive desertsâ of Texas
170,12,12_women_health_pill_cycles,https://techcrunch.com/2019/06/10/what-top-vcs-look-for-in-womens-fertility-startups/,"What top VCs look for in women’s fertility startups Investors are starting to pour millions into women's health tech. Does your startup have what it takes to get their attention?A number of promising women’s health tech companies have popped up in the last few years, from fertility apps to ovulation bracelets — even Apple has jumped into the subject with the addition of period tracking built into the latest edition of the watch. But there hasn’t been much in the way of innovation in women’s sexual health for decades.In-vitro fertilization (IVF) is now a 40-year-old invention and even the top pharmaceutical companies have spent a pittance on research and development. Subjects like polycystic ovarian syndrome, endometriosis and menopause have taken a backseat to other, more fatal concerns. Fertility is itself oftentimes a mysterious black box as well, though a full 10% of the female population in the United States has difficulty getting or staying pregnant.That’s all starting to change as startups are now bringing in millions in venture capital to gather and treat women’s health. While it’s early days (no unicorns just yet) interest in the subject has been jumping steadily higher each year.To shine a better light on the importance of tech’s role in spurring more innovation for women’s fertility, we asked five VCs passionate about the space for their investment strategies, including Sarah Cone (Social Impact Capital), Vanessa Larco (NEA), Anu Duggal (Female Founders Fund), Jess Lee (Sequoia) and Nancy Brown (Oak HC/FT).Sarah Cone, Social Impact CapitalWe’re interested in companies that create large data sets in women’s health and fertility, enabling personalized medicine, clinical trial virtualization, better patient outcomes, and the application of modern AI/ML techniques to generate hypotheses that discover new targets and molecules.",What top VCs look for in womenâs fertility startups
52,12,12_women_health_pill_cycles,https://gizmodo.com/youtube-health-sources-doctors-nurses-1849711635,"In its latest effort to limit health misinformation, YouTube is trying to make it easier for users to identify and differentiate reliable, factual videos made by certified healthcare professionals from those made by wellness gurus and their ilk. Starting today, the platform will let doctors and nurses apply for verified provider labels and showcase their videos on special healthcare carousels in search results.The announcement was made on Thursday by YouTube’s global head of health Dr. Garth Graham, who pointed out in a blog post that this is the first time the platform will extend its health product features to individual healthcare providers. Previously, the features were available to organizations like universities and government agencies. YouTube has historically struggled to rein in medical misinformation, most recently during the pandemic, and has been criticized for dragging its feet on implementing fixes for the problem.AdvertisementAt the moment, only individuals in the U.S. and Germany will be able to apply to be YouTube Health Sources, although YouTube stated that the company hopes to be able to open up the process to more countries and regions in the future.“This is a big step towards helping people more easily find and connect with content that comes from the extraordinary community of healthcare creators on YouTube–the smart, dedicated and creative folks who are transforming the ways that we share medical information,” Graham said in a YouTube blog post.The company’s global head of health pointed to channels helmed by licensed individuals that provide helpful health content on the platform, such as Doctor Mike, a family doctor who debunks medical myths, and Doctor Ali, a psychologist who makes easily accessible videos on mental health.AdvertisementWhat will qualify someone to be a YouTube Health Source? According to the platform’s support page, the application process is open to licensed doctors, nurses, registered nurses, psychologists, marriage and family therapists, and licensed clinical social workers. All applicants must be licensed, YouTube notes, adding that third-party partner LegitScript will be in charge of coordinating license verification by working with licensing bodies like the Federation of State Medical Boards in the U.S.Besides obtaining license verification, YouTube states that channels must follow its monetization policies, even if it’s not monetizing content. Applicants must also have more than 2,000 valid public watch hours in the past 12 months and primarily focus on covering health information. Finally, individuals interested in being a Health Source must have no active community guidelines strikes.AdvertisementGetting accepted doesn’t mean that you’ll be a YouTube Health Source forever, though. On its support page, the platform points out that channels are periodically reassessed and may lose access to YouTube health features if the company finds they no longer meet the criteria.Interested individuals can apply here. YouTube states that it will review applicants’ channels and get back to them with an answer in within one or two months. Individuals who are accepted may start getting access to the features in early 2023.",YouTube Is Making It Easier to Tell the Difference Between Real Doctors and Quacks
176,12,12_women_health_pill_cycles,https://techcrunch.com/2019/09/29/badass-millennial-women-are-supercharging-startup-investments/,"Across the political, social and economic stage, women’s issues are finally receiving heightened attention and priority.There are more women than ever seeking political office; funding for female-founded startups is reaching record levels (even if they still have a long way to go to reach gender parity); a sizable cohort of female-founded and led companies have achieved billion-dollar unicorn valuations; and several women-led companies, including PagerDuty, The RealReal, and Eventbrite, have entered the public markets with successful IPOs.What’s driving so much positive change?Clearly, broadened awareness of gender and power issues, largely due to #MeToo, as well as an increase in the number of female investors, thanks to groups like All Raise, are all contributing catalysts. In addition, women now outnumber men in college, a majority of American moms are in the workforce, and in 40 percent of households those women are the breadwinners. But it’s more than that; I believe that there’s a profound generational shift afloat, and that this first wave of female-led unicorns is just the tip of the NASDAQ iceberg.Unlike previous generations who may have either looked at self-investment as self-indulgence or who simply didn’t have the resources or technology available to make supplementary investments in themselves, today’s badass millennial women are unapologetic about their desire to invest in their own success and well-being. Determined to succeed without compromising their values or physical and mental wellness, these uber-empowered millennial women are making viable a new generation of startups to help them realize their dreams and feel comfortable in their skin. I refer to this economic wave as She-conomy 2.0.For decades now there have been tech companies, which I refer to as She-conomy 1.0 , catering to traditional and homogeneous identities of women primarily as shoppers and caregivers. In contrast, these new modern She-conomy 2.0 brands address latent, historically unmet, often un-discussed and under-served needs that speak to the multitude of other facets of our identities.These companies have less to do with what women buy and more to do with their willingness to invest in themselves — in their careers and in their physical and emotional health and well-being. They are seeking and are willing to pay for products and services that help them advance their careers, feel comfortable about their bodies, and provide the physical and emotional support they’re seeking.Women are taking control of their careers and supporting each other.More than two decades ago, when I had my first child, I joined a mom’s group at Stanford Hospital. We were all working moms trying to juggle career and motherhood. It was a truly challenging time for each of us. The group provided such helpful support that we met every Monday evening for five years until our kids were in kindergarten. Why Mondays? Because Mondays are especially hard for working parents, marking yet another week in search of balance. We realized that meeting on Monday evenings provided us with the support we needed to make it through the work week. Perhaps even more critically, it gave us something about Mondays to look forward to.There’s something incredibly empowering about experiencing a major transition like a new job or new parenthood as part of a cohort. Sheryl Sandberg famously sought to institutionalize this kind of support for working women with her non-profit Lean In. It has dramatically raised awareness around working women’s struggles. However, individual Lean In group leaders are usually volunteers running these sessions on the side while working and shouldering life’s endless list of other responsibilities.Now a new generation of organizations is offering this support — for a fee. As for-profit organizations, they’re doing so in a scalable, consistent and reliable way. Women don’t have to worry about whether the organizer will be able to carve out time to orchestrate a meeting because doing so is the organizer’s job. Chief, Declare, The Assembly * , The Wing and The Riveter are all examples of companies that are growing and thriving because they’re offering valuable space, support and services that women are willing to pay for. Most of these organizations initially targeted millennials, but women of all generations are benefiting and participating.Women are changing the narrative around previously taboo topics and promoting inclusiveness and acceptance of oneself.It wasn’t long ago that mannequins, much like cover models, only came in one size. Now mainstream brands not only sell broader offerings; they increasingly showcase them in magazines, catalogs, stores and the runway. For example, Nike’s flagship store in London featured both plus-sized mannequins and para-sport mannequins for people with physical and intellectual abilities, and Rhianna’s new inclusive lingerie line regularly presents both plus-size and pregnant models.Millennials (like all of us) don’t want to feel shamed; they want to feel empowered and beautiful. Instead of settling for frumpy, ill-fitting clothing or outdated product design, millennials are using their social media megaphones to tell the market what they want. Traditional companies like Victoria’s Secret have moved at a molasses-like pace to evolve from treating women as objects of fantasy to celebrating their right to feel great about themselves. Their antiquated practices have created the opportunity for new startups to create brands centered on body positivity. Some companies are filling largely underserved market needs by catering exclusively to larger and specialty sizes, and others are addressing previously taboo topics like body hair, which also contribute strongly to feelings around body positivity. Eloquii offers extended clothing sizes, Ruby Ribbon * and Third Love provide a wide sizing range of under garments and bras, and Fur addresses body hair and grooming.Women are dedicating more attention to their own health and relationships.Self-help books have been around for ages, but tech is paving the way for a new generation of services to provide guidance and support that are more convenient and targeted. At the same time, women are increasingly willing to discuss health issues that were previously taboo, like menstruation, menopause and perimenopause, fertility, and depression. Advancements in technology are making health-related self-care more accessible from the convenience of our wristbands and phones. Meanwhile, people are spending a disproportionate amount of their wealth on health, making the entire healthcare industry ripe for disruption.All of these factors are making femtech big business. Countless new companies are helping women take more active control of their sexual health, including birth control and STI testing (Pill Club and Nurx), period tracking (Flo Health), fertility and egg freezing (Kind Body and Carrot Fertility), menopause (Rory, Genneve), postpartum depression and miscarriage (Maven) and even our relationships (Relish* and Bumble). In addition, no shortage of femtech companies are addressing period care, such as Lola, Cora, The Flex Company, Thinx, and Sustain Natural.These companies are only viable because so many women — beginning with millennials but expanding out to the rest of us — are now willing and able to invest in themselves. United across a shared mission of female empowerment and inclusivity, She-onomy 2.0 is making it more realistic than ever to empower us to advance our careers, feel good about ourselves and stay healthy. Hats off to the badass millennial women leading this charge; we’re all better off professionally, emotionally and even physically thanks to you!*Denotes portfolio company for Trinity Ventures",Badass millennial women are supercharging startup investments
188,12,12_women_health_pill_cycles,https://techcrunch.com/2020/08/28/femtech-poised-for-growth-beyond-fertility/,"The market for female-focused health products (aka “femtech”) is set for growth via segmentation, per an analyst note from PitchBook which identifies opportunities for entrepreneurs to target a growing number of health issues that specifically affect women or affect women in a specific way — broadening out from a traditional focus on reproductive health.Femtech remains a “significantly underdeveloped” slice of healthtech, according to the analysis, which highlights the disparity between how much women spend annually on medical expenses — estimated at ~$500 billion — versus how little healthcare R&D is targeted specifically at women’s health issues (a mere 4%).Last year the global market for female-focused health products generated $820.6 million, per the note, and is estimated to reach at least $3 billion by the end of 2030. It says femtech posted $592.1 million in VC investment in 2019, slightly down on 2018’s $620.3 million. But so far this year it’s racked up $376.2 million in VC across 57 deals — putting it on pace to match 2019’s funding levels.Areas of growth opportunity PitchBook sees for femtech outside its traditional focus on reproductive health are: Endometriosis, a painful disorder of the womb lining affecting one in 10 women; what it calls “personalized and female-oriented approaches to general health & disease management,” with a specific focus on heart health, pain management, and diabetes and weight management within that; and the life-stage transition of menopause.“While we still view femtech as a niche industry, we believe secular drivers could help propel new growth opportunities in the space,” write analysts Kaia Colban and Andrew Akers. “These include the increasing representation of women in the venture-backed technology community, rising awareness and acceptance of women’s health issues, and the growing prevalence of infectious diseases among women in some countries in Africa and Asia.“Furthermore, while the majority of femtech products have traditionally focused on reproductive health, we believe new approaches to women’s health research will help open the door to new products and services.”Expansion of the vertical is being driven by universal growth of the personalized medicine industry — which PitchBook notes is expected to reach $3.2 trillion by 2025, registering a CAGR of 10.6% over the forecast period.While the massive underrepresentation of women in the venture community goes a long way to explaining the relative lack of attention investors have paid to products addressing women’s health — with the note acknowledging pitching to male investors remains a challenge for femtech startups — it suggests investors have also been cool on the subcategory because of a relatively poor track record of “sizable” exits.“Only six femtech exits were completed in 2019; however, this still represents a 64% increase in exit value compared to 2018,” it writes. “The largest exits in recent years include Progyny’s $130M IPO and Procter & Gamble’s acquisition of This is L. for $100M. Progyny’s stock has roughly doubled in the eight months since it went public.”PitchBook says it expects just 14% of VC to go toward female-founded startups this year — further noting that only 17% of startups have at least one female founder. (For femtech startups the figure is considerably higher — yet still only 69% of those PitchBook tracks; NB, this does not include startups building products targeted at women where there isn’t a medical need, such as skincare & beauty etc.)“However, we believe these barriers may be subsiding as male investors begin to recognize the femtech market opportunity and as the VC world becomes more gender-diverse,” it adds, noting that female-founded companies deliver over twice as much per dollar invested than their male-owned counterparts, which it reckons could help to turn more investors’ heads.Other key industry growth drivers the note points to are a conducive regulatory environment; a rise in preventative medicine & holistic health; and advancements in health technology that have made personalized products more accessible and affordable, such as AI and “cloud-based infomatics.”On the M&A front, PitchBook notes this is most common for femtech startups in the general health & wellness category. And while most remain single-product companies, it says it expects a maturing femtech industry to lead to product diversification — “potentially driven by M&A” — noting recent examples of pregnancy-focused apps tapping into the menopause market, which it says suggests an expanding opportunity for fertility startups.",Femtech poised for growth beyond fertility
185,12,12_women_health_pill_cycles,https://techcrunch.com/2020/02/09/hannah-seal-interview/,"Index Fund’s portfolio is driving long-overdue innovation in femcare 'It’s a category that’s been underserved for a long time,' says principal Hannah SealU.K. startup Daye is rethinking female intimate care from a woman’s perspective, starting with a tampon infused with cannabidiol that tackles period pain.It’s also quietly demolishing the retrograde approach to “femcare” product design that not only peddles stale and sexist stereotypes, but also can harm women’s bodies.Those perfumed sanitary pads stinking out the supermarket shelf? Whomever came up with that idea has obviously never experienced thrush or bacterial vaginosis nor spoken to a health professional who could have told them vaginal infections can be triggered by perfumed products.The missing link: There are few people with a vagina in positions leading product strategy. And that’s the disruptive opportunity female-led femcare businesses like Daye are closing in on.The Index Ventures-backed startup is shaking up a tired category by selling the flip-side: thoughtfully designed products for period care that do no harm and take aim at actual problems women have, starting with dysmenorrhea (otherwise known as menstrual cramps). The overarching strand is building community to help women better understand what’s going on with their bodies and reinforce shifting product expectations in the process.We chatted with Index principal Hannah Seal about the fund’s investment in Daye and to get her thoughts more broadly on a new generation of female-focused startups that are driving long-overdue innovation.The interview has been edited for length and clarity.",Index Fundâs portfolio is driving long-overdue innovation in femcare
199,12,12_women_health_pill_cycles,https://techcrunch.com/2021/05/25/acuitymd-raises-7m-to-better-track-the-evolving-world-of-medical-hardware/,"In a world defined by tons of noise and little signal, startups that make it easier for consumers to make a choice just make sense. Career Karma helps students pick a tech bootcamp, Stackin’ helps millennials navigate the world of neobank and savings apps and a new Boston-based company is helping doctors keep track of the most up-to-date medical devices on the market.AcuityMD, founded in 2019 by Mike Monovoukas, Lee Smith and Robert Coe, is an enterprise software company that wants to unlock the often siloed world of medical device data. And to do so, it landed a $7 million seed round this week, led by Benchmark.As part of the deal, Benchmark GP Eric Vishria will join AcuityMD’s board of directors. Ajax Health, which closed $100 million in 2019 to back medtech companies, also participated in the round. With seed capital, Monovoukas said that AcuityMD plans to double its eight-person team by the end of the year, and invest heavily in one specific area: “product, product, product.”AcuityMD is a data platform that tracks the entire medical device lifecycle — from the sale of an item to a patient’s outcome after a surgery. It aggregates industry and market data on individual medical devices to give a metadata of sorts on a singular product.“There are thousands of products being launched each year and so it’s almost impossible for a surgeon, after they’ve graduated fellowship or residency to keep track of the latest and greatest medical technology out there,” Monovoukas said. “We view this as a software and coordination problem, where you have all this data out there and it’s inefficient in getting to the decision-maker.”Monovoukas experienced inefficiencies in medical device management firsthand when a family member needed to go through a series of surgeries.“What was fascinating to me is that the manufacturer holds a lot of information about what to use, [and] sometimes that didn’t get disseminated to the right surgeon at the right point of time,” he said. “The fundamental realization I had was that the information flow in this industry was a little bit broken…It wasn’t an issue of being competent doctors or surgeons, but a lack of information transfer, which is kind of crazy in a data-driven world.”So, the entrepreneur, who worked at Bain & Company as well as a medical device company before that, began thinking of how to use data to make medical devices more responsive to long-term patient outcomes. While doctors were a key stakeholder set to benefit from more information, AcuityMD’s team landed on selling to device manufacturers as their key customer.Now, part of that reason might be because hospitals and doctors are notoriously a pain to sell to. The other reason, Monovoukas tells me, is that performance data on medical devices is a key signal that sales teams within manufacturers can use to beef up, and better target, their pitches. The co-founder explained how manufacturers want visibility into the market for their products, ranging from data on where high-volume surgeons might need one of their devices to long-term outcomes on certain devices over time.The data could help a sales rep pre-emptively figure out how targeting 10 surgeons for a specific product impacts the manufacturer financially and in context with the rest of the market.AcuityMD is teeing itself up to become a real-time database of medical devices. Long-term, it could position itself as a Same Day Shipping service connecting manufacturers to surgeons in high-demand and vital transactions. Monovaukus says that while logistics and inventory is a “visceral” problem for the medical device industry, it doesn’t have a solution in place yet. He could see the startup getting to a point in the future where they can predict inventory levels required at each facility — similar to how some companies like Medinas, co-founded by Chloe Alpert, operate and manage within hospital systems.But for now, AcuityMD thinks it can best use its platform and millions of venture-backed capital outside the provider system. It sources a lot of its data on hospitals and surgeons from Medicare CMS and insurance companies, so leaving no action required on the end of providers.A challenge will be making sure those data sources are good enough to extract true signals. The startup is still defining good.“I heard someone once say that any digital health company eventually becomes a healthcare data company,” he said. “We’re approaching things a little bit differently.”",AcuityMD raises $7M to better track the evolving world of medical hardware
149,12,12_women_health_pill_cycles,https://techcrunch.com/2017/03/21/the-mednet-launches-its-quora-for-cancer-an-online-medical-knowledge-base/,"A New York City startup called The Mednet today launched a platform that gives physicians a knowledge-sharing tool that’s as easy to use as Quora, but provides them with expert answers about the latest research in their field. The site has focused, so far, strictly on cancer.Due to rules governing medical information and patients’ privacy, questions posted to The Mednet cannot be case-based. They are situation-based only, meaning doctors don’t share patient info, not even blurred photos.While the startup’s site, theMednet.org, has been in development for about 2.5 years, the company officially launched today and is part of the latest batch of the Y Combinator accelerator. Co-founders Nadine Housri, a radiation oncologist, and her brother CEO Samir Housri told TechCrunch their company has raised some grant funding and equity funding to date, including from YC and The Hope Foundation.Results on the platform have been hope-inspiring so far to Nadine Housri, she said. Cancer experts regularly help each other there to figure out complex issues that will immediately impact their patients. For example, a new study published in the prestigious New England Journal of Medicine came out, and an investigator in that study, Howard Sandler, answered questions about it on The Mednet, helping other doctors decide whether or not to use hormones, along with radiation, to treat a prostate cancer patient.His study had found that adding hormonal therapy to radiation treatment improved the average long-term survival of men with prostate cancer who have had their prostate gland removed. But of course, the science was complicated and the regimen wasn’t recommended for any or every patient.Nadine Housri said when theMednet.org first launched, she wasn’t sure if oncologists, professors, department chairs and other cancer researchers would be too busy to give away their expertise on some new online platform. But, she explained, “Experts are willing to give away info to random people here because they constantly field phone calls, emails and answer questions at medical conferences anyway. One reason you go into academia or medicine is to have a great impact on people, on your community. On Mednet experts can put their answers out there, clear up misconceptions on research and clinical practices.”Like many startups, The Mednet has spent its early years focused on building an expert user community, and becoming a vital resource to users, as well. The Housris said their network will always be accessible for free to doctors, and they want it to serve as a knowledge base for medicine broadly. The company plans to expand beyond oncology over the long run.It is exploring the potential to generate revenue by helping companies raise awareness of and enroll patients in their clinical trials. It could also aggregate some of its expert answers into automated decision support for clinics, the founder said. Depending on its monetization strategy, The Mednet could compete with online resources for physicians like Figure 1 or UpToDate over the long run. But when it comes to medicine, there probably can’t be too many tools to help people get good, up-to-date information.","The Mednet launches its âQuora for cancer,â an online medical knowledge base"
