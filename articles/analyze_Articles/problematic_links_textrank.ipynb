{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import pytextrank\n",
    "import ssl\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For ignoring SSL ceritficate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class infine_scroll(object): \n",
    "  def __init__(self, last):\n",
    "    self.last = last\n",
    "\n",
    "  def __call__(self, driver):\n",
    "    new = driver.execute_script('return document.body.scrollHeight')  \n",
    "    if new > self.last:\n",
    "        return new\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_html_from_url(url):\n",
    "  chrome_options = Options()\n",
    "  chrome_options.add_argument(\"--headless\")\n",
    "  browser = webdriver.Chrome('C:\\Program Files\\chromedriver_win32 (1)\\chromedriver', options=chrome_options) \n",
    "  browser.set_page_load_timeout(30) \n",
    "  browser.get(url)\n",
    "   \n",
    "  last_height = browser.execute_script('return document.body.scrollHeight')\n",
    "\n",
    "  flag=1\n",
    "\n",
    "  while flag==1:\n",
    "    \n",
    "    try:\n",
    "       browser.execute_script('window.scrollTo(0,document.body.scrollHeight)')\n",
    "       wait = WebDriverWait(browser, 10)\n",
    "\n",
    "       new_height = wait.until(infine_scroll(last_height))\n",
    "       last_height = new_height\n",
    "\n",
    "    except:\n",
    "        flag = 0\n",
    "  html = browser.page_source\n",
    "  return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pytextrank.base.BaseTextRankFactory at 0x161c5358250>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load a spaCy model, depending on language, scale, etc.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# add PyTextRank to the spaCy pipeline\n",
    "nlp.add_pipe(\"textrank\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df = pd.read_csv(r\"C:\\Users\\Ravit\\Documents\\rnd\\horizon_scanning_lab\\articles\\analyze_Articles\\reddit_articles_analysis\\reddit_problematic_articles.txt\", sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df = prob_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_urls = prob_df.index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, url in enumerate(prob_urls):\n",
    "    prob_urls[i] = url.split('\"')[0].strip(',').strip('[').strip(']').replace(\"'\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.newsweek.com/baby-alpaca-learns-walk-again-prosthetic-leg-adorable-clip-1749410?',\n",
       " 'https://academic.oup.com/humupd/article/28/4/457/6555833?login=false',\n",
       " 'https://www.fiercebiotech.com/research/ut-austin-scientists-design-safer-cas9-improved-crispr-gene-editing-accuracy',\n",
       " 'https://www.wsj.com/articles/metas-facebook-says-its-new-vr-headset-could-replace-workers-pcs-11665521118?st=w9xk0vs5je9lyo1&reflink=share_mobilewebshare',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S1877050922012777',\n",
       " 'https://www.lightreading.com/security/the-cloud-and-5g-security-apocalypse-is-only-matter-of-time/d/d-id/781259',\n",
       " 'https://www.abc.net.au/news/science/2022-10-26/extreme-miyake-radiation-events-tree-rings-solar-storms/101563738',\n",
       " 'https://onlinelibrary.wiley.com/doi/full/10.1002/smj.3459',\n",
       " 'https://www.wsj.com/articles/solar-rollout-rouses-resistance-in-europes-countryside-11665234001',\n",
       " 'https://www.autoevolution.com/news/harvard-engineers-invent-a-solid-state-battery-that-never-dies-it-s-a-game-changer-198518.html',\n",
       " 'https://www.science.org/doi/10.1126/sciadv.abq0135',\n",
       " 'https://www.pnas.org/doi/10.1073/pnas.2210525119',\n",
       " 'https://www.science.org/doi/10.1126/sciadv.add3726',\n",
       " 'https://thehill.com/policy/energy-environment/3669504-supreme-court-to-hear-case-that-could-have-massive-impact-on-us-water-quality/',\n",
       " 'https://www.wsj.com/articles/methane-emissions-from-oil-and-gas-wells-are-much-higher-than-thought-study-shows-11664474402?mod=hp_lead_pos6',\n",
       " 'https://www.sciencedirect.com/science/article/abs/pii/S0025326X22007627?via%3Dihub',\n",
       " 'https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2022JD036761',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0269749122011022?via%3Dihub',\n",
       " 'https://www.science.org/doi/10.1126/sciadv.abq6974',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0091743522003140',\n",
       " 'https://academic.oup.com/eurheartj/advance-article/doi/10.1093/eurheartj/ehac613/6770665?login=false',\n",
       " 'https://www.sciencedirect.com/science/article/abs/pii/S0925492722001172',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0306460322002908',\n",
       " 'https://doi.org/10.1016/j.humov.2022.103016',\n",
       " 'https://onlinelibrary.wiley.com/doi/full/10.1111/obr.13462',\n",
       " 'https://www.uq.edu.au/news/article/2022/09/research-shows-water-fluoridation-safe-children',\n",
       " 'https://www.sciencedirect.com/science/article/abs/pii/S0149763422003967',\n",
       " 'https://www.sciencedirect.com/science/article/abs/pii/S0379073822002687',\n",
       " 'https://onlinelibrary.wiley.com/doi/epdf/10.1111/add.15911?%2F%3Futm_source=google&utm_medium=cpc&utm_campaign=Campaign_Sciad_R3MR425_Hybrid_NursingDentistry',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0091743522003115',\n",
       " 'https://www.tomshardware.com/news/lithograhy-tool-russia-7nm-2028',\n",
       " 'https://www.tomshardware.com/news/1200w-power-requirement-rtx-4090',\n",
       " 'https://academic.oup.com/zoolinnean/advance-article-abstract/doi/10.1093/zoolinnean/zlac089/6768673',\n",
       " 'https://www.techdirt.com/2022/10/26/signal-says-it-will-exit-india-rather-than-compromise-its-encryption/',\n",
       " 'https://journals.sagepub.com/doi/10.1177/09567976221118541',\n",
       " 'https://www.tandfonline.com/doi/abs/10.1080/02699931.2022.2128064?journalCode=pcem20',\n",
       " 'https://journals.sagepub.com/doi/10.1177/09567976221112936',\n",
       " 'https://siliconangle.com/2022/10/05/intel-hits-major-milestone-moves-toward-mass-production-quantum-computer-chips/',\n",
       " 'https://www.wsj.com/articles/chinas-factories-accelerate-robotics-push-as-workforce-shrinks-11663493405',\n",
       " 'https://www.thegamer.com/discord-bans-68000-servers-55-million-accounts/',\n",
       " 'https://www.techdirt.com/2022/10/19/yes-buyout-of-parler-looks-very-much-like-a-failed-company-taking-advantage-of-troubled-rich-guy/',\n",
       " 'https://journals.sagepub.com/doi/10.1177/00197939221129261',\n",
       " 'https://www.tandfonline.com/doi/abs/10.1080/03057240.2022.2109606?journalCode=cjme20',\n",
       " 'https://www.sciencedirect.com/science/article/abs/pii/S009265662200126X',\n",
       " 'https://academic.oup.com/nsr/advance-article/doi/10.1093/nsr/nwac200/6712344?login=false',\n",
       " 'https://thehill.com/policy/energy-environment/3685583-nasa-suggests-new-space-cooling-technology-could-charge-electric-cars-in-5-minutes/',\n",
       " 'https://thehill.com/opinion/technology/3647216-china-has-returned-helium-3-from-the-moon-opening-door-to-future-technology/',\n",
       " 'https://apnews.com/article/business-kamala-harris-seattle-washington-pollution-16405c66d405103374d6f78db6ed2a04',\n",
       " 'https://apnews.com/article/technology-germany-europe-berlin-climate-and-environment-75afb2820d22c3d53b136b2ab2841767']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get websites. This is to learn what parsing tags are required to extract text\n",
    "\n",
    "websites = []\n",
    "for url in prob_urls:\n",
    "    websites.append(url.split('/')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = list(set(websites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text_textrank(doc, rank_threshold=0.0):\n",
    "    # examine the top-ranked phrases in the document\n",
    "    rank_d = {\"text\":[], \"rank\":[], \"count\":[]}\n",
    "\n",
    "    for phrase in doc._.phrases:\n",
    "        if phrase.rank >= rank_threshold:\n",
    "            rank_d['text'].append(phrase.text)\n",
    "            rank_d['rank'].append(phrase.rank)\n",
    "            rank_d['count'].append(phrase.count)\n",
    "    \n",
    "    return rank_d "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find parsers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'siliconangle.com'\n",
    "title = soup.find('h3', {'class': 'sa-post-title'}).text\n",
    "body = soup.find('div', {'class': 'single-post-content'}).find_all('p')\n",
    "text = \"\".join([p.find_next(text=True).strip() for p in body])\n",
    "\n",
    "\n",
    "'www.newsweek.com' \n",
    "title =  soup.find('h1', {'class':'title'}).text\n",
    "body = soup.find('div', {'class':'article-body v_text'}).find_all('p')\n",
    "text = \"\".join([p.find_next(text=True).strip() for p in body])\n",
    "\n",
    "'www.thegamer.com'\n",
    "title = soup.find('h1', {'class':'heading_title'}).text\n",
    "body = soup.find('section', {'id':'article-body'}).find_all('p')\n",
    "text = \"\".join([p.find_next(text=True).strip() for p in body])\n",
    "\n",
    "'apnews.com'\n",
    "title = soup.find('h1', {\"class\":\"Component-heading-0-2-16\"}).text\n",
    "body = soup.find('div', {\"class\":\"Article\"}).find_all('p')\n",
    "text = \"\".join([p.find_next(text=True).strip() for p in body])\n",
    "\n",
    "'www.techdirt.com'\n",
    "#didn't succeed\n",
    "\n",
    "'agupubs.onlinelibrary.wiley.com'\n",
    "#didn't succeed\n",
    "\n",
    "'www.uq.edu.au'\n",
    "title = soup.find('h1', {'id':\"page-title\"}).text\n",
    "body = soup.find('div', {'id':\"content\"}).find_all('p')\n",
    "text = \"\".join([p.find_next(text=True).strip() for p in body])\n",
    "\n",
    "'www.fiercebiotech.com'\n",
    "#didn't succeed\n",
    "\n",
    "academic.oup.com\n",
    "#didn't succeed\n",
    "\n",
    "www.science.org\n",
    "#didn't succeed\n",
    "\n",
    "www.wsj.com\n",
    "title = soup.find('h1', {\"class\":\"css-1lvqw7f-StyledHeadline e1ipbpvp0\"}).text\n",
    "text = soup.find('section', {'class':'css-az2xkl-Container-Container e1d75se20'}).text\n",
    "\n",
    "ww.tandfonline.com\n",
    "title = soup.find(\"span\", {\"class\":\"NLM_article-title hlFld-title\"}).text\n",
    "text = soup.find(\"div\", {\"class\":\"abstractSection abstractInFull\"}).text\n",
    "\n",
    "www.abc.net.au\n",
    "title = soup.find(\"h1\", {\"class\":\"_1EAJU hMmqO WL4Yr n-Wqw _18EFj _2ZOIT _3HiTE x9R1x pDrMR hmFfs _390V1\"}).text\n",
    "body = soup.find(\"div\", {\"class\":\"_3P3cP _3sFAh\"}).find_all('p')\n",
    "text = \"\".join([p.find_next(text=True).strip() for p in body])\n",
    "\n",
    "onlinelibrary.wiley.com\n",
    "#didn't succeed\n",
    "\n",
    "'www.sciencedirect.com'\n",
    "#didn't succeed\n",
    "\n",
    "www.autoevolution.com\n",
    "#didn't succeed\n",
    "\n",
    "www.pnas.org\n",
    "#didn't succeed\n",
    "\n",
    "www.tomshardware.com\n",
    "title=soup.find(\"title\").text\n",
    "body = soup.find(\"div\",{\"id\":\"article-body\"})\n",
    "text = \"\".join([p.find_next(text=True).strip() for p in body])\n",
    "\n",
    "thehill.com\n",
    "title=soup.find(\"title\").text\n",
    "body = soup.find(\"div\", {\"class\":\"article__text | body-copy | flow\"}).find_all('p')\n",
    "text = \"\".join([p.find_next(text=True).strip() for p in body])\n",
    "\n",
    "\n",
    "\n",
    "journals.sagepub.com\n",
    "#not working\n",
    "\n",
    "www.lightreading.com\n",
    "#not working\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#websites that work with requests\n",
    "requests_webs = ['siliconangle.com', 'www.newsweek.com', 'www.thegamer.com',  'apnews.com', 'www.uq.edu.au', 'www.wsj.com', \n",
    "'www.tandfonline.com', 'www.abc.net.au', 'www.tomshardware.com', 'thehill.com']\n",
    "\n",
    "#websites that don't\n",
    "no_requests_webs = ['www.techdirt.com', 'agupubs.onlinelibrary.wiley.com', 'www.fiercebiotech.com', 'academic.oup.com', 'www.science.org', 'onlinelibrary.wiley.com', 'www.sciencedirect.com', 'www.autoevolution.com', 'www.pnas.org', 'journals.sagepub.com', 'www.lightreading.com']\n",
    "\n",
    "#doi.org => link not extracted properly\n",
    "#remove from list\n",
    "prob_urls.remove('https://doi.org/10.1016/j.humov.2022.103016')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_urls = [url for url in prob_urls if url.split('/')[2] in requests_webs]\n",
    "still_prob_links = [url for url in prob_urls if url.split('/')[2] in no_requests_webs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:404: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(request_urls[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.wsj.com/articles/solar-rollout-rouses-resistance-in-europes-countryside-11665234001'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_title_df = ['article_keywords', 'title_keywords', 'count']\n",
    "title_df = pd.DataFrame([], columns=cols_title_df)\n",
    "\n",
    "cols_text_df = ['sentence', 'sentence_rank', 'count']\n",
    "text_df = pd.DataFrame([], columns=cols_text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ravit\\AppData\\Local\\Temp\\ipykernel_14116\\981321935.py:4: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome('C:\\Program Files\\chromedriver_win32 (1)\\chromedriver', options=chrome_options)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cols_text_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Ravit\\Documents\\rnd\\horizon_scanning_lab\\articles\\analyze_Articles\\problematic_links_textrank.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ravit/Documents/rnd/horizon_scanning_lab/articles/analyze_Articles/problematic_links_textrank.ipynb#X20sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m title_dict \u001b[39m=\u001b[39m analyze_text_textrank(title_doc, threshold_rank)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ravit/Documents/rnd/horizon_scanning_lab/articles/analyze_Articles/problematic_links_textrank.ipynb#X20sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m temp_text_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame\u001b[39m.\u001b[39mfrom_dict(text_dict)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Ravit/Documents/rnd/horizon_scanning_lab/articles/analyze_Articles/problematic_links_textrank.ipynb#X20sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m temp_text_df\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m cols_text_df\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ravit/Documents/rnd/horizon_scanning_lab/articles/analyze_Articles/problematic_links_textrank.ipynb#X20sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m temp_text_df[\u001b[39m'\u001b[39m\u001b[39marticle_link\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m [url]\u001b[39m*\u001b[39m\u001b[39mlen\u001b[39m(temp_text_df)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ravit/Documents/rnd/horizon_scanning_lab/articles/analyze_Articles/problematic_links_textrank.ipynb#X20sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m text_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([text_df, temp_text_df])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cols_text_df' is not defined"
     ]
    }
   ],
   "source": [
    "threshold_rank =0.05\n",
    "\n",
    "for i, url in enumerate(request_urls):\n",
    "    print(i)\n",
    "    web = url.split('/')[2] \n",
    "    if web in requests_webs:\n",
    "        try:\n",
    "            result = get_html_from_url(url)\n",
    "        except:\n",
    "            print(\"url crashed\")\n",
    "            still_prob_links.append(url)\n",
    "     \n",
    "        soup = BeautifulSoup(result)\n",
    "        if soup is None:\n",
    "            still_prob_links.append(url)\n",
    "            continue\n",
    "\n",
    "        if web == 'siliconangle.com':\n",
    "            try:\n",
    "                title = soup.find('h3', {'class': 'sa-post-title'}).text\n",
    "                body = soup.find('div', {'class': 'single-post-content'}).find_all('p')\n",
    "                text = \"\".join([p.find_next(text=True).strip() for p in body])\n",
    "            except:\n",
    "                still_prob_links.append(url)\n",
    "\n",
    "\n",
    "        if web=='www.newsweek.com':\n",
    "            try:\n",
    "                title =  soup.find('h1', {'class':'title'}).text\n",
    "                body = soup.find('div', {'class':'article-body v_text'}).find_all('p')\n",
    "                text = \"\".join([p.find_next(text=True).strip() for p in body])\n",
    "            except:\n",
    "                still_prob_links.append(url)\n",
    "        \n",
    "        if web=='www.thegamer.com':\n",
    "            try:\n",
    "                title = soup.find('h1', {'class':'heading_title'}).text\n",
    "                body = soup.find('section', {'id':'article-body'}).find_all('p')\n",
    "                text = \"\".join([p.find_next(text=True).strip() for p in body])\n",
    "            except:\n",
    "                still_prob_links.append(url)\n",
    "            \n",
    "        if web=='apnews.com':\n",
    "            try:\n",
    "                title = soup.find('h1', {\"class\":\"Component-heading-0-2-16\"}).text\n",
    "                body = soup.find('div', {\"class\":\"Article\"}).find_all('p')\n",
    "                text = \"\".join([p.find_next(text=True).strip() for p in body])\n",
    "            except:\n",
    "                still_prob_links.append(url)\n",
    "\n",
    "        if web=='www.uq.edu.au':\n",
    "            try:\n",
    "                title = soup.find('h1', {'id':\"page-title\"}).text\n",
    "                body = soup.find('div', {'id':\"content\"}).find_all('p')\n",
    "                text = \"\".join([p.find_next(text=True).strip() for p in body])\n",
    "            except:\n",
    "                still_prob_links.append(url)\n",
    "        if web=='www.wsj.com':\n",
    "            try:\n",
    "                title = soup.find('h1', {\"class\":\"css-1lvqw7f-StyledHeadline e1ipbpvp0\"}).text\n",
    "                text = soup.find('section', {'class':'css-az2xkl-Container-Container e1d75se20'}).text\n",
    "            except:\n",
    "                still_prob_links.append(url)\n",
    "        if web=='www.tandfonline.com':\n",
    "            try:\n",
    "                title = soup.find(\"span\", {\"class\":\"NLM_article-title hlFld-title\"}).text\n",
    "                text = soup.find(\"div\", {\"class\":\"abstractSection abstractInFull\"}).text\n",
    "            except:\n",
    "                still_prob_links.append(url)\n",
    "        if web=='www.abc.net.au':\n",
    "            try:\n",
    "                title = soup.find(\"h1\", {\"class\":\"_1EAJU hMmqO WL4Yr n-Wqw _18EFj _2ZOIT _3HiTE x9R1x pDrMR hmFfs _390V1\"}).text\n",
    "                body = soup.find(\"div\", {\"class\":\"_3P3cP _3sFAh\"}).find_all('p')\n",
    "                text = \"\".join([p.find_next(text=True).strip() for p in body])\n",
    "            except:\n",
    "                still_prob_links.append(url)\n",
    "            \n",
    "        if web=='www.tomshardware.com':\n",
    "            try:\n",
    "                title=soup.find(\"title\").text\n",
    "                body = soup.find(\"div\",{\"id\":\"article-body\"})\n",
    "                text = \"\".join([p.find_next(text=True).strip() for p in body])\n",
    "            except:\n",
    "                still_prob_links.append(url)\n",
    "        \n",
    "        if web == 'thehill.com':\n",
    "            try:\n",
    "                title=soup.find(\"title\").text\n",
    "                body = soup.find(\"div\", {\"class\":\"article__text | body-copy | flow\"}).find_all('p')\n",
    "                text = \"\".join([p.find_next(text=True).strip() for p in body])\n",
    "            except:\n",
    "                still_prob_links.append(url)\n",
    "\n",
    "        text_doc = nlp(text)\n",
    "        title_doc = nlp(title)\n",
    "\n",
    "        text_dict = analyze_text_textrank(text_doc, threshold_rank)\n",
    "        title_dict = analyze_text_textrank(title_doc, threshold_rank)\n",
    "\n",
    "        temp_text_df = pd.DataFrame.from_dict(text_dict)\n",
    "        temp_text_df.columns = cols_text_df\n",
    "        temp_text_df['article_link'] = [url]*len(temp_text_df)\n",
    "        text_df = pd.concat([text_df, temp_text_df])\n",
    " \n",
    "        temp_title_df = pd.DataFrame.from_dict(title_dict)\n",
    "        temp_title_df.columns = cols_title_df\n",
    "        temp_title_df['article_link'] = [url]*len(temp_title_df)\n",
    "        title_df = pd.concat([title_df, temp_title_df])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_df.to_csv(r\"C:\\Users\\Ravit\\Documents\\rnd\\horizon_scanning_lab\\articles\\analyze_Articles\\reddit_articles_analysis\\problematic_reddit_articles_titles.csv\")\n",
    "text_df.to_csv(r\"C:\\Users\\Ravit\\Documents\\rnd\\horizon_scanning_lab\\articles\\analyze_Articles\\reddit_articles_analysis\\problematic_reddit_articles_texts.csv\")\n",
    "\n",
    "#save questions that selenium didn't succeed either\n",
    "stubborn_df = pd.DataFrame(still_prob_links, columns=['url'])\n",
    "stubborn_df.to_csv(r\"C:\\Users\\Ravit\\Documents\\rnd\\horizon_scanning_lab\\articles\\analyze_Articles\\reddit_articles_analysis\\prob_articles_after_second_try.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
